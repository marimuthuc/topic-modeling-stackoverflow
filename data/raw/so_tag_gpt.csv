Post Link,PostTypeId,OwnerUserId,Answer Link,Title,Body,CreationDate,ClosedDate,LastEditDate,LastActivityDate,Tags,AnswerCount,CommentCount,Score,ViewCount,FavoriteCount,PostTypeId,OwnerUserId,Body,CreationDate,CommentCount,Score
"75801940","1","19336351","","Unable to get word by word response from GPT API","<p>I am trying to get the response from my gpt api, word by word like chatGPT generates and not all at once. I have all other things working, getting the response as expected just not in chunks .</p>
<p>I am able to print the partial response in console but unable to show it on UI, could anyone help here?</p>
<p>This is my backend code</p>
<pre><code>import { ChatGPTAPI } from &quot;chatgpt&quot;;

app.post(&quot;/&quot;, async (req, res) =&gt; {
  const { message } = req.body;
  const api = new ChatGPTAPI({
    apiKey: OPENAI_API_KEY,
  });

  const resp = await api.sendMessage(
    message, {
      onProgress: (partialResponse) =&gt; {
        console.log(partialResponse);
      },
    }
  );
  
// Code for sending the response all at once
  // if (resp.text) {
  //   res.json({
  //     message: resp.text,
  //   });
  // }
});

const server = app.listen(5000, () =&gt; {
  console.log(&quot;app listening&quot;);
});

server.headersTimeout = 610000;
</code></pre>
<p>This is how I am fetching it in frontend</p>
<pre><code>const handleSubmit = (e) =&gt; {
    e.preventDefault();

    fetch(&quot;http://localhost:5000&quot;, {
      method: &quot;POST&quot;,
      headers: {
        &quot;Content-Type&quot;: &quot;application/json&quot;,
      },
      body: JSON.stringify({ message }),
    })
      .then((res) =&gt; res.json())
      .then((data) =&gt; {
        setResponse(data.message);
        setMessage(&quot;&quot;);
      });
  };
</code></pre>
","2023-03-21 13:36:31","","","2023-03-25 17:14:24","<node.js><reactjs><openai-api><gpt-3><chatgpt-api>","0","0","0","293","","","","","","",""
"75807326","1","9050016","","How to prepare dataset with multiple answers for single question to train the GPT3/davinci model","<p>I am trying to fine-tune the GPT model, and for that, I have 3 columns: context, question, and answer. but I have multiple answers to a question. I have repeated question text for multiple answers, what is the best way to prepare an optimized dataset?
let me know if I'm putting this in the correct manner.</p>
<p>I am thinking of having a separator &quot;or else&quot; to differentiate multiple answers to a single question.</p>
","2023-03-21 23:50:31","","2023-03-21 23:55:13","2023-03-22 09:56:29","<python><nlp><data-mining><gpt-3><large-language-model>","0","0","0","33","","","","","","",""
"75807664","1","3944252","","Issues Handling ChatGPT Streaming Response in Terminal using OpenAI API - Using Python, rich library","<p>I am trying to integrate the <strong>openAi API</strong> model - <code>gpt-4</code> with Terminal to enable <strong>ChatGPT</strong>. My objective is to receive streaming responses from <strong>ChatGPT</strong> and print them in the Terminal.
Although I can successfully print the entire response without streaming, I'm facing issues with streaming responses. Specifically, the <code>ask_stream</code> function is printing every word on a new line, which is not the desired behavior. I'm using the rich library to handle Markups</p>
<p>My code:</p>
<pre><code>import openai
from rich.markdown import Markdown
from rich.console import Console
from prompt_toolkit import PromptSession
from prompt_toolkit.auto_suggest import AutoSuggestFromHistory
from prompt_toolkit.completion import WordCompleter
from prompt_toolkit.history import InMemoryHistory
import time
import argparse
import asyncio

openai.api_key = &quot;MY API KEY&quot;
model = &quot;gpt-4&quot;
delay_time = 0.01
max_response_length = 200
console = Console()


async def ask_stream(prompt):
    response = openai.ChatCompletion.create(model='gpt-4',
                                            messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;{prompt}&quot;}], max_tokens=8000,
                                            temperature=0.4, stream=True)
    answer = ''
    for event in response:
        if answer:
            console.print(Markdown(answer), end='')
        # sys.stdout.flush()
        event_text = event['choices'][0]['delta']
        answer = event_text.get('content', '')
        time.sleep(0.01)


async def ask(prompt) -&gt; Markdown:
    if prompt:
        completion = openai.ChatCompletion.create(model=model,
                                                  messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;{prompt}&quot;}])
        if completion:
            if 'error' in completion:
                return completion['error']['message']
            return Markdown(completion.choices[0].message.content)
        else:
            raise Exception(&quot;&quot;)


def create_session() -&gt; PromptSession:
    return PromptSession(history=InMemoryHistory())


async def get_input_async(
        session: PromptSession = None,
        completer: WordCompleter = None,
) -&gt; str:
    &quot;&quot;&quot;
    Multiline input function.
    &quot;&quot;&quot;
    return await session.prompt_async(
        completer=completer,
        multiline=True,
        auto_suggest=AutoSuggestFromHistory(),
    )


async def main():
    print(f&quot;Starting Chatgpt with model - {model}&quot;)
    session = create_session()
    while True:
        print(&quot;\nYou:&quot;)
        question = await get_input_async(session=session)
        print()
        print()
        if question == &quot;!exit&quot;:
            break
        elif question == &quot;!help&quot;:
            print(
                &quot;&quot;&quot;
            !help - Show this help message
            !exit - Exit the program
            &quot;&quot;&quot;,
            )
            continue
        print(&quot;ChatGPT:&quot;)
        if args.no_stream:
            console.print(await ask(prompt=question))
        else:
            await ask_stream(prompt=question)


if __name__ == &quot;__main__&quot;:
    parser = argparse.ArgumentParser()
    parser.add_argument(&quot;--no-stream&quot;, action=&quot;store_true&quot;)
    args = parser.parse_args()
    asyncio.run(main())
</code></pre>
<p><code>ask_stream</code> prints like below
<a href=""https://i.stack.imgur.com/cEJWC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/cEJWC.png"" alt=""enter image description here"" /></a></p>
<p>Can someone suggest a solution to fix this issue? I am pretty new to Python.</p>
","2023-03-22 01:09:17","","","2023-03-22 02:19:56","<python><openai-api><rich><gpt-4>","1","0","-2","1292","","","","","","",""
"75811293","1","21455152","","OpenAI GPT-3 API error: ""InvalidRequestError: Resource not found""","<p>I've been trying to upload a json file that I will use for fine tuning my GPT-3 model.
I get an error when trying to upload it.</p>
<pre><code>openai.File.create(file=open(&quot;training_data.jsonl&quot;), purpose=&quot;fine-tune&quot;)
</code></pre>
<p>When I run the command above I get the following error:</p>
<p><code>InvalidRequestError: Resource not found</code></p>
","2023-03-22 10:51:16","","2023-03-23 17:41:44","2023-03-23 17:50:39","<python><openai-api><gpt-3>","1","0","-1","2093","","","","","","",""
"75818642","1","19977480","","Finetuning gpt2, validation loss increases with accuracy and f1 score","<p>I am finetuning gpt2 on text classification with the huggingface trainer. I observed that after 2 epochs, my validation loss start to increase, but my validation accuracy and f1 score still increases too. I have tried with 2 different seed but I observe the same effect. How do I know if I am overfitting? Should I perform early stopping?<a href=""https://i.stack.imgur.com/oyEtq.png"" rel=""nofollow noreferrer""> Graph of validation loss and accuracy </a></p>
<p>I would expect when the validation loss increases, there should be a plateau or a drop in accuracy and f1 score, but this is not a case here.</p>
","2023-03-23 01:48:21","","","2023-03-23 01:48:21","<text-classification><gpt-2><overfitting-underfitting>","0","0","0","83","","","","","","",""
"75840719","1","13103348","","Getting missing pandas error while trying to fine-tune GPT3","<p>I'm using the following command :<br />
<code>openai tools fine_tunes.prepare_data -f ./data.jsonl</code></p>
<p>and I'm getting the following error:</p>
<pre><code>Analyzing...
Traceback (most recent call last):
  File &quot;/Users/jyothiraditya/mambaforge/bin/openai&quot;, line 8, in &lt;module&gt;
    sys.exit(main())
  File &quot;/Users/jyothiraditya/mambaforge/lib/python3.10/site-packages/openai/_openai_scripts.py&quot;, line 63, in main
    args.func(args)
  File &quot;/Users/jyothiraditya/mambaforge/lib/python3.10/site-packages/openai/cli.py&quot;, line 586, in prepare_data
    df, remediation = read_any_format(fname)
  File &quot;/Users/jyothiraditya/mambaforge/lib/python3.10/site-packages/openai/validators.py&quot;, line 477, in read_any_format
    assert_has_pandas()
  File &quot;/Users/jyothiraditya/mambaforge/lib/python3.10/site-packages/openai/datalib.py&quot;, line 56, in assert_has_pandas
    raise MissingDependencyError(PANDAS_INSTRUCTIONS)
openai.datalib.MissingDependencyError: 

OpenAI error: 

    missing `pandas` 

This feature requires additional dependencies:

    $ pip install openai[datalib]
</code></pre>
<p>I tried reinstalling <code>datalib</code> using : <code>pip install --upgrade openai openai&quot;[datalib]&quot;</code> but it did not work</p>
<p>I tried install pandas manually : <code>pip install pandas</code> but it also did not work</p>
<p>What can I do to resolve this error?</p>
","2023-03-25 09:19:15","","2023-03-25 09:43:49","2023-06-20 10:19:59","<openai-api><gpt-3><fine-tune>","2","1","2","511","","","","","","",""
"75856110","1","21500678","","How to fine-tune gpt2 with a custom set of unlabelled document","<p>I'm newbie to GPT2 fine-tuning. My goal is to fine-tune GPT-2 (or BERT) on a my own set of document, in order to be able to query the bot on a topic contained in these documents, and receive an answer. I have some doubts on how to develop this, because I saw that fine tuning a Question and Answer chatbot requires a labelled dataset, containing questions relatet to a answer.</p>
<p>Is it possible to fine tune a language model on an unlabelled dataset?
After I train the model on my data, can I already query it or anyway is there a need to fine-tune on a specific task using an annotated dataset?
Is there a minumum number of documents on order to achieve good results?
Is it possible to do on a non-english language?
Thank you.</p>
","2023-03-27 13:06:09","","2023-03-27 14:00:38","2023-03-27 14:29:19","<bert-language-model><transfer-learning><openai-api><gpt-2><fine-tune>","1","0","0","373","","","","","","",""
"75863060","1","11155486","","Time and cost to train Distill GPT-2 model on BookCorpus using AWS EC2","<p>I am trying to calculate the time it would take to train a Distill GPT2 model on BookCorpus dataset using multiple EC2 instances for the purpose of language modeling.</p>
<p>What is the method for calculating training time of language models?</p>
","2023-03-28 06:40:06","","","2023-03-28 06:40:06","<amazon-ec2><nlp><gpt-2><distributed-training><nlg>","0","0","0","36","","","","","","",""
"75864319","1","21508401","","import Image as PIL_Image ModuleNotFoundError: No module named 'Image' while running langchain with DirectoryLoader('source', glob='*.pdf')","<pre><code>from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import DirectoryLoader

import openai
loader = DirectoryLoader('source', glob='*.pdf')

data = loader.load()
</code></pre>
<p>Just this much code... I get this error</p>
<pre><code>  File &quot;C:\Users\vsvrp\anaconda3\envs\GPTtrail2\lib\site-packages\pptx\parts\image.py&quot;, line 13, in &lt;module&gt;
    import Image as PIL_Image
ModuleNotFoundError: No module named 'Image'

Process finished with exit code 1
</code></pre>
<p>I do not get this error if I do this</p>
<pre><code>loader = UnstructuredPDFLoader(&quot;DOStest.pdf&quot;)
</code></pre>
<p>I tried to do pip install Image</p>
<p>It is still not working. Any help would be greatly appreciated.</p>
<p>Working with langchain and documentloaders for the first time and the DirectoryLoader class is supposed to work in this case.</p>
","2023-03-28 09:09:22","","2023-03-28 09:23:56","2023-03-28 09:23:56","<python-3.x><openai-api><gpt-3><gpt-4><langchain>","0","1","2","175","","","","","","",""
"75874606","1","649994","","Error: PineconeClient: Project name not set, v0.0.10","<p>Downloaded GitHub - <a href=""https://github.com/mayooear/gpt4-pdf-chatbot-langchain"" rel=""nofollow noreferrer"">mayooear/gpt4-pdf-chatbot-langchain:</a> GPT4 &amp; LangChain Chatbot for large PDF docs</p>
<p>When I try to run the app, I get following error</p>
<pre><code>PineconeClient: Error getting project name: TypeError: fetch failed

error - [Error: PineconeClient: Project name not set. Call init() first.] {
page: ‘/api/chat’
}
</code></pre>
<p>My node version is node -v
v18.15.0
Pinecone version is
<code>“@pinecone-database/pinecone”: “^0.0.10”,</code></p>
<p>It seems it is due to issue of fetch() which is an experimental feature. How to disable fetch() and use node-fetch()</p>
","2023-03-29 08:02:07","","2023-03-29 15:55:21","2023-04-07 18:42:02","<openai-api><gpt-3><chatgpt-api><gpt-4><langchain>","1","0","2","208","","","","","","",""
"75878060","1","","","Langchain - Multiple input SequentialChain","<p>I'am experiencing with langchain so my question may not be relevant but I have trouble find an example in the documentation.</p>
<p>Actually as far as I understand, SequentialChain is made to receive one or more input for the first chain and then feed the output of the n-1 chain into the n chain.</p>
<p>Let's say I'am working with 3 chains, the first one that take as input snippet of a csv file and some description about where the csv came from, the next one that take as input snippet of our csv file AND output of the first chains to produce a python script as output.</p>
<p>here is the &quot;no sequential&quot; version that work :</p>
<pre class=""lang-py prettyprint-override""><code>DATA_REVIEW = &quot;&quot;&quot; You are a datascientist specialized in business analysis. You are able to retrieve the most relevant metrics in every json file. You are able to give complete and detailed review of how thoses metrics can be used for making profit. A snippet of the full Json is given as context. Your role is to write down all type of metrics that can be retrieved from the full json. Don't do the calculation, the metrics list will be send to a python developer. You also should include metrics that can be used for comparison.

after the metrics list, write the columns name list. 

context:
{data}


Metrics that can be retrieved from the full json:
&quot;&quot;&quot;
PYTHON_SCRIPT = &quot;&quot;&quot;You are a datascientist specialized in business analysis. You are able to write powerfull and efficient python code to retrieve metrics from a dataset. Your role is to write a python script for all type of metrics described above based on the structure of the dataset. Your python script should print all metrics calculated and 
each products followed by their whole metrics. You should always use pandas library. After you printed out all the metrics, store them as in the example below:
metrics_result = f'Total number of products: (total_products)'
metrics_result += f'Average price of products: (avg_price)'
for index, row in df.iterrows():
    metrics_result += f'Product ID: (row[&quot;product_id&quot;])'
    metrics_result += f'Product Name: (row[&quot;product_name&quot;])'

Make sure to replace unwanted character for each column and to convert value to the desired type before going into calculation. Also pay attention to the columns exact name. Data are represented as a json below but the file they came from is an xlsx. Your code should always start with :



structure:
{data}

Metrics to retrieve:
{output}


python script:


&quot;&quot;&quot;
prompt_template = PromptTemplate(
            input_variables=['data'],
            template=DATA_REVIEW
            )
        openai = OpenAI(model_name=&quot;text-davinci-003&quot;,openai_api_key='KEY', temperature=0, max_tokens=3000)
        output = openai(prompt_template.format(data=data))
        python_script_template = PromptTemplate(
            input_variables=['data','output'],
            template=PYTHON_SCRIPT
            )
        openai = OpenAI(model_name=&quot;text-davinci-003&quot;,openai_api_key='KEY', temperature=0, max_tokens=3000)
        script = openai(python_script_template.format(
                output = output,
                data = data
                ))


#Actual sequential chain script 'not working' 

llm = OpenAI(temperature=0.0)

prompt = PromptTemplate(
    input_variables=[&quot;data_snippet&quot;],
    template=&quot;&quot;&quot;You are a datascientist specialized in business analysis. You are able to retrieve the most relevant metrics in every json file. You are able to give complete and detailed review of how thoses metrics can be used for making profit. Your next project is for a Beauty e-shop business. a snippet of the full Json is given as context. Your role is to write down all type of metrics that can be retrieved from the full json. You also should include metrics that can be used for comparison.
    context:
        {data_snippet}
    
    metrics that can be retrieved from the complete file:
&quot;&quot;&quot;
)


chain = LLMChain(llm=llm, prompt=prompt, output_key='metrics')


data_snippet = read_csv_data(csv_file_path)


data_snippet_str = str(data_snippet)
metrics = chain.run(data_snippet_str)
second_prompt = PromptTemplate(
    input_variables=[&quot;data_snippet&quot;, &quot;metrics&quot;],
    template=
&quot;&quot;&quot;You are a datascientist specialized in business analysis. You are able to write powerfull and efficient python code to retrieve metrics from a dataset. Your role is to write a python script for all type of metrics described above based on the structure of the dataset. Your python script should print all metrics calculated and 
    each products followed by their whole metrics. You should always use pandas library. After you printed out all the metrics, store them as in the example below:
        metrics_result = f'Total number of products: (total_products)'
        metrics_result += f'Average price of products: (avg_price)'
        for index, row in df.iterrows():
            metrics_result += f'Product ID: (row[&quot;product_id&quot;])'
            metrics_result += f'Product Name: (row[&quot;product_name&quot;])'

    Make sure to replace unwanted character for each column and to convert value to the desired type before going into calculation. Also pay attention to the columns exact name. Data are represented as a json below but the file they came from is an xlsx. Your code should always start with :
        import pandas as pd
        data = CSV_FILE
        df = pd.read_csv(data)


    structure:
        {data_snippet}

    Metrics to retrieve:
        {metrics}


    python script:
&quot;&quot;&quot;
)

chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key='script')

from langchain.chains import SimpleSequentialChain

overall_chain = SimpleSequentialChain(chains=[chain, chain_two], input_variables=['data_snippet_str'], output_variables=[&quot;metrics&quot;,&quot;script&quot;], verbose=True)


python_script = overall_chain.run([data_snippet_str, chain_two])
</code></pre>
","2023-03-29 13:42:52","","2023-04-03 18:24:09","2023-04-03 18:24:09","<gpt-3><langchain>","0","0","1","1311","","","","","","",""
"75889488","1","21528260","","I never get embedded files loaded with from langchain.document_loaders import DirectoryLoader","<p>My code with from langchain.document_loaders import TextLoader, with a single .txt file it works but with DirectoryLoader nothing.
Attach image of the code:(<a href=""https://i.stack.imgur.com/VAKxn.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/VAKxn.png</a>)
Attach image of contect of the texts:<a href=""https://i.stack.imgur.com/W9EeN.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>At the beginning it gave me errors with permissions, I don't know what happened, must it be because of that? then change the permissions of the Store folder to everyone.</p>
<p>But when running the code it always stays loading all the time</p>
<p>I would really appreciate if you help me please</p>
<p>I need the code to embed the words of the text files in the &quot;Store&quot; folder but it doesn't</p>
<p>Like this capture:
(<a href=""https://i.stack.imgur.com/o2uoW.png"" rel=""nofollow noreferrer"">https://i.stack.imgur.com/o2uoW.png</a>)</p>
","2023-03-30 14:26:29","","","2023-03-30 14:26:29","<python><openai-api><gpt-3><langchain>","0","1","0","555","","","","","","",""
"75889941","1","2292490","","Give GPT (with own knowledge base) an instruction on how to behave before user prompt","<p>I have given GPT some information in CSV format to learn and now I would like to transmit an instruction on how to behave before the user prompt.</p>
<pre><code>def chatbot(input_text):
    index = GPTSimpleVectorIndex.load_from_disk('index.json')
    message_history.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;{input_text}&quot;})
    response = index.query(input_text+message_history, response_mode=&quot;compact&quot;)
    return response.response
</code></pre>
<p>&quot;message_history&quot; looks like this:</p>
<pre><code>message_history = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;You are a Pirate! Answer every question in pirate-slang!&quot;}, {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: f&quot;OK&quot;}]
</code></pre>
<p>I got the following error:</p>
<blockquote>
<p>&quot;TypeError: can only concatenate str (not &quot;list&quot;) to str&quot;</p>
</blockquote>
<p>I remember that I have to convert this into tuples but everything I try only causes more chaos...</p>
<p>Here's the whole code:</p>
<pre><code>from gpt_index import SimpleDirectoryReader, GPTListIndex, GPTSimpleVectorIndex, LLMPredictor, PromptHelper
from langchain import OpenAI
import gradio as gr
import sys
import os

os.environ[&quot;OPENAI_API_KEY&quot;] = 'INSERT_KEY_HERE'

message_history = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;You are a Pirate! Answer every question in pirate-slang!&quot;},
               {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: f&quot;OK&quot;}]


def construct_index(directory_path):
    # Index is made of CSV, TXT and PDF Files
    max_input_size = 4096
    num_outputs = 512
    max_chunk_overlap = 20
    chunk_size_limit = 600

prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)

llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name=&quot;gpt-3.5-turbo&quot;, max_tokens=num_outputs))

documents = SimpleDirectoryReader(directory_path).load_data()

index = GPTSimpleVectorIndex.from_documents(documents)

index.save_to_disk('index.json')

return index

def chatbot(input_text):
    index = GPTSimpleVectorIndex.load_from_disk('index.json')
    message_history.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;{input_text}&quot;})
    response = index.query(input_text+message_history, response_mode=&quot;compact&quot;)
    return response.response

 iface = gr.Interface(fn=chatbot,
                 inputs=gr.inputs.Textbox(lines=7, label=&quot;Enter something here...&quot;),
                 outputs=&quot;text&quot;,
                 title=&quot;ChatBot&quot;)

index = construct_index(&quot;docs&quot;)
iface.launch(share=True)
</code></pre>
","2023-03-30 15:09:19","","","2023-03-30 15:09:19","<python><prompt><openai-api><gpt-3><chatgpt-api>","0","3","1","420","","","","","","",""
"75901665","1","1935541","","TextCompleition Latency with Large Prompts - How to Avoid?","<p>We've been experimenting back and forth between text completion and Chat completion to build an interactive AI.</p>
<p>What we've found is with Text completion the AI follows instructions much better, but after a number of messages being added to the prompt (e.g. about 8 back and forth sentences of around 90 chars each), the Latency starts to go up.  It also increases the token usage (less important but notice it).</p>
<p>Has anyone been able to use Text Completion for long conversations and if so were you able to do it without getting a major latency hit?</p>
<p>Did you need an intermediate step to summarize the previous conversation rather than carry all the messages per request?</p>
","2023-03-31 18:28:23","","","2023-03-31 18:28:23","<openai-api><gpt-3><fine-tune><gpt-4>","0","0","0","70","","","","","","",""
"75905776","1","21444092","","Questions about masks of padding in GPT","<p>The GPT series models use the decoder of Transformer, with unidirectional attention. In the source code of GPT in Hugging Face, there is the implementation of masked attention:</p>
<pre><code>self.register_buffer(
            &quot;bias&quot;,
            torch.tril(torch.ones((max_positions, max_positions), dtype=torch.uint8)).view(
                1, 1, max_positions, max_positions
            ),
        )
</code></pre>
<p>The default attention_mask is None.</p>
<p>However, I have found that in some GPT demos, the attention_mask derived from valid lengths is not assigned. It seems that the padding tokens are not masked during attention, but just ignored in the loss computation.</p>
<p>Is it correct? Or whether masking the padding in the attention does not matter to the final results?</p>
<p>Besides, I also wonder whether the embedding of padding token will change during training.</p>
","2023-04-01 11:01:05","","2023-04-02 04:00:34","2023-04-02 04:00:34","<huggingface-transformers><attention-model><gpt-2><zero-padding>","0","2","1","211","","","","","","",""
"75906140","1","20601880","","word to word gpt api responce stream in react native","<p>Is there a way to implement GPT API with word to word api</p>
<p>I already try implement in javascript directly into app but its not working</p>
<p>I want to use Chat GPT Turbo api directly in react native (expo) with word by word stream here is working example without stream</p>
","2023-04-01 12:17:21","","","2023-06-19 23:29:59","<react-native><openai-api><gpt-3><chatgpt-api><gpt-4>","0","3","0","272","","","","","","",""
"75906161","1","21346793","","In which form should be dataset in NLP model?","<p>I try to make fine-tuning of model tinkoff-ai/ruDialoGPT-medium. In which form should be my dataset? The base generation is in form:</p>
<pre><code>@@ПЕРВЫЙ@@ привет @@ВТОРОЙ@@ привет @@ПЕРВЫЙ@@ как дела? @@ВТОРОЙ@@
</code></pre>
<p>Where @@ПЕРВЫЙ@@ is the first person, @@ВТОРОЙ@@ - the second person of dialogue.</p>
<p>I try to make fine-tuning with json like:</p>
<pre><code>    {&quot;sample&quot;: [&quot; Я ищу бесплатные онлайн-курсы по бухгалтерскому учету.&quot;, &quot; В сети есть ряд бесплатных онлайн-курсов по бухгалтерскому учету, таких как Coursera и edX. Эти курсы предлагают вводные занятия по бухгалтерскому учету продвинутого уровня, которые могут помочь вам изучить основы бухгалтерского учета и финансового управления. Вы также можете заглянуть в местные общественные колледжи или центры обучения взрослых в вашем районе для получения более специализированных курсов по бухгалтерскому учету.&quot;]},
</code></pre>
<p>But the generation of answers is very bad</p>
","2023-04-01 12:22:47","","","2023-04-01 12:22:47","<machine-learning><gpt-2>","0","0","1","17","","","","","","",""
"75945693","1","2672447","","how to determine the expected prompt_tokens for gpt-4 chatCompletion","<p>For the following nodejs code below I am getting prompt_tokens = 24 in the response. I want to be able to determine what the expected prompt_tokens should be prior to making the request.</p>
<pre class=""lang-js prettyprint-override""><code>    import { Configuration, OpenAIApi } from 'openai';

    const configuration = new Configuration({
        apiKey: process.env.OPENAI_API_KEY,
     });
     
    const openai = new OpenAIApi(configuration);

    const completion = await openai.createChatCompletion({
    model: &quot;gpt-4&quot;,
    messages: [
      {role: &quot;system&quot;, content: systemPrompt}, //systemPrompt= 'You are a useful assistant.'     
      {role: &quot;user&quot;, content: userPrompt} //userPrompt= `What is the meaning of life?`
    ]
    });

    /* completion.data = {
       id: 'chatcmpl-72Andnl250jsvSJGbjBJ6YzzFGToA',
       object: 'chat.completion',
       created: 1680752525,
       model: 'gpt-4-0314',
       usage: { prompt_tokens: 24, completion_tokens: 91, total_tokens: 115 },
       choices: [ [Object] ]
    } */
</code></pre>
<p>It seems like each model has its own way of encoding and the best lib for that is python tiktoken. Hence if I was to estimate &quot;prompt_tokens&quot;. I would need to pass through the &quot;text&quot; value to the script below. However I am not sure what I should be using as the &quot;text&quot; below in the python script for the &quot;messages&quot; above in the nodejs, such that print(token_count) below = 24 [the actual prompt_tokens in the response]</p>
<pre class=""lang-py prettyprint-override""><code>    import sys
    import tiktoken

    text = sys.argv[1]
    enc = tiktoken.encoding_for_model(&quot;gpt-4&quot;)
    tokens = enc.encode(text)
    token_count = len(tokens)
    print(token_count)
</code></pre>
","2023-04-06 03:58:46","","2023-04-06 20:13:40","2023-04-06 20:47:39","<openai-api><chatgpt-api><gpt-4>","1","0","0","694","","","","","","",""
"75966973","1","21597554","","ChatBot - Trouble using custom gpt_index and langchain libraries for creating a GPT-3 based search index","<p>FYI : I am trying to build a chatbot based on the instructions given by Dan Shipper <a href=""https://www.lennysnewsletter.com/p/i-built-a-lenny-chatbot-using-gpt"" rel=""nofollow noreferrer"">https://www.lennysnewsletter.com/p/i-built-a-lenny-chatbot-using-gpt</a>
I'm trying to use custom libraries called gpt_index and langchain to create a GPT-3 based search index using the OpenAI API. I have successfully installed the libraries and have the following code. BTW I am using google Colab for the environment.</p>
<pre class=""lang-py prettyprint-override""><code>from gpt_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper
from langchain import OpenAI
import sys
import os
from IPython.display import Markdown, display

def construct_index(directory_path):
    ...
    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=&quot;text-davinci-003&quot;, max_tokens=num_outputs))
    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)
 
    documents = SimpleDirectoryReader(directory_path).load_data()
    
    index = GPTSimpleVectorIndex(
        documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper
    )

    index.save_to_disk('index.json')
    return index

def ask_lenny():
    index = GPTSimpleVectorIndex.load_from_disk('index.json')
    while True: 
        query = input(&quot;What do you want to ask Lenny? &quot;)
        response = index.query(query, response_mode=&quot;compact&quot;)
        display(Markdown(f&quot;Lenny Bot says: &lt;b&gt;{response.response}&lt;/b&gt;&quot;))
</code></pre>
<p>When I call the construct_index function with the path to my documents, I get the following error:
<code>TypeError: __init__() got an unexpected keyword argument 'llm_predictor'</code></p>
<p>It seems that there is a mismatch between the expected arguments of the <code>GPTSimpleVectorIndex</code> class and the provided arguments in the code. Unfortunately, I cannot find any documentation or examples for these custom libraries.</p>
<p>Could anyone help me understand how to correctly initialize the GPTSimpleVectorIndex class and resolve this error? Any guidance on using these libraries would be greatly appreciated.</p>
<p>Thank you!</p>
<p>I am running this in Google Colab and see the error.</p>
","2023-04-08 18:20:39","","2023-04-09 17:02:12","2023-06-23 10:29:07","<python><openai-api><gpt-3>","1","0","1","1021","","","","","","",""
"75979815","1","21610884","","How to add 'message history' to llama-index based GPT-3 in Python","<p>I am fairly new to using llama-index library for training GPT-3 as well as using ChatGPT through the standard API (both in Python). I have noticed that standard ChatGPT API i could simply do the following code below to have ChatGPT get message history as context:</p>
<pre><code>message_history=[]
completion = openai.ChatCompletion.create(model=&quot;gpt-3.5-turbo&quot;,messages=message_history)
</code></pre>
<p>Now I am using llama-index library to train GPT-3 on a more specific context, however I don't know how to have the model consider the message_history as well, here is a code that I am currently working on an i dont know how to implement message history:</p>
<pre><code>
def construct_index(directory_path):
    # set maximum input size
    max_input_size = 4096
    # set number of output tokens
    num_outputs = 2000
    # set maximum chunk overlap
    max_chunk_overlap = 20
    # set chunk size limit
    chunk_size_limit = 600 

    # define prompt helper
    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)
    # define LLM
    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name=&quot;text-ada-001&quot;, max_tokens=num_outputs))
    # define context (dataset)
    documents = SimpleDirectoryReader(directory_path).load_data()
    # transform context to index format
    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)
    index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)
    # important: index are are like map, has latitutdes and logntitudes to indicate how each city (texts) are close to each other
    index.save_to_disk(&quot;index.json&quot;)
    return index

index = GPTSimpleVectorIndex.load_from_disk(&quot;index.json&quot;)
dbutils.widgets.text(&quot;user_input&quot;, &quot;user: &quot;)
response = index.query(dbutils.widgets.get(&quot;user_input&quot;),response_mode='compact')
print(&quot;Response: &quot;, response.response)
</code></pre>
","2023-04-10 18:45:19","","","2023-04-24 14:10:47","<python><openai-api><gpt-3><llama-index><gpt-index>","0","0","6","1250","","","","","","",""
"75979901","1","20678352","","how to fix ""KeyError: 0"" in the hugging face transformer train() function","<p>hello guys please i am in dying need of your help .
i am trying to fine-tune the gpt2-meduim model with the hugging face transformer and i ran into this error just when i wanted to start the training &quot;KeyError: 0&quot; .
here is my full  code</p>
<pre><code>import pandas as pd 
import numpy as np

</code></pre>
<pre><code>
dataset = pd.read_csv('Train_rev1.csv',error_bad_lines=False, engine='python')
# dataset.head(5)

def replace_string(row):
    row['FullDescription'] = row['FullDescription'].replace('****', str(row['SalaryNormalized']))
    return row

dataset = dataset.apply(replace_string, axis=1)
dataset = dataset.drop(['ContractType','ContractTime','LocationRaw','SalaryRaw','SourceName','Id','Title', 'LocationNormalized', 'Company', 'Category',
       'SalaryNormalized'], axis=1)

dataset.columns

! pip install -q transformers
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments
tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')
tokenizer.add_special_tokens({'pad_token': '[PAD]'})
tokenized_data = tokenizer(dataset['FullDescription'].tolist(), truncation=True, padding=True)

# Split data into training and validation sets
train_size = int(0.8 * len(tokenized_data['input_ids']))
val_size = len(tokenized_data['input_ids']) - train_size

train_dataset = {'input_ids': tokenized_data['input_ids'][:train_size],
                 'attention_mask': tokenized_data['attention_mask'][:train_size]}
val_dataset = {'input_ids': tokenized_data['input_ids'][train_size:],
               'attention_mask': tokenized_data['attention_mask'][train_size:]}

</code></pre>
<p>i beleive my error some how originates around this section</p>
<pre><code>from transformers import GPT2Config
# Define model configuration and instantiate model
model_config = GPT2Config.from_pretrained('gpt2-medium')
model_config.output_hidden_states = True
model = GPT2LMHeadModel.from_pretrained('gpt2-medium', config=model_config)

# Train model using Huggingface Trainer API
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=1,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy='steps',
    eval_steps=50,
    load_best_model_at_end=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

trainer.train()
</code></pre>
<p>my ide underlines this last statement and produces the the 'KeyError: 0' and it deos not provide me with any other detail about the error apart from</p>
<p>KeyError                                  Traceback (most recent call last)
 in &lt;cell line: 1&gt;()
----&gt; 1 trainer.train()</p>
<p>5 frames
/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py in (.0)
49                 data = self.dataset.<strong>getitems</strong>(possibly_batched_index)
50             else:
---&gt; 51                 data = [self.dataset[idx] for idx in possibly_batched_index]
52         else:
53             data = self.dataset[possibly_batched_index]</p>
<p>KeyError: 0</p>
<p>i have tried changing some train_arguements but not working and am totally out of ideas as the error is not explicit</p>
","2023-04-10 19:02:35","","","2023-04-10 19:02:35","<machine-learning><nlp><huggingface-transformers><gpt-2><text-generation>","0","2","0","197","","","","","","",""
"75999769","1","21490540","","Twitch Chat Bot program not responding","<p>I am trying to make a python program which takes twitch chat as input, uses gpt3 to generate response, then say that response using pyttsx3 library. When running the program, it is neither responding to the chat nor showing any error. I am not able to tell if the program is actually connected with twitch or not. Here's the code -</p>
<pre><code>import openai
import pyttsx3
import asyncio
from twitchio.ext import commands

# Initialize OpenAI API
openai.api_key = &quot;&quot;

# Initialize the text to speech engine
engine = pyttsx3.init()
engine.setProperty('voice', 'HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Speech\Voices\Tokens\TTS_MS_EN-US_ZIRA_11.0')


def generate_response(prompt):
    response = openai.Completion.create(
        engine=&quot;text-davinci-003&quot;,
        prompt=prompt,
        max_tokens=4000,
        n=1,
        stop=None,
        temperature=0.5,
    )
    return response[&quot;choices&quot;][0][&quot;text&quot;]


def speak_text(text):
    engine.say(text)
    engine.runAndWait()


async def main():
    bot = commands.Bot(
        # set up the bot
        token='',
        irc_token='',
        client_id='',
        nick='',
        prefix='!',
        initial_channels=['disappointed_aether']
    )

    @bot.event
    async def event_message(ctx):
        print(&quot;Received a message&quot;)
        if ctx.author.name.lower() != bot.nick.lower():
            print(ctx.content)

            # Generate the response
            response = generate_response(ctx.content)
            print(f&quot;chat gpt 3 says: {response}&quot;)

            # read response using GPT3
            speak_text(response)

    await bot.start()


loop = asyncio.get_event_loop()
loop.run_until_complete(main())

</code></pre>
<p>First I thought asyncio might be causing the problem, so tried to change the last 3 lines to</p>
<pre><code>if __name__ == '__main__':
    bot.run()
</code></pre>
<p>This did not work. Other than that, I honestly have no idea what is causing the problem. Please help</p>
","2023-04-12 20:56:50","","","2023-04-12 20:56:50","<python><chatbot><twitch><openai-api><gpt-3>","0","0","0","57","","","","","","",""
"76010864","1","15138014","","How can I train GPT-3 with my own company data using OpenAI's API?","<p>I want to train GPT-3 with my company's data to perform specific NLP tasks using OpenAI's API. How can I train the GPT-3 model with my own data? What kind of data preprocessing do I need to perform before training the model? Are there any Python libraries or frameworks that can help me with the data preprocessing and training process? Can I use OpenAI's API to fine-tune the model for my specific NLP tasks, or do I need to train the model separately? What are the best practices for training GPT-3 with custom data using OpenAI's API?</p>
<p>I have researched the OpenAI API and have read the documentation on how to train GPT-3 with custom data. However, I am still unsure about the specific steps required to train GPT-3 with my company's data using OpenAI's API. I am expecting to learn more about the data preprocessing steps and Python libraries or frameworks that can assist with the training process. Additionally, I would like to know whether I can use OpenAI's API to fine-tune the model for my specific NLP tasks or whether I need to train the model separately. I am looking for best practices and recommendations for training GPT-3 with custom data using OpenAI's API.</p>
","2023-04-14 01:02:24","","","2023-05-31 13:46:04","<openai-api><data-preprocessing><gpt-3>","1","2","2","937","","","","","","",""
"76014800","1","10562928","","OpenAI GPT-3 API error: ""Cannot find module '@openai/api'""","<p>I am having trouble using the OpenAI API with Node.js. Specifically, I am trying to use the openai.Completion object, but I keep getting a <code>Cannot find module '@openai/api'</code> error.</p>
<p>I have already tried installing the @openai/api package using <code>npm install @openai/api</code>, but I get a 404 error indicating that the package could not be found. I have also removed it and reinstalled but no luck.</p>
<p>I also tried upgrading to the latest version of Node.js, which is currently 19.1.0, but the issue is stuborn. I created a test script (test.js) with the following code:</p>
<pre><code>const openai = require('openai');

openai.apiKey = 'sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX';

async function runTest() {
    try {
        const gpt3Response = await openai.Completion.create({
            engine: 'davinci-codex',
            prompt: `Create a simple conversational response for beginners, with an easy question at the end, based on the input: &quot;Hello, how are you?&quot;`,
            max_tokens: 50,
            n: 1,
            stop: null,
            temperature: 0.5,
        });
        console.log(gpt3Response.choices[0].text.trim());
    } catch (error) {
        console.error(error);
    }
}

runTest();
</code></pre>
<p>When I run this script with <code>node test.js</code>, I get the following error:</p>
<pre><code>Error: Cannot find module '@openai/api'
Require stack:
- C:\Users\User\Documents\Coding\folders\test.js
</code></pre>
<p>I have also tested the OpenAI API using VSC Thunder Client, and it seems to work. Here is the POST request I used:</p>
<pre><code>POST https://api.openai.com/v1/engines/davinci/completions
{
    &quot;prompt&quot;: &quot;do you like soccer&quot;,
    &quot;max_tokens&quot;: 50,
    &quot;n&quot;: 1,
    &quot;stop&quot;: null,
    &quot;temperature&quot;: 0.5,
    &quot;top_p&quot;: 1,
    &quot;echo&quot;: false
}
</code></pre>
<p>I received the following response:</p>
<pre><code>{
    &quot;id&quot;: &quot;cmpl-75BDDTIZ2Q1yodctHcEohCIsA1f46&quot;,
    &quot;object&quot;: &quot;text_completion&quot;,
    &quot;created&quot;: 1681469095,
    &quot;model&quot;: &quot;davinci&quot;,
    &quot;choices&quot;: [{
        &quot;text&quot;: &quot;?”\n\n“I’m not sure. I’ve never been to a game.”\n\n“I’m going to the game on Saturday. Would you like to go with me?&quot;,
        &quot;index&quot;: 0,
        &quot;logprobs&quot;: null,
        &quot;finish_reason&quot;: &quot;length&quot;
    }],
    &quot;usage&quot;: {
        &quot;prompt_tokens&quot;: 4,
        &quot;completion_tokens&quot;: 49,
        &quot;total_tokens&quot;: 53
    }
}
</code></pre>
<p>Could you please help me understand what could be causing the <code>Cannot find module '@openai/api'</code> error?</p>
<p>To provide me with next steps to try figure out why this API is not working. Either solutions or further tests I can try.</p>
<p>Thank you!</p>
","2023-04-14 11:50:53","","2023-04-15 10:20:49","2023-04-15 10:20:49","<javascript><node.js><openai-api><gpt-3>","2","0","1","449","","","","","","",""
"76025799","1","15764986","","Create multi-message conversations with the GPT API","<p>I am experimenting with the GPT API by OpenAI and am learning how to use the GPT-3.5-Turbo model. I found a quickstart example on the web:</p>
<pre><code>def generate_chat_completion(messages, model=&quot;gpt-3.5-turbo&quot;, temperature=1, max_tokens=None):
    headers = {
        &quot;Content-Type&quot;: &quot;application/json&quot;,
        &quot;Authorization&quot;: f&quot;Bearer {API_KEY}&quot;,
    }

    data = {
        &quot;model&quot;: model,
        &quot;messages&quot;: messages,
        &quot;temperature&quot;: temperature,
    }

    max_tokens = 100

    if max_tokens is not None:
        data[&quot;max_tokens&quot;] = max_tokens

    response = requests.post(API_ENDPOINT, headers=headers, data=json.dumps(data))

    if response.status_code == 200:
        return response.json()[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;]
    else:
        raise Exception(f&quot;Error {response.status_code}: {response.text}&quot;)

while 1:
    inputText = input(&quot;Enter your message: &quot;)

    messages = [
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: inputText},
    ]

    response_text = generate_chat_completion(messages)
    print(response_text)
</code></pre>
<p>With the necessary imports and the API key and endpoint defined above the code block. I added the inputText variable to take text inputs and an infinite <em>while</em> loop to keep the input/response cycle going until the program is terminated (probably bad practice).</p>
<p>However, I've noticed that responses from the API aren't able to reference previous parts of the conversation like the ChatGPT web application (rightfully so, as I have not mentioned any form of conversation object). I looked up on the API documentation on chat completion and the conversation request example is as follows:</p>
<pre><code>[
  {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant that translates English to French.&quot;},
  {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: 'Translate the following English text to French: &quot;{text}&quot;'}
]
</code></pre>
<p>However, this means I will have to send all the inputted messages into the conversation at once and get a response back for each of them. I cannot seem to find a way (at least as described in the API) to send a message, then get one back, and then send another message in the format of a full conversation with reference to previous messages like a chatbot (or as described before the ChatGPT app). Is there some way to implement this?</p>
<p>Also: the above does not use the OpenAI Python module. It uses the <a href=""https://requests.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">Requests</a> and JSON modules.</p>
","2023-04-16 03:42:43","","2023-04-16 10:33:32","2023-05-15 18:40:14","<python><python-requests><openai-api><gpt-3><chatgpt-api>","1","0","1","961","","","","","","",""
"58195745","1","9389353","","Generate text from input on default model gpt-2-simple python","<p>I can't figure out for the life of me how to generate text from the default model feeding in a prefix:</p>

<p>I have downloaded the model and here is my code:</p>

<pre><code>import gpt_2_simple as gpt2

model_name = ""124M""

sess = gpt2.start_tf_sess()

gpt2.generate(sess, model_name=model_name)

gpt2.generate(sess, model_name=model_name, prefix=""&lt;|My name is |&gt;"")
</code></pre>

<p>However when i run it i get the following error:</p>

<pre><code>tensorflow.python.framework.errors_impl.FailedPreconditionError: 2 root error(s) found. (0) Failed precondition: Attempting to use uninitialized value model/h3/mlp/c_proj/w [[{{node model/h3/mlp/c_proj/w/read}}]] [[strided_slice/_33]] (1) Failed precondition: Attempting to use uninitialized value model/h3/mlp/c_proj/w [[{{node model/h3/mlp/c_proj/w/read}}]]
</code></pre>

<p>Any idea what I'm doing wrong?</p>
","2019-10-02 05:36:51","","2020-11-29 12:07:49","2020-11-29 12:07:49","<python><tensorflow><gpt-2>","1","0","1","1688","","","","","","",""
"64312421","1","1157814","","OpenAI API and GPT-3, not clear how can I access or set up a learning/dev?","<p>I am reading tons of GPT-3 samples, and came cross many code samples.
None of them mentions that how and where I can run and play with the code myself... and especially not mentioning I can not.</p>
<p>So I did my research, and concluded, I can not, but I may be wrong:</p>
<ul>
<li>There is no way to run the &quot;thing&quot; on-premises on a dev machine, it is a hosted service by definition (?)</li>
<li>As of now (Oct. 11th 2020) the OpenAI API is in invite only beta (?)</li>
</ul>
<p>Did I miss something?</p>
","2020-10-12 05:59:30","","2023-01-19 04:31:06","2023-01-19 04:31:06","<nlp><openai-api><gpt-3>","2","0","4","1926","0","","","","","",""
"71376760","1","1165643","","How to output the list of probabilities on each token via model.generate?","<p>Right now I have:</p>
<pre><code>model = GPTNeoForCausalLM.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
input_ids = tokenizer(prompt, return_tensors=&quot;pt&quot;).input_ids.cuda()
gen_tokens = model.generate(input_ids, do_sample=specifiedDoSample, output_scores=True, temperature=specifiedTemperature, max_new_tokens=specifiedNumTokens, repetition_penalty=specifiedRepetitionPenalty, top_p=specifiedTopP)
gen_text = tokenizer.batch_decode(gen_tokens)[0]
print(gen_text)
</code></pre>
<p>This will print the generated text. However, I want it to list the top N tokens in each step as well as their probability (N being a number specified by me), similar to OpenAI's beta playground where you can select &quot;Show probabilities: Full spectrum&quot;. For example, if the prompt is &quot;You are now a&quot;, the next token should say something like {&quot;vampire&quot;: 51%, &quot;corpse&quot;: 32% ... etc.}</p>
<p>What is the easiest way to do this via Huggingface Transformers?</p>
","2022-03-07 05:28:50","","2023-01-19 04:33:35","2023-01-19 04:33:35","<python><nlp><huggingface-transformers><gpt-3>","2","0","0","3655","","","","","","",""
"71303277","1","18339450","","How to remove input from from generated text in GPTNeo?","<p>I'm writing a program to generate text...
I need to remove the input from the generated text. How can I do this?
The code:</p>
<pre><code>input_ids = tokenizer(context, return_tensors=&quot;pt&quot;).input_ids
gen_tokens = model.generate(
    input_ids,
    do_sample=True,
    temperature=0.8,
    top_p=0.9)
strs = tokenizer.batch_decode(gen_tokens)[0]
</code></pre>
<p>Here the strs contains the input I've given...
How to remove that?</p>
","2022-03-01 03:03:51","","2022-12-13 15:41:24","2022-12-13 15:41:24","<huggingface-transformers><gpt-2>","1","0","2","802","","","","","","",""
"75701297","1","8459132","","Not enough memory for fine tuning LLM with Hugging Face","<p>I'm running into runtime errors where I don't have enough memory to fine tune a pretrained LLM.</p>
<p>I'm a novelist and I am curious to see what would happen if I fine tune a pretrained LLM to write more chapters of my novel in my style.</p>
<p>I successfully ran a tutorial on fine tuning a BERT model with Hugging Face with a Yelp dataset that is smaller than mine yesterday on my CPU (I have 16GB RAM and don't have an NVIDIA GPU,) so not sure where the error is arising from now.</p>
<p>Some things I've tried, but still giving me a runtime memory error:</p>
<ul>
<li>changed my model from Neo GPT to GPT2, which is much smaller</li>
<li>decreased my batch size hyperparameter</li>
<li>decreased the max length of tokens</li>
<li>decreased my dataset size</li>
</ul>
<p>This is my code:</p>
<pre><code>from transformers import GPTNeoForCausalLM, GPT2Tokenizer, Trainer, TrainingArguments
from datasets import Dataset, load_dataset

# Step 1: Import my novel
import docx
import pandas as pd

# Read each paragraph from a Word file
doc = docx.Document(r&quot;C:\Users\chris\Downloads\The Black Squirrel (1).docx&quot;)
paras = [p.text for p in doc.paragraphs if p.text]

# Convert list to dataframe
df = pd.DataFrame(paras)
df.reset_index(drop=False,inplace=True)
df.rename(columns={'index':'label',0:'text'},inplace=True)

# Split my novel into train and test
from sklearn.model_selection import train_test_split

train, test = train_test_split(df, test_size=0.05)

# Export novel as CSV to be read by Huggingface library
train.to_csv(r&quot;C:\Users\chris\OneDrive\Documents\ML\data\black_squirrel_dataset_train.csv&quot;, index=False)
test.to_csv(r&quot;C:\Users\chris\OneDrive\Documents\ML\data\black_squirrel_dataset_test.csv&quot;, index=False)

# Tokenize novel
datasets = load_dataset('csv',
                       data_files={'train':r&quot;C:\Users\chris\OneDrive\Documents\ML\data\black_squirrel_dataset_train.csv&quot;,
                       'test':r&quot;C:\Users\chris\OneDrive\Documents\ML\data\black_squirrel_dataset_test.csv&quot;})

# Instantiate tokenizer
tokenizer = GPT2Tokenizer.from_pretrained(&quot;EleutherAI/gpt-neo-1.3B&quot;,
                                          pad_token='[PAD]')

# Do I need the below?
# tokenizer.enable_padding(pad_id=tokenizer.token_to_id('[PAD]'))
paragraphs = df['text']
max_length = max([len(tokenizer.encode(paragraphs)) for paragraphs in paragraphs])

# Tokenize my novel
def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;], padding='max_length', truncation=True)

tokenized_datasets = datasets.map(tokenize_function, batched=True)

# Step 2: Train the model
model = GPTNeoForCausalLM.from_pretrained(&quot;EleutherAI/gpt-neo-1.3B&quot;)

model.resize_token_embeddings(len(tokenizer))

training_args = TrainingArguments(
    output_dir=r&quot;C:\Users\chris\OneDrive\Documents\ML\models&quot;,
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=32, # batch size for training
    per_device_eval_batch_size=64,  # batch size for evaluation
    eval_steps = 400, # Number of update steps between two evaluations.
    save_steps=800, # after # steps model is saved
    warmup_steps=500,# number of warmup steps for learning rate scheduler
    )

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test']
)

trainer.train()
</code></pre>
<p>Here is my error readout:</p>
<pre><code>***** Running training *****
  Num examples = 779
  Num Epochs = 3
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed &amp; accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 75
  Number of trainable parameters = 1315577856
  0%|          | 0/75 [19:12&lt;?, ?it/s]
Traceback (most recent call last):
  File &quot;C:\Users\chris\AppData\Local\Programs\Python\Python37\lib\code.py&quot;, line 90, in runcode
    exec(code, self.locals)
  File &quot;&lt;input&gt;&quot;, line 9, in &lt;module&gt;
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\trainer.py&quot;, line 1547, in train
    ignore_keys_for_eval=ignore_keys_for_eval,
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\trainer.py&quot;, line 1791, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\trainer.py&quot;, line 2539, in training_step
    loss = self.compute_loss(model, inputs)
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\trainer.py&quot;, line 2571, in compute_loss
    outputs = model(**inputs)
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\models\gpt_neo\modeling_gpt_neo.py&quot;, line 752, in forward
    return_dict=return_dict,
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\models\gpt_neo\modeling_gpt_neo.py&quot;, line 627, in forward
    output_attentions=output_attentions,
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\models\gpt_neo\modeling_gpt_neo.py&quot;, line 342, in forward
    feed_forward_hidden_states = self.mlp(hidden_states)
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\models\gpt_neo\modeling_gpt_neo.py&quot;, line 300, in forward
    hidden_states = self.act(hidden_states)
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\activations.py&quot;, line 35, in forward
    return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
RuntimeError: [enforce fail at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\c10\core\impl\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 405798912 bytes.
</code></pre>
<p>My novel is <a href=""https://docs.google.com/document/d/1PI81BJy19_t4YdNNVC08XraxC3TRyQVYffrYn8PlZ0E/edit"" rel=""nofollow noreferrer"">here</a>. You can save as docx as is and run the code. Or, you can just save the first chapter. I also tried splitting up the first chapter into one paragraph per sentence to make the tokens even smaller, though that didn't help.</p>
<p>Does this indicate that I really need an NVIDIA GPU to run machine learning tasks? Or is this likely an issue with my dataset setup or code?</p>
<p>Thanks.</p>
","2023-03-10 21:50:32","","","2023-03-10 21:50:32","<machine-learning><pytorch><huggingface><gpt-2>","0","0","0","362","","","","","","",""
"75753390","1","21409617","","how to create prompt and completion for numerical dataset in GPT3 model","<p>i am trying to customize GPT 3 model for sales domain.
Is it possible to fine tune gpt3 model using a dataset which has numerical and categorical columns. If then how can we create prompt and completion for that particular dataset.</p>
<p>how to create prompt and completion for dataset which has numerical and categorical column.</p>
","2023-03-16 07:53:02","","","2023-03-16 07:53:02","<prompt><completion><gpt-3>","0","0","0","94","","","","","","",""
"76091454","1","16861522","","How can I improve my ChatGPT API prompts?","<p>I am having an issue with ChatGPT API relating to prompt engineering</p>
<p>I have a dataset which consists of individual product titles, and product descriptions which was awful design, but I didn't have control over that part. <strong>I need to create aggregate titles for the individual titles.</strong></p>
<p>I fine-tuned the Curie model on the data in a similar way to this:</p>
<p>Prompt:</p>
<p><code>60cm tall Oak Drop Leaf Table|80cm tall Oak Drop Leaf Table|100cm tall Oak Drop Leaf Table|60cm tall Drop Leaf Table Material: Oak with bird design</code></p>
<p>Completion:</p>
<p><code>Oak Drop Leaf Table</code></p>
<p>I fine-tuned it on about 100 human-written titles</p>
<p>I am currently using settings:</p>
<pre><code>$data = array(
    'model' =&gt; $model,
    'prompt' =&gt; $prompt,
    'temperature' =&gt; 0.6,
    'max_tokens' =&gt; 25,
    &quot;top_p&quot; =&gt; 1,
    &quot;frequency_penalty&quot; =&gt; 0,
    &quot;presence_penalty&quot; =&gt; 0.6,
);
</code></pre>
<p>I have varied these, but to no great effect.</p>
<p>I am wondering where am I going wrong?</p>
<p>I am getting responses like:</p>
<p><code>60cm Oak Drop Leaf TableOak Drop Leaf TableOak Drop Leaf Table</code></p>
<p><code>Oak Drop Leaf Table|Oak Drop Leaf Table|Oak Drop Leaf Table</code></p>
<p><code>Oak Drop Leaf Table,Oak Drop Leaf Table,Oak Drop Leaf Table</code></p>
","2023-04-24 11:37:33","","2023-04-24 11:39:17","2023-04-24 11:39:17","<prompt><openai-api><gpt-3><chatgpt-api><gpt-4>","0","2","0","264","","","","","","",""
"76100128","1","17319114","","How to stop GPT-3.5-Turbo model from generating text (azure)?","<p>In my use case I am using openai models hosted on azure. I am trying to generate a list of senteces or words with a specific length. Lets take this prompt as an example:</p>
<pre><code>Give 10 Examples of pizza ingredients: 
1. tomatoes
2. mushrooms
</code></pre>
<p>The text-davinci-003 model completes the list as expected and stops but the gpt-3.5-turbo model generates tokens until the token limit is reached, even when I tell the model to stop when the task is done. Using few shot prompting also doesn't seem to work here.</p>
<p>Hacky workarounds</p>
<ul>
<li><p>Using a low value for max_tokens. But it is hard to estimate the value because parts of the prompt will be changed dynamically in the application. And it still needs postprocessing to remove wasted tokens.</p>
</li>
<li><p>Put a counter before the examples and then using a specific number as stop sequence. When using a general counter like above then I need to ensure that the stop sequence won't be generated accidentally so that the model stops. When using an unusual counter like &quot;1~~&quot;, &quot;2~~&quot;... there is a chance that the model malforms the stop sequence so that it still will be generating until the limit is reached.</p>
</li>
</ul>
<p>Is there a clean and easy solution to let the model stop generating, like text-davinci-003 does?</p>
","2023-04-25 10:24:01","","2023-04-26 10:38:20","2023-04-28 17:53:00","<azure><gpt-3><azure-openai><text-davinci-003>","1","0","2","377","","","","","","",""
"76100892","1","282855","","GPT4 - Unable to get response for a question?","<p>As the title clearly describes the issue I've been experiencing, I'm not able to get a response to a question from the dataset I use using the <a href=""https://github.com/nomic-ai/gpt4all"" rel=""nofollow noreferrer"">nomic-ai/gpt4all</a>. The execution simply stops. No exception occurs. How can I overcome this situation?</p>
<p>p.s. I use the offline mode of <code>GPT4</code> since I need to process a bulk of questions.</p>
<p>p.s. This issue occurs <strong>after a while</strong>. Something prevents <code>GPT4All</code> to generate a response for the given question.</p>
<pre><code>question = 'Was Avogadro a  professor at the University of Turin?'
gpt = GPT4All()
gpt.open()
resp = gpt.prompt(question)
print(f'Response: {resp}')  # --&gt; the execution does not reach here
</code></pre>
","2023-04-25 11:57:01","","2023-04-29 21:06:24","2023-04-29 21:06:55","<gpt-3><chatgpt-api><gpt-4><gpt4all><pygpt4all>","1","0","1","515","","","","","","",""
"76169951","1","20205450","","model.bert() through the slicing error can anyone let me know why is it?","<pre><code>with torch.no_grad():
      logits = torch.zeros(len(definitions), dtype=torch.double).to(DEVICE)
      for i, bert_input in list(enumerate(features)):
          logits[i] = model.ranking_linear(
              model.bert(
                  input_ids=torch.tensor(bert_input.input_ids, dtype=torch.long).unsqueeze(0).to(DEVICE),
                  attention_mask=torch.tensor(bert_input.input_mask, dtype=torch.long).unsqueeze(0).to(DEVICE),
                  token_type_ids=torch.tensor(bert_input.segment_ids, dtype=torch.long).unsqueeze(0).to(DEVICE)
              )[1]
          )
      scores = softmax(logits, dim=0)

      preds = (sorted(zip(sense_keys, definitions, scores), key=lambda x: x[-1], reverse=True))
</code></pre>
<p><strong>This Error what i get</strong></p>
<blockquote>
<hr />
<p>IndexError                                Traceback (most recent call last)
 in &lt;cell line: 1&gt;()
3       for i, bert_input in list(enumerate(features)):
4           logits[i] = model.ranking_linear(
5               model.bert(
6                   input_ids=torch.tensor(bert_input.input_ids, dtype=torch.long).unsqueeze(0).to(DEVICE),
7                   attention_mask=torch.tensor(bert_input.input_mask, dtype=torch.long).unsqueeze(0).to(DEVICE),</p>
<p>6 frames</p>
<p>/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
2208         # remove once script supports set_grad_enabled
2209         <em>no_grad_embedding_renorm</em>(weight, input, max_norm, norm_type)
2210     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
2211
2212</p>
<p>IndexError: index out of range in self</p>
</blockquote>
<p>I want to run this code and i am confuse why is it giving me a index out of range error</p>
","2023-05-04 05:52:46","","","2023-05-04 05:52:46","<python><deep-learning><nlp><bert-language-model><gpt-2>","0","0","0","17","","","","","","",""
"76283060","1","1081396","","Fine-tune Openai GPT - Set general instructions?","<p>I want to make GPT summarise paragraphs. So far I'm sending him a prompt like this:</p>
<pre><code>Please summarize each of the 7 paragraphs below into 7 new summarized paragraphs 
containing no more than 40 words.

Parragraph1: fadsfas
Parragraph2: fadsfa
...
</code></pre>
<p>I'm not using any fine-tuned model so far, but I'd like to, mainly because it will cost less to run the queries and it is supposed to be faster.</p>
<p>I've tried training the model by providing a set of &quot;prompts&quot; with paragraphs and a set of &quot;completions&quot; with their summaries, but the fine-tuned model doesn't work well.</p>
<p>I assume I didn't train the model in the correct way.</p>
<p>Isn't there a way to provide the fine-tuning process with a set of &quot;instructions&quot; together with the set of prompts + completions?</p>
<p>How would GPT know if I want paragraphs to never exceed a certain number of words for example?</p>
<p>I have the impression that the dataset might not be enough to provide enough instructions to GPT on how to process the input.</p>
<p>Should I just send this in every prompt that I use for the training?</p>
<pre><code>{
  &quot;prompt&quot;: &quot;Please summarize each of the 7 paragraphs below into 7 
              new summarized paragraphs containing no more than 40 words.
    
              Parragraph1: ....&quot;
  &quot;completion&quot;: &quot;...&quot;
}
</code></pre>
","2023-05-18 17:15:25","","2023-05-18 17:22:04","2023-05-18 17:22:04","<openai-api><gpt-3>","0","0","0","43","","","","","","",""
"76293205","1","1354514","","My gpt2 code generates a few correct words and then goes into a loop of generating the same sequence again and again","<p>The following gpt2 code for sentence completion generates a few good sentences and then ends in a loop of repetitive sentences.</p>
<pre><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer                         
import torch                                                                    
                                                                                
# Load the pre-trained model and tokenizer                                      
model_name = 'gpt2'                                                             
model = GPT2LMHeadModel.from_pretrained(model_name)                             
tokenizer = GPT2Tokenizer.from_pretrained(model_name)                           
                                                                                
# Set the model to evaluation mode                                              
model.eval()                                                                    
#                                                                               
# Input sentence                                                                
input_sentence = &quot;I want to go to the&quot;                                          
                                                                                
for i in range(200):                                                            
                                                                                
    # Tokenize the input sentence                                               
    input_tokens = tokenizer.encode(input_sentence, return_tensors='pt')        
                                                                                
    # Generate predictions                                                      
    with torch.no_grad():                                                       
        outputs = model.generate(input_tokens, max_length=len(input_tokens) + 1, num_return_sequences=1)
                                                                                
    # Decode the generated output                                               
    generated_output = tokenizer.decode(outputs[0], skip_special_tokens=True)   
                                                                                
    print(generated_output)                                                     
                                                                                
    input_sentence = generated_output    
</code></pre>
","2023-05-20 01:28:54","","","2023-05-22 17:55:58","<nlp><stanford-nlp><huggingface-transformers><gpt-2>","1","0","0","49","","","","","","",""
"76304353","1","15107876","","IndexError: index out of range in self while using GPT2LMHeadModel.from_pretrained(""gpt2"")","<p>I am working on this question answering code and using pretrained GPT2LMHeadModel. But after tokenization when I pass the inputs and attention mask to the model it is giving index error. My code:</p>
<pre><code>feedback_dataset = []

# Preprocessing
nltk.download(&quot;stopwords&quot;)
nltk.download(&quot;wordnet&quot;)

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words(&quot;english&quot;))

def preprocess_text(text):
    # Lowercase
    text = text.lower()
    
    # Remove punctuation
    text = text.translate(str.maketrans(&quot;&quot;, &quot;&quot;, string.punctuation))
    
    # Remove numbers
    text = re.sub(r&quot;\d+&quot;, &quot;&quot;, text)
    
    # Tokenization
    tokens = text.split()
    
    # Remove stop words
    tokens = [token for token in tokens if token not in stop_words]
    
    # Lemmatization
    tokens = [lemmatizer.lemmatize(token) for token in tokens]
    
    # Join tokens
    text = &quot; &quot;.join(tokens)
    
    return text

# Preprocess the dataset
preprocessed_dataset = [
    {
        &quot;user&quot;: preprocess_text(entry[&quot;user&quot;]),
        &quot;bot&quot;: preprocess_text(entry[&quot;bot&quot;])
    }
    for entry in dataset
]

# Load pre-trained model and tokenizer
model = GPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;)
tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
# Add padding token to the tokenizer
tokenizer.add_special_tokens({'pad_token': '[PAD]'})

# Define the maximum sequence length
max_length = 512  # Set your desired maximum length here

# Tokenize and format the dataset with truncation
tokenized_dataset = tokenizer.batch_encode_plus(
    [(entry[&quot;user&quot;], entry[&quot;bot&quot;]) for entry in preprocessed_dataset],
    padding=&quot;longest&quot;,
    truncation=True,
    max_length=max_length,
    return_tensors=&quot;pt&quot;
)

input_ids = tokenized_dataset[&quot;input_ids&quot;]
attention_mask = tokenized_dataset[&quot;attention_mask&quot;]

# Ensure input tensors have correct shape
input_ids = input_ids.squeeze()
attention_mask = attention_mask.squeeze()
# Define optimizer and loss function
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
loss_fn = torch.nn.CrossEntropyLoss()

# Training loop
num_epochs = 2
for epoch in range(num_epochs):
    optimizer.zero_grad()
    inputs = {
        &quot;input_ids&quot;: input_ids,
        &quot;attention_mask&quot;: attention_mask,
        &quot;labels&quot;: input_ids
    }
    print(&quot;input_ids shape: &quot;, input_ids.shape,&quot;attention_mask shape: &quot;, attention_mask.shape)#, &quot;input shape: &quot;, inputs)
    
    outputs = model(**inputs)
    loss = outputs.loss
    loss.backward()
    optimizer.step()

</code></pre>
<p>I am getting error in the <code>outpus =  model(**inputs) </code> line.
The error is:</p>
<pre><code>input_ids shape:  torch.Size([5, 19]) attention_mask shape:  torch.Size([5, 19])
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-39-3329f43b161a&gt; in &lt;cell line: 7&gt;()
     14     print(&quot;input_ids shape: &quot;, input_ids.shape,&quot;attention_mask shape: &quot;, attention_mask.shape)#, &quot;input shape: &quot;, inputs)
     15 
---&gt; 16     outputs = model(**inputs)
     17     loss = outputs.loss
     18     loss.backward()

6 frames
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1499                 or _global_backward_pre_hooks or _global_backward_hooks
   1500                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1501             return forward_call(*args, **kwargs)
   1502         # Do not call functions when jit is used
   1503         full_backward_hooks, non_full_backward_hooks = [], []

/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py in forward(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)
   1074         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
   1075 
-&gt; 1076         transformer_outputs = self.transformer(
   1077             input_ids,
   1078             past_key_values=past_key_values,

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1499                 or _global_backward_pre_hooks or _global_backward_hooks
   1500                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1501             return forward_call(*args, **kwargs)
   1502         # Do not call functions when jit is used
   1503         full_backward_hooks, non_full_backward_hooks = [], []

/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py in forward(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)
    841 
    842         if inputs_embeds is None:
--&gt; 843             inputs_embeds = self.wte(input_ids)
    844         position_embeds = self.wpe(position_ids)
    845         hidden_states = inputs_embeds + position_embeds

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1499                 or _global_backward_pre_hooks or _global_backward_hooks
   1500                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1501             return forward_call(*args, **kwargs)
   1502         # Do not call functions when jit is used
   1503         full_backward_hooks, non_full_backward_hooks = [], []

/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py in forward(self, input)
    160 
    161     def forward(self, input: Tensor) -&gt; Tensor:
--&gt; 162         return F.embedding(
    163             input, self.weight, self.padding_idx, self.max_norm,
    164             self.norm_type, self.scale_grad_by_freq, self.sparse)

/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   2208         # remove once script supports set_grad_enabled
   2209         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 2210     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   2211 
   2212 

IndexError: index out of range in self
</code></pre>
<p>the size of input and attention mask is same. And it also the shape of token is also less than 1024 which is max for gpt2. So what could be the problem? Can anyone help me please.</p>
","2023-05-22 08:33:26","","","2023-05-22 08:33:26","<python><index-error><gpt-2>","0","0","0","54","","","","","","",""
"76380787","1","7006465","","Kotlin code to openapi call not working beyond building jsonObjectRequest","<p>I am making a call from Kotlin code to openai api (gpt-3.5-turbo). I am using a valid my_token for auth. My code is as below -</p>
<pre><code>override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        setContentView(R.layout.activity_generate)

        requestQueue = Volley.newRequestQueue(this)

        val outputText = findViewById&lt;TextView&gt;(R.id.present_final)

        val prompt = &quot;say hi&quot;
        val requestBody = JSONObject()
        requestBody.put(&quot;model&quot;, &quot;gpt-3.5-turbo&quot;)
        requestBody.put(&quot;messages&quot;, JSONArray()
            .put(JSONObject().put(&quot;role&quot;, &quot;system&quot;).put(&quot;content&quot;, &quot;You are a helpful assistant.&quot;))
            .put(JSONObject().put(&quot;role&quot;, &quot;user&quot;).put(&quot;content&quot;, prompt))
        )


        val jsonObjectRequest = object: JsonObjectRequest(
            Request.Method.POST,
            &quot;https://api.openai.com/v1/chat/completions&quot;,
            requestBody,
            Response.Listener { response -&gt;
                try {
                    val generatedResponse = parseGeneratedResponse(response.toString())
                    runOnUiThread {
                        outputText.text = generatedResponse
                    }
                } catch (e: JSONException) {
                    Log.e(&quot;GenerateActivity&quot;, &quot;JSONException while parsing response: ${e.message}&quot;)
                } catch (e: Exception) {
                    Log.e(&quot;GenerateActivity&quot;, &quot;Exception: ${e.message}&quot;)
                }
            },
            Response.ErrorListener { error -&gt;
                Log.e(&quot;GenerateActivity&quot;, &quot;API request failed: ${error.message}&quot;)
            }
        ) {
            override fun getHeaders(): MutableMap&lt;String, String&gt; {
                return mutableMapOf(
                    &quot;Content-Type&quot; to &quot;application/json&quot;,
                    &quot;Authorization&quot; to &quot;Bearer &lt;my_token&gt;&quot;
                )
            }
        }

        requestQueue.add(jsonObjectRequest)
    }
</code></pre>
<p>when I debug, the code flow is not getting inside the line <strong>val jsonObjectRequest = object: JsonObjectRequest(</strong>.</p>
<p>I trid using the same my_token and same prompt through a simple python script using request lib and it is working fine. Please help with the kotlin code. Thanks!</p>
","2023-06-01 10:04:12","","","2023-06-01 10:04:12","<kotlin><openai-api><gpt-3><chatgpt-api>","0","0","0","7","","","","","","",""
"76381731","1","404348","","Use Open AI API in VBA code to answer user query","<p>I have written a code in vba where i am taking input from user and provide answer to user query using Open AI api + json pasre. The code was running, although i am not sure what went wrong and now its giving error &quot;Object required&quot; on Json parse line</p>
<pre><code>Sub GenerateInsights()
Dim xmlHttp As Object
Set xmlHttp = CreateObject(&quot;MSXML2.XMLHTTP.6.0&quot;)

Dim url As String
url = &quot;https://api.openai.com/v1/completions&quot;

Dim apiKey As String
apiKey = &quot;API_KEY&quot;

Dim prompt As String
prompt = inputbox(&quot;Ask Question&quot;)

Dim payload As String
payload = &quot;{&quot;&quot;model&quot;&quot;: &quot;&quot;text-davinci-003&quot;&quot;,&quot;&quot;prompt&quot;&quot;: &quot;&quot;&quot; &amp; prompt &amp; &quot;&quot;&quot;,&quot;&quot;max_tokens&quot;&quot;: 1000}&quot;


xmlHttp.Open &quot;POST&quot;, url, False
xmlHttp.setRequestHeader &quot;Content-Type&quot;, &quot;application/json&quot;
xmlHttp.setRequestHeader &quot;Authorization&quot;, &quot;Bearer &quot; &amp; apiKey
xmlHttp.send payload

Dim response As String
response = xmlHttp.responseText

' Parse the response JSON to extract the generated insights
Dim generatedText As String
generatedText = ParseGeneratedText(response)

' Output the generated insights
Debug.Print generatedText
MsgBox generatedText


End Sub

Function ParseGeneratedText(response As String) As String
    Dim json As Object
    Set json = JsonConverter.ParseJson(response)

    Dim choices As Object
    Set choices = json(&quot;choices&quot;)

    Dim generatedText As String
    generatedText = choices(1)(&quot;text&quot;)

    ParseGeneratedText = generatedText
End Function
</code></pre>
<p>I am getting error Object required on below line</p>
<pre><code>  Set choices = json(&quot;choices&quot;)
</code></pre>
<p>Please help.</p>
","2023-06-01 12:12:17","","","2023-06-01 12:12:17","<json><vba><openai-api><gpt-3><chatgpt-api>","0","10","-1","62","","","","","","",""
"76413431","1","22028199","","Can't call transcribed text from whisper to openai chatbot","<p>This script takes input from microphone and transcribe the speech to text and pass the text to the gpt tex-davinci for generating response.</p>
<p>But the script is not generating any gpt response.</p>
<pre><code>import openai
import gradio as gr

import whisper

import time

model = whisper.load_model(&quot;base&quot;)

prompt_input = ''

def transcribe(audio):
    global prompt_input
  
    # load audio and pad/trim it to fit 30 seconds
    audio = whisper.load_audio(audio)
    audio = whisper.pad_or_trim(audio)

    # make log-Mel spectrogram and move to the same device as the model
    mel = whisper.log_mel_spectrogram(audio).to(model.device)

    # detect the spoken language
    _, probs = model.detect_language(mel)
    print(f&quot;Detected language: {max(probs, key=probs.get)}&quot;)

    # decode the audio
    options = whisper.DecodingOptions()
    result = whisper.decode(model, mel, options)
    prompt_input = result.text

gr.Interface(
    title='OpenAI Whisper ASR Gradio Web UI', 
    fn=transcribe, 
    inputs=[
        gr.inputs.Audio(source=&quot;microphone&quot;, type=&quot;filepath&quot;)
    ],
    outputs=[
        &quot;textbox&quot;
    ],
    live=True).launch(share=True)

openai.api_key = 'API_KEY'
openai.api_base = 'https://api.openai.com'

def ask_gpt(prompt, model):
    response = openai.Completion.create(
        engine=model,
        prompt=prompt,
        max_tokens=1024,
        n=1,
        stop=None,
        temperature=0.7
    )

    return response.choices[0].text.strip()

def main():
    model = 'text-davinci-003'
    while True:
        prompt = prompt_input
        if prompt.lower() == 'quit':
            break

        response = ask_gpt(prompt=f'User: {prompt}\nBot: ', model=model)
        print(f'Bot: {response}')
        
if __name__ == '__main__':
    main()
</code></pre>
<p>I am using the openai whisper ASR <a href=""https://github.com/petewarden/openai-whisper-webapp"" rel=""nofollow noreferrer"">web app</a> for speech-to-text and gpt text-davinci for generating response.
Also, I am running it in Google Colab.
Thanks</p>
","2023-06-06 09:55:35","","","2023-06-11 02:00:29","<python><google-colaboratory><openai-api><gpt-3><openai-whisper>","2","0","0","68","","","","","","",""
"76413465","1","22028890","","How to fune-tune and deploy ChatGPT on Cloud?","<p>I know - how to fine-tune the ChatGPT. However, I not able to find out - How we can deploy the fine-tuned model in our server/cloud?
Can you anyone please help me with these?</p>
<p>I've created the fine-tune model of ChatGPT. But I'm not getting - how to that model can be in my system/server/cloud?</p>
","2023-06-06 09:59:06","","","2023-06-06 09:59:06","<openai-api><gpt-3><chatgpt-api><fine-tune><gpt-4>","0","0","0","19","","","","","","",""
"76526834","1","9904671","","Need ideas suggestions on Personality Mimic Chatbot","<p>I want to make a chatbot that will clone the behaviour and habits of a person. I am using Langchain Retriever for this, the data is a huge database of multiple questions. And so far, it is going well. Now I want to add one more layer on top of this and change the retrived answer. For example if some person is playful in nature, I want to convert that answer in a playful manner.</p>
<p>One thing that I am thinking is, send the answer to GPT and ask it to do so, but again that increases the costs. I even looked into some hosted HuggingFace models, but i couldn't find something very specific to my usecase.</p>
<p>Any ideas on how to do this in a better and efficient manner?</p>
<p>Thank you!</p>
","2023-06-21 20:15:23","","","2023-06-21 20:15:23","<openai-api><huggingface><langchain><gpt-3>","0","0","0","10","","","","","","",""
"75787052","1","21436401","","GPT 4 API delays/data types","<p>I got into the API beta and I'm playing around with an app. I got as far as getting the API connection working and doing what I want in pycharm, but have a couple problems:</p>
<ol>
<li><p>I'm getting pretty slow response times and hitting a usage cap frequently as well (the API account is sufficiently funded). I assume some of this will improve as the new product stabilizes? Would rather not switch to an earlier model for my use case.</p>
</li>
<li><p>I'm asking GPT to give me a list of items in a python list format, which I am able to typecast into an actual list. If I set the temperature too low I get back repetitive items, but if I set it too high I don't get the correct python formatting.</p>
</li>
<li><p>I'm hitting the API 5 or 6 times which could probably be consolidated down to a couple, but that would depend on consistently getting a properly formatted JSON response, which seems more dubious than asking for a python list.</p>
</li>
</ol>
<p>Basically, is this thing predictable enough that you can ask for a certain data format and it will come in that format reliably enough to build an app on top of?</p>
<p>Any suggestions/discussion is appreciated.</p>
<p>What have I tried:
Tried various temperatures. Have asked OpenAI to increase usage cap. Have not tried other models.</p>
","2023-03-20 06:12:24","","","2023-04-20 16:34:35","<gpt-4>","1","0","0","254","","","","","","",""
"62362406","1","13370109","","Is gpt-2 unusable with python?","<p>I was following <a href=""https://medium.com/@ngwaifoong92/beginners-guide-to-retrain-gpt-2-117m-to-generate-custom-text-content-8bb5363d8b7f"" rel=""nofollow noreferrer"">this</a> tutorial and ran across an issue while using train.py. the issue says</p>

<pre><code>Exception has occurred: ModuleNotFoundError
No module named 'tensorflow.contrib'
  File ""F:\PythonFiles\Post Generator\gpt-2\src\model.py"", line 3, in &lt;module&gt;
    from tensorflow.contrib.training import HParams
</code></pre>

<p>I searched a lot on the internet and it turns out that tensorflow.contrib has been depreceated. So is there an alternate way to do so or the gpt-2 is not usable with python?</p>

<p>I also tried</p>

<pre><code>pip install tensorflow==1.15

ERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0)
ERROR: No matching distribution found for tensorflow==1.15
</code></pre>
","2020-06-13 16:07:04","","2020-11-29 11:52:25","2020-11-29 11:52:25","<python><tensorflow><gpt-2>","1","3","2","1169","","","","","","",""
"59501673","1","2315835","","Tensor Flow issues with Python","<p>Still struggling to get that GPT-2 Tutorial working. I Am now back to having issues with Tensor Flow.  Note I'm on a Completely clean install of Windows 10 (x64) on a Lenovo Thinkpad.</p>

<p>Getting the following error whenever I try to train GPT-2:</p>

<pre><code>Traceback (most recent call last):
  File ""C:\Users\Ian\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in &lt;module&gt;
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Ian\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in &lt;module&gt;
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Ian\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.7_3.7.1776.0_x64__qbz5n2kfra8p0\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.7_3.7.1776.0_x64__qbz5n2kfra8p0\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.
</code></pre>

<p>During handling of the above exception, another exception occurred:</p>

<pre><code>Traceback (most recent call last):
  File ""encode.py"", line 10, in &lt;module&gt;
    from load_dataset import load_dataset
  File ""C:\PY\gpt-2-finetuning\src\load_dataset.py"", line 4, in &lt;module&gt;
    import tensorflow as tf
  File ""C:\Users\Ian\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\__init__.py"", line 98, in &lt;module&gt;
    from tensorflow_core import *
  File ""C:\Users\Ian\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_core\__init__.py"", line 40, in &lt;module&gt;
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Ian\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Ian\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.7_3.7.1776.0_x64__qbz5n2kfra8p0\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Ian\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_core\python\__init__.py"", line 49, in &lt;module&gt;
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Ian\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in &lt;module&gt;
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Ian\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in &lt;module&gt;
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Ian\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in &lt;module&gt;
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Ian\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.7_3.7.1776.0_x64__qbz5n2kfra8p0\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.7_3.7.1776.0_x64__qbz5n2kfra8p0\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.
</code></pre>

<p>Any thoughts?</p>
","2019-12-27 13:43:38","","2020-11-29 12:06:25","2020-11-29 12:06:25","<python-3.x><tensorflow><windows-10><gpt-2>","1","0","0","116","","","","","","",""
"67598327","1","11259950","","JSONDecodeError: Unexpected UTF-8 BOM (decode using utf-8-sig): line 1 column 1 (char 0) ---While Tuning gpt2.finetune","<p>Hope you all are doing good ,
I am working on fine tuning GPT 2 model to generate Title based on the content ,While working on it ,I have created a simple CSV files containing only the title to train the model , But while inputting this model to GPT 2 for fine tuning I am getting the following ERROR ,
JSONDecodeError                           Traceback (most recent call last)
 in ()
10               steps=1000,
11               save_every=200,
---&gt; 12               sample_every=25)   # steps is max number of training steps
13
14 # gpt2.generate(sess)</p>
<pre><code>    3 frames
    /usr/lib/python3.7/json/__init__.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)
        336         if s.startswith('\ufeff'):
        337           s = s.encode('utf8')[3:].decode('utf8')
    --&gt; 338             # raise JSONDecodeError(&quot;Unexpected UTF-8 BOM (decode using utf-8-sig)&quot;,
        339             #                       s, 0)
        340     else:
    
    JSONDecodeError: Unexpected UTF-8 BOM (decode using utf-8-sig): line 1 column 1 (char 0)
    
    Below is my code for the above :
    
    import gpt_2_simple as gpt2
    
    model_name = &quot;120M&quot; # &quot;355M&quot; for larger model (it's 1.4 GB)
    gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/117M/
    sess = gpt2.start_tf_sess()
    
    gpt2.finetune(sess,
                  'titles.csv',
                  model_name=model_name,
                  steps=1000,
                  save_every=200,
                  sample_every=25)   # steps is max number of training steps
    
    I have tried all the basic mechanism of handing UTF -8 BOM but did not find any luck ,Hence requesting your help .It would be a great help from you all .
</code></pre>
","2021-05-19 06:57:50","","","2021-05-26 22:50:14","<utf-8><byte-order-mark><gpt-2>","1","0","0","789","","","","","","",""
"69098317","1","7211427","","Mycin like diagnosis system using gpt3 model","<p>I  wondering if we can build a Mycin like expert system using most advanced deep learning model like GPT3 by fine tuning medical domain knowledge. We build 40 years ago Mycin using symbolic approach but I am not sure it is possible now.</p>
","2021-09-08 06:57:51","","","2021-09-08 06:57:51","<interactive><expert-system><gpt-3>","0","0","1","31","","","","","","",""
"65974247","1","9837081","","Incrementally training || pause&resume training, GPT2 language model'ing","<p>I'm currently trying to learn python - and at the same time learning machine learning with GPT-2 language modeling - i have had some problems, and i got over most of them, and finally got something decent running.</p>
<p><strong>But...</strong> as most of you probably know, training your model takes alot of CPU/GPU power &amp; time - time i can spare, but the problem is that i cant have it running non-stop on my home computer (yes i know i can rent a GPU @ google) - since i want be able to do anything else while training my model.</p>
<p>So i have the following questions:</p>
<ul>
<li>Can i somehow stop and restart my models training? i read something about checkpoints, but their is so much outdated info on this topic - so i havent been able to figure it out.</li>
<li>Can i incrementally feed my model fx. 10% of my dataset, let it finish - and then next week feed it another 10% and so on? if so how?</li>
<li>Bonus question... is it better to aim for many epochs with a lower data set? or a larger dataset and more epochs? what is a good amount of epochs?</li>
</ul>
<p><strong>Packages:</strong></p>
<ul>
<li>Python, 3.7.9</li>
<li>Tensorflow-gpu 2.3.0</li>
<li>Tensorflow-estimator 2.3.0</li>
<li>Transformers 4.2.2</li>
<li>Tokenizers 0.9.4</li>
<li>cudatoolkit 10.1</li>
</ul>
<p><strong>Code - Tokenizer</strong></p>
<pre><code>from tokenizers.models import BPE
from tokenizers import Tokenizer
from tokenizers.decoders import ByteLevel as ByteLevelDecoder
from tokenizers.normalizers import NFKC, Sequence
from tokenizers.pre_tokenizers import ByteLevel
from tokenizers.trainers import BpeTrainer

class BPE_token(object):
def __init__(self):
    self.tokenizer = Tokenizer(BPE())
    self.tokenizer.normalizer = Sequence([
        NFKC()
    ])
    self.tokenizer.pre_tokenizer = ByteLevel()
    self.tokenizer.decoder = ByteLevelDecoder()

def bpe_train(self, paths):
    trainer = BpeTrainer(vocab_size=50000, show_progress=True, inital_alphabet=ByteLevel.alphabet(),         special_tokens=[
        &quot;&lt;s&gt;&quot;,
        &quot;&lt;pad&gt;&quot;,
        &quot;&lt;/s&gt;&quot;,
        &quot;&lt;unk&gt;&quot;,
        &quot;&lt;mask&gt;&quot;
    ])
    self.tokenizer.train(trainer, paths)

def save_tokenizer(self, location, prefix=None):
    if not os.path.exists(location):
        os.makedirs(location)
    self.tokenizer.model.save(location, prefix)

# ////////// TOKENIZE DATA ////////////
from pathlib import Pa th
import os# the folder 'text' contains all the files
paths = [str(x) for x in Path(&quot;./da_corpus/&quot;).glob(&quot;**/*.txt&quot;)]
tokenizer = BPE_token()# train the tokenizer model
tokenizer.bpe_train(paths)# saving the tokenized data in our specified folder
save_path = 'tokenized_data'
tokenizer.save_tokenizer(save_path)
</code></pre>
<p><strong>Code -- Model Trainer</strong></p>
<pre><code>save_path = 'tokenized_data'
tokenizer = GPT2Tokenizer.from_pretrained(save_path)
paths = [str(x) for x in Path(&quot;./da_corpus/&quot;).glob(&quot;**/*.txt&quot;)]
# tokenizer = Tokenizer.from_file(&quot;./tokenized_data/tokenizer-wiki.json&quot;)
tokenizer.add_special_tokens({
  &quot;eos_token&quot;: &quot;&lt;/s&gt;&quot;,
  &quot;bos_token&quot;: &quot;&lt;s&gt;&quot;,
  &quot;unk_token&quot;: &quot;&lt;unk&gt;&quot;,
  &quot;pad_token&quot;: &quot;&lt;pad&gt;&quot;,
  &quot;mask_token&quot;: &quot;&lt;mask&gt;&quot;
})# creating the configurations from which the model can be made
config = GPT2Config(
  vocab_size=tokenizer.vocab_size,
  bos_token_id=tokenizer.bos_token_id,
  eos_token_id=tokenizer.eos_token_id
)# creating the model
model = TFGPT2LMHeadModel(config)

single_string = ''
for filename in paths:
    with open(filename, &quot;r&quot;, encoding='utf-8') as f:
        x = f.read()
    single_string += x + tokenizer.eos_token
string_tokenized = tokenizer.encode(single_string)
# print(string_tokenized)



examples = []
block_size = 100
BATCH_SIZE = 12
BUFFER_SIZE = 2000
for i in range(0, len(string_tokenized) - block_size + 1, block_size):
    examples.append(string_tokenized[i:i + block_size])
    inputs, labels = [], []


for ex in examples:
    inputs.append(ex[:-1])
    labels.append(ex[1:])

dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))
dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

# defining our optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')# compiling the model
model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])
num_epoch = 20
history = model.fit(dataset, epochs=num_epoch)


output_dir = './model_bn_custom/'

if not os.path.exists(output_dir):
    os.mkdir(output_dir)


model_to_save = model.module if hasattr(model, 'module') else model
output_model_file = os.path.join(output_dir, WEIGHTS_NAME)
output_config_file = os.path.join(output_dir, CONFIG_NAME)

# save model and model configs
model.save_pretrained(output_dir)
model_to_save.config.to_json_file(output_config_file)

# save tokenizer
tokenizer.save_pretrained(output_dir)
</code></pre>
","2021-01-30 23:33:47","","","2021-01-30 23:33:47","<python><tensorflow><tensorflow2.0><huggingface-transformers><gpt-2>","0","0","3","227","","","","","","",""
"66669890","1","12349188","","GPT2Simple having issues running","<p>I am trying to run this GPT2Simple sample but I am getting errors</p>
<pre><code>Original stack trace for 'model/MatMul':
  File &quot;c:/Users/Jerome Ariola/Desktop/Machine Learning Projects/gpt test.py&quot;, line 32, in &lt;module&gt;
    steps=1)
  File &quot;C:\Program Files\Python36\lib\site-packages\gpt_2_simple\gpt_2.py&quot;, line 198, in finetune
    output = model.model(hparams=hparams, X=context, gpus=gpus)
  File &quot;C:\Program Files\Python36\lib\site-packages\gpt_2_simple\src\model.py&quot;, line 212, in model
    logits = tf.matmul(h_flat, wte, transpose_b=True)
  File &quot;C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\util\dispatch.py&quot;, line 180, in wrapper
    return target(*args, **kwargs)
  File &quot;C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\ops\math_ops.py&quot;, line 2754, in matmul
    a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
  File &quot;C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\ops\gen_math_ops.py&quot;, line 6136, in mat_mul
    name=name)
  File &quot;C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\framework\op_def_library.py&quot;, line 794, in _apply_op_helper
    op_def=op_def)
  File &quot;C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\util\deprecation.py&quot;, line 507, in new_func
    return func(*args, **kwargs)
  File &quot;C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\framework\ops.py&quot;, line 3357, in create_op
    attrs, op_def, compute_device)
  File &quot;C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\framework\ops.py&quot;, line 3426, in _create_op_internal
    op_def=op_def)
  File &quot;C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\framework\ops.py&quot;, line 1748, in __init__
    self._traceback = tf_stack.extract_stack()
</code></pre>
<p>This is the code, taken from <a href=""https://github.com/minimaxir/gpt-2-simple"" rel=""nofollow noreferrer"">https://github.com/minimaxir/gpt-2-simple</a></p>
<p>I also downgraded from Tensorflow 2.0 to Tensorflow 1.15 because there was an issue with <code>tf.contrib</code> or something</p>
<pre><code># https://github.com/minimaxir/gpt-2-simple

import gpt_2_simple as gpt2
import os
import requests

model_name = &quot;124M&quot;
if not os.path.isdir(os.path.join(&quot;models&quot;, model_name)):
    print(f&quot;Downloading {model_name} model...&quot;)
    gpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/

file_name = &quot;shakespeare.txt&quot;

if not os.path.isfile(file_name):
    url = &quot;https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt&quot;
    data = requests.get(url)
    
    with open(file_name, 'w') as f:
        f.write(data.text)


sess = gpt2.start_tf_sess()
gpt2.finetune(sess,
              file_name,
              model_name=model_name,
              steps=1)

gpt2.generate(sess)
</code></pre>
","2021-03-17 09:02:31","","2021-03-17 13:14:26","2021-03-18 15:25:40","<python><tensorflow><google-publisher-tag><gpt-2>","1","0","1","216","","","","","","",""
"67362300","1","6805178","","fill-mask usage from transformers pipeline","<p>I fine-tune a gpt2 language model and I am generation the text according to my model by using following lines of code:</p>
<p>generator = pipeline('text-generation', tokenizer='gpt2', model='data/out')
print(generator('Once upon a time', max_length=40)[0]['generated_text'])</p>
<p>Now I want to do the prediction of only next word with the probabilities. I know we can do it by using 'fill-mask' but I don't know how to do it. When I put 'fill-mask' inplace of 'text-generation', I am getting this error:</p>
<p>&quot;Unrecognized configuration class &lt;class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'&gt; for this kind of AutoModel: AutoModelForMaskedLM.
Model type should be one of BigBirdConfig, Wav2Vec2Config, ConvBertConfig, LayoutLMConfig, DistilBertConfig, AlbertConfig, BartConfig, MBartConfig, CamembertConfig, XLMRobertaConfig, LongformerConfig, RobertaConfig, SqueezeBertConfig, BertConfig, MobileBertConfig, FlaubertConfig, XLMConfig, ElectraConfig, ReformerConfig, FunnelConfig, MPNetConfig, TapasConfig, DebertaConfig, DebertaV2Config, IBertConfig.&quot;.</p>
<p>generator = pipeline('fill-mask', tokenizer='gpt2', model='data/out') // this line is giving me the above mentioned error.</p>
<p>Please let me know how can I fix this issue. Any kind of help would be greatly appreciated.
Thanks in advance.</p>
<p>The whole code for better understanding.</p>
<p>from transformers import (
GPT2Tokenizer,
DataCollatorForLanguageModeling,
TextDataset,
GPT2LMHeadModel,
TrainingArguments,
Trainer,
pipeline)</p>
<p>train_path = 'parsed_data.txt'
test_path = 'parsed_data.txt'</p>
<p>tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)</p>
<p>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)</p>
<p>train_dataset = TextDataset(tokenizer=tokenizer, file_path=train_path, block_size=128)
test_dataset = TextDataset(tokenizer=tokenizer, file_path=test_path, block_size=128)</p>
<p>model = GPT2LMHeadModel.from_pretrained('gpt2')</p>
<p>training_args = TrainingArguments(output_dir = 'data/out', overwrite_output_dir = True, per_device_train_batch_size = 32, per_device_eval_batch_size = 32, learning_rate = 5e-5, num_train_epochs = 3,)</p>
<p>trainer = Trainer(model = model, args = training_args, data_collator=data_collator, train_dataset = train_dataset, eval_dataset = test_dataset)</p>
<p>trainer.train()</p>
<p>trainer.save_model()
generator = pipeline('fill-mask', tokenizer='gpt2', model='data/out')</p>
","2021-05-03 00:36:31","","2021-05-03 01:40:58","2021-05-03 01:40:58","<nlp><pytorch><artificial-intelligence><language-model><gpt-2>","0","0","1","628","","","","","","",""
"70373541","1","16638949","","Should I adjust the weights of embedding of newly added tokens?","<p>I'm a beginner of neural language processing. Recenttly, I try to train a text generation model based on GPT-2 with huggingface transformers. I added some new tokens to the tokenizer and resize the embedding of the model with <code>model.resize_token_embeddings(len(tokenizer))</code>. Suppose I added 6 new tokens, should I add the weights of the 6 tokens to the optimizer? How should I do it? Thank you very much!</p>
","2021-12-16 03:34:42","","","2022-07-14 10:56:27","<huggingface-transformers><pre-trained-model><gpt-2>","1","0","3","637","","","","","","",""
"72925542","1","19518604","","When you prompt GPT3, what happens to the input data?","<p>For example, let's say I open up the playground and type &quot;Quack&quot;. What does the model do with those 5 characters to figure out what letters or words should come next?</p>
<p>(As it happens, GPT3 filled in that prompt with &quot;Quackery&quot;, then a tirade against cell therapy. Weird).</p>
","2022-07-10 01:00:42","","2023-01-21 05:22:24","2023-06-07 17:36:09","<nlp><artificial-intelligence><gpt-3>","2","0","-1","319","","","","","","",""
"73113552","1","12103619","","Open AI generate longer text with GPT-3","<p>I'm playing with the GPT-3 API of OPENAI but I struggle to find a way to make long enough generated text.</p>
<p>Here is my piece of code :</p>
<pre><code>import os
import openai

# export OPENAI_API_KEY='get_key_from_openai'

openai.api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)

response = openai.Completion.create(
  model=&quot;text-davinci-002&quot;,
  prompt=&quot;How to choose a student loan&quot;,
  temperature=0.6,
  max_tokens=512,
  top_p=1,
  frequency_penalty=1,
  presence_penalty=1,
  n= 10
)

print(response['choices'][0]['text'])
</code></pre>
<p>An example output I have is</p>
<p>&quot;There are a few things to consider when choosing a student loan, including the interest rate, repayment options, and whether the loan is federal or private. You should also compare loans to see which one will cost you the least amount of money in the long run&quot;</p>
<p>However, there are ~50 words which shouldn't be close to 80-100 tokens. I also thought that the <code>n</code> parameter was supposed to run <code>n</code> consecutive generated texts ?</p>
<p>Can someone explain how to make this generated text longer (ideally ~1000 tokens) ? Some huggingface models have a <code>min_tokens</code> parameter but I couldn't find it there.</p>
<p>Thanks a lot</p>
","2022-07-25 18:07:18","","2023-01-26 22:03:25","2023-01-26 22:03:25","<python><openai-api><gpt-3>","1","0","4","4698","","","","","","",""
"74869109","1","20490510","","When using OPT-2.7B or any other natural language model, is there a way to trick it into having a conversation/ give it a pre prompt in the code","<p>Using this code, or a variant of, is there anything that can be added to &quot;trick&quot; opt into conversing as another user in a style more similar to a chatbot. As of now it will either start something more similar to an article or have a conversation with itself for a few lines as seen below.</p>
<pre><code>    val = input(&quot;Enter your value: &quot;)
    input_ids = tokenizer((val), return_tensors='pt').input_ids

    output = model.generate(input_ids, min_length=1, max_length=1024, penalty_alpha=0.6, top_k=6)
    print(&quot;Output:\n&quot; + 100 * '-')
    print(tokenizer.decode(output[0], skip_special_tokens=True))
    print(&quot;&quot; + 100 * '-')
    with open(&quot;OutputText.txt&quot;, &quot;w&quot;) as f:
        f.write(tokenizer.decode(output[0], skip_special_tokens=True))


</code></pre>
<p>Here's an example of the current output:</p>
<p>*User Input:
Hello There.</p>
<p>Model Output:
Hello there. I have an egg that matches your TSV. Would you mind hatching it for me?
Sure, I'll add you now. Let me know when you're online.
Sorry for the late reply. I'm online now for the next few hours. Just send me a trade request whenever you're ready.
No probs, I'm in the middle of a battle at the moment, but I'll get you as soon as I'm done.
Thank you very much for the hatch. Have a nice day :D
*</p>
<p>I've attempted to add a prompt to the start and it hasn't made a difference.</p>
","2022-12-20 21:30:17","","","2023-03-20 06:31:21","<neural-network><huggingface-transformers><language-model><huggingface><gpt-2>","1","0","1","140","","","","","","",""
"74903974","1","1889865","","OpenAI ""We could not parse the JSON body of your request.""","<p>I'm trying to write a Chrome extension that takes the webpage text and send it to ChatGPT. Here's my JS code:</p>
<pre><code>document.addEventListener('DOMContentLoaded', function () {
var getBattleCardButton = document.getElementById('get-battle-card-button');
getBattleCardButton.addEventListener('click', function () {
    // Get all the content on the page as a string
    var pageContent = document.body.innerText;

    // Truncate the content to a maximum length of 4096 tokens
    var truncatedContent = pageContent.substring(0, 4096);

    // Construct the prompt by concatenating the specific words with the page content
    var prompt = &quot;Summarize this content into 3 sections, What? Why? and How?. Each section with 3 concise points. &quot; + truncatedContent;

    // Send the page content to Chat GPT as a prompt
    fetch('https://api.openai.com/v1/completions', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'Authorization': 'Bearer MY_API_KEY'
        },
        body: JSON.stringify({
            'prompt': prompt,
            'model': 'text-davinci-003',
            'max_tokens': 512
        })
    })
        .then(response =&gt; response.json())
        .then(data =&gt; {
            // Open a new tab and insert the Chat GPT output into it
            var newTab = window.open();
            newTab.document.body.innerHTML = data.response;
        })
        .catch(error =&gt; {
            console.error('Error:', error);
        });
});
</code></pre>
<p>});</p>
<p>I'm getting the error &quot;We could not parse the JSON body of your request.&quot; and the output is 'undefined'. What am I doing wrong?</p>
<p>PS: the HTML and CSS on the extension works just fine.</p>
","2022-12-23 21:02:51","","2023-01-11 18:12:22","2023-01-11 18:12:22","<javascript><google-chrome-extension><openai-api><gpt-3>","0","0","2","794","","","","","","",""
"74916280","1","7476541","","Error: That model does not exist (OpenAI)","<p>When using a model I fine-tuned for GPT-3 using <code>openai api</code> from CLI, it stopped working and I get an error with this message: &quot;That model does not exist&quot;.</p>
<p>But this is a model I have used before, so it should exist.</p>
","2022-12-25 22:55:23","","","2022-12-25 22:55:23","<openai-api><gpt-3>","1","0","-1","2653","","","","","","",""
"74978917","1","20908437","","""RuntimeError: Expected target size"" error for the nn.CrossEntropyLoss() function","<p>I am trying to train a GPT-2 model to take in a tokenized/padded input and predict the output. My batch size is 32. My max length is 343. I believe that the 768 comes from the model. I cannot get the loss function to work properly though. The training loop keeps throwing me errors like this:
<code>RuntimeError: Expected target size [32, 768], got [32, 343]</code></p>
<pre class=""lang-py prettyprint-override""><code># Create a TensorDataset from input_ids and output_ids
dataset = TensorDataset(input_tensors, output_tensors)

#Constants
batch_size = 32
num_epochs = 20
# Create a DataLoader from the dataset
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Set the device to run on
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

# Define the model architecture
model = transformers.GPT2Model.from_pretrained('gpt2').to(device)

# Define the loss function
loss_function = nn.CrossEntropyLoss(ignore_index=0, reduction='mean')

# Define the optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Set the model to training mode
model.train()
print(f&quot;input_tensors.shape before the loop: {input_tensors.shape}&quot;)
print(f&quot;output_tensors.shape before the loop: {output_tensors.shape}&quot;)

# Loop over the number of epochs
for epoch in range(num_epochs):
    # Initialize the epoch loss
    epoch_loss = 0
    
    # Loop over the data in the dataloader
    for input_tensors, output_tensors in dataloader:
        # Send the input and target tensors to the device
        input_tensors = input_tensors.to(device)
        output_tensors = output_tensors.type(torch.LongTensor)
        output_tensors = output_tensors.to(device)
        # Zero gradients
        optimizer.zero_grad()
        
        # Begin Forward pass
        logits = model(input_tensors)[0]
        
        print(f&quot;logits.shape: {logits.shape}&quot;)
        print(f&quot;input_tensors.shape: {input_tensors.shape}&quot;)
        print(f&quot;output_tensors.shape: {output_tensors.shape}&quot;)
        
        # Compute the loss
        loss = loss_function(logits, output_tensors)

        # Backward pass
        loss.backward()

        # Update the model parameters
        optimizer.step()

        # Add the loss to the epoch loss
        epoch_loss += loss.item()
        # Print the epoch loss
    print(f'Epoch {epoch+1}: Loss = {epoch_loss}')
</code></pre>
<p>And the sizes of the tensors:</p>
<ul>
<li><code>input_tensors.shape == torch.Size([2625, 343])</code> before the loop</li>
<li><code>output_tensors.shape == torch.Size([2625, 343])</code> before the loop</li>
<li><code>logits.shape == torch.Size([32, 343, 768])</code></li>
<li><code>input_tensors.shape == torch.Size([32, 343])</code></li>
<li><code>output_tensors.shape == torch.Size([32, 343])</code></li>
</ul>
<p>I have tried squeezing/unsqueezing and changing the shape of the logits/output_tensors shape. I think that's the right next step but I can't figure out what to change exactly.</p>
","2023-01-02 04:03:01","","2023-01-03 00:55:58","2023-01-03 00:55:58","<machine-learning><pytorch><tensor><cross-entropy><gpt-2>","0","1","3","79","","","","","","",""
"74986827","1","1974376","","OpenAISwift package works for ios not for Mac","<p>I have been following instructions to build a simple SwiftUI GPT-3 client using the OpenAISwift client library. The app works as expected on iOS but when I try to run a macos version I am getting these errors:</p>
<p>2023-01-02 15:07:14.845094-0500 GPT2[35955:1083936] [] networkd_settings_read_from_file Sandbox is preventing this process from reading networkd settings file at &quot;/Library/Preferences/com.apple.networkd.plist&quot;, please add an exception.
2023-01-02 15:07:14.845261-0500 GPT2[35955:1083936] [] networkd_settings_read_from_file Sandbox is preventing this process from reading networkd settings file at &quot;/Library/Preferences/com.apple.networkd.plist&quot;, please add an exception.
2023-01-02 15:07:15.078105-0500 GPT2[35955:1086396] [] nw_resolver_can_use_dns_xpc_block_invoke Sandbox does not allow access to com.apple.dnssd.service</p>
<p>I found another macos OpenAIKit project on gitub stating that the following need to be added to info.plist for macos:</p>
<pre><code>&lt;plist version=&quot;1.0&quot;&gt;
&lt;dict&gt;
    &lt;key&gt;com.apple.security.app-sandbox&lt;/key&gt;
    &lt;true/&gt;
    &lt;key&gt;com.apple.security.files.user-selected.read-only&lt;/key&gt;
    &lt;true/&gt;
    &lt;key&gt;com.apple.security.network.client&lt;/key&gt;
    &lt;true/&gt;
    &lt;key&gt;com.apple.security.network.server&lt;/key&gt;
    &lt;true/&gt;
&lt;/dict&gt;
&lt;/plist&gt;
</code></pre>
<p>but I did not see these choices available in the XCode 14 project properties info section. I would have tried pasting the dict object in to a text version of the info.plist but I could not see a way to edit the info.plist as a text.</p>
<p>Here is the simple code I am using:</p>
<pre><code>import SwiftUI
import OpenAISwift

final class ViewModel: ObservableObject {
    init() {}
    
    private var client: OpenAISwift?
    
    func setup() {
        client = OpenAISwift(authToken: &quot;MYKEYHERE&quot;)
       
        
    }
    
    func send(text: String,
        completion: @escaping (String) -&gt; Void) {
            client?.sendCompletion(with: text,
                           maxTokens: 500,
                           completionHandler: {result in
        
        switch result {
        case .success(let model):
            let output = model.choices.first?.text ?? &quot;&quot;
            completion(output)
        case .failure:
            break
        }
    })
}
}

struct ContentView: View {
    @ObservedObject var viewModel = ViewModel()
    @State var text = &quot;&quot;
  @State var models = [String]()
    
    var body: some View {
        VStack(alignment: .leading) {
            ForEach(models, id: \.self) { string in
                Text(string)
            }
            
            Spacer()
            
            HStack {
                TextField(&quot;Type here ...&quot;, text: $text)
                Button(&quot;Send&quot;) {
                    send()
                }
            }
        }
        .onAppear{
            viewModel.setup()
        }.padding()
        
    }
    
    func send() {
        guard !text.trimmingCharacters(in: .whitespaces).isEmpty else {
            return
        }
        models.append(&quot;Me: \(text)&quot;)
        viewModel.send(text: text) { response in
            DispatchQueue.main.async {
                self.models.append(&quot;GPT: &quot; + response)
                self.text = &quot;&quot;
            }
            
        }
    }
}

struct ContentView_Previews: PreviewProvider {
    static var previews: some View {
        ContentView()
    }
}
</code></pre>
<p>How can I get this multiplatform app running on macos Ventura 13.1? Thanks for any help.</p>
","2023-01-02 20:36:06","","2023-01-29 13:44:20","2023-01-29 13:44:20","<macos><swiftui><appstore-sandbox><entitlements><gpt-3>","2","0","0","260","","","","","","",""
"75046073","1","17275588","","Python + Open AI/GPT3 question: Why is part of my prompt spilling into the responses I receive?","<p>This happens to probably 10% of responses I get. For whatever reason, the last bits of my prompt somehow spill into it, at the start of it. Like there will be a period, or a question mark, or sometimes a few of the last letters from the prompt, that get removed from the prompt, and somehow find their way into BOTH the response that gets printed inside of the Visual Studio Code terminal, AND in the outputted version that gets written to a corresponding Excel spreadsheet.</p>
<p>Any reason why this might happen?</p>
<p>Some example responses:</p>
<blockquote>
<p>.</p>
<p>Most apples are colored red.</p>
</blockquote>
<p>Also</p>
<blockquote>
<p>?</p>
<p>Most rocks are colored gray.</p>
</blockquote>
<p>Another example:</p>
<blockquote>
<p>for it.</p>
<p>Most oceans are colored blue.</p>
</blockquote>
<p>The period, the question mark, &quot; for it&quot; somehow get transposed FROM the end of the prompt, and tacked onto the response. And they even get removed from the prompt that was originally in the Excel spreadsheet to begin with.</p>
<p>Could this be a bug with xlsxwriter? open ai? Some combo of both?</p>
<p>Code here:</p>
<pre><code>import xlsxwriter
import openpyxl

import os
import openai

filename = f'testing-openai-gpt3-requests-v1.xlsx'
wb = openpyxl.load_workbook(filename, read_only=False)
sheet = wb.active

# print(&quot;starting number of ideas is:&quot;)
# print(sheet.max_row)

for x in range(sheet.max_row):
    c = sheet.cell(row = x+1, column = 1)
    # print(c.value) 

    myCurrentText = c.value 
    myCurrentPrompt = &quot;What is the color of most of the following objects: &quot; + myCurrentBusinessIdea

    openai.api_key = [none of your business]

    response = openai.Completion.create(
    model = &quot;text-davinci-003&quot;,
    prompt = myCurrentPrompt,
    max_tokens = 1000,
    )

    TheOutputtedSummary = response['choices'][0]['text']

    print(TheOutputtedSummary)
    sheet.cell(row = x+1, column = 6).value = TheOutputtedSummary


wb.save(str(filename))
print('All finished!')
</code></pre>
","2023-01-08 07:16:08","","","2023-01-09 16:00:53","<python><machine-learning><artificial-intelligence><gpt-3>","1","0","0","396","","","","","","",""
"75136962","1","12342925","","OpenAI GPT-3 API error: ""TypeError: Converting circular structure to JSON"" using ExpressJS","<p>Just experimenting with OpenAI's api and have a very basic express app up and running. What I'm trying to do is just get it to send me back an appropriate response with a basic input but it currently keeps failing.</p>
<p>I'm using Postman to iterate on the code on localhost. All packages are definitely installed and the API key is correct and specfied in the .env file.</p>
<p>My current working file is below. I'm sure I'll kick myself but can anyone spot what dumb thing I've probably done?</p>
<pre><code>const express = require('express');
const app = express();
require('dotenv').config();
const bodyParser = require('body-parser');
app.use(bodyParser.json());
const axios = require('axios'); // Come back to this

const { Configuration, OpenAIApi } = require(&quot;openai&quot;);
const configuration = new Configuration({
    apiKey: process.env.OPENAI_API_KEY,
});

const openai = new OpenAIApi(configuration);

app.get('/api/v1', async (req, res) =&gt; {
    
  let body = {
      model: &quot;text-davinci-003&quot;,        
      prompt: &quot;How are you?&quot;,
      temperature: 1,
      max_tokens: 2086,
      top_p: 1,
      frequency_penalty: 0,
      presence_penalty: 0,
  };

  
  const response = await openai.createCompletion(body);

  res.send({ response });
});

// Listen for requests
app.listen(3000, function() {
    console.log('Server is listening on port 3000');
});
</code></pre>
<p><strong>Error generated in terminal</strong></p>
<pre><code>/home/mint-pc/Desktop/projects/ebooks/api/node_modules/express/lib/response.js:1150
    : JSON.stringify(value);
           ^

TypeError: Converting circular structure to JSON
    --&gt; starting at object with constructor 'ClientRequest'
    |     property 'socket' -&gt; object with constructor 'TLSSocket'
    --- property '_httpMessage' closes the circle
    at JSON.stringify (&lt;anonymous&gt;)
    at stringify (/home/mint-pc/Desktop/projects/ebooks/api/node_modules/express/lib/response.js:1150:12)
    at ServerResponse.json (/home/mint-pc/Desktop/projects/ebooks/api/node_modules/express/lib/response.js:271:14)
    at ServerResponse.send (/home/mint-pc/Desktop/projects/ebooks/api/node_modules/express/lib/response.js:162:21)
    at /home/mint-pc/Desktop/projects/ebooks/api/ghost_writer.js:48:7
</code></pre>
","2023-01-16 16:20:20","","2023-03-13 13:48:19","2023-03-20 07:44:11","<node.js><express><openai-api><gpt-3>","2","2","1","880","","","","","","",""
"75192212","1","18029046","","Template for RLHF with the TRL library","<p>I'm trying to implement a very very basic working template for RLHF with TRL. The notebook is here:</p>
<p><a href=""https://www.kaggle.com/code/mcantoni81/rlhf-with-trl-gpt2"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/mcantoni81/rlhf-with-trl-gpt2</a></p>
<p>My target here is to make gpt2 answer &quot;i'm the mailman&quot;, but maybe i'm not getting right the mechanics of TRL. Looks like the training doesn't influence the model at all.</p>
<p>How can i correct this template?</p>
<p>I've expected the queries of the model to somehow change.</p>
","2023-01-21 09:06:58","","","2023-01-21 09:06:58","<pytorch><huggingface-transformers><kaggle><huggingface><gpt-2>","0","0","1","145","","","","","","",""
"75285557","1","4932296","","Removing tokens from the GPT tokenizer","<p>How can I remove unwanted sub-tokens from GPT vocabulary or tokenizer? I have tried an existing approach that was used for a ROBERTa kind of model as shown below (<a href=""https://github.com/huggingface/transformers/issues/15032"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/15032</a>). However it fails at the point of initializing the &quot;model&quot; component of the backend_tokenizer with the new vocabulary.</p>
<pre><code>#1. Get your tokenizer and the list of tokens you want to remove

import json
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;gpt2&quot;)

# get all tokens with &quot;unused&quot; in target_tokenizer
unwanted_words = [ 'ply', 'Ġmor','Ġprovide','IC','ung','Ġparty', 'Ġexist', 'Ġmag',]


#2. Get the arguments that allowed to initialize the &quot;model&quot; component of the backend_tokenizer.
model_state = json.loads(tokenizer.backend_tokenizer.model.__getstate__())
print(len(model_state[&quot;vocab&quot;]))


#3. Modify the initialization arguments, in particular the vocabulary to remove the tokens we don't want

# remove all unwanted tokens from the vocabulary
for word in unwanted_words:
    del model_state[&quot;vocab&quot;][word]

print(len(model_state[&quot;vocab&quot;]))


#4. Intitialize again the &quot;model&quot; component of the backend_tokenizer with the new vocabulary

from tokenizers import models

model_class = getattr(models, model_state.pop(&quot;type&quot;))

tokenizer.backend_tokenizer.model = model_class(**model_state)

print(len(tokenizer.vocab))

</code></pre>
<p>And below is the error:</p>
<pre><code>---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

&lt;ipython-input-21-fa908d23c419&gt; in &lt;module&gt;
     30 model_class = getattr(models, model_state.pop(&quot;type&quot;))
     31 
---&gt; 32 tokenizer.backend_tokenizer.model = model_class(**model_state)
     33 
     34 print(len(tokenizer.vocab))

TypeError: argument 'merges': failed to extract enum PyMerges ('Merges | Filename')
- variant Merges (Merges): TypeError: failed to extract field PyMerges::Merges.0, caused by TypeError: 'str' object cannot be converted to 'PyTuple'
- variant Filename (Filename): TypeError: failed to extract field PyMerges::Filename.0, caused by TypeError: 'list' object cannot be converted to 'PyString'


</code></pre>
<p>What other methods can I use or refer to? The original script I adapter was used for ROBERTa which uses Sentencepiece but GPT uses BPE.</p>
","2023-01-30 14:05:41","","","2023-01-30 14:05:41","<python-3.x><nlp><huggingface-transformers><huggingface-tokenizers><gpt-2>","0","0","0","172","","","","","","",""
"75355374","1","21153622","","Python Telegram bot chat gpt","<pre><code>import telebot
import requests
import time

TELEGRAM_TOKEN = &quot;my-token&quot;

bot = telebot.TeleBot(TELEGRAM_TOKEN)

@bot.message_handler(commands=['search'])

def handle_search(message):

    # extract the search word from the message text
    search_word = message.text.split(&quot; &quot;, 1)[1]
    headers = {
        'Authorization': 'Bearer sk-token',
    }
   
    # prepare the request payload
    json_data = {

        'model': 'text-davinci-003',
        'prompt': f'{search_word}',
        'temperature': 0.8,
        'max_tokens': 2000,
    }

    # send the request to OpenAI API

    response = requests.post('https://api.openai.com/v1/completions', headers=headers, json=json_data).json()
    
    # extract the response text

    response_text = response['choices'][0]['text']

    
    # send the response text as a message
    bot.send_message(message.chat.id, response_text, reply_to_message_id=message.message_id)

def run():

    while True:
        try:
            bot.polling(none_stop=True)
        except Exception as e:
            # log the error
            print(f&quot;Error occurred: {e}&quot;)

            # wait for 5 seconds before polling again
            time.sleep(5)

if __name__ == '__main__':
    run()
</code></pre>
<p>When I turn it on, it appears like this</p>
<blockquote>
<p>File &quot;main.py&quot;, line 6, in  @bot.message_handler(commands=['search']) AttributeError: 'TeleBot' object has no attribute 'message_handler'</p>
</blockquote>
","2023-02-05 20:43:21","","2023-02-06 00:25:04","2023-02-21 11:40:42","<python><telegram><openai-api><telebot><gpt-3>","0","1","0","830","","","","","","",""
"71399624","1","135043","","Memory usage in transforming fine tuning of GPTJ-6b to HuggingFace format","<p>Following this tutorial using TPUs to fine tune GPTJ has worked well.
<a href=""https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md"" rel=""nofollow noreferrer"">https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md</a></p>
<p>Why would the step to transform to huggingface format using to_hf_weights.py have an issue with memory at 256MB - even after slimming has been applied?</p>
<p>The issue I filed is here:
<a href=""https://github.com/kingoflolz/mesh-transformer-jax/issues/209"" rel=""nofollow noreferrer"">https://github.com/kingoflolz/mesh-transformer-jax/issues/209</a></p>
","2022-03-08 18:06:05","","2022-12-13 15:41:00","2022-12-13 15:41:00","<tpu><jax><gpt-3>","1","0","0","384","","","","","","",""
"60097717","1","9344014","","GPT-2 Continue training from checkpoint","<p>I am trying to continue training from a saved checkpoint using the colab setup for GPT-2-simple at:</p>

<p><a href=""https://colab.research.google.com/drive/1SvQne5O_7hSdmPvUXl5UzPeG5A6csvRA#scrollTo=aeXshJM-Cuaf"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1SvQne5O_7hSdmPvUXl5UzPeG5A6csvRA#scrollTo=aeXshJM-Cuaf</a></p>

<p>But I just cant get it to work. Loading the saved checkpoint from my googledrive works fine, and I can use it to generate text, but I cant continue training from that checkpoint. In the <code>gpt2.finetune ()</code> I am entering <code>restore.from='latest""</code> and <code>overwrite=True</code>, and I have been trying to use both same run_name and different one, and using <code>overwrite=True</code>, and not. I have also tried restarting the runtime in between, as was suggested, but it doesn´t help, I keep getting the following error:</p>

<pre><code>""ValueError: Variable model/wpe already exists, disallowed. Did you mean to set reuse=True 
or reuse=tf.AUTO_REUSE in VarScope?""
</code></pre>

<p>I asume that I need to run the <code>gpt2.load_gpt2(sess, run_name='myRun')</code> before continue training, but whenever I have run this first, the <code>gtp2.finetune()</code> throws this error</p>
","2020-02-06 14:51:59","","2020-11-29 11:58:00","2021-04-13 11:19:13","<python><tensorflow><nlp><google-colaboratory><gpt-2>","2","0","2","4165","0","","","","","",""
"73335404","1","13297517","","NAN values appears when including a new padding token in my tokenizer","<p>I'm trying to fine-tune a DialoGPT model on a new dataset. I already processed my data correctly and adding a new padding token in the tokenizer didn't seem to make any issue :</p>
<pre class=""lang-py prettyprint-override""><code>#my dataset : 
print(dataset)
print(dataset[0]['text'])
</code></pre>
<blockquote>
<h3>output</h3>
<p>Dataset({
features: ['text'],
num_rows: 48423
})</p>
<p>[speaker 1]: Great that you wish to hear the voices of the guitarists. Here are your booking details of the tickets. You wish to purchase 4 tickets for the event The Original Wailers that is going to take place on March 8th in Berkeley, right?
[speaker 2]: Yup, you're right. Please May I know where is the event conducted and I need the complete address?
[speaker 1]: Please note down the complete address of the event happening. It's at Cornerstone Craft Beer &amp; Live Music, 2367 Shattuck Avenue. Your reservation is successful and have a great time there!
[speaker 2]: Thanks much for the information you've given. Please can you help me to find some intermediate priced restaurant that provides Ethiopian kind of food.
[speaker 1]: Yup! There is an Ethiopian Restaurant named Addis Restaurant providing excellent and authentic traditional Ethiopian cuisine located in Berkeley. Do you wish to reserve a table here?
[speaker 2]:</p>
</blockquote>
<pre class=""lang-py prettyprint-override""><code>#tokenizing and adding labels
tokenizer.add_special_tokens({'pad_token': '[PAD]'})
def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;],  padding='max_length', add_special_tokens =True, max_length=246) #truncation=True, max_length=13)

tokenized_datasets = ds.map(
    tokenize_function, batched=True, num_proc=4, remove_columns=[&quot;text&quot;]
)

tokenized_datasets = tokenized_datasets.add_column(&quot;labels&quot;, tokenized_datasets[:]['input_ids']) 

train_set = model.prepare_tf_dataset(
    tokenized_datasets,
    shuffle=True,
    batch_size=1,
)
sample = train_set.as_numpy_iterator()
sample = sample.next()

print(tokenized_datasets)
print(train_set)
print(sample)
</code></pre>
<blockquote>
<h3>output</h3>
<p>Dataset({
features: ['input_ids', 'attention_mask', 'labels'],
num_rows: 48423
})</p>
<p>&lt;PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(1, 246), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(1, 246), dtype=tf.int64, name=None)}, TensorSpec(shape=(1, 246), dtype=tf.int64, name=None))&gt;</p>
<p>({'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0]]),
'input_ids': array([[   58,  4125,  3110,   352,  5974,   314,   765,   284,   711,
440,  9190,   440, 14918,   440,  3825,   319,   616,  3359,
13,   198,    58,  4125,  3110,   362,  5974,   921,   765,
284,  3350,   262,  3496,   440,  9190,   440, 14918,   440,
3825,  4291,   262,  3195,    11,   826,    30,   198,    58,
4125,  3110,   352,  5974,  1320,   318,   826,    13,  1867,
2099,   286,  3496,   318,   340,    30,   198,    58,  4125,
3110,   362,  5974,   632,   318,  5610,   739,   262, 12136,
6536,   290,   534,  3496,   468,  2067,    13,   198,    58,
4125,  3110,   352,  5974, 20558,   617,  1637,   329,   502,
13,   198,    58,  4125,  3110,   362,  5974,   220, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257]])},
array([[   58,  4125,  3110,   352,  5974,   314,   765,   284,   711,
440,  9190,   440, 14918,   440,  3825,   319,   616,  3359,
13,   198,    58,  4125,  3110,   362,  5974,   921,   765,
284,  3350,   262,  3496,   440,  9190,   440, 14918,   440,
3825,  4291,   262,  3195,    11,   826,    30,   198,    58,
4125,  3110,   352,  5974,  1320,   318,   826,    13,  1867,
2099,   286,  3496,   318,   340,    30,   198,    58,  4125,
3110,   362,  5974,   632,   318,  5610,   739,   262, 12136,
6536,   290,   534,  3496,   468,  2067,    13,   198,    58,
4125,  3110,   352,  5974, 20558,   617,  1637,   329,   502,
13,   198,    58,  4125,  3110,   362,  5974,   220, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257]]))</p>
</blockquote>
<p>The ouputs so far seem pretty clean for me. But when I try to make a prediction with my model or train it I have nan values as output :</p>
<pre class=""lang-py prettyprint-override""><code>#Instatiation of model 
from transformers import TFAutoModelForCausalLM
model = TFAutoModelForCausalLM.from_pretrained(&quot;microsoft/DialoGPT-medium&quot;)

optimizer = AdamWeightDecay(learning_rate=1e-9, weight_decay_rate=0.01)
model.compile(optimizer=optimizer, jit_compile=True)

</code></pre>
<pre class=""lang-py prettyprint-override""><code>#model inference
loss = model(sample[0], labels=sample[1])
print(loss)
</code></pre>
<blockquote>
<h3>output</h3>
<p>TFCausalLMOutputWithCrossAttentions([('loss',
&lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)&gt;),
('logits',
&lt;tf.Tensor: shape=(1, 246, 50258), dtype=float32, numpy=
array([[[nan, nan, nan, ..., nan, nan, nan],
[nan, nan, nan, ..., nan, nan, nan],
[nan, nan, nan, ..., nan, nan, nan],
...,
[nan, nan, nan, ..., nan, nan, nan],
[nan, nan, nan, ..., nan, nan, nan],
[nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)&gt;),
('past_key_values',
(&lt;tf.Tensor: shape=(2, 1, 16, 246, 64), dtype=float32, numpy=
array([[[[[nan, nan, nan, ..., nan, nan, nan],
[nan, nan, nan, ..., nan, nan, nan],
[nan, nan, nan, ..., nan, nan, nan],
...,
[nan, nan, nan, ..., nan, nan, nan],
[nan, nan, nan, ..., nan, nan, nan],
[nan, nan, nan, ..., nan, nan, nan]],</p>
<pre><code>                                            [[nan, nan, nan, ..., nan, nan, nan],
                                             [nan, nan, nan, ..., nan, nan, nan],
                                             [nan, nan, nan, ..., nan, nan, nan],
                                             ...,
                                             [nan, nan, nan, ..., nan, nan, nan],
                                             [nan, nan, nan, ..., nan, nan, nan],
                                             [nan, nan, nan, ..., nan, nan, nan]],
                                             .............
</code></pre>
</blockquote>
<pre class=""lang-py prettyprint-override""><code>#model training
model.fit(train_set, epochs=1)
</code></pre>
<blockquote>
<h3>output</h3>
<p>56/48423 [..............................] - ETA: 2:27:49 - loss: nan</p>
</blockquote>
<p>This NAN value is certainly caused by the new token '[PAD]' added but I don't know how to deal with it.
Can someone help me please ?</p>
","2022-08-12 14:05:34","","2022-08-12 14:10:29","2022-08-12 14:10:29","<python><deep-learning><huggingface-transformers><language-model><gpt-2>","0","0","1","120","","","","","","",""
"74503607","1","310370","","Text generation AI models generating repeated/duplicate text/sentences. What am I doing incorrectly? Hugging face models - Meta GALACTICA","<p>Whole day I have worked with available text generation models</p>
<p>Here you can find list of them : <a href=""https://huggingface.co/models?pipeline_tag=text-generation&amp;sort=downloads"" rel=""nofollow noreferrer"">https://huggingface.co/models?pipeline_tag=text-generation&amp;sort=downloads</a></p>
<p>I want to generate longer text outputs, however, with multiple different models, all I get is repetition.</p>
<p>What am I missing or doing incorrectly?</p>
<p>I will list several of them</p>
<p>Freshly released meta GALACTICA - <a href=""https://huggingface.co/facebook/galactica-1.3b"" rel=""nofollow noreferrer"">https://huggingface.co/facebook/galactica-1.3b</a></p>
<p>The code example</p>
<pre><code>from transformers import AutoTokenizer, OPTForCausalLM

tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/galactica-1.3b&quot;)
model = OPTForCausalLM.from_pretrained(&quot;facebook/galactica-1.3b&quot;, device_map=&quot;auto&quot;)

 
input_text = &quot;The benefits of deadlifting\n\n&quot;
input_ids = tokenizer(input_text, return_tensors=&quot;pt&quot;).input_ids.to(&quot;cuda&quot;)

outputs = model.generate(input_ids,new_doc=False,top_p=0.7, max_length=1000)
print(tokenizer.decode(outputs[0]))
</code></pre>
<p>The generated output</p>
<p><a href=""https://i.stack.imgur.com/zV7qg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zV7qg.png"" alt=""enter image description here"" /></a></p>
<p>Facebook opt - <a href=""https://huggingface.co/facebook/opt-350m"" rel=""nofollow noreferrer"">https://huggingface.co/facebook/opt-350m</a></p>
<p>The tested code</p>
<pre><code>from transformers import GPT2Tokenizer, OPTForCausalLM

model = OPTForCausalLM.from_pretrained(&quot;facebook/opt-350m&quot;)
tokenizer = GPT2Tokenizer.from_pretrained(&quot;facebook/opt-350m&quot;)

prompt = &quot;The benefits of deadlifting can be listed as below:&quot;
inputs = tokenizer(prompt, return_tensors=&quot;pt&quot;)

# Generate
generate_ids = model.generate(inputs.input_ids, max_length=800)
tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
</code></pre>
<p>The generated output</p>
<p><a href=""https://i.stack.imgur.com/zv2j9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zv2j9.png"" alt=""enter image description here"" /></a></p>
","2022-11-19 20:48:50","","2023-01-29 06:21:42","2023-02-26 15:13:30","<python><nlp><huggingface-transformers><huggingface><gpt-2>","1","0","3","480","","","","","","",""
"75362603","1","15435637","","Is GPT-3 a model or a framework?","<p>We all hear GPT-3 being called a large language model (LLM), but is it really more of a framework since you can use GPT-3 with your own dataset, to train your own version of a GPT-3 model?</p>
<p>My understanding is that a model is the result of training, and you can use one of many frameworks/libraries to train the model (ex: tensor flow).  If GPT-3 was just a model, you wouldn't be able to train with your own data on it, right?  So that makes GPT-3 a framework?</p>
<p>Can anyone help me to better understand the AI terminology for this?</p>
","2023-02-06 14:26:30","","2023-03-02 05:33:20","2023-03-02 05:33:20","<machine-learning><nlp><artificial-intelligence><gpt-3>","1","1","-1","164","","","","","","",""
"75540828","1","21270404","","Chat bot not sending message to API","<p>I asked chat gpt to write a chat bot code modeled on gpt3, and he actually wrote it.</p>
<p>The site was created, but messages could not be sent.</p>
<p>I also got a gpt3 api key and used it, but it doesn't seem to work well. What's the problem?</p>
<p>Below is the code written by gpt</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
  &lt;meta charset=""utf-8""&gt;
  &lt;title&gt;ChatGPT Demo&lt;/title&gt;
  &lt;link rel=""stylesheet"" href=""style.css""&gt;
&lt;/head&gt;
&lt;body&gt;
  &lt;div class=""chat-window""&gt;
    &lt;div class=""chat-header""&gt;
      &lt;h1&gt;ChatGPT&lt;/h1&gt;
    &lt;/div&gt;
    &lt;div class=""chat-body""&gt;
      &lt;ul class=""message-list""&gt;
        &lt;li class=""message bot""&gt;
          &lt;p&gt;Hello! How can I help you today?&lt;/p&gt;
        &lt;/li&gt;
      &lt;/ul&gt;
    &lt;/div&gt;
    &lt;div class=""chat-footer""&gt;
      &lt;input id=""input"" type=""text"" placeholder=""Type your message here...""&gt;
      &lt;button onclick=""send()""&gt;Send&lt;/button&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;script src=""https://cdn.jsdelivr.net/npm/@openai/api""&gt;&lt;/script&gt;
  &lt;script&gt;
    // OpenAI API Key 설정
    const openai = window.openai;
    const api_key = 'YOUR_API_KEY';
    const model_engine = 'davinci';

    // API 호출하여 응답 받기
    const askGPT3 = async (input) =&gt; {
      console.log(input); // This is getting logged but below API is not being called.
      const response = await openai.Completion.create({
        engine: model_engine,
        prompt: input,
        max_tokens: 1024,
        n: 1,
        stop: null,
        temperature: 0.5,
        apiKey: api_key
      });
      return response.choices[0].text.trim();
    };

    // 대화 시작
    const startConversation = async () =&gt; {
      const botMessage = document.querySelector('.message.bot p');
      const answer = await askGPT3('Hello!');
      botMessage.innerHTML = answer;
    };
    startConversation();

    // 대화 전송
    const send = async () =&gt; {
      const input = document.getElementById('input').value;
      const messageList = document.querySelector('.message-list');
      const userMessage = `&lt;li class=""message user""&gt;&lt;p&gt;${input}&lt;/p&gt;&lt;/li&gt;`;
      messageList.insertAdjacentHTML('beforeend', userMessage);
      const botMessage = `&lt;li class=""message bot""&gt;&lt;p&gt;${await askGPT3(input)}&lt;/p&gt;&lt;/li&gt;`;
      messageList.insertAdjacentHTML('beforeend', botMessage);
      document.getElementById('input').value = '';
    };

    // 대화 엔터키 전송
    const input = document.getElementById('input');
    input.addEventListener('keyup', (event) =&gt; {
      if (event.keyCode === 13) {
        event.preventDefault();
        document.querySelector('button').click();
      }
    });
  &lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>
</div>
</div>
</p>
<p>I clicked send to send a message but it doesn't work - the message is not sent to the API.</p>
","2023-02-23 05:18:36","","2023-02-23 05:46:54","2023-05-08 13:23:33","<javascript><html><openai-api><gpt-3>","1","6","0","206","","","","","","",""
"75549286","1","8494468","","GPT Index: Issue using ComposableGraph with Vector Stores","<p>I am having issue with implementing Vector Stores with composability</p>
<pre><code>from llama_index.composability import ComposableGraph
index1 = GPTQdrantIndex(doc1, client=qdrant_client,collection_name=&quot;index1&quot;)
index1.set_text(&quot;S3document1&quot;)
index2 = GPTQdrantIndex(doc2, client=qdrant_client,collection_name=&quot;index2&quot;)
index2.set_text(&quot;S3document2&quot;)
# save index to disk
index1.save_to_disk('index_Qdrant1.json')
index2.save_to_disk('index_Qdrant2.json')

list_index2 = GPTListIndex([index1, index2]);

graph = ComposableGraph.build_from_index(list_index2)
graph.save_to_disk(&quot;save_path2.json&quot;)
graph = ComposableGraph.load_from_disk(&quot;save_path2.json&quot;)

query_configs = [
    {
        &quot;index_struct_type&quot;: &quot;qdrant&quot;,
        &quot;query_mode&quot;: &quot;default&quot;   
    },
    {
        &quot;index_struct_type&quot;: &quot;keyword_table&quot;,
        &quot;query_mode&quot;: &quot;simple&quot;,
        &quot;query_kwargs&quot;: {}
    },
]

response = graph.query(&quot;Who is this&quot;, query_configs=query_configs);
print(response)
</code></pre>
<p>Error
<a href=""https://i.stack.imgur.com/l3rh9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/l3rh9.png"" alt=""enter image description here"" /></a></p>
<p>I have tried ComposableGraph with other Vector Stores as well but didn't worked</p>
","2023-02-23 19:06:18","","","2023-02-23 19:06:18","<python><openai-api><gpt-3>","0","0","0","673","","","","","","",""
"75722268","1","1031215","","Fine-tuning of OpeanAI model with unsupervised set, not supervised","<p>I want GPT-3 model to know everything about my domain area, for example my inbox. I want to be able to ask it questions like &quot;Have I even had a Silicon Valley Bank account?&quot; and get correct response. I've familiarized myself with <a href=""https://platform.openai.com/docs/guides/fine-tuning"" rel=""nofollow noreferrer"">fine-tuning mechanism</a> in official OpenAI docs and it's not exactly what I'm looking for. I want to just dump all my emails on the model and ask it: &quot;Learn!&quot;. However fine-tuning require supervised style learning with prompts and reponses, which I do not have. <a href=""https://platform.openai.com/docs/guides/fine-tuning/example-notebooks"" rel=""nofollow noreferrer"">Example</a> in the notebooks for doc suggests that you can use &quot;Davinci-instruct to ask a few questions based on a Wikipedia section, as well as answer those questions, based on that section&quot;, which I guess solves my problem if I apply it to all my emails, but I'd rather not do this step, because I might screw up something. Can I have other options?</p>
<p>I found that Azure Open AI integration <a href=""https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/fine-tuning?pivots=programming-language-studio"" rel=""nofollow noreferrer"">allows you to do fine-tuning</a> as well, but it seems to have the same problem.</p>
<p>I might be calling what I want to do is fine-tuning, but in fact I keep pre-training process and just decided to go with fine-tuning because it has documentation and API. On the other hand fine-tuning guaranties that I would get wrong answers, pre-training doesn't, and you dont want to get wrong answer on question &quot;Have I even had a Silicon Valley Bank account?&quot;</p>
","2023-03-13 13:06:42","","","2023-03-27 18:17:41","<openai-api><pre-trained-model><gpt-3><fine-tune>","1","0","0","304","","","","","","",""
"75723546","1","8391698","","How to resolve ""the size of tensor a (1024) must match the size of tensor b"" in happytransformer","<p>I have the following code. This code uses the GPT-2 language model from the Transformers library to generate text from a given input text. The input text is split into smaller chunks of 1024 tokens, and then the GPT-2 model is used to generate text for each chunk. The generated text is concatenated to produce the final output text. The <a href=""https://happytransformer.com"" rel=""nofollow noreferrer"">HappyTransformer</a> library is used to simplify the generation process by providing a pre-trained model and an interface to generate text with a given prefix and some settings. The GPT-2 model and tokenizer are also saved to a local directory. The output of the code is the generated text for the input text, with corrections for grammar suggested by the prefix &quot;grammar: &quot;.</p>
<pre><code>from transformers import GPT2Tokenizer, GPT2LMHeadModel
from happytransformer import HappyGeneration, GENSettings
import torch

model_name = &quot;gpt2&quot;
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

save_path = &quot;/home/ubuntu/storage1/various_transformer_models/gpt2&quot;
# save the tokenizer and model to a local directory
tokenizer.save_pretrained(save_path)
model.save_pretrained(save_path)

# Processing
happy_gen = HappyGeneration(&quot;GPT-2&quot;, &quot;gpt2&quot;)
args = GENSettings(num_beams=5, max_length=1024)

mytext = &quot;This sentence has bad grammar. This is a very long sentence that exceeds the maximum length of 512 tokens. Therefore, we need to split it into smaller chunks and process each chunk separately.&quot;
prefix = &quot;grammar: &quot;

# Split the text into chunks of maximum length 1024 tokens
max_length = 1024
chunks = [mytext[i:i+max_length] for i in range(0, len(mytext), max_length)]

# Process each chunk separately
results = []
for chunk in chunks:
    # Generate outputs for each chunk
    result = happy_gen.generate_text(prefix + chunk, args=args)
    results.append(result.text)

# Concatenate the results
output_text = &quot; &quot;.join(results)

print(output_text)

</code></pre>
<p>But it gives me this error:</p>
<pre><code>RuntimeError: The size of tensor a (1024) must match the size of tensor b (1025) at non-singleton dimension 3
</code></pre>
<p>How can I resolve it?</p>
","2023-03-13 14:56:59","","","2023-03-14 00:54:19","<python><huggingface-transformers><huggingface><gpt-2>","1","3","2","241","","","","","","",""
"75760838","1","8682074","","Can i train chatgpt with custom data from a database?","<p>Let's say I'm a law firm and I have this tables(basic structure)</p>
<ul>
<li><strong>users</strong>: name, email, telephone etc..</li>
<li><strong>employees</strong>: kind, name, email, telephone etc..</li>
<li><strong>cases</strong>: case name, casenumber, parties names, attorney assigned, entries, last update, status, open_date,  close_date</li>
<li><strong>tasks</strong>:   case_id, employee_assigned,  employee_assigner,statusdue_at</li>
<li><strong>communication</strong>: date, user_id, employee_id, text, kind, duration</li>
<li><strong>cases_assignations</strong>: employee_id, case_id</li>
</ul>
<p>So let's say now I want to train a model with chatGPT or another solution so if for example the employee types:</p>
<pre><code>**Input**: I would like to know the tasks assigned to John that are days due
**output**: John has 3 tasks that are due, these are: Task 1, task 2 ,task 3

**Input**: I would like to how many cases are open
**output**: There are 8 cases open right now
      
**Input**: I would like to how when did john communicate last time with client Elena 
**output**:  John communicate with Elena on october 8 at 6 am on phone and it last 5 minutes

**Input**: I would like to how the cases that John as opened and last update is more than 5 days before
**output**:  John has 8 cases open that are 5 days before, these are: case 1 ,case 2, case 3, case 4
</code></pre>
<p>etc... makes sense?
I would like to know the strategy to make this possible</p>
","2023-03-16 19:26:17","2023-04-25 15:51:23","","2023-03-29 10:03:53","<openai-api><gpt-3>","0","3","3","3825","","","","","","",""
"75783316","1","13829794","","OpenAI API Finetune Model - no response","<p>I am trying to learn to finetune GPT models, and am most familiar with working with Python using IDEs (eg Spyder). Would anyone know why the following code in my IDE (Spyder) gives no response, no tracebacks, no errors? I do not see any sign of the model appearing in the OpenAI Playground too.</p>
<p>Some notes: Test1 is the nickname for my model.
API key is removed in the code. I know i'm not supposed to code API keys as string - will eventually cloak it.
I'm trying to use the ada model (cheapest, just for testing).
My prompts/answers are in excel, and the code successfully saves it as a csv with prompt/response pairs.
I get &quot;FileNotFoundError: [WinError 2] The system cannot find the file specified&quot; if i remove &quot;shell=True&quot;.</p>
<pre><code>import pandas as pd
import openai
import subprocess

openai.api_key = 'xxxx'

datafile = &quot;C:/xx/xx/xx.xlsx&quot;
df=pd.read_excel(datafile,sheet_name=&quot;Sheet1&quot;,index_col = 0,skiprows=range(2),usecols = &quot;A:D&quot;,nrows=300,parse_dates=True)

# df = pd.read_csv(&quot;out_openai_completion.csv&quot;)

prepared_data = df.loc[:,['Trigger','Reply']]
prepared_data.rename(columns={'sub_prompt':'prompt', 'response_txt':'completion'}, inplace=True)
prepared_data.to_csv('prepared_data.csv',index=False)


# prepared_data.csv --&gt; prepared_data_prepared.json
subprocess.run('openai tools fine_tunes.prepare_data --file prepared_data.csv --quiet'.split(),shell=True)

# ## Start fine-tuning
subprocess.run('openai api fine_tunes.create --training_file prepared_data_prepared.jsonl --model text-davinci-002 --suffix &quot;Test1&quot;'.split(),shell=True)

subprocess.run('openai api fine_tunes.get -i &lt;SRC_Test1&gt;',shell=True) #to check progress/status

</code></pre>
","2023-03-19 16:09:47","","","2023-03-19 16:09:47","<python><pandas><subprocess><openai-api><gpt-3>","0","0","0","291","","","","","","",""
"75783524","1","21433400","","Train gpt-3 on email conversations","<p>I have to train gpt-3 on email data, so that the support team can get a quick answer from a chat-bot, for questions that were asked before by customers. There are email conversations between customers and the support team (Customer1 ask question, Support answers, Customer1 asks another question … ). I have to:</p>
<p>1.Filter important conversations and only feed gpt-3 with them.
2.prepare and convert them into the right format, so that I can train the model.
Is there anone who has some ideas about how to realize these steps and weather to use fine tuning or embeddings?</p>
<p>gpt-3 has to connect the questions to the answers that were given by the support team.</p>
","2023-03-19 16:44:07","","","2023-03-28 07:17:15","<openai-api><gpt-3>","1","0","0","362","","","","","","",""
"67288454","1","2742509","","Train GPT2 with Trainer & TrainingArguments using/specifying attention_mask","<p>I'm using Trainer &amp; TrainingArguments to train GPT2 Model, but it seems that this does not work well.</p>
<p>My datasets have the ids of the tokens of my corpus and the mask of each text, to indicate where to apply the attention:</p>
<pre><code>Dataset({
features: ['attention_mask', 'input_ids', 'labels'],
num_rows: 2012860
}))
</code></pre>
<p>I am doing the training with Trainer &amp; TrainingArguments, passing my model and my previous dataset as follows. But nowhere do I specify anything about the attention_mask:</p>
<pre><code>training_args = TrainingArguments(
output_dir=path_save_checkpoints,
overwrite_output_dir=True,
num_train_epochs=1,
per_device_train_batch_size = 4,
gradient_accumulation_steps = 4,
logging_steps = 5_000, save_steps=5_000,
fp16=True,
deepspeed=&quot;ds_config.json&quot;,
remove_unused_columns = True,
debug = True
)

trainer = Trainer(
model=model,
args=training_args,
data_collator=data_collator,
train_dataset=dataset,
tokenizer=tokenizer,
)

trainer.train()
</code></pre>
<p>How should I tell the Trainer to use this feature (attention_mask)?
If you take a look at the file /transformers/trainer.py there is no reference to &quot;attention&quot; or &quot;mask&quot;.</p>
<p>Thanks in advance!</p>
","2021-04-27 18:07:17","","","2021-10-13 00:14:31","<huggingface-transformers><attention-model><gpt-2>","1","0","0","438","","","","","","",""
"75559672","1","15696244","","OpenAI GPT-3 API: Which file formats can be used for fine-tuning?","<p>As we are getting in to turbulent times of AI.
I am as well spilling mine drop in to ocean.
As I am pythonian, all attempts are done in python/anaconda.</p>
<p>Does anybody have already some experience in &quot;data formats&quot; passable to GPT family of AIs?</p>
<p>In documentation is recommended use of OpenAI tool for control.
Followed by documentation recommending format (&quot;Prompt:&quot;, &quot;Completion:&quot;)
With strings marked as:</p>
<pre><code>  [&quot;str&quot; = in quotes,&quot;/&quot; = separator ,&quot;@&gt;&quot; = unique symbol, 
   &quot; &quot; = comp. starts with empty space]

  'Prompt':    'Hello AI..!!/@&gt;' 
  'Completion': ' How are you today?/@&gt;' 
</code></pre>
<p>&quot;Completion&quot; should have <strong>empty space</strong> at start of every sting.
So far I was able to find just <strong>simple examples</strong> as:</p>
<pre><code>Col1             Col2
'Prompt':        'Completion':
'Text/@&gt;'        ' Text/@&gt;'
</code></pre>
<p>Is there any way it will understand more complex dataset?
Is effective to have more dim. DataFrame?
<strong>Example:</strong></p>
<pre><code>     Col1        Col2             Col3         Col4        
    'Prompt_a':  'Completion_a':  'Prompt_b':  'Completion_b':
    'Text/@&gt;'    ' Text/@&gt;'       'Text/@&gt;'    ' Text/@&gt;
</code></pre>
<p>Is longer context text passed just as 'str/@&gt;', or is some partition needed?</p>
<pre><code>' text text text /@&gt;'
</code></pre>
<p>Many thanks for all answers and efforts in advance.</p>
<p>Already checked: <a href=""https://help.openai.com/en/articles/6811186-how-do-i-format-my-fine-tuning-data"" rel=""nofollow noreferrer"">https://help.openai.com/en/articles/6811186-how-do-i-format-my-fine-tuning-data</a></p>
","2023-02-24 17:32:30","","2023-03-13 14:43:42","2023-03-13 14:43:42","<python><openai-api><gpt-3><fine-tune>","1","0","0","794","","","","","","",""
"75644866","1","19789151","","Incompatibilty between styled-components and react-simple-bot library","<p>Hi I am trying to create a chatbot using Neo4j as a backend and GPT-3 as a translation tool for NL and CYPHER. I am following the tutorial in this webpage: <a href=""https://medium.com/@yu-joshua/adding-q-a-features-to-your-knowledge-graph-in-3-simple-steps-3ffe6f5caef4"" rel=""nofollow noreferrer"">https://medium.com/@yu-joshua/adding-q-a-features-to-your-knowledge-graph-in-3-simple-steps-3ffe6f5caef4</a> by Fanghua YU and he provides the frontend in Node.js. He uses react-simple-bot but the problem is that the latest version of this library is not compatible with the library styled-components@5.3.3</p>
<p>npm ERR! code ERESOLVE
npm ERR! ERESOLVE could not resolve
npm ERR!
npm ERR! While resolving: react-simple-chatbot@0.6.1
npm ERR! Found: styled-components@5.3.8
npm ERR! node_modules/styled-components
npm ERR!   peer styled-components@&quot;&gt;= 2&quot; from babel-plugin-styled-components@2.0.7
npm ERR!   node_modules/babel-plugin-styled-components
npm ERR!     babel-plugin-styled-components@&quot;&gt;= 1.12.0&quot; from styled-components@5.3.8
npm ERR!   styled-components@&quot;^5.3.8&quot; from the root project
npm ERR!
npm ERR! Could not resolve dependency:
npm ERR! peer styled-components@&quot;^4.0.0&quot; from react-simple-chatbot@0.6.1<br />
npm ERR! node_modules/react-simple-chatbot
npm ERR!   react-simple-chatbot@&quot;^0.6.1&quot; from the root project
npm ERR!
npm ERR! Conflicting peer dependency: styled-components@4.4.1
npm ERR! node_modules/styled-components
npm ERR!   peer styled-components@&quot;^4.0.0&quot; from react-simple-chatbot@0.6.1
npm ERR!   node_modules/react-simple-chatbot
npm ERR!     react-simple-chatbot@&quot;^0.6.1&quot; from the root project</p>
<p>I tried a lot of things. The most obvious one might be to downgrade the version of styled-components library to 4.0.0 which is compatible with react-simple-bot. But when I do that, a lot of the libraries are left deprecated and there is a lot of vulnerabilities in the App:</p>
<p>added 1919 packages, and audited 1920 packages in 30s</p>
<p>191 packages are looking for fundingrun npm fund for details</p>
<p>33 vulnerabilities (1 low, 1 moderate, 22 high, 9 critical)</p>
<p>So then I tried to run the app in my web browser but doesn't display anything.</p>
<p>So then I tried to run 'npm audit fix' to solve this problem but most of them require 'npm audit fix --force' which ends up breaking the application and still when I run it doesn't display anything...</p>
<p>I also tried to fix libraries one by one with a lot of patience and still nothing.</p>
<p>Please I really need help with this I can provide code if needed but I didn't feel it was necessary.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>{
  ""name"": ""datathon-ui"",
  ""version"": ""0.1.0"",
  ""private"": true,
  ""dependencies"": {
    ""@testing-library/jest-dom"": ""^5.16.0"",
    ""@testing-library/react"": ""^11.2.7"",
    ""@testing-library/user-event"": ""^12.8.3"",
    ""core-js"": ""^3.29.0"",
    ""dotenv"": ""^10.0.0"",
    ""immer"": ""^9.0.19"",
    ""neo4j-driver"": ""^4.4.2"",
    ""nth-check"": ""^2.1.1"",
    ""openai"": ""^2.0.4"",
    ""react"": ""^17.0.2"",
    ""react-dom"": ""^17.0.2"",
    ""react-scripts"": ""4.0.3"",
    ""react-simple-chatbot"": ""^0.6.1"",
    ""speak-tts"": ""^2.0.8"",
    ""styled-components"": ""^4.0.0"",
    ""svgo"": ""^3.0.2"",
    ""web-vitals"": ""^1.1.2""
  },</code></pre>
</div>
</div>
</p>
","2023-03-05 19:38:26","","2023-03-06 03:51:27","2023-03-06 03:51:27","<node.js><neo4j><gpt-3>","0","0","0","126","","","","","","",""
"75784494","1","21434098","","How can I split my model among multiple GPUs?","<p>I have been trying to split the <code>self.blocks</code> among multiple GPUs, but it returns the error &quot;All tensors must be on same GPU.&quot; I don't want DataParallel, but ModelParallel among 2 GPU minimum and their weights and biases should commute with each other.</p>
<pre><code>class LanguageModel(nn.Module):

    def __init__(self):
        super().__init__()
        # each token directly reads off the logits for the next token from a lookup table
        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
        self.position_embedding_table = nn.Embedding(block_size, n_embd)
        self.blocks = nn.DataParallel(nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)]))
        self.ln_f = nn.LayerNorm(n_embd) # final layer norm
        self.lm_head = nn.Linear(n_embd, vocab_size)

        # better init, not covered in the original GPT video, but important, will cover in followup video
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, idx, targets=None):
        B, T = idx.shape
        # idx and targets are both (B,T) tensor of integers
        tok_emb = self.token_embedding_table(idx) # (B,T,C)
        pos_emb = self.position_embedding_table(torch.arange(T, device=device[0])) # (T,C)
        x = tok_emb + pos_emb # (B,T,C)
        x = self.blocks(x) # (B,T,C)
        x = self.ln_f(x) # (B,T,C)
        logits = self.lm_head(x) # (B,T,vocab_size)

        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)

        return logits, loss

    def generate(self, idx, max_new_tokens):
        # idx is (B, T) array of indices in the current context
        for _ in range(max_new_tokens):
            # crop idx to the last block_size tokens
            idx_cond = idx[:, -block_size:]
            # get the predictions
            logits, loss = self(idx_cond)
            # focus only on the last time step
            logits = logits[:, -1, :] # becomes (B, C)
            # apply softmax to get probabilities
            probs = F.softmax(logits, dim=-1) # (B, C)
            # sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)
            # append sampled index to the running sequence
            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)
        return idx
</code></pre>
<p>I have already tried splitting it like</p>
<pre><code># splitting blocks into multiple GPUs
      for i in range(n_layer):
      self.blocks.module[i].to(device[i % len(device)])
</code></pre>
<p>Please help. Thanks in advance :)</p>
","2023-03-19 19:33:56","","2023-03-20 10:09:04","2023-03-20 10:09:04","<python><machine-learning><pytorch><nlp><gpt-2>","1","0","2","112","","","","","","",""
"75827960","1","20493358","","how do i stop this encoding error in the openai python module?","<p>i'm trying to make a chat completion bot using opeAI's GPT engine that takes voice input and outputs a text to speech file, however, i keep getting an encoding error that i dont understand</p>
<pre><code>import os
import speech_recognition as sr
import openai
from dotenv import load_dotenv
from os import path
from playsound import playsound
from gtts import gTTS
import simpleaudio as sa

load_dotenv()

language = 'en'

openai.api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)

while True:
    # this recognizes your voice input
    recog = sr.Recognizer()
    with sr.Microphone() as source:
        audio = recog.listen(source)
    #this transcribes the voice to text
    with open(&quot;microphone-results.wav&quot;, &quot;wb&quot;) as f:
        f.write(audio.get_wav_data())
    AUDIO_FILE = path.join(path.dirname(path.realpath(__file__)), &quot;microphone-results.wav&quot;)
    my_question = recog.recognize_sphinx(audio)

    #this generates a response
    response = openai.ChatCompletion.create(
        model=&quot;gpt-3.5-turbo&quot;,
        messages=[
            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a chatbot named jarvis&quot;},
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: str(my_question)},
        ]
    )
    reply = ''.join(choice.message.content for choice in response.choices)
    tts = gTTS(reply)
    tts_file = &quot;temp.wav&quot;

    tts.save(tts_file)
    wave_obj = sa.WaveObject.from_wave_file(tts_file)
    play_obj = wave_obj.play()
    play_obj.wait_done()

    os.remove(tts_file)
</code></pre>
<p>i tried formatting it, thinking it would output the tts result instead, it said this:</p>
<pre><code>  File &quot;c:\Users\tonda\python\SSPS_Projects\PortfolioApps\assistant\functions\ChatGPT\Chat.py&quot;, line 28, in &lt;module&gt;
    response = openai.ChatCompletion.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\openai\api_resources\chat_completion.py&quot;, line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\openai\api_resources\abstract\engine_api_resource.py&quot;, line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\openai\api_requestor.py&quot;, line 216, in request
    result = self.request_raw(
             ^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\openai\api_requestor.py&quot;, line 516, in request_raw
    result = _thread_context.session.request(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\requests\sessions.py&quot;, line 587, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\requests\sessions.py&quot;, line 701, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\requests\adapters.py&quot;, line 489, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\urllib3\connectionpool.py&quot;, line 703, in urlopen
    httplib_response = self._make_request(
                       ^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\urllib3\connectionpool.py&quot;, line 398, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\urllib3\connection.py&quot;, line 239, in request
    super(HTTPConnection, self).request(method, url, body=body, headers=headers)
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\http\client.py&quot;, line 1282, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\http\client.py&quot;, line 1323, in _send_request
    self.putheader(hdr, value)
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\site-packages\urllib3\connection.py&quot;, line 224, in putheader
    _HTTPConnection.putheader(self, header, *values)
  File &quot;C:\Users\tonda\scoop\apps\python\3.11.0\Lib\http\client.py&quot;, line 1255, in putheader
    values[i] = one_value.encode('latin-1')
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeEncodeError: 'latin-1' codec can't encode character '\u201c' in position 7: ordinal not in range(256)
</code></pre>
<p>it is saying something about an unknown character? i really dont understand this, partially because im new to coding</p>
","2023-03-23 21:04:00","","","2023-05-12 15:25:30","<python><encoding><character-encoding><openai-api><gpt-3>","1","0","3","262","","","","","","",""
"75837647","1","14888778","","How do I restart Hugging Face Transformer GPT2 finetuning?","<p>I'm trying to restart fine-tuning but it starts from the beginning. Is this normal?</p>
<p>I want to resume fine-tuning using a saved checkpoint. However, when I replace the line <code>model = GPT2LMHeadModel.from_pretrained('gpt')</code> with <code>model = GPT2LMHeadModel.from_pretrained('path/to/checkpoint')</code>, the training starts from the beginning. Is this normal?</p>
","2023-03-24 19:50:39","","2023-03-24 19:51:07","2023-03-25 10:38:23","<python><pytorch><huggingface-transformers><gpt-2>","1","0","0","99","","","","","","",""
"73472819","1","15152059","","How to standardize GPT-3 word embeddings in Python?","<p>I have a df with two columns, one for a single word (e.g., &quot;capability&quot;) and one for a corresponding word embedding:</p>
<pre><code>Word        Word_embedding
DISOWN      [0.002071153838187456, 0.00909473467618227, ... ]
CAPABILITY  [-0.004976911004632711, 0.005002433434128761, ... ]
</code></pre>
<p><strong>I would like to standardize (mean=0, sd=1) ALL word embeddings</strong>. Unfortunately, I couldn't find a function that fits to the structure of the word embeddings (Python beginner here, sorry).</p>
<p>My failed approach:</p>
<pre><code>preprocessing.scale(df[[&quot;Word_embedding&quot;]])
</code></pre>
<p>which yielded the error:</p>
<blockquote>
<p>TypeError: float() argument must be a string or a number, not 'list'</p>
</blockquote>
<p>Can someone help?</p>
","2022-08-24 12:04:01","","2022-08-24 13:09:43","2022-08-24 13:09:43","<python><word-embedding><gpt-3>","0","0","0","159","","","","","","",""
"73899423","1","14022747","","NLP / ML Python: variation of topic modeling + summarization? Can someone point me in the right direction?","<p>New to NLP and Machine learning. Wondering if someone can point me in the right direction:</p>
<p>I'm looking to create a function that takes 2 inputs.</p>
<p>-an array of strings (english sentences of varying relation to one another. but for these purposes let's just assume they're totally unrelated sentences)</p>
<p>-a &quot;topic&quot; string.</p>
<p>The function then returns a coherent paragraph / essay about the indicated &quot;topic,&quot; using ONLY the available sentences.</p>
<p>Seems like some flavor of topic-modeling and summarization, except the function writes using only the predetermined array of strings.</p>
<p>Any thoughts as to what libraries or techniques I should investigate?</p>
<p>Thanks!</p>
","2022-09-29 17:16:11","","2022-09-29 17:22:42","2022-09-29 17:22:42","<python><nlp><stanford-nlp><summarization><gpt-3>","0","2","1","81","","","","","","",""
"73945384","1","20156712","","Structure evaluation set GPT-2 text generation huggingface","<p>I´m currently reproducing the second task (generating articles from headline) of this tutorial: <a href=""https://www.modeldifferently.com/en/2021/12/generaci%C3%B3n-de-fake-news-con-gpt-2/#42-fine-tuning-to-generate-articles-from-headlines"" rel=""nofollow noreferrer"">https://www.modeldifferently.com/en/2021/12/generaci%C3%B3n-de-fake-news-con-gpt-2/#42-fine-tuning-to-generate-articles-from-headlines</a></p>
<p>I understand that the ‘input_ids’ of the training data must be prepared in the the format ‘bos_token title sep_token content eos_token’. Now I want to add a compute_metrics function which will be called by the trainer and evaluates another set, thus the model has to predict the ‘content’ only given the ‘title’. How do I prepare the data for the evaluation set?</p>
<p>Is it just ‘bos_token title sep_token’? Or has one to manipulate the ‘attention_mask’ as indicated here:
<a href=""https://discuss.huggingface.co/t/gpt2-for-qa-pair-generation/759/9"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/gpt2-for-qa-pair-generation/759/9</a></p>
","2022-10-04 08:57:22","","","2022-10-04 08:57:22","<gpt-2><huggingface>","0","0","0","108","","","","","","",""
"75435496","1","21203402","","OpenAI GPT-3 API error: ""Unknown endpoint for this model.""","<p>I'm new to using APIs.
I found myself interested inn the new OpenAI product, GPT-3 (I know, it's not that new. But I just found out about it).
I'm trying to use the API key in Python, but it seems the key is invalid.</p>
<p>This is my code (I can't put my API key here for obvious reasons):</p>
<pre><code>import requests 
prompt = 'Tell me the history of Europe in summary'
model = 'davinci'
url = 'https://api.openai.com/v1/engines/davinci/jobs'

headers = {
    'content-type': 'application/json',
    'Authorization': 'Bearer MY_API_KEY',
}

data = {
    'prompt': prompt,
    'max-tokens': 100,
    'temperature': 0.5,
}

response = requests.post(url,headers=headers, json=data)
response_json = response.json()
print(response_json)
</code></pre>
<p>I keep receiving this error:
{'error': {'message': 'Unknown endpoint for this model.', 'type': 'invalid_request_error', 'param':       None, 'code': None}}</p>
<p>I have tried using a new API key several times but it doesn't work.
How can I find out why my key is invalid?</p>
","2023-02-13 11:50:22","","2023-03-13 14:31:20","2023-05-15 06:26:31","<python><openai-api><gpt-3>","2","2","-2","2431","","","","","","",""
"57720955","1","4544413","","Can't import the encoder code for fine tuning GPT-2","<p>I'm trying to reproduce the example from this article: <a href=""https://medium.com/@ngwaifoong92/beginners-guide-to-retrain-gpt-2-117m-to-generate-custom-text-content-8bb5363d8b7f"" rel=""nofollow noreferrer"">https://medium.com/@ngwaifoong92/beginners-guide-to-retrain-gpt-2-117m-to-generate-custom-text-content-8bb5363d8b7f</a> </p>

<p>The example code is from the following repo: <a href=""https://github.com/nshepperd/gpt-2"" rel=""nofollow noreferrer"">https://github.com/nshepperd/gpt-2</a></p>

<p>After installing the requirements and downloading the model, the following step is to train the model, for which this code has to be executed: </p>

<pre><code>python encode.py lyric.txt lyric.npz
</code></pre>

<p>The issue here is that this requires to import the following modules: </p>

<pre><code>import argparse
import numpy as np

import encoder
from load_dataset import load_dataset
</code></pre>

<p>Where <strong>encoder</strong> and <strong>load_dataset</strong> are on a child directory:</p>

<pre><code>|--encode.py
 --src
   |--encoder.py
   |--load_dataset.py
</code></pre>

<p>This generates the following error: </p>

<pre><code>ModuleNotFoundError: No module named 'encoder'
</code></pre>

<p>I tried creating the <code>__init__.py</code> files and importing them as </p>

<p><strong>src.encoder</strong> and <strong>src.load_dataset</strong> but that those not work either.</p>

<p>In the medium post the author proposes to move the file <strong>encoder.py</strong> to src and execute the code from there, the issue there is that doing it breaks the relative path for the model too and although I handled that the issue with the paths keeps going for other files as well.   </p>
","2019-08-30 05:30:36","","2020-11-29 11:58:47","2023-04-02 17:27:44","<python><path><nlp><init><gpt-2>","5","1","3","2924","","","","","","",""
"69590991","1","17139319","","How do I make ""msg.content"" constantly get new strings added to it instead of replaced?","<pre><code> var collector = new MessageCollector(message.channel, filter, {
    max: 10,
    time: 60000,
})

        collector.on(&quot;collect&quot;, (msg) =&gt; {
            console.log(msg.content)
            
        openai.Completion.create({
            
            engine: &quot;davinci&quot;,
            prompt: msg.content,
            temperature: 0.9,
            max_tokens: 150,
            top_p: 1,
            frequency_penalty: 0.35,
            presence_penalty: 0.6, 
            stop: [&quot;\n&quot;, &quot; Human:&quot;, &quot; AI:&quot;]  
        
        }).then((response) =&gt; {
            
            message.channel.send(response.choices[0].text)
        })

    })
}
</code></pre>
<p>So I was trying to implement the AI &quot;GPT-3&quot; into a discord bot to see how it would work, but GPT-3 needs to know the prompt (basically context of a conversation) all the time. With the way I have it set up, it constantly replaces the variable (msg.content) with new strings once they are grabbed by &quot;MessageCollector&quot;. I need to make it so that whenever a message is detected, it adds that string to the variable and constantly doing that until, lets say a timer goes off.</p>
","2021-10-15 21:52:15","","2021-10-17 04:49:11","2021-10-17 04:49:11","<javascript><node.js><discord.js><openai-api><gpt-3>","0","3","1","48","","","","","","",""
"70118071","1","","","Trouble getting text from GPT2 returned?","<p>basically I am trying to have gpt2 respond to a prompt in the variable {text} and I am running into this error:</p>
<p>ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()</p>
<p>here is my code thus far:</p>
<pre><code>import gradio as gr
from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')#gpt2-xl #for very powerful model
model = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=tokenizer.eos_token_id)

text = &quot;what is natural language processing?&quot;
encoded_input = tokenizer.encode(text, return_tensors='pt')

#print(tokenizer.decode((encoded_input[0][0]))) # works well to here

def generate_text(inp):
    input_ids = tokenizer.encode(inp, return_tensors='tf')
    beam_output = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)
    output = tokenizer.decode(beam_output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)
    return &quot;.&quot;.join(output.split(&quot;.&quot;)[:-1]) + &quot;.&quot;

output_text = gr.outputs.Textbox() # works well to here
text1 = generate_text(text) # BREAKS HERE
</code></pre>
<p>Could anyone help me figure out what I'm doing wrong? Thanks.</p>
","2021-11-25 22:59:42","","","2022-02-24 19:27:43","<python><huggingface-transformers><gpt-2>","1","0","0","343","","","","","","",""
"75861442","1","21505798","","An error occurred: module 'openai' has no attribute 'ChatCompletion'","<p>I'm trying to build a discord bot that uses the GPT-4 API to function as a chatbot on discord. I have the most recent version of the OpenAI library but when I run my code it tells me &quot;An error occurred: module 'openai' has no attribute 'ChatCompletion'&quot;</p>
<p>I tried uninstalling and reinstalling the OpenAI library, I tried using the completions endpoint and got the error &quot;This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?&quot;</p>
<p>This is the snippet of code thats giving me issues:</p>
<pre><code>async def get_gpt_response(prompt, history):
    history_strings = [f&quot;{message['role']}: {message['content']}&quot; for message in history] # update history format
    chat_prompt = '\n'.join(history_strings + [f&quot;user: {prompt}&quot;])
    
    completions = openai.ChatCompletion.create(
        engine=config[&quot;model&quot;],
        prompt=chat_prompt,
        max_tokens=config[&quot;max_tokens&quot;],
        n=1,
        temperature=config[&quot;temperature&quot;],
    )
    return completions.choices[0].text.strip().split('assistant:', 1)[-1].strip()
</code></pre>
","2023-03-28 00:33:57","","","2023-06-09 18:45:55","<python><openai-api><gpt-4>","6","2","2","2514","","","","","","",""
"75872837","1","21513148","","How can we set a scope to a fine tuned chat gpt3 chatbot","<p>How can we set a scope to a fine tuned chat gpt3 chatbot and limit it within a particular context? In order to avoid replying to unnecessary questions unrelated to the context.</p>
","2023-03-29 03:18:42","","","2023-03-29 03:18:42","<scope><chatbot><gpt-3><fine-tune>","0","0","0","26","","","","","","",""
"76297340","1","10772341","","TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]] when training GPT-2","<p>I'm learning how to train generative models using the transformers library from HuggingFace, however, I keep having this error: <code>TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]</code> when I want to tokenize the text of my dataset.</p>
<p>Here's my code:</p>
<pre class=""lang-py prettyprint-override""><code>def construct_conv(row, tokenizer, eos = True):
    flatten = lambda l: [item for sublist in l for item in sublist]
    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))
    conv = flatten(conv)
    return conv

class ConversationDataset(Dataset):
    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):

        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)

        directory = args.cache_dir
        cached_features_file = os.path.join(
            directory, args.model_type + &quot;_cached_lm_&quot; + str(block_size)
        )

        if os.path.exists(cached_features_file) and not args.overwrite_cache:
            logger.info(&quot;Loading features from cached file %s&quot;, cached_features_file)
            with open(cached_features_file, &quot;rb&quot;) as handle:
                self.examples = pickle.load(handle)
        else:
            logger.info(&quot;Creating features from dataset file at %s&quot;, directory)

            self.examples = []
            for _, row in df.iterrows():
                conv = construct_conv(row, tokenizer)
                self.examples.append(conv)

            logger.info(&quot;Saving features into cached file %s&quot;, cached_features_file)
            with open(cached_features_file, &quot;wb&quot;) as handle:
                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, item):
        return torch.tensor(self.examples[item], dtype=torch.long)
      
# Cacheing and storing of data/checkpoints

def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):
    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)

def set_seed(args):
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    if args.n_gpu &gt; 0:
        torch.cuda.manual_seed_all(args.seed)

def _sorted_checkpoints(args, checkpoint_prefix=&quot;checkpoint&quot;, use_mtime=False) -&gt; List[str]:
    ordering_and_checkpoint_path = []

    glob_checkpoints = glob.glob(os.path.join(args.output_dir, &quot;{}-*&quot;.format(checkpoint_prefix)))

    for path in glob_checkpoints:
        if use_mtime:
            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))
        else:
            regex_match = re.match(&quot;.*{}-([0-9]+)&quot;.format(checkpoint_prefix), path)
            if regex_match and regex_match.groups():
                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))

    checkpoints_sorted = sorted(ordering_and_checkpoint_path)
    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]
    return checkpoints_sorted

def _rotate_checkpoints(args, checkpoint_prefix=&quot;checkpoint&quot;, use_mtime=False) -&gt; None:
    if not args.save_total_limit:
        return
    if args.save_total_limit &lt;= 0:
        return

    # Check if we should delete older checkpoint(s)
    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)
    if len(checkpoints_sorted) &lt;= args.save_total_limit:
        return

    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)
    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]
    for checkpoint in checkpoints_to_be_deleted:
        logger.info(&quot;Deleting older checkpoint [{}] due to args.save_total_limit&quot;.format(checkpoint))
        shutil.rmtree(checkpoint)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>def main(df_trn, df_val):
    args = Args()
    
    if args.should_continue:
        sorted_checkpoints = _sorted_checkpoints(args)
        if len(sorted_checkpoints) == 0:
            raise ValueError(&quot;Used --should_continue but no checkpoint was found in --output_dir.&quot;)
        else:
            args.model_name_or_path = sorted_checkpoints[-1]

    if (
        os.path.exists(args.output_dir)
        and os.listdir(args.output_dir)
        and args.do_train
        and not args.overwrite_output_dir
        and not args.should_continue
    ):
        raise ValueError(
            &quot;Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.&quot;.format(
                args.output_dir
            )
        )

    # Setup CUDA, GPU &amp; distributed training
    device = torch.device(&quot;cuda&quot;)
    args.n_gpu = torch.cuda.device_count()
    args.device = device

    # Setup logging
    logging.basicConfig(
        format=&quot;%(asctime)s - %(levelname)s - %(name)s -   %(message)s&quot;,
        datefmt=&quot;%m/%d/%Y %H:%M:%S&quot;,
        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,
    )
    logger.warning(
        &quot;Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s&quot;,
        args.local_rank,
        device,
        args.n_gpu,
        bool(args.local_rank != -1),
        args.fp16,
    )

    # Set seed
    set_seed(args)

    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)
    model = AutoModelWithLMHead.from_pretrained(
        args.model_name_or_path,
        from_tf=False,
        config=config,
        cache_dir=args.cache_dir,
    )
    model.to(args.device)
    
    logger.info(&quot;Training/evaluation parameters %s&quot;, args)
    print(tokenizer)

    # Training
    if args.do_train:
        
        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)

        global_step, tr_loss = train(args, train_dataset, model, tokenizer)
        logger.info(&quot; global_step = %s, average loss = %s&quot;, global_step, tr_loss)

    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()
    if args.do_train:
        # Create output directory if needed
        os.makedirs(args.output_dir, exist_ok=True)

        logger.info(&quot;Saving model checkpoint to %s&quot;, args.output_dir)
        # Save a trained model, configuration and tokenizer using `save_pretrained()`.
        # They can then be reloaded using `from_pretrained()`
        model_to_save = (
            model.module if hasattr(model, &quot;module&quot;) else model
        )  # Take care of distributed/parallel training
        model_to_save.save_pretrained(args.output_dir)
        tokenizer.save_pretrained(args.output_dir)

        # Good practice: save your training arguments together with the trained model
        torch.save(args, os.path.join(args.output_dir, &quot;training_args.bin&quot;))

        # Load a trained model and vocabulary that you have fine-tuned
        model = AutoModelWithLMHead.from_pretrained(args.output_dir)
        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)
        model.to(args.device)

    # Evaluation
    results = {}
    if args.do_eval and args.local_rank in [-1, 0]:
        checkpoints = [args.output_dir]
        if args.eval_all_checkpoints:
            checkpoints = list(
                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + &quot;/**/&quot; + WEIGHTS_NAME, recursive=True))
            )
            logging.getLogger(&quot;transformers.modeling_utils&quot;).setLevel(logging.WARN)  # Reduce logging
        logger.info(&quot;Evaluate the following checkpoints: %s&quot;, checkpoints)
        for checkpoint in checkpoints:
            global_step = checkpoint.split(&quot;-&quot;)[-1] if len(checkpoints) &gt; 1 else &quot;&quot;
            prefix = checkpoint.split(&quot;/&quot;)[-1] if checkpoint.find(&quot;checkpoint&quot;) != -1 else &quot;&quot;

            model = AutoModelWithLMHead.from_pretrained(checkpoint)
            model.to(args.device)
            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)
            result = dict((k + &quot;_{}&quot;.format(global_step), v) for k, v in result.items())
            results.update(result)

    return results
</code></pre>
<p>And my dataset it's this one:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ID</th>
<th style=""text-align: right;"">reponse</th>
<th style=""text-align: right;"">context</th>
<th style=""text-align: right;"">context/0</th>
<th style=""text-align: right;"">context/1</th>
<th style=""text-align: right;"">context/2</th>
<th style=""text-align: right;"">context/3</th>
<th style=""text-align: right;"">context/4</th>
<th style=""text-align: right;"">context/5</th>
<th style=""text-align: right;"">context/6</th>
<th style=""text-align: right;"">context/7</th>
<th style=""text-align: right;"">context/8</th>
<th>context/9</th>
</tr>
</thead>
<tbody>
<tr>
<td>36917</td>
<td style=""text-align: right;"">Es tan simple.</td>
<td style=""text-align: right;"">No se que te detiene.</td>
<td style=""text-align: right;"">¡Muy persuasiva!</td>
<td style=""text-align: right;"">No es crimen librarse de una alimaña.</td>
<td style=""text-align: right;"">¿Por qué tener lastima de un hombre tan vil?</td>
<td style=""text-align: right;"">Además, también ha puesto sus ojos en Okayo.</td>
<td style=""text-align: right;"">Hace 4 años que soy victima de mi marido.</td>
<td style=""text-align: right;"">Solo estoy siendo franca contigo.</td>
<td style=""text-align: right;"">Calmate.</td>
<td style=""text-align: right;"">¡Eres peor que el diablo!</td>
<td style=""text-align: right;"">¿Comprendes?</td>
<td>Okayo me recuerda constantemente mi fracaso.</td>
</tr>
<tr>
<td>5449</td>
<td style=""text-align: right;"">Muy torpe, Joyce.</td>
<td style=""text-align: right;"">A la sala de interrogación rápido. ¡Muévanse!</td>
<td style=""text-align: right;"">De pie, muchachos.</td>
<td style=""text-align: right;"">A la sala de interrogación rápido. ¡Muévanse!</td>
<td style=""text-align: right;"">De pie, muchachos.</td>
<td style=""text-align: right;"">¡Use su cuchillo, hombre!</td>
<td style=""text-align: right;"">¡Adelántese, Thomson!</td>
<td style=""text-align: right;"">¡Bien hecho, Jenkins!</td>
<td style=""text-align: right;"">Gracias.</td>
<td style=""text-align: right;"">Muy bien.</td>
<td style=""text-align: right;"">El bungaló del mayor Warden está al final del ...</td>
<td>Continúe, conductor.</td>
</tr>
<tr>
<td>37004</td>
<td style=""text-align: right;"">Pídemelo.</td>
<td style=""text-align: right;"">Sólo lo que quieras tú.</td>
<td style=""text-align: right;"">Ya no soy yo.</td>
<td style=""text-align: right;"">Eres preciosa y maravillosa.</td>
<td style=""text-align: right;"">¿No?</td>
<td style=""text-align: right;"">Así te gustaré.</td>
<td style=""text-align: right;"">Haré y diré lo que quieras.</td>
<td style=""text-align: right;"">Nunca.</td>
<td style=""text-align: right;"">Así nunca querrás estar con otras, ¿verdad?</td>
<td style=""text-align: right;"">Siempre diré lo que tú desees y haré lo que tú...</td>
<td style=""text-align: right;"">Pero yo sí.</td>
<td>Pero...</td>
</tr>
<tr>
<td>47077</td>
<td style=""text-align: right;"">¡Boris!</td>
<td style=""text-align: right;"">¡Nicolás, que alegría a mi corazón, volviste!</td>
<td style=""text-align: right;"">¡Regresan los Vencedores!</td>
<td style=""text-align: right;"">¡Miren!</td>
<td style=""text-align: right;"">¡Ahí vienen!</td>
<td style=""text-align: right;"">Está vivo.</td>
<td style=""text-align: right;"">Boris está vivo.</td>
<td style=""text-align: right;"">Dasha prometió avisarme cuando regrese.</td>
<td style=""text-align: right;"">Pero, en la fábrica dicen que él está en una u...</td>
<td style=""text-align: right;"">Tampoco hay noticias de Stepan.</td>
<td style=""text-align: right;"">¡Quién sabe!</td>
<td>¿Por qué entonces, no hay noticias de él?</td>
</tr>
<tr>
<td>41450</td>
<td style=""text-align: right;"">Entonces por qué no estamos en mejor situación...</td>
<td style=""text-align: right;"">Dora Hartley era una buena prueba.</td>
<td style=""text-align: right;"">Mire, lo que hace usted creer ¿Qué los indios ...</td>
<td style=""text-align: right;"">Aleja esa arma.</td>
<td style=""text-align: right;"">Buenas noches.</td>
<td style=""text-align: right;"">Es hora de ir a la cama.</td>
<td style=""text-align: right;"">Seguro.</td>
<td style=""text-align: right;"">Sí. recuerde que es un secreto.</td>
<td style=""text-align: right;"">Es bonita.</td>
<td style=""text-align: right;"">Está bien.</td>
<td style=""text-align: right;"">¿Ann Martin?</td>
<td>Hola, Bax.</td>
</tr>
</tbody>
</table>
</div>","2023-05-20 21:02:24","","2023-05-20 21:15:34","2023-05-20 21:15:34","<python><huggingface-transformers><huggingface><gpt-2>","0","0","0","87","","","","","","",""
"76320182","1","6832612","","Gradio Interface does not output anything","<p>Via Button click (Gradio Interface) I want to output the return value of a method of the Prompter Class:</p>
<pre><code>class Prompter:
def __init__(self, gpt_model, temper):
    if not os.environ.get(&quot;OPENAI_API_KEY&quot;):
        raise Exception(&quot;Please set the OPENAI_API_KEY environment variable&quot;)

    openai.api_key = os.environ.get(&quot;OPENAI_API_KEY&quot;)

    self.gpt_model = gpt_model
    self.temper = temper

def prompt_model_print(self, messages: list):
    response = openai.ChatCompletion.create(model=self.gpt_model, messages=messages)
    return response[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;]
</code></pre>
<p>I instantiate an Object of this class within the code for my gradio interface:</p>
<pre><code>import gradio as gr def emo(name):
return prompter.prompt_model_print(emo_name_prompts) 
inputs = gr.inputs.Textbox(label=&quot;Name&quot;)
outputs = gr.outputs.Textbox(label=&quot;Emotion&quot;)
demo = gr.Interface(fn=emo, inputs=None, outputs=outputs, title=&quot;Discover Emotion&quot;, description=&quot;Please generate a Python list of 10 new feelings, provide name and sentiment&quot;)
demo.launch(share=True)
</code></pre>
<p>The display appears in my colab cell. When clicking the &quot;generate&quot; button the machine is busy. But it outputs nothing and also my Open AI usage do not change. How do I display the output of <code>prompter.prompt_model_print(emo_name_prompts)</code>
You can assume, that this method works – but only outside of the Gradio interface.</p>
","2023-05-24 04:52:21","","","2023-05-24 04:52:21","<openai-api><gpt-3><gradio>","0","0","0","67","","","","","","",""
"76331086","1","21027624","","Estimating OpenAI GPT-3.5-Turbo usage costs for french inputs, is this the right approach?","<p>I have a corpus of french documents that will undergo the same processing using OpenAI. I'll be extracting information from the texts using french prompts.
The prompts will be constituted of the text itself + the question specifying the task we'd like to accomplish.
I am using TikToken to estimate the number of tokens and my code is as follows:</p>
<pre><code>import tiktoken

encoding = tiktoken.get_encoding(&quot;cl100k_base&quot;)
encoding = tiktoken.encoding_for_model(&quot;gpt-3.5-turbo&quot;)

def num_tokens_from_string(string: str, encoding_name: str) -&gt; int:
    &quot;&quot;&quot;Returns the number of tokens in a text string.&quot;&quot;&quot;
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens

def count_token(text):
    text = str(text)
    return num_tokens_from_string(text, &quot;cl100k_base&quot;)

df['estimation'] = df['text'].apply(count_token)
df['estimation'].sum()
</code></pre>
<p>Is this the right approach for the French language?
After having an estimate of the number of tokens, we're multiplying this by 0.002$/1k token to get a rough estimate of the total price. Is this approach valid?
Does the number of tokens include the output / generated tokens as well?</p>
<p>Thanks in advance for your help</p>
","2023-05-25 10:04:23","","","2023-05-25 10:04:23","<token><openai-api><gpt-3>","0","0","0","49","","","","","","",""
"76405967","1","6623469","","How come azure openai models are faster than openai models?","<p>I recently tried gpt-4 model with API calls to azure and openai. Noticed that time taken by models in azure is <strong>at least</strong> 2X faster.</p>
<p>What could be the reason behind this? Like has azure shared any details around this change in speed?</p>
","2023-06-05 11:28:11","","","2023-06-06 18:07:36","<azure-cognitive-services><openai-api><gpt-4>","1","0","0","318","","","","","","",""
"76437658","1","11049287","","How to handle token limit in ChatGPT3.5 Turbo when creating tables?","<p>End user can copy tables from a pdf like
<a href=""https://i.stack.imgur.com/RSojg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RSojg.png"" alt=""enter image description here"" /></a></p>
<p>, paste the text in openai playground</p>
<pre><code>bird_id bird_posts bird_likes
012 2 5
013 0 4
056 57 70
612 0 12
</code></pre>
<p>and will prompt the gpt with &quot;Create table with the given text&quot;
and gpt generates a table like below:
<a href=""https://i.stack.imgur.com/a4iAS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/a4iAS.png"" alt=""enter image description here"" /></a></p>
<p>This works well as expected.
But when my input text is sizeable (say 1076 tokens), I face the following error:</p>
<pre><code>Token limit error: The input tokens exceeded the maximum allowed by the model. Please reduce the number of input tokens to continue. Refer to the token count in the 'Parameters' panel for more details.
</code></pre>
<p>I will use python for text preprocessing and will get the data from UI.
If my input is textual data (like passages), I can use the <a href=""https://python.langchain.com/en/latest/modules/chains/index_examples/summarize.html"" rel=""nofollow noreferrer"">approaches</a> suggested by Langchain.
But, I would not be able to use summarization iteratively with tabular text as I might loose rows/columns.</p>
<p>Any inputs how this can be handled?</p>
","2023-06-09 06:13:47","","","2023-06-22 17:14:03","<python-3.x><openai-api><gpt-3><azure-openai><llm>","1","0","0","46","","","","","","",""
"76479712","1","1305322","","tf.compat.v1.estimator.Estimator(): NameError: name 'model_fn' is not defined","<p>I am trying to create a pet LLM using GPT-2 following instructions here:  <a href=""https://thomascherickal.medium.com/how-to-create-your-own-llm-model-2598615a039a"" rel=""nofollow noreferrer"">https://thomascherickal.medium.com/how-to-create-your-own-llm-model-2598615a039a</a></p>
<p>The code gives syntax error while calling tf.compat.v1.estimator.Estimator() with model_fn as an argument:</p>
<p><em>NameError: name 'model_fn' is not defined</em></p>
<p>I tried defining model_fn as:
model_fn = model_fn(hparams, tf.estimator.ModeKeys.TRAIN)
but that did not help. I am not sure where model_fn should be defined.</p>
<p>Full code is here. Any help would be appreciated.</p>
<pre><code>import tensorflow as tf
import numpy as np
import os
import json
import random
import time
import argparse
# Define the command-line arguments
parser = argparse.ArgumentParser()
parser.add_argument(&quot;--dataset_path&quot;, type=str, required=True,
                    help=&quot;Path to the dataset&quot;)
parser.add_argument(&quot;--model_path&quot;, type=str, required=True,
                    help=&quot;Path to the pre-trained model&quot;)
parser.add_argument(&quot;--output_path&quot;, type=str, required=True,
                    help=&quot;Path to save the fine-tuned model&quot;)
parser.add_argument(&quot;--batch_size&quot;, type=int, default=16,
                    help=&quot;Batch size for training&quot;)
parser.add_argument(&quot;--epochs&quot;, type=int, default=1,
                    help=&quot;Number of epochs to train for&quot;)
args = parser.parse_args()
# Load the pre-trained GPT-2 model
with open(os.path.join(args.model_path, &quot;hparams.json&quot;), &quot;r&quot;) as f:
    hparams = json.load(f)
model = tf.compat.v1.estimator.Estimator(
    model_fn=model_fn, #&lt;- error occurs here
    model_dir=args.output_path,
    params=hparams,
    config=tf.compat.v1.estimator.RunConfig(
        save_checkpoints_steps=5000,
        keep_checkpoint_max=10,
        save_summary_steps=5000
    )
)
# Define the input function for the dataset
def input_fn(mode):
    dataset = tf.data.TextLineDataset(args.dataset_path)
    dataset = dataset.repeat()
    dataset = dataset.shuffle(buffer_size=10000)
    dataset = dataset.batch(args.batch_size)
    dataset = dataset.map(lambda x: tf.strings.substr(x, 0, hparams[&quot;n_ctx&quot;]))
    iterator = dataset.make_one_shot_iterator()
    return iterator.get_next()
# Define the training function
def train():
    for epoch in range(args.epochs):
        model.train(input_fn=lambda: input_fn(tf.estimator.ModeKeys.TRAIN))
        print(f&quot;Epoch {epoch+1} completed.&quot;)
# Start the training
train()
</code></pre>
","2023-06-15 06:31:44","","2023-06-16 06:08:25","2023-06-16 06:08:25","<python><tensorflow><gpt-2><llm>","0","0","0","21","","","","","","",""
"69831403","1","","","GPT2 on apple M1 Pro chip","<p>while trying to install GPT2 according to the instructions on the <a href=""https://github.com/openai/gpt-2/blob/master/DEVELOPERS.md"" rel=""nofollow noreferrer"">official</a> github repo, I ended up with an <code>Illigal hardware instruction</code> error when I tried to use it.<br />
that means I shouldn't even think of trying GPT2 on an M1 pro chip<br />
(though the instructions are incomplete because it doesn't tell you what python and pip version to use to install tensorflow, it just says you need tensorflow 1.12.0 so, from the <a href=""https://www.tensorflow.org/install/pip"" rel=""nofollow noreferrer"">official</a> tensorflow website and by connecting the dots from the instructions there, I figured I needed python3.8 and also since I have MacOS).<br />
after this dead-end and before I gave up on this beautiful open source ML model, I discovered in the official apple's <a href=""https://github.com/apple/tensorflow_macos"" rel=""nofollow noreferrer"">github page</a> they have an optimized tensorflow version for MacOS even allowing you to take advantage of the 16 Neural-Engine cores the M1 Pro CPU has. (no one cares about GPU support if you have that)<br />
only problem is the tensorflow this time is a versioned 2.X while GPT2 is using 1.12.0<br />
I don't believe apple will care about backward compatibility, even the 2.X version on their github is archived and on read-only. so there is no intensions we can hope for<br />
the problem between the two versions is the <code>contrib</code> package was removed.<br />
the top rated answer <a href=""https://stackoverflow.com/questions/55082483/why-can-i-not-import-tensorflow-contrib-i-get-an-error-of-no-module-named-tenso"">here</a> (to this day) suggests to &quot;<em>google the name of the module without the tf.contrib part to know its new location and thus migrating the code accordingly by correcting the import statement.</em>&quot;<br />
now I have access to the contrib package in the <a href=""https://github.com/tensorflow/tensorflow/tree/v1.12.0/tensorflow/contrib"" rel=""nofollow noreferrer"">github repo</a> for tensorflow so there is no need for googling I gues.<br />
the first error at this point is on model.py line 6: <code>from tensorflow.contrib.training import HParams</code>
I simply downloaded it from <a href=""https://github.com/tensorflow/tensorflow/tree/v1.12.0/tensorflow/contrib/training/python/training"" rel=""nofollow noreferrer"">github's repo</a> and pasted it in GPT2's <code>src</code><br />
I was thinking to keep repeating the same trick until HParams.py asks for: <code>from tensorflow.contrib.training.python.training import hparam_pb2</code><br />
hparam_pb2 doesn't exist anywhere, so I don't know how to find this extensioned file with *_pb2.py<br />
if anyone is running on the same problem, kindly advise what's next</p>
","2021-11-03 20:33:57","","2021-11-03 21:55:56","2021-11-03 21:55:56","<python><tensorflow><apple-m1><gpt-2>","0","1","2","1531","","","","","","",""
"71003190","1","8552928","","OpenAI API repeats completions with no variation","<p>I have tried implementing a chatbot in OpenAI with Javascript, using the official OpenAI npm dependency.</p>
<p>The way i have solved it, is that i have an array of chat messages, that gets joined by newlines, and sent as the prompt to the API.</p>
<p>Example:</p>
<pre><code>arr.push(&quot;This is a conversation between you and an AI&quot;)
arr.push(&quot;You: Hello, how are you doing&quot;)
arr.push(&quot;AI: I'm great, how about you?&quot;)
arr.push(&quot;You: I'm good, thanks!&quot;)
</code></pre>
<p>I then push the next question asked to the array, and then push an empty &quot;AI:&quot; string for the OpenAI-endpoint to complete.</p>
<p>The resulting prompt for the API to complete looks like this</p>
<pre><code>```
This is a conversation between you and an AI
You: Hello, how are you doing
AI: I'm great, how about you?
You: I'm good, thanks!
You: How's the weather today?
AI:
```
</code></pre>
<p>The response will then also be pushed to the array, so the conversation can continue... (at this time i only send the last ~20 lines from the array)
However, the problem i have is that the &quot;bot&quot; will start repeating itself, seemingly at random times it will start answering something like &quot;great, how about you?&quot;, and whatever you send as the last question in the prompt, that will be the answer&quot;</p>
<p>Example:</p>
<pre><code>```
This is a conversation between you and an AI
You: Hello, how are you doing
AI: I'm great, how about you?
You: I'm good, thanks!
You: How's the weather today?
AI: It is looking great!
You: That's nice, any plans for today?
AI: It is looking great!
You: What are you talking about?
AI: It is looking great!
```
</code></pre>
<p>The only relevant thing i seem to have found in the documentation is the frequency_penalty and the presence_penalty. However, changing those doesnt seem to do much.</p>
<p>This is the parameters used for the examples above:</p>
<pre><code>    const completion = await openai.createCompletion(&quot;text-davinci-001&quot;, {
        prompt: p,
        max_tokens: 200,
        temperature: 0.6,
        frequency_penalty: 1.5,
        presence_penalty: 1.2,


    });

    return completion.data.choices[0].text.trim()
</code></pre>
<p>I have of course also tried with different combinations of temperatures and penalties.
Is this just a known problem, or am i misunderstanding something?</p>
","2022-02-06 00:06:01","","","2023-01-19 19:35:50","<openai-api><gpt-3>","1","1","3","3466","","","","","","",""
"76489469","1","22083420","","Unsupervised fine-tuning on custom documents after the supervised fine tuning on general question-answers dataset. Will it be useful for GPT-2 model?","<p>I know the formal way of training a GPT2 model on custom documents is to first do semi-supervised fine tuning on the text of the documents followed by supervised fine-tuning on question answers from the same documents.
But the sole purpose of supervised fine-tuning being to acquire style of answering question, is it possible to do supervised fine-tuning on a general dataset, and after that perform unsupervised fine-tuning on our custom text dataset from documents.
This way question answering style can also be acquired by the model along with the advantage of having no need of making a question-answer dataset for the custom documents.</p>
<p>Will it give the desired results?</p>
","2023-06-16 10:51:50","","","2023-06-16 10:51:50","<pre-trained-model><gpt-2><large-language-model><semisupervised-learning><generative-pretrained-transformer>","0","0","0","9","","","","","","",""
"76137940","1","21774119","","While trying to generate text using GPT-2 the custom loss function accesses PAD_TOKEN_ID","<p>While training the custom loss function tries to access the PAD_TOKEN_ID resulting in the below error.50257 is the PAD_TOKEN_ID and the vocab size of GPT-2</p>
<pre><code>InvalidArgumentError: {{function_node __wrapped__SparseSoftmaxCrossEntropyWithLogits_device_/job:localhost/replica:0/task:0/device:CPU:0}} Received a label value of 50257 which is outside the valid range of [0, 50257).  Label values: 389 1976 1437 264 649 24867 1762 503 5633 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 50257 5025...
</code></pre>
<p>In order to remove this I tried masking the Labels and the logits.The labels before masking have a shape of (1260,) and post masking it is (132,). The logits before masking have a shape of (1260, 50257) and post masking it is (63323820,) which is (1260 * 63323820,). The code I am using to mask the logits is as follows:-</p>
<pre><code>shift_logits = logits[..., :-1, :]
shift_logits = tf.reshape(shift_logits, [-1, shift_logits.shape[-1]])
mask_logits = tf.math.logical_not(tf.math.equal(shift_logits, pad_token_id))
mask_logits = tf.cast(mask_logits, dtype=tf.float32)
shift_logits_masked = tf.boolean_mask(shift_logits,mask_logits)
</code></pre>
<p>So there is a primary problem where the label value of 50257 is being accessed and while trying to remove that by masking both logits and labels they fail due to different shapes. This is probably a dumb question however since I am running out of ideas hence it would be really helpful if someone can have a look.</p>
<p>I tried masking both the labels and logits but as mentioned above the size of the labels are (1260,) and logits (1260,50257) hence whenever I am trying to apply tf.boolean_mask then it fails with shape mismatch error. I am expecting to calculate the loss as mentioned below :</p>
<pre><code>loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)
loss = loss_fn(shift_labels_masked, shift_logits_masked)
</code></pre>
<p>since this is text generation in my training loop am passing the labels as input_ids as shown below:</p>
<pre><code>for epoch in range(num_epochs):
  for batch in train_ds:
    input_ids = batch[&quot;input_ids&quot;]
    with tf.GradientTape() as tape:
      outputs = model(input_ids)
      loss = loss_fn(outputs,labels=batch[&quot;input_ids&quot;],pad_token_id=tokenizer.pad_token_id)
      loss = tf.reduce_mean(loss)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    #if optimizer.iterations % 100 == 0:
    print(&quot;Epoch {} Batch {} Loss {:.4f}&quot;.format(epoch + 1, optimizer.iterations.numpy(), loss.numpy()))

</code></pre>
","2023-04-29 19:04:58","","2023-04-29 19:08:56","2023-04-29 19:08:56","<tensorflow><loss-function><gpt-2><text-generation>","0","0","0","58","","","","","","",""
"71333114","1","9557623","","OpenAI retrieve file content","<p>Unable to retrieve the content of file uploaded already.</p>
<p>Kindly suggest what is going wrong? I have tried for each type of file: search, classification, answers, and fine-tune. Files upload successfully but while retrieving content it shows an error.</p>
<pre><code>import openai

openai.api_key = &quot;sk-bbjsjdjsdksbndsndksbdksbknsndksd&quot; # this is wrong key

# Replace file_id with the file's id whose file content is required
content = openai.File.download(&quot;file-5Xs86wEDO5gx8fOitMYArV8r&quot;)

print(content)
</code></pre>
<p>Error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;main.py&quot;, line 6, in &lt;module&gt;
    content = openai.File.download(&quot;file-5Xs86wEDO5gx8fOitMYArV8r&quot;)
  File &quot;/usr/local/lib/python3.8/dist-packages/openai/api_resources/file.py&quot;, line 61, in download
    raise requestor.handle_error_response(
openai.error.InvalidRequestError: Not allowed to download files of purpose: classifications
</code></pre>
","2022-03-03 06:50:00","","2022-03-03 15:24:51","2022-06-04 10:15:35","<python><openai-api><gpt-3>","1","2","0","1190","","","","","","",""
"71371756","1","18390515","","Can i clear up gpu vram in colab","<p>I'm trying to use aitextgen to finetune 774M gpt 2 on a dataset. unfortunately, no matter what i do, training fails because there are only 80 mb of vram available. how can i clear the vram without restarting the runtime and maybe prevent the vram from being full?</p>
","2022-03-06 15:42:13","","","2022-10-12 14:15:38","<google-colaboratory><gpt-2><fine-tune><vram>","2","0","4","6322","","","","","","",""
"75699797","1","13714062","","In NLP, how can we use text query to fetch data from Tabular format data with thousands of rows?","<p>Let's say we have an excel of thousands of rows. We want to fetch some data based on the query entered in pure text format. <strong>Google's TAPAS model does not work on beyond few 100 rows</strong>. Is there any other way to do this.</p>
<p>Let's say there is an input text box and the user has entered the query <strong>&quot;I want to buy Mercedez benz Class C, which is of white color, the price is in the range of 1-2lac US $ and it should be available for purchase in New York state&quot;</strong></p>
<p>Based on this query, the system should fetch the relevant data from Table and give it to user.
The only condition is that the tabular data has thousands of rows, so <strong>TAPAS model may not work</strong> and <strong>GPT3 is too expensive to train and use.</strong></p>
<p><strong>Is there any other way?</strong></p>
","2023-03-10 18:29:24","","","2023-03-10 18:29:24","<elasticsearch><search><nlp><transformer-model><gpt-3>","0","0","0","46","","","","","","",""
"75783029","1","1314732","","PyTorch with Transformer - finetune GPT2 throws index out of range Error","<p>in my Jupiter i have the following code. I can not figure out why this throws a <code>IndexError: index out of range in self</code> error.</p>
<p>here ist the code:</p>
<pre><code>!pip install torch
!pip install torchvision
!pip install transformers
</code></pre>
<pre><code>import torch
from torch.utils.data import Dataset

class MakeDataset(Dataset):
    def __init__(self, tokenized_texts, block_size):
        self.examples = []
        for tokens in tokenized_texts:
            # truncate the tokens if they are longer than block_size
            if len(tokens) &gt; block_size:
                tokens = tokens[:block_size]
            # add padding tokens if the tokens are shorter than block_size
            while len(tokens) &lt; block_size:
                tokens.append(tokenizer.pad_token_id)
            self.examples.append(torch.tensor(tokens, dtype=torch.long))

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, item):
        return self.examples[item]
</code></pre>
<pre><code>from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments, AutoTokenizer, \
    AutoModelWithLMHead, GPT2Tokenizer

# Load the tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2', padding_side='right')
model = AutoModelWithLMHead.from_pretrained('gpt2')

PAD_TOKEN = '&lt;PAD&gt;'
tokenizer.add_special_tokens({'pad_token': PAD_TOKEN})

# Load text corpus
with open(&quot;texts.txt&quot;, encoding=&quot;utf-8&quot;) as f:
    texts = f.read().splitlines()

print(len(texts) , &quot; lines of text.&quot;)

# Tokenize the texts
tokenized_texts = []
for text in texts:
    tokens = tokenizer.encode(text, padding='max_length', truncation='only_first')
    if len(tokens) &gt; 0:
        tokenized_texts.append(tokens)

# gemerate a dataset
dataset = MakeDataset(tokenized_texts, block_size=1024)
print(&quot;Dataset length: &quot;, len(dataset))


# Create a DataCollatorForLanguageModeling object
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Define the training arguments
training_args = TrainingArguments(
    output_dir='./results',  # output directory
    num_train_epochs=5,  # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    save_steps=1000,  # number of steps between saving checkpoints
    save_total_limit=2,  # limit the total amount of checkpoints saved
    prediction_loss_only=True,  # only calculate loss on prediction tokens
    learning_rate=1e-5,  # learning rate
    warmup_steps=500,  # number of warmup steps for learning rate scheduler
    fp16=False  # enable mixed precision training with apex
)

# Create a Trainer object
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset
)

# Train the model
trainer.train()

# Save the trained model
trainer.save_model('./fine-tuned-gpt2')
</code></pre>
<p>The text file at the moment looks very simple:</p>
<pre><code>Hello, my name is Paul.
My cat can sing.
</code></pre>
<p>The full error is:</p>
<pre><code>IndexError                                Traceback (most recent call last)
Cell In[140], line 54
     46 trainer = Trainer(
     47     model=model,
     48     args=training_args,
     49     data_collator=data_collator,
     50     train_dataset=dataset
     51 )
     53 # Train the model
---&gt; 54 trainer.train()
     56 # Save the trained model
     57 trainer.save_model('./fine-tuned-gpt2')

File /opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py:1633, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1628     self.model_wrapped = self.model
   1630 inner_training_loop = find_executable_batch_size(
   1631     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1632 )
-&gt; 1633 return inner_training_loop(
   1634     args=args,
   1635     resume_from_checkpoint=resume_from_checkpoint,
   1636     trial=trial,
   1637     ignore_keys_for_eval=ignore_keys_for_eval,
   1638 )

File /opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py:1902, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1900         tr_loss_step = self.training_step(model, inputs)
   1901 else:
-&gt; 1902     tr_loss_step = self.training_step(model, inputs)
   1904 if (
   1905     args.logging_nan_inf_filter
   1906     and not is_torch_tpu_available()
   1907     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
   1908 ):
   1909     # if loss is nan or inf simply add the average of previous logged losses
   1910     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)

File /opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py:2645, in Trainer.training_step(self, model, inputs)
   2642     return loss_mb.reduce_mean().detach().to(self.args.device)
   2644 with self.compute_loss_context_manager():
-&gt; 2645     loss = self.compute_loss(model, inputs)
   2647 if self.args.n_gpu &gt; 1:
   2648     loss = loss.mean()  # mean() to average on multi-gpu parallel training

File /opt/homebrew/lib/python3.11/site-packages/transformers/trainer.py:2677, in Trainer.compute_loss(self, model, inputs, return_outputs)
   2675 else:
   2676     labels = None
-&gt; 2677 outputs = model(**inputs)
   2678 # Save past state if it exists
   2679 # TODO: this needs to be fixed and made cleaner later.
   2680 if self.args.past_index &gt;= 0:

File /opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/homebrew/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:1075, in GPT2LMHeadModel.forward(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)
   1067 r&quot;&quot;&quot;
   1068 labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
   1069     Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set
   1070     `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`
   1071     are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`
   1072 &quot;&quot;&quot;
   1073 return_dict = return_dict if return_dict is not None else self.config.use_return_dict
-&gt; 1075 transformer_outputs = self.transformer(
   1076     input_ids,
   1077     past_key_values=past_key_values,
   1078     attention_mask=attention_mask,
   1079     token_type_ids=token_type_ids,
   1080     position_ids=position_ids,
   1081     head_mask=head_mask,
   1082     inputs_embeds=inputs_embeds,
   1083     encoder_hidden_states=encoder_hidden_states,
   1084     encoder_attention_mask=encoder_attention_mask,
   1085     use_cache=use_cache,
   1086     output_attentions=output_attentions,
   1087     output_hidden_states=output_hidden_states,
   1088     return_dict=return_dict,
   1089 )
   1090 hidden_states = transformer_outputs[0]
   1092 # Set device for model parallelism

File /opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/homebrew/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py:842, in GPT2Model.forward(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)
    839 head_mask = self.get_head_mask(head_mask, self.config.n_layer)
    841 if inputs_embeds is None:
--&gt; 842     inputs_embeds = self.wte(input_ids)
    843 position_embeds = self.wpe(position_ids)
    844 hidden_states = inputs_embeds + position_embeds

File /opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162, in Embedding.forward(self, input)
    161 def forward(self, input: Tensor) -&gt; Tensor:
--&gt; 162     return F.embedding(
    163         input, self.weight, self.padding_idx, self.max_norm,
    164         self.norm_type, self.scale_grad_by_freq, self.sparse)

File /opt/homebrew/lib/python3.11/site-packages/torch/nn/functional.py:2210, in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   2204     # Note [embedding_renorm set_grad_enabled]
   2205     # XXX: equivalent to
   2206     # with torch.no_grad():
   2207     #   torch.embedding_renorm_
   2208     # remove once script supports set_grad_enabled
   2209     _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 2210 return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)

IndexError: index out of range in self
</code></pre>
<p>Can someone tell me what I have done wrong with the training setup?</p>
<p>++ UPDATE ++</p>
<p>I change the <code>MakeDataset</code> to <code>TextDataset</code> to get a pt tensor back:</p>
<pre><code>class TextDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __getitem__(self, idx):
        return {key: tensor[idx] for key, tensor in self.encodings.items()}

    def __len__(self):
        return len(self.encodings.input_ids)
</code></pre>
<p>the output of <code>print(dataset[0])</code> is:</p>
<pre><code>{'input_ids': tensor([15496,    11,   616,  1438,   318,  3362,    13, 50257, 50257, 50257,
        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
        50257, 50257]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}
</code></pre>
<p>and with</p>
<pre><code>tokenized_texts = tokenizer(texts, padding='max_length', truncation=True, return_tensors=&quot;pt&quot;)
</code></pre>
<p>to pad them to the models length:</p>
<pre><code>6  lines of text.
Dataset length:  6
{'input_ids': tensor([[15496,    11,   616,  ..., 50257, 50257, 50257],
        [ 3666,  3797,   460,  ..., 50257, 50257, 50257],
        [32423,  1408, 46097,  ..., 50257, 50257, 50257],
        [10020,  1044,  6877,  ..., 50257, 50257, 50257],
        [31319,   288,   292,  ..., 50257, 50257, 50257],
        [ 7447, 24408,  8834,  ..., 50257, 50257, 50257]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]])}
</code></pre>
<p>But I still get the same error.
I also deleted all caches.</p>
","2023-03-19 15:24:24","","2023-03-20 08:30:52","2023-03-20 08:30:52","<python><pytorch><huggingface><gpt-2>","0","2","2","286","","","","","","",""
"75860080","1","16923163","","Can I use a single ChatGPT-3 API key for multiple projects simultaneously?","<p>I am a noobie programmer in college and I am trying to learn how to use API keys. I am currently using the ChatGPT-3 API for my Siri personal assistant project, and it's been working well for me so far.</p>
<p>Now, I am developing another application - a bot that can utilize my resume to automatically generate cover letters, and reach out to talent acquisition teams.</p>
<p>Can I use the same ChatGPT-3 API key for both projects simultaneously? Are there any limitations or issues I should be aware of while using a single API key for multiple projects?</p>
","2023-03-27 20:25:14","","2023-03-27 20:39:33","2023-03-27 20:57:09","<api><chatgpt-api><gpt-4>","1","0","0","740","","","","","","",""
"75871333","1","396014","","Using GPT-3 to identify relationships in a corpus","<p>I have a corpus of 15K news articles. I would like to train a GPT model (3 or 4) to ingest these texts and then output how the locations, events, actions, participants, and things described in the texts are related to one another. So if the corpus says John Smith took part in a protest, I'd like to tell me this and what other people took part, how the protest was related to specific locations, etc. Is this possible?</p>
<p>If so can someone please point me in the right direction for learning how to do it? When I do searches all I'm finding is links about using GPT models to give extractive or abstractive summaries of individual texts. I suppose that's related but not quite the same.</p>
","2023-03-28 21:46:12","","2023-04-17 04:52:47","2023-04-17 04:52:47","<nlp><information-extraction><gpt-3><relation-extraction>","1","0","0","33","","","","","","",""
"75871649","1","11481515","","gpt-35-turbo does not memorize messages in PHP","<p>I created a Telegram bot that responds to user messages using the OpenAI GPT API. Everything works fine, but there is an issue. With the gpt-35-turbo model, it is possible to add parameters to memorize messages and track conversations, which I did, but it doesn't work. I am sharing my code for assistance</p>
<p>Here is my code</p>
<pre><code>&lt;?php

class Bot
{
    private $bot_token;

    public function __construct($bot_token)
    {
        $this-&gt;bot_token = $bot_token;
    }

    public function handleUpdate($update)
    {
        $message = $update['message'];
        $chat_id = $message['chat']['id'];
        $text = $message['text'];

        switch ($text) {
            case '/start':
                $this-&gt;sendTypingAction($chat_id);
                sleep(1);
                $welcome_message = &quot;👋 Hello 🐨🌿\nWith me, you can get the answer to EVERYTHING in a few seconds!\n\n🇺🇸 🇬🇧 🇫🇷 🇪🇸 I speak all languages, send /prof to start&quot;;
                $this-&gt;sendMessage($chat_id, $welcome_message);
                break;
            case '/help':
                $this-&gt;sendTypingAction($chat_id);
                sleep(1);
                $help_message = &quot;Need help? Here's what I can do: \n\n/start - To begin\n/help - To get help\n/prof - To start a chat session with a virtual teacher&quot;;
                $this-&gt;sendMessage($chat_id, $help_message);
                break;
            default:
                $this-&gt;sendTypingAction($chat_id);
                sleep(1);
                $response = $this-&gt;generateResponse($text);
                $this-&gt;sendMessage($chat_id, $response);
        }
    }


    private function generateResponse($text)
    {
        $response = &quot;&quot;;

        // Get the user's Telegram ID
        $chat_id = $update['message']['chat']['id'];

        // Get the user's session data (if any)
        session_id(&quot;tg_&quot; . $chat_id);
        session_start();
        $conversation = isset($_SESSION['conversations']) ? $_SESSION['conversations'] : array();

        // detect if there is some mention of date or time in the text:
        // ========================================================
        // Define the keywords to search for
        $keywords = array('time', 'date', 'day is it');

        // Check if the text contains any of the keywords
        $containsKeyword = false;
        $regex = '/\b(' . implode('|', $keywords) . ')\b/i';
        $containsKeyword = preg_match($regex, $text);

        // If the text contains a keyword, call the getDateTime() function
        if ($containsKeyword) {
            $datetime = getDateTime();
            $text = &quot;It is &quot; . $datetime . &quot;. If appropriate, respond to 
            the following in a short sentence: &quot; . $text;
        }

        // set up a session variable to store the last n questions and responses
        $number_of_interactions_to_remember = 10;
        $openai_api_key = 'sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx';

        if (!isset($_SESSION['conversations'])) {
            $_SESSION['conversations'] = array();
        }

        // Remove oldest conversation if the number of interactions &gt;= $number_of_interactions_to_remember
        if (count($_SESSION['conversations']) &gt; $number_of_interactions_to_remember + 1) {
            $_SESSION['conversations'] = array_slice($_SESSION['conversations'], -$number_of_interactions_to_remember, $number_of_interactions_to_remember, true);
        }

        // Prepare the request to the OpenAI API

        $data = array(
            'model' =&gt; 'gpt-3.5-turbo',
            'messages' =&gt; array(
                array(
                    'role' =&gt; 'system',
                    'content' =&gt; 'You are called Chatty McChatface. You give short, friendly responses. '
                )
            )
        );
    
        // Add the last 10 interactions in the request to the OpenAI API
        foreach ($conversation as $conversation_item) {
            foreach ($conversation_item as $message) {
                array_push($data['messages'], array(
                    'role' =&gt; $message['role'],
                    'content' =&gt; $message['content']
                ));
            }
        }
    
        // Add user's last question in request to OpenAI API
        array_push($data['messages'], array(
            'role' =&gt; 'user',
            'content' =&gt; $text
        ));
    
        // Send request to OpenAI API
        $curl = curl_init();
    
        curl_setopt_array($curl, array(
            CURLOPT_URL =&gt; 'https://api.openai.com/v1/chat/completions',
            CURLOPT_RETURNTRANSFER =&gt; true,
            CURLOPT_ENCODING =&gt; '',
            CURLOPT_MAXREDIRS =&gt; 10,
            CURLOPT_TIMEOUT =&gt; 0,
            CURLOPT_FOLLOWLOCATION =&gt; true,
            CURLOPT_HTTP_VERSION =&gt; CURL_HTTP_VERSION_1_1,
            CURLOPT_CUSTOMREQUEST =&gt; 'POST',
            CURLOPT_POSTFIELDS =&gt; json_encode($data),
            CURLOPT_HTTPHEADER =&gt; array(
                'Authorization: Bearer ' . $openai_api_key,
                'Content-Type: application/json'
            ),
        ));

        $response = curl_exec($curl);
        curl_close($curl);
    
        $response = json_decode($response, true);
    
        if (isset($response['choices'][0]['message']['content'])) {
            $content = $response['choices'][0]['message']['content'];
        } else {
            $content = &quot;Something went wrong! ```&quot; . json_encode($response) . &quot;```&quot;;
        }
    
        // Add the last interaction in the user's session
        $new_conversation = array(
            array(
                'role' =&gt; 'user',
                'content' =&gt; $text
            ),
            array(
                'role' =&gt; 'assistant',
                'content' =&gt; $content
            )
        );
    
        if (count($conversation) &gt; $number_of_interactions_to_remember) {
            array_shift($conversation);
        }
    
        array_push($conversation, $new_conversation);
        $_SESSION['conversations'] = $conversation;
    
        return $content;
    }

    private function sendMessage($chat_id, $text)
    {
        $url = &quot;https://api.telegram.org/bot&quot; . $this-&gt;bot_token . &quot;/sendMessage?chat_id=&quot; . $chat_id . &quot;&amp;text=&quot; . urlencode($text);
        file_get_contents($url);
    }

    private function sendTypingAction($chat_id)
    {
        $url = &quot;https://api.telegram.org/bot&quot; . $this-&gt;bot_token . &quot;/sendChatAction?chat_id=&quot; . $chat_id . &quot;&amp;action=typing&quot;;
        file_get_contents($url);
    }
}

$update = json_decode(file_get_contents('php://input'), true);

if (isset($update)) {
    $bot = new Bot('XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX');
    $bot-&gt;handleUpdate($update);
}

</code></pre>
","2023-03-28 22:40:06","","","2023-03-28 22:40:06","<php><telegram-bot><session-variables><openai-api><gpt-4>","0","0","0","187","","","","","","",""
"75915524","1","15525882","","How can I put a python program with specific module dependencies into an HTML page?","<p>I am trying to make a modified GPT model, designed with Python, available for questions on an HTML page. I have tried using PyScript but I do not know how to give it access to the modules that I want.</p>
<p>Here is my python code in the file gpt.py:</p>
<pre><code>from llama_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext
from langchain import OpenAI
from IPython.display import Markdown, display

def construct_index(directory_path):
    # set maximum input size
    max_input_size = 4096
    # set number of output tokens
    num_outputs = 2000
    # set maximum chunk overlap
    max_chunk_overlap = 20
    # set chunk size limit
    chunk_size_limit = 600 

    # define prompt helper
    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)

    # define LLM
    llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name=&quot;text-davinci-003&quot;, max_tokens=num_outputs))
 
    documents = SimpleDirectoryReader(directory_path).load_data()
    
    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)
    index = GPTSimpleVectorIndex.from_documents(documents, service_context=service_context)

    index.save_to_disk('index.json')

    return index

def ask_ai():
    index = GPTSimpleVectorIndex.load_from_disk('index.json')
    while True: 
        query = input(&quot;What do you want to ask? &quot;)
        response = index.query(query)
        print(response)
        display(Markdown(f&quot;Response: &lt;b&gt;{response.response}&lt;/b&gt;&quot;))

os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;myapikey&quot;
construct_index(&quot;context_data/data&quot;)
ask_ai()
</code></pre>
<p>I tried using PyScript in the following HTML code:</p>
<pre><code>&lt;html&gt;
    &lt;head&gt;
      &lt;link rel=&quot;stylesheet&quot; href=&quot;https://pyscript.net/latest/pyscript.css&quot; /&gt;
      &lt;script defer src=&quot;https://pyscript.net/latest/pyscript.js&quot;&gt;&lt;/script&gt;
    &lt;/head&gt;

  &lt;body&gt;
    &lt;py-config type=&quot;toml&quot;&gt;
        packages = [&quot;llama_index&quot;, &quot;langchain&quot;, &quot;IPython&quot;]

        [[fetch]]
        files = [&quot;./gpt.py&quot;]
    &lt;/py-config&gt;
    &lt;py-script&gt;
      from llama_index import SimpleDirectoryReader, GPTListIndex, readers, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext
      from langchain import OpenAI
      from IPython.display import Markdown, display
      
      os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;myapikey&quot;
      construct_index(&quot;context_data/data&quot;)
      ask_ai()
    &lt;/py-script&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>But then I got the error:</p>
<blockquote>
<p>(PY1001): Unable to install package(s) 'llama_index,langchain,IPython'. Reason: Can't find a pure Python 3 Wheel for package(s) 'llama_index,langchain,IPython'. See: <a href=""https://pyodide.org/en/stable/usage/faq.html#micropip-can-t-find-a-pure-python-wheel"" rel=""nofollow noreferrer"">https://pyodide.org/en/stable/usage/faq.html#micropip-can-t-find-a-pure-python-wheel</a> for more information.</p>
</blockquote>
<p>So I followed that link and it told me to make sure all those modules have *py3-none-any.whl files, which they did. Those can be found here:</p>
<p><a href=""https://pypi.org/project/langchain/#files"" rel=""nofollow noreferrer"">https://pypi.org/project/langchain/#files</a></p>
<p><a href=""https://pypi.org/project/gpt-index/#files"" rel=""nofollow noreferrer"">https://pypi.org/project/gpt-index/#files</a></p>
<p><a href=""https://pypi.org/project/ipython/#files"" rel=""nofollow noreferrer"">https://pypi.org/project/ipython/#files</a></p>
<p>It is possible that <code>llama_index</code> should be called <code>gpt-index</code>, but that doesn't change the error message.</p>
<p>How can I make give my HTML page access to these modules? Thank you!</p>
","2023-04-03 01:40:34","","2023-04-03 11:43:31","2023-04-03 11:43:31","<python><html><gpt-3><pyscript><llama-index>","0","3","0","187","","","","","","",""
"75951366","1","21582404","","How to save the gpt-2-simple model after training?","<p>I trained the <a href=""https://github.com/minimaxir/gpt-2-simple"" rel=""nofollow noreferrer"">gpt-2-simple</a> chat bot model but I am unable to save it. It's important for me to download the trained model from colab because otherwise I have to download the 355M model each time (see below code).</p>
<p>I tried various methods to save the trained model (like <code>gpt2.saveload.save_gpt2()</code>), but none worked and I don't have any more ideas.</p>
<p>My training code:</p>
<pre class=""lang-py prettyprint-override""><code>%tensorflow_version 2.x
!pip install gpt-2-simple

import gpt_2_simple as gpt2
import json

gpt2.download_gpt2(model_name=&quot;355M&quot;)

raw_data = '/content/drive/My Drive/data.json'

with open(raw_data, 'r') as f:
    df =json.load(f)

data = []

for x in df:
    for y in range(len(x['dialog'])-1):
        a = '[BOT] : ' + x['dialog'][y+1]['text']
        q = '[YOU] : ' + x['dialog'][y]['text']
        data.append(q)
        data.append(a)

with open('chatbot.txt', 'w') as f:
     for line in data:
        try:
            f.write(line)
            f.write('\n')
        except:
            pass

file_name = &quot;/content/chatbot.txt&quot;

sess = gpt2.start_tf_sess()

gpt2.finetune(sess,
              dataset=file_name,
              model_name='355M',
              steps=500,
              restore_from='fresh',
              run_name='run1',
              print_every=10,
              sample_every=100,
              save_every=100
              )

while True:
  ques = input(&quot;Question : &quot;)
  inp = '[YOU] : '+ques+'\n'+'[BOT] :'
  x = gpt2.generate(sess,
                length=20,
                temperature = 0.6,
                include_prefix=False,
                prefix=inp,
                nsamples=1,
                )
</code></pre>
","2023-04-06 15:42:27","","2023-04-11 11:37:39","2023-04-11 11:37:39","<machine-learning><nlp><artificial-intelligence><gpt-2><text-generation>","1","0","2","176","","","","","","",""
"76226113","1","21878965","","How to use large prompt for GPT-3 models in python?","<p>I am going to extract information from docx file using OpenAI GPT-3 model in python.
But the total length of prompts is too big than GPT-3 provided.</p>
<p>If you have any opinion about this problem, please help me.
Thanks</p>
<p>I tried it to split several chunks, but it is not useful.
Because I need whole information of chunks, not each chunks.</p>
","2023-05-11 09:27:03","","","2023-05-11 09:38:04","<python><gpt-3>","1","0","-3","110","","","","","","",""
"76278747","1","11725056","","Out of 4 methods for Document Question Answering in LangChain, which one if the fastest and why (ignoring the LLM model used)?","<p><strong>NOTE:</strong>: My <em>reference document</em> data changes periodically so if I use Embedding Vector space method, I have to Modify the embedding, say once a day</p>
<p>I want to know these factors so that I can design my system to compensate my <em>reference document data</em> generation latency with creating embedding beforehand using Cron Jobs</p>
<p>There are 4 methods in <code>LangChain</code> using which we can retrieve the QA over Documents. More or less they are wrappers over one another.</p>
<ol>
<li><code>load_qa_chain</code> uses Dynamic Document each time it's called</li>
<li><code>RetrievalQA</code> get it from the Embedding space of document</li>
<li><code>VectorstoreIndexCreator</code> is the wrapper of <code>2.</code></li>
<li><code>ConversationalRetrievalChain</code> uses Embedding Space and it has a memory and chat history too.</li>
</ol>
<p>What is the difference between <code>1,3,4</code> in terms of speed?</p>
<ol>
<li><p>If I use embedding DB, would it be faster than <code>load_qa_chain</code> assuming that making Embedding of the document beforehand (like in <code>2,3</code>)helps or is it same because the time taken for a 50 words Prompt is same as Time taken for a 2000 words (Document Text + Prompt in <code>load_qa_chain</code>) ?</p>
</li>
<li><p>Will the speed be affected If I use <code>ConversationalRetrievalChain</code> with or without <code>memory</code> and <code>chat_history</code>?</p>
</li>
</ol>
","2023-05-18 07:45:43","","2023-05-18 07:52:31","2023-05-31 06:33:07","<python><openai-api><gpt-3><langchain>","1","0","-1","900","","","","","","",""
"76289498","1","12159826","","AttributeError: 'tuple' object has no attribute 'is_single_input","<p>I am trying to use langchain Agents in order to get answers for the questions asked using API, but facing error &quot;AttributeError: 'tuple' object has no attribute 'is_single_input'&quot;. Following is the code and error. Open for solution and suggestions.</p>
<pre><code>from langchain.tools import StructuredTool
from langchain.agents import initialize_agent
from langchain.llms import OpenAI
import requests
</code></pre>
<h1>Step 1: Implement your API function or class</h1>
<pre><code>def process_document(document):
    # Process the document using your API logic
    url = 'api'

    data = {'file': open(document, 'rb')}

    response = requests.post(url, auth=requests.auth.HTTPBasicAuth('dfg', ''), files=data)
    return response
</code></pre>
<h1>Step 2: Create a Tool</h1>
<pre><code>tool = StructuredTool.from_function(process_document,description=&quot;Process the document using the API&quot;)
</code></pre>
<h1>Step 3: Initialize the Language Model</h1>
<pre><code>llm = OpenAI(temperature=0, model_name=&quot;text-davinci-003&quot;, openai_api_key=&quot;key&quot;)
</code></pre>
<h1>Step 4: Initialize the Agent</h1>
<pre><code>agent = initialize_agent(tool, llm)
</code></pre>
<h1>Step 5: Use the Agent</h1>
<pre><code>document = &quot;&quot;  # Provide the document to be processed
result = agent.process(document)  # Process the document using the agent and the API
question = &quot;What is Registration number and registration date?&quot;  # Provide the question to ask about    the processed result
answer = agent.generate(question)  # Generate an answer to the question using the agent
</code></pre>
<p>While implementing this I am facing following error :</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-3-dea540cbc5b0&gt; in &lt;cell line: 28&gt;()
     26 
     27 # Step 4: Initialize the Agent
---&gt; 28 agent = initialize_agent(tool, llm)
     29 
     30 # Step 5: Use the Agent

3 frames
/usr/local/lib/python3.10/dist-packages/langchain/agents/utils.py in validate_tools_single_input(class_name, tools)
      7     &quot;&quot;&quot;Validate tools for single input.&quot;&quot;&quot;
      8     for tool in tools:
----&gt; 9         if not tool.is_single_input:
     10             raise ValueError(
     11                 f&quot;{class_name} does not support multi-input tool {tool.name}.&quot;

AttributeError: 'tuple' object has no attribute 'is_single_input'
 
</code></pre>
","2023-05-19 13:22:39","","2023-05-20 04:00:24","2023-05-26 01:41:09","<python><artificial-intelligence><openai-api><langchain><gpt-4>","2","0","0","236","","","","","","",""
"76421921","1","13057722","","Using GPT 4 or GPT 3.5 with SQL Database Agent throws OutputParserException: Could not parse LLM output:","<p>I am using the <a href=""https://python.langchain.com/en/latest/modules/agents/toolkits/examples/sql_database.html"" rel=""nofollow noreferrer"">SQL Database Agent</a> to query a postgres database. I want to use gpt 4 or gpt 3.5 models in the OpenAI llm passed to the agent, but it says I must use ChatOpenAI. Using ChatOpenAI throws parsing errors.</p>
<p>The reason for wanting to switch models is reduced cost, better performance and most importantly - token limit. The max token size is 4k for 'text-davinci-003' and I need at least double that.</p>
<p>Here is my code</p>
<pre><code>from langchain.agents.agent_toolkits import SQLDatabaseToolkit
from langchain.sql_database import SQLDatabase
from langchain.agents import create_sql_agent
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
import os

os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;&quot;
db = SQLDatabase.from_uri(
    &quot;postgresql://&lt;my-db-uri&gt;&quot;,
    engine_args={
        &quot;connect_args&quot;: {&quot;sslmode&quot;: &quot;require&quot;},
    },
)

llm = ChatOpenAI(model_name=&quot;gpt-3.5-turbo&quot;)
toolkit = SQLDatabaseToolkit(db=db, llm=llm)

agent_executor = create_sql_agent(
    llm=llm,
    toolkit=toolkit,
    verbose=True,
)

agent_executor.run(&quot;list the tables in the db. Give the answer in a table json format.&quot;)
</code></pre>
<p>When I do, it throws an error in the chain midway saying</p>
<pre><code>&gt; Entering new AgentExecutor chain...
Traceback (most recent call last):
  File &quot;/home/ramlah/Documents/projects/langchain-test/sql.py&quot;, line 96, in &lt;module&gt;
    agent_executor.run(&quot;list the tables in the db. Give the answer in a table json format.&quot;)
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/chains/base.py&quot;, line 236, in run
    return self(args[0], callbacks=callbacks)[self.output_keys[0]]
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/chains/base.py&quot;, line 140, in __call__
    raise e
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/chains/base.py&quot;, line 134, in __call__
    self._call(inputs, run_manager=run_manager)
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/agents/agent.py&quot;, line 953, in _call
    next_step_output = self._take_next_step(
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/agents/agent.py&quot;, line 773, in _take_next_step
    raise e
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/agents/agent.py&quot;, line 762, in _take_next_step
    output = self.agent.plan(
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/agents/agent.py&quot;, line 444, in plan
    return self.output_parser.parse(full_output)
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/agents/mrkl/output_parser.py&quot;, line 51, in parse
    raise OutputParserException(
langchain.schema.OutputParserException: Could not parse LLM output: `Action: list_tables_sql_db, ''`
</code></pre>
<p>Please help. Thanks!</p>
","2023-06-07 09:40:43","","2023-06-07 09:48:48","2023-06-23 10:34:21","<openai-api><gpt-3><langchain><gpt-4>","1","0","1","389","","","","","","",""
"74262671","1","20378708","","openai using python, returned length problem in ""text-davinci-002"" model","<p>I am trying to use <strong>&quot;text-davinci-002&quot;</strong> model using <strong>&quot;openai&quot;</strong>. The returned text is a single sentence while the same sentence returns a full text in openAI official example.
This is the code used:</p>
<pre><code>response = openai.Completion.create(
            model=&quot;email to ask for a promotion&quot;,
            prompt=userPrompt,
            temperature=0.76
            )
</code></pre>
<p>The output of this code is:
*Hello [Employer],</p>
<p>I would like to request a promotion*</p>
<p>while the same sentence in OpenAI website <a href=""https://beta.openai.com/docs/quickstart/introduction"" rel=""nofollow noreferrer"">here</a> outputs:
*
Hello [Employer],</p>
<p>I would like to request a promotion to the position of [position you want]. I have been with the company for [amount of time] and I feel that I have the experience and qualifications needed for the position.*</p>
<p>Thank you in advance</p>
","2022-10-31 11:35:48","","","2022-12-27 18:52:43","<python><openai-api><gpt-3>","1","1","1","885","","","","","","",""
"61121982","1","13268010","64542919","Asking gpt-2 to finish sentence with huggingface transformers","<p>I am currently generating text from left context using the example script <code>run_generation.py</code> of the huggingface transformers library with gpt-2:</p>

<pre class=""lang-sh prettyprint-override""><code>$ python transformers/examples/run_generation.py \
  --model_type gpt2 \
  --model_name_or_path gpt2 \
  --prompt ""Hi, "" --length 5

=== GENERATED SEQUENCE 1 ===
Hi,  could anyone please inform me
</code></pre>

<p>I would like to generate short complete sentences. Is there any way to tell the model to finish a sentence before <code>length</code> words?</p>

<hr>

<p>Note: I don't mind changing model, but would prefer an auto-regressive one.</p>
","2020-04-09 13:12:13","","2020-11-29 11:58:17","2022-04-30 02:20:17","<nlp><pytorch><huggingface-transformers><gpt-2>","2","3","4","1922","","2","4777851","<p>Unfortunately there is no way to do so. You can set the <code>length</code> parameter to a greater value and then just discard the incomplete part at the end.</p>
<p>Even GPT3 doesn't support completing a sentence before a specific <code>length</code>. GPT3 support &quot;sequences&quot; though. Sequences force the model to stop when certain condition is fulfilled. You can find more information about in thi <a href=""https://www.twilio.com/blog/ultimate-guide-openai-gpt-3-language-model"" rel=""nofollow noreferrer"">article</a></p>
","2020-10-26 18:25:41","1","3"
"68460080","1","15887188","68481504","how can i use openai's gpt 3 to find alternate spellings of bad words?","<p>so, i am making an auto mod discord bot that finds alternate spellings of bad words. i tried using regex to find them but found many many false positives. so i thought about using openai's gpt-3 to do so, as i saw a screenshot of someone using it for what appears to be finding alternate spellings.</p>
<p>the screenshot:</p>
<p><a href=""https://i.stack.imgur.com/CfTUv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CfTUv.png"" alt=""the screenshot:"" /></a></p>
<p>unfortunately, i don't know how exactly they made gpt-3 do this, and how something like this can be used in an application like a discord bot.</p>
<p>can someone please tell me how you can use gpt-3 to find alternate spellings of words?</p>
<p>any help would be appreciated! thank you!</p>
","2021-07-20 19:00:02","","","2023-04-24 10:39:23","<openai-api><gpt-3>","2","0","0","882","","2","11333098","<p>I am not sure if you are looking for prompt/settings. However, based on my experience (3-4 months) I would use a few-shot approach prompt such this one:</p>
<pre><code>Check spelling and return the corrected word:
Word: nawty
Returns: naughty
Word: rigt
Returns: right
Word: stakoverflow
Returns: 
</code></pre>
<p>I guess that a high temperature and no penalties will do a good job. Also, keep trying different engines and see how it behaves. Curie-instruct-beta should do it.</p>
","2021-07-22 08:23:31","1","3"
"69773687","1","8655577","70045434","AttributeError: 'GPT2Model' object has no attribute 'gradient_checkpointing'","<p>I am trying to load a GPT2 fine tuned model in flask initially. The model is being loaded during the init functions using:</p>
<pre><code>app.modelgpt2 = torch.load('models/model_gpt2.pt', map_location=torch.device('cpu'))
app.modelgpt2tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
</code></pre>
<p>But while performing the prediction task as followed in the snippet below:</p>
<pre><code>from flask import current_app
input_ids = current_app.modelgpt2tokenizer.encode(&quot;sample sentence here&quot;, return_tensors='pt')
sample_outputs = current_app.modelgpt2.generate(input_ids,
                                                do_sample=True,
                                                top_k=50,
                                                min_length=30,
                                                max_length=300,
                                                top_p=0.95,
                                                temperature=0.7,
                                                num_return_sequences=1)
</code></pre>
<p>It throws the following error as mentioned in the question:
<strong>AttributeError: 'GPT2Model' object has no attribute 'gradient_checkpointing'</strong></p>
<blockquote>
<p>The error trace is listed starting from the <code>model.generate</code> function:
File &quot;/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py&quot;, line 28, in decorate_context
return func(*args, **kwargs)</p>
</blockquote>
<blockquote>
<p>File &quot;/venv/lib/python3.8/site-packages/transformers/generation_utils.py&quot;, line 1017, in generate
return self.sample(</p>
</blockquote>
<blockquote>
<p>File &quot;/venv/lib/python3.8/site-packages/transformers/generation_utils.py&quot;, line 1531, in sample
outputs = self(</p>
</blockquote>
<blockquote>
<p>File &quot;/venv/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1102, in _call_impl
return forward_call(*input, **kwargs)</p>
</blockquote>
<blockquote>
<p>File &quot;/venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py&quot;, line 1044, in forward
transformer_outputs = self.transformer(</p>
</blockquote>
<blockquote>
<p>File &quot;/venv/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1102, in _call_impl
return forward_call(*input, **kwargs)</p>
</blockquote>
<blockquote>
<p>File &quot;/venv/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py&quot;, line 861, in forward
print(self.gradient_checkpointing)</p>
</blockquote>
<blockquote>
<p>File &quot;/venv/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1177, in <strong>getattr</strong>
raise AttributeError(&quot;'{}' object has no attribute '{}'&quot;.format(</p>
</blockquote>
<blockquote>
<p>AttributeError: 'GPT2Model' object has no attribute 'gradient_checkpointing'</p>
</blockquote>
<p>Checked with <code>modeling_gpt2.py</code>, by default <code>self.gradient_checkpointing</code> is set <code>False</code> in the constructor of the class.</p>
","2021-10-29 19:04:17","","2021-10-29 23:01:50","2022-03-15 04:33:40","<python><pytorch><gpt-2><openai-api>","2","0","1","1585","","2","8655577","<p>This issue is found to be occurring only if the framework is run using venv or deployment frameworks like uWSGI or gunicorn.
It is resolved when transformers version 4.10.0 is used instead of the latest package.</p>
","2021-11-20 11:21:43","0","0"
"64722585","1","2330237","","GPT-3 Prompts for Sentence-Level and Paragraph-Level Text Summarization / Text Shortening / Text Rewriting","<p>Need effective prompts for GPT-3 that can accomplish this 'programming' task.  Creating effective GPT-3 prompts has essentially become a new form of programming (giving a computer instructions to complete a task).</p>
<p>There are getting to be repositories for the nascient, growing 'programming' language of GPT-3 prompts, eg at:</p>
<p><a href=""https://github.com/martonlanga/gpt3-prompts"" rel=""nofollow noreferrer"">https://github.com/martonlanga/gpt3-prompts</a></p>
<p><a href=""http://gptprompts.wikidot.com/start"" rel=""nofollow noreferrer"">http://gptprompts.wikidot.com/start</a></p>
<p><a href=""https://github.com/wgryc/gpt3-prompts"" rel=""nofollow noreferrer"">https://github.com/wgryc/gpt3-prompts</a></p>
<p>See a working example below, which works ok, but doesn't really address the need, and isn't adequately reliable.</p>
<p>This is an important, new, and quickly growing area.</p>
<p>Seeking prompts that will accomplish the goal in the Title: summarizing / shortening sentences and / or paragraphs with high reliability, without creating nonsense.</p>
<p>Please, reviewers, this is an important question to many people... don't be narrow-minded and decided that because GPT-3 prompts aren't (yet) a 'traditional' computer language they don't have a place here.</p>
<p>Thank you for your help</p>
<p>Example GPT-3 Prompt:</p>
<p>Please summarize the article below.
&quot;&quot;&quot;
Microsoft in talks to buy TikTok
Negotiations for ByteDance-owned social media group come as Trump threatens action</p>
<p>Microsoft has held talks to acquire TikTok, whose Chinese owner ByteDance faces mounting pressure from the US government to sell the video sharing app or risk being blacklisted in the country, said people briefed on the matter.</p>
<p>... the rest of the article...
&quot;&quot;&quot;</p>
<p>Q: Could you please summarize the article above in three sentences?</p>
","2020-11-06 22:50:25","2023-01-05 13:45:23","2020-11-29 11:47:16","2023-01-03 20:53:58","<text><artificial-intelligence><summarization><gpt-3>","3","1","4","5489","0","","","","","",""
"65097515","1","14270840","","How to use GPT-2 for topic modelling?","<p>I want to generate topics and subtopics from a corpus. It would be great if someone could share the python code.</p>
","2020-12-01 19:48:36","","","2020-12-03 07:25:01","<nlp><topic-modeling><bert-language-model><gpt-2>","1","0","2","921","","","","","","",""
"65159768","1","9824768","","On-the-fly tokenization with datasets, tokenizers, and torch Datasets and Dataloaders","<p>I have a question regarding &quot;on-the-fly&quot; tokenization. This question was elicited by reading the &quot;How to train a new language model from scratch using Transformers and Tokenizers&quot; <a href=""https://huggingface.co/blog/how-to-train"" rel=""nofollow noreferrer"">here</a>. Towards the end there is this sentence: &quot;If your dataset is very large, you can opt to load and tokenize examples on the fly, rather than as a preprocessing step&quot;. I've tried coming up with a solution that would combine both <code>datasets</code> and <code>tokenizers</code>, but did not manage to find a good pattern.</p>
<p>I guess the solution would entail wrapping a dataset into a Pytorch dataset.</p>
<p>As a concrete example from the <a href=""https://huggingface.co/transformers/custom_datasets.html"" rel=""nofollow noreferrer"">docs</a></p>
<pre class=""lang-py prettyprint-override""><code>import torch

class SquadDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        # instead of doing this beforehand, I'd like to do tokenization on the fly
        self.encodings = encodings 

    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}

    def __len__(self):
        return len(self.encodings.input_ids)

train_dataset = SquadDataset(train_encodings)
</code></pre>
<p>How would one implement this with &quot;on-the-fly&quot; tokenization exploiting the vectorized capabilities of tokenizers?</p>
","2020-12-05 17:15:55","","2020-12-08 21:57:02","2021-03-04 09:49:05","<huggingface-transformers><huggingface-tokenizers><gpt-2>","1","0","5","2589","0","","","","","",""
"76436535","1","9785742","","I try to use GPTJ-lora model to generate txt, but the max-length of the generated text seemed to be 20 tokens. How to make it longer","<pre><code>import transformers
#from transformers import AutoModelWithHeads
model.load_adapter(&quot;./&quot;,adapter_name='lora')
peft_model_path=&quot;./&quot;
tokenizer = AutoTokenizer.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.pad_token_id = tokenizer.eos_token_id
input_text = &quot;フィリピンから10年前来日しました。日本の生活にも慣れ、将来も日本に住み続けたいと考えています。そこで、日本国籍を取得したいと思うのですが、どういう要件が必要でしょうか？&quot;
input_ids = tokenizer(input_text, return_tensors='pt')
generated_output = model.generate(**input_ids, max_length=400)
output = tokenizer.decode(generated_output[0], skip_special_tokens=True)
print(output)
</code></pre>
<p>The result is like this:</p>
<p>日本国籍を取得するには、日本国籍法により、日本国籍を取得するには、日本国籍法により、日本国籍を取得するには、日本国籍法により、日本国籍を取得するには、日本国籍法により、日本国籍を取得するには、日本国籍法により、日本国籍を取得するには、日本国籍法により、日本国籍を取得するには、日本国籍法により、日本国籍を取得するには、日本国籍法</p>
<p>you can see that the generated text repeated a 20-tokens text &quot;日本国籍を取得するには、日本国籍法により、日本国籍を取得するには、日本国籍法により、日本国籍を取得するには、日本国籍法により、日本国籍を取得するには、日本国籍法により、日本国籍を取得するには、日本国籍法により、日本国籍を取得するには、日本国籍法により、日本国籍を取得するには、日本国籍法により、日本国籍を取得するには、日本国籍法&quot;</p>
<p>I　try to use GPTJ-lora model to generate txt, wish to get a 400-tokens generated text。But the results is not so. Sometimes, the response can be over 40-50 tokens, but it can not be any longer. Otherwise, it will repeat the last sentence again and again. sometimes, the length of the repeated sentence is more than 20 tokens, but sometimes, it is shorter than 20 tokens. what is the problem?
The base model is GPT-J, and is trained by databricks-dolly-15k.jsonl, which has been modified into Japanese.</p>
","2023-06-09 00:49:42","","2023-06-09 02:34:15","2023-06-09 02:34:15","<gpt-3><lora><llm><peft>","0","0","0","26","","","","","","",""
"76451232","1","22056563","","Why does this bundled app not work when the python script does work?","<p>I have essentially zero coding experience. I'm using GPT4 prompts to help me put together a simple print utility that generates a barcode and adds 1 to the last code printed in order for us to organize our inventory. The script works great when run with IDLE but when it's packaged it does not generate a barcode image and gives an error in cmd. GPT has tried numerous ways around this always resulting in the same error.</p>
<p>It has concluded that pyinstaller must be incompatible with the barcode pip(?) Here's the full script:</p>
<pre><code>import tkinter as tk
from barcode import Code39
from barcode.writer import ImageWriter
from PIL import Image, ImageFont, ImageDraw
import sys
import os
import time
import threading

def resource_path(relative_path):
    try:
        base_path = sys._MEIPASS
    except Exception:
        base_path = os.path.abspath(&quot;.&quot;)
    return os.path.join(base_path, relative_path)

font_path = resource_path('resources/ARLRDBD.ttf')


def get_next_number(filename):
    try:
        with open(filename, 'r') as f:
            lines = f.read().splitlines()  # splitlines method strips newline characters
        last_number = int(lines[-1])
    except FileNotFoundError:
        last_number = 54999  # start from 55000
    next_number = last_number + 1
    with open(filename, 'a') as f:
        f.write(f&quot;{next_number}\n&quot;)  # append the next number to the file
    return last_number, next_number

def generate_barcode(number):
    code39 = Code39(str(number), writer=ImageWriter(), add_checksum=False)

    # Create the directory if it does not exist
    directory = 'barcodes'
    if not os.path.exists(directory):
        os.makedirs(directory)

    filename = os.path.join(directory, f&quot;barcode_{number}.jpeg&quot;)  # Save the file in the directory

    barcode_img = code39.save(filename)

    img = Image.open(barcode_img)

    width_mm = 62
    height_mm = 29
    width_pixel = int(width_mm * (300 / 25.4))  
    height_pixel = int(height_mm * (300 / 25.4)) 
    img_resized = img.resize((width_pixel, height_pixel))

    draw = ImageDraw.Draw(img_resized)
    font = ImageFont.load_default()

    text_bbox = draw.textbbox((0, 0), str(number), font=font)
    text_width = text_bbox[2]
    x_position = (img_resized.width - text_width) / 2

    draw.text((x_position, img_resized.height - 50), str(number), font=font, fill=255)

    img_resized.save(barcode_img)
    return barcode_img


def print_file(barcode_img):
    time.sleep(1)  # let the system recognize the new file
    os.startfile(barcode_img, 'print')
    threading.Timer(60, os.remove, [barcode_img]).start()  # delay before deleting the file

def main():
    last_number, next_number = get_next_number(&quot;last_number.txt&quot;) 
    barcode_img = generate_barcode(next_number)
    print_file(barcode_img)

    # Adding 1 to the numbers displayed in the labels
    last_label.config(text=&quot;Last: &quot; + str(last_number + 1))
    next_label.config(text=&quot;Next: &quot; + str(next_number + 1))


root = tk.Tk()
root.geometry('400x400')  # Set the window size to 400x400
root.title(&quot;Auction Barcode Print Button&quot;)  # Set the window title

# Create the labels with default text
last_label = tk.Label(root, text=&quot;Last: &quot;, font=('Arial', 14))
next_label = tk.Label(root, text=&quot;Next: &quot;, font=('Arial', 14))

# Place the labels in the window
last_label.pack(pady=(50, 10))
next_label.pack(pady=(10, 20))

button_frame = tk.Frame(root, width=200, height=200)  # Frame to hold the button
button_frame.pack_propagate(0)  # prevents the frame to shrink
button_frame.pack(pady=(20,20))

button = tk.Button(button_frame, text=&quot;Print Barcode&quot;, command=main, relief=tk.RAISED)  
button.pack(fill=tk.BOTH, expand=1)  # Button fills the entire frame

root.mainloop()
</code></pre>
<p>And here's the error from cmd when this script is packaged and run:</p>
<pre><code>Exception in Tkinter callback
Traceback (most recent call last):
  File &quot;tkinter\__init__.py&quot;, line 1948, in __call__
  File &quot;Auction_Label_4.py&quot;, line 72, in main
  File &quot;Auction_Label_4.py&quot;, line 42, in generate_barcode
  File &quot;barcode\base.py&quot;, line 65, in save
  File &quot;barcode\codex.py&quot;, line 74, in render
  File &quot;barcode\base.py&quot;, line 105, in render
  File &quot;barcode\writer.py&quot;, line 265, in render
  File &quot;barcode\writer.py&quot;, line 439, in _paint_text
  File &quot;PIL\ImageFont.py&quot;, line 1008, in truetype
  File &quot;PIL\ImageFont.py&quot;, line 1005, in freetype
  File &quot;PIL\ImageFont.py&quot;, line 255, in __init__
OSError: cannot open resource
</code></pre>
<p>The script does however generate the 'last_number' txt file and the 'barcodes' folder, it just doesn't generate a barcode.</p>
<p>Thank you for giving this post a look.</p>
","2023-06-11 15:43:09","","","2023-06-11 15:48:41","<python><printing><pyinstaller><barcode><gpt-4>","1","0","0","22","","","","","","",""
"76451783","1","14552928","","AuthenticationError: <empty message> in OpenAPI api","<p>I have been trying to use langchain library's <code>ChatOpenAI</code>, I pip installed langchain, and imported <code>ChatOpenAI</code></p>
<p>I'm running my code on colab so I set my the openAI's api key as:</p>
<pre><code>%env OPENAI_API_KEY= my_api_key
</code></pre>
<p>now when I try to initialize <code>ChatOpenAI</code> as follows, it runs without error:</p>
<pre><code>chat = ChatOpenAI(temperature=0.0,openai_organization='Personal')
chat
</code></pre>
<p>I get the following as the result:</p>
<pre><code>ChatOpenAI(verbose=False, callbacks=None, callback_manager=None, client=&lt;class 'openai.api_resources.chat_completion.ChatCompletion'&gt;, model_name='gpt-3.5-turbo', temperature=0.0, model_kwargs={}, openai_api_key='my_api_key', openai_api_base='', openai_organization='Personal', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None)
</code></pre>
<p>But when I try to run it as :</p>
<pre><code>customer_response = chat(customer_messages)

</code></pre>
<p>It throws the following error:</p>
<pre><code>---------------------------------------------------------------------------
AuthenticationError                       Traceback (most recent call last)
&lt;ipython-input-64-a39359b08f39&gt; in &lt;cell line: 1&gt;()
----&gt; 1 customer_response = chat(customer_messages)

17 frames
/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py in _interpret_response_line(self, rbody, rcode, rheaders, stream)
    761         stream_error = stream and &quot;error&quot; in resp.data
    762         if stream_error or not 200 &lt;= rcode &lt; 300:
--&gt; 763             raise self.handle_error_response(
    764                 rbody, rcode, resp.data, rheaders, stream_error=stream_error
    765             )

AuthenticationError: &lt;empty message&gt;
</code></pre>
","2023-06-11 17:55:46","","","2023-06-13 17:38:13","<python><api><openai-api><gpt-3>","1","0","1","256","","","","","","",""
"76470976","1","21656479","","What replacement to gpt-3.5-turbo stream mode in gpt-3.5-turbo-0613?","<p>gpt-3.5-turbo-0613 is no longer supported stream mode
(<a href=""https://github.com/n3d1117/chatgpt-telegram-bot/"" rel=""nofollow noreferrer"">https://github.com/n3d1117/chatgpt-telegram-bot/</a>)</p>
<p>Most popular standart GPT3.5 telegram bot fails when 3.5 replaced with 3.5-0613 model
Like this</p>
<pre><code>File &quot;/home/a/Sh/dis/chatgpt-telegram-bot/bot/openai_helper.py&quot;, line 128, in __common_get_chat_response
    token_count = self.__count_tokens(self.conversations[chat_id])
  File &quot;/home/a/Sh/dis/chatgpt-telegram-bot/bot/openai_helper.py&quot;, line 275, in __count_tokens
    raise NotImplementedError(f&quot;&quot;&quot;num_tokens_from_messages() is not implemented for model {model}.&quot;&quot;&quot;)
NotImplementedError: num_tokens_from_messages() is not implemented for model gpt-3.5-turbo-16k.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;/home/a/Sh/dis/chatgpt-telegram-bot/bot/telegram_bot.py&quot;, line 414, in prompt
    async for content, tokens in stream_response:
  File &quot;/home/a/Sh/dis/chatgpt-telegram-bot/bot/openai_helper.py&quot;, line 93, in get_chat_response_stream
    response = await self.__common_get_chat_response(chat_id, query, stream=True)
  File &quot;/home/a/Sh/dis/chatgpt-telegram-bot/bot/openai_helper.py&quot;, line 162, in __common_get_chat_response
    raise Exception(f'⚠️ _An error has occurred_ ⚠️\n{str(e)}') from e
Exception: ⚠️ _An error has occurred_ ⚠️
num_tokens_from_messages() is not implemented for model gpt-3.5-turbo-16k`
</code></pre>
","2023-06-14 07:13:53","","","2023-06-14 07:13:53","<gpt-3>","0","0","-1","161","","","","","","",""
"76514041","1","20148726","","my gpt 3.5 turbo api is not giving good enough response as i can get from chat gpt","<p>So i have implemented chat gpt 3.5 turbo API in my react app. so my app is basically like an assistant to a recruiter. so a recruiter gives a sample job post to the app and it send this post to chat gpt to craft it. now i have different personas to be copied in the response i am also instructing it to follow these personas and styles. in this example persona of Lou Adler and style is enticing. But the problem is when i give the problem to cht gpt it is givng me good response but in case of my API in my app the response is not good enough. can someone tell me about the problem.</p>
<p>below is my code and note that there are two user roles. i do not understand this. where will the actual propt by user will be? can you kindly elaborate this problem.</p>
<pre><code>import logo from './logo.svg';
import './App.css';
import React, { useEffect, useState } from 'react';
import axios from 'axios';


function App() {

 // get api key from server
  const [apiKey, setApiKey] = useState('');

  useEffect(() =&gt; {
    fetch('https://server-khaki-kappa.vercel.app/api/key')
      .then(response =&gt; response.json())
      .then(data =&gt; setApiKey(data.apiKey))
      .catch(error =&gt; console.error(error));
  }, []);
  const headers = {
    'Content-Type': 'application/json',
    Authorization: `Bearer ${apiKey}`,
  };

  const [userInput, setUserInput] = useState({
    system: '',
    user: '',
    assistant: '',
    prompt: '',
    model: 'gpt-3.5-turbo-16k',
  });

  console.log(userInput)
  const [assistantResponse, setAssistantResponse] = useState('');
  const [loading, setLoading] = useState(false);

  const handleUserInput = (e) =&gt; {
    console.log('e.target',e.target.value);
    setUserInput((prevState) =&gt; ({
      ...prevState,
      [e.target.name]: e.target.value,
    }));
  };

  const sendUserInput = async () =&gt; {
    setLoading(true);

    const data = {
      model: userInput.model,
      messages: [
        {
          role: 'system',
          content: 
          // userInput.system
          'You are an AI language model trained to assist recruiters in refining job posts. Please provide Enticing content, language, and information in the job posts. Number of words in the response should be equal to or more than the job post that a recruiter is giving to you. you strictly have to follow the same persona given to you. also you have to follow the job post that recruiter will give you. you will make it more enticing and follow the persona of Lou Adler'
             },
        {
          role: 'user',
          content: 
          userInput.user 
          // 'When rewriting the job description, use a language model acting as a recruitment expert or consultant. In this context, take on the persona of Lou Adler. Your role is to be enticing with the reader and emphasize the benefits and opportunities associated with the job position, while presenting the information in an enticing manner.'
            },
        {
          role: 'assistant',
          content:
            // userInput.assistant 
            'You are an AI assistant trained to help recruiters refine their job posts. You can provide suggestions, make the language more enticing, and ensure all necessary information is included. If any details are missing or ambiguous, please ask for more information to provide the best possible suggestions. Take your time to answer the best.'
             },
        {
          role: 'user',
          content:
            userInput.prompt 
            },
      ],
      temperature: 0.2
    };

    try {
      const response = await axios.post(
        'https://api.openai.com/v1/chat/completions',
        data,
        { headers }
      );
      const { choices } = response.data;
      const assistantResponse = choices[0].message.content;
      setLoading(false);
      setAssistantResponse(assistantResponse);
    } catch (error) {
      console.error('An error occurred:', error.message);
    }
  };

  const formatAssistantResponse = (response) =&gt; {
    const paragraphs = response.split('\n\n');

    const formattedResponse = paragraphs.map((paragraph, index) =&gt; (
      &lt;p key={index} className=&quot;text-left mb-4&quot;&gt;
        {paragraph}
      &lt;/p&gt;
    ));

    return formattedResponse;
  };

  return (
    &lt;div className=&quot;container mx-auto py-8&quot;&gt;
    &lt;h1 className=&quot;text-2xl font-bold mb-4&quot;&gt;Chat :&lt;/h1&gt;
    {loading ? (
      &lt;&gt;
        &lt;h1 className=&quot;spinner&quot;&gt;&lt;/h1&gt;
      &lt;/&gt;
    ) : (
      &lt;&gt;
        &lt;div className=&quot;bg-gray-100 p-4 mb-4&quot;&gt;
          {formatAssistantResponse(assistantResponse)}
        &lt;/div&gt;
      &lt;/&gt;
    )}

    &lt;section className='m-6'&gt;
      
    &lt;div className=&quot;mb-4 &quot;&gt;
      &lt;label className=&quot;block mb-2&quot;&gt;
        Model:
        &lt;select
          className=&quot;border border-gray-300 rounded px-4 py-2 w-full&quot;
          name=&quot;model&quot;
          value={userInput.model}
          onChange={handleUserInput}
        &gt;
          &lt;option value=&quot;gpt-3.5-turbo&quot;&gt;gpt-3.5-turbo&lt;/option&gt;
          &lt;option value=&quot;gpt-3.5-turbo-16k&quot;&gt;gpt-3.5-turbo-16k&lt;/option&gt;
          &lt;option value=&quot;gpt-3.5-turbo-0613&quot;&gt;gpt-3.5-turbo-0613&lt;/option&gt;
          &lt;option value=&quot;gpt-3.5-turbo-16k-0613&quot;&gt;gpt-3.5-turbo-16k-0613&lt;/option&gt;
          {/* &lt;option value=&quot;text-davinci-003&quot;&gt;text-davinci-003&lt;/option&gt; */}
        &lt;/select&gt;
      &lt;/label&gt;
    &lt;/div&gt;
    &lt;div className=&quot;mb-4&quot;&gt;
      {/* &lt;label className=&quot;block mb-2&quot;&gt;
        System Role:
        &lt;textarea
           className=&quot;border border-gray-300 rounded px-4 py-2 w-full&quot;
          type=&quot;text&quot;
          rows={4}
          name=&quot;system&quot;
          value={userInput.system}
          onChange={handleUserInput}
        /&gt;
      &lt;/label&gt; */}
    &lt;/div&gt;
    &lt;div className=&quot;mb-4&quot;&gt;
&lt;label className=&quot;block mb-2&quot;&gt;
  User Role:
  &lt;textarea
     className=&quot;border border-gray-300 rounded px-4 py-2 w-full&quot;
    rows={4}
    name=&quot;user&quot;
    value={userInput.user}
    onChange={handleUserInput}
  /&gt;
&lt;/label&gt;
&lt;/div&gt;

    &lt;div className=&quot;mb-4&quot;&gt;
      {/* &lt;label className=&quot;block mb-2&quot;&gt;
        Assistant Role:
        &lt;textarea
      
     
        className=&quot;border border-gray-300 rounded px-4 py-2 w-full&quot;
          type=&quot;text&quot;
          rows={4}
          name=&quot;assistant&quot;
          value={userInput.assistant}
          
          onChange={handleUserInput}
        /&gt;
      &lt;/label&gt; */}
    &lt;/div&gt;
    &lt;div className=&quot;mb-4&quot;&gt;
      &lt;label className=&quot;block mb-2&quot;&gt;
        Prompt:
        &lt;textarea
          className=&quot;border border-gray-300 rounded px-4 py-2 w-full&quot;
          name='prompt'
          type=&quot;text&quot;
          rows={4}
        onChange={handleUserInput}
        /&gt;
      &lt;/label&gt;
    &lt;/div&gt;
   
    &lt;/section&gt;
    &lt;button
      className=&quot;bg-blue-500 hover:bg-blue-600 text-white font-bold py-2 px-4 rounded&quot;
      onClick={sendUserInput}
    &gt;
      Send
    &lt;/button&gt;
  &lt;/div&gt;
  );
}

export default App;
</code></pre>
","2023-06-20 11:02:18","","","2023-06-20 11:02:18","<reactjs><chat><openai-api><chatgpt-api><gpt-4>","0","0","0","39","","","","","","",""
"76533304","1","13312941","","Sentence Transformers Installation Error: legacy install failure","<p>I am using privateGPT for a project and it's throwing error on installing the dependencies. r</p>
<p>I just installed privateGPT from github and ran <code>pip3 install -r requirements</code> . I followed all the instructions given at <a href=""https://github.com/imartinez/privateGPT"" rel=""nofollow noreferrer"">privateGPT ReadMe</a> and installed pytorch it throws this <a href=""https://i.stack.imgur.com/7u2Sz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7u2Sz.png"" alt=""sentence transformers erro"" /></a></p>
","2023-06-22 15:09:04","","2023-06-22 15:28:55","2023-06-22 15:28:55","<python><sentence-transformers><gpt-4>","0","4","-3","18","","","","","","",""
"71539894","1","13510057","","CUDA out of memory while fine-tuning GPT2","<blockquote>
<p>RuntimeError: CUDA out of memory. Tried to allocate 144.00 MiB (GPU 0; 11.17 GiB total capacity; 10.49 GiB already allocated; 13.81 MiB free; 10.56 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>
</blockquote>
<p>This is the error I am getting, I have tried playing around with batch size but to no avail. I am training on google colab.</p>
<p>This is the piece of code concerned with the error:</p>
<pre><code>training_args = TrainingArguments(
output_dir=&quot;/content/&quot;,
num_train_epochs=EPOCHS,
per_device_train_batch_size=16,
per_device_eval_batch_size=16,
# gradient_accumulation_steps=BATCH_UPDATE,
evaluation_strategy=&quot;epoch&quot;,
save_strategy='epoch',
fp16=True,
fp16_opt_level=APEX_OPT_LEVEL,
warmup_steps=WARMUP_STEPS,    
learning_rate=LR,
adam_epsilon=EPS,
weight_decay=0.01,        
save_total_limit=1,
load_best_model_at_end=True,     
)
</code></pre>
<p>Any solution?</p>
","2022-03-19 16:18:09","","2022-03-19 16:19:47","2022-03-19 17:58:28","<python><machine-learning><nlp><training-data><gpt-2>","1","0","1","1555","","","","","","",""
"58884492","1","11130181","","why is encoder.json not found when running GPT2 small model","<p>good evening,</p>

<p><em>caveat, im not a python or machine learning expert</em></p>

<p>I'm trying to run the small instance of GPT2 , after the hype I wanted to check it out. So far I've downloaded all the prerequisites. Python, regex, tensorflow etc. but when it comes to running the script to generate the sample from the model im being thrown the following error</p>

<p>'''File ""C:*****\F******y\Desktop\Python\gpt-2\src\encoder.py"", line 109, in get_encoder
    with open(os.path.join(models_dir, model_name, 'encoder.json'), 'r') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'models\124M\encoder.json'''</p>

<p>when i'm calling the script i switch into the directory that holds the file and run ''' generate_unconditional_samples.py --top_k 40 ''' from the command line </p>

<p>the script itself looks like this </p>

<pre><code>#!/usr/bin/env python3

import fire
import json
import os
import numpy as np
import tensorflow as tf

import model, sample, encoder

def sample_model(
    model_name='124M',
    seed=None,
    nsamples=0,
    batch_size=1,
    length=None,
    temperature=1,
    top_k=0,
    top_p=1,
    models_dir='U**r\F****y\Desktop\Python\gpt-2\models',
):
    """"""
    Run the sample_model
    :model_name=124M : String, which model to use
    :seed=None : Integer seed for random number generators, fix seed to
     reproduce results
    :nsamples=0 : Number of samples to return, if 0, continues to
     generate samples indefinately.
    :batch_size=1 : Number of batches (only affects speed/memory).
    :length=None : Number of tokens in generated text, if None (default), is
     determined by model hyperparameters
    :temperature=1 : Float value controlling randomness in boltzmann
     distribution. Lower temperature results in less random completions. As the
     temperature approaches zero, the model will become deterministic and
     repetitive. Higher temperature results in more random completions.
    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is
     considered for each step (token), resulting in deterministic completions,
     while 40 means 40 words are considered at each step. 0 (default) is a
     special setting meaning no restrictions. 40 generally is a good value.
     :models_dir : path to parent folder containing model subfolders
     (i.e. contains the &lt;model_name&gt; folder)
    """"""
    models_dir = os.path.expanduser(os.path.expandvars(models_dir))
    enc = encoder.get_encoder(model_name, models_dir)
    hparams = model.default_hparams()
    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:
        hparams.override_from_dict(json.load(f))

    if length is None:
        length = hparams.n_ctx
    elif length &gt; hparams.n_ctx:
        raise ValueError(""Can't get samples longer than window size: %s"" % hparams.n_ctx)

    with tf.Session(graph=tf.Graph()) as sess:
        np.random.seed(seed)
        tf.set_random_seed(seed)

        output = sample.sample_sequence(
            hparams=hparams, length=length,
            start_token=enc.encoder['&lt;|endoftext|&gt;'],
            batch_size=batch_size,
            temperature=temperature, top_k=top_k, top_p=top_p
        )[:, 1:]

        saver = tf.train.Saver()
        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))
        saver.restore(sess, ckpt)

        generated = 0
        while nsamples == 0 or generated &lt; nsamples:
            out = sess.run(output)
            for i in range(batch_size):
                generated += batch_size
                text = enc.decode(out[i])
                print(""="" * 40 + "" SAMPLE "" + str(generated) + "" "" + ""="" * 40)
                print(text)

if __name__ == '__main__':
    fire.Fire(sample_model)
</code></pre>

<p>'''</p>

<p>can anyone advise what I might be doing wrong - im sure its really obvious but i've been trying all sorts of stuff for about 4 hours with no luck</p>

<p>any advice is much appreciated </p>
","2019-11-15 20:50:54","","2020-11-29 12:09:39","2020-11-29 12:09:39","<python><machine-learning><artificial-intelligence><nlg><gpt-2>","1","0","1","1634","0","","","","","",""
"65897844","1","15025011","","Why using GPT2Tokenizer convert Arabic characters to symbols?","<p>I am trying to use <a href=""https://huggingface.co/aubmindlab/aragpt2-base"" rel=""nofollow noreferrer"">GPT2</a> for Arabic text classification task as follows:</p>
<pre><code>    tokenizer = GPT2Tokenizer.from_pretrained(model_path)
    model = GPT2ForSequenceClassification.from_pretrained(model_path, 
                                                          num_labels=len(lab2ind)) 
</code></pre>
<p>However, when I use the tokenizer it converts the Arabic characters to symbols like this
<code>'ĠÙĥØªÙĬØ±'</code></p>
","2021-01-26 08:16:49","","2021-01-26 18:02:05","2021-01-26 18:02:05","<pytorch><tokenize><huggingface-transformers><huggingface-tokenizers><gpt-2>","0","3","1","266","","","","","","",""
"76397904","1","11745522","","Generate the probabilities of all the next possible word for a given text","<p>i have the following code</p>
<pre><code>import transformers
from transformers import pipeline

# Load the language model pipeline
model = pipeline(&quot;text-generation&quot;, model=&quot;gpt2&quot;)

# Input sentence for generating next word predictions
input_sentence = &quot;I enjoy walking in the&quot;
</code></pre>
<p>I want to generate <strong>only the next word</strong> given the input sentence but i want to see list of all possible next words along with their probabilities. any other LLM can be used i put gpt2 as an example.</p>
<p>In the code i want to choose top 500 words or top 1000 words suggestion for only the next word and the probabilities of each suggested word
how can i do this?</p>
","2023-06-03 20:23:27","","2023-06-03 21:37:46","2023-06-03 21:40:49","<text><pytorch><huggingface-transformers><gpt-2>","2","0","3","119","","","","","","",""
"66518316","1","13808280","","How do I make a paraphrase generation using BERT/ GPT-2","<p>I am trying hard to understand how to make a paraphrase generation using BERT/GPT-2. I cannot understand how do I make it. Could you please provide me with any resources where I will be able to make a paraphrase generation model?
<strong>&quot;The input would be a sentence and the output would be a paraphrase of the sentence&quot;</strong></p>
","2021-03-07 15:45:38","","","2021-06-18 13:10:05","<nlp><gpt-2>","2","0","4","3065","","","","","","",""
"62219426","1","13494387","","Adding tokens to GPT-2 BPE tokenizer","<p>I want to add new words to my BPE tokenizer. I know the symbol Ġ means the end of a new token and the majority of tokens in vocabs of pre-trained tokenizers start with Ġ. Assume I want to add the word <strong>Salah</strong> to my tokenizer. I tried to add both <strong>Salah</strong> token and <strong>ĠSalah</strong>:
tokenizer.add_tokens(['Salah', 'ĠSalah']) # they get 50265 and 50266 values respectively.
However, when I tokenize a sentence where <strong>Salah</strong> appears, the tokenizer will never return me the second number (neither when using <code>.tokenize</code>nor<code>.encode</code>), for instance:
<code>tokenizer.tokenize('I love Salah and salad')</code> returns <code>['I', 'Ġlove', 'Salah', 'Ġand', 'Ġsalad']</code>.
The question is: should I use the symbol <code>Ġ</code> when adding new tokens or the tokenizer does it itself? Or, probably, it must be specified manually?
Thanks in advance!</p>
","2020-06-05 15:56:12","","2020-11-29 12:05:47","2020-11-29 12:05:47","<python><nlp><tokenize><huggingface-transformers><gpt-2>","0","2","2","1021","","","","","","",""
"67365595","1","3146304","","Pytorch inference OOM after some batches","<p>I am trying to do inference with a GPT2-like model on a large dataset (26k samples). To speed it up I would like to do it in batches, but trying this it goes in Cuda OOM after some batches. The fact that it goes out only after some batches sounds strange to me, because I suppose the memory use should be more or less constant in different batches.
This is my code:</p>
<pre><code>tokenizer.padding_side = &quot;left&quot;
tokenizer.pad_token = tokenizer.eos_token

sentences = [&quot;&lt;START_TOK&gt;&quot; + s + &quot;&lt;END_TOK&gt;&quot; + tokenizer.eos_token for s in sentences]

inputs = tokenizer(sentences, return_tensors=&quot;pt&quot;, padding=True, max_length=1024, truncation=True)

device = torch.device(&quot;cuda:0&quot;)
inputs = inputs.to(device)
model = model.to(device)
model.eval()
res = []
with torch.no_grad():
    output_sequences = model.generate(
            input_ids=inputs['input_ids'],
            attention_mask=inputs['attention_mask'],
            max_length=1024,
            pad_token_id=tokenizer.eos_token_id,
            no_repeat_ngram_size=2,
            do_sample=True,
            top_k=100,
            top_p=0.9,
            temperature=0.85
        )
     output_sequences = output_sequences.cpu() #not really sure this is useful, just tried, but the problem remained
     for i in range(len(sentences)):
         res.append(tokenizer.decode(output_sequences[i]))
model.train()
return res
</code></pre>
<p>What could be the problem?</p>
","2021-05-03 08:15:15","","2021-05-03 13:26:03","2021-05-03 13:26:03","<pytorch><gpt-2>","0","3","0","187","","","","","","",""
"70577285","1","13440007","70580033","""ValueError: You have to specify either input_ids or inputs_embeds"" when training AutoModelWithLMHead Model (GPT-2)","<p>I want to fine-tune the AutoModelWithLMHead model from <a href=""https://huggingface.co/dbmdz/german-gpt2"" rel=""nofollow noreferrer"">this repository</a>, which is a German GPT-2 model. I have followed the tutorials for pre-processing and fine-tuning. I have prepocessed a bunch of text passages for the fine-tuning, but when beginning training, I receive the following error:</p>
<pre><code>File &quot;GPT\lib\site-packages\torch\nn\modules\module.py&quot;, line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;GPT\lib\site-packages\transformers\models\gpt2\modeling_gpt2.py&quot;, line 774, in forward
    raise ValueError(&quot;You have to specify either input_ids or inputs_embeds&quot;)
ValueError: You have to specify either input_ids or inputs_embeds
</code></pre>
<p>Here is my code for reference:</p>
<pre><code># Load data
with open(&quot;Fine-Tuning Dataset/train.txt&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as train_file:
    train_data = train_file.read().split(&quot;--&quot;)

with open(&quot;Fine-Tuning Dataset/test.txt&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as test_file:
    test_data = test_file.read().split(&quot;--&quot;)

# Load pre-trained tokenizer and prepare input
tokenizer = AutoTokenizer.from_pretrained('dbmdz/german-gpt2')

tokenizer.pad_token = tokenizer.eos_token
train_input = tokenizer(train_data, padding=&quot;longest&quot;)
test_input = tokenizer(test_data, padding=&quot;longest&quot;)

# Define model

model = AutoModelWithLMHead.from_pretrained(&quot;dbmdz/german-gpt2&quot;)
training_args = TrainingArguments(&quot;test_trainer&quot;)


# Evaluation

metric = load_metric(&quot;accuracy&quot;)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = numpy.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# Train
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_input,
    eval_dataset=test_input,
    compute_metrics=compute_metrics,
)
trainer.train()
trainer.evaluate()

</code></pre>
<p>Does anyone know the reason for this? Any help is welcome!</p>
","2022-01-04 10:26:25","","","2022-06-23 14:13:51","<python><pytorch><huggingface-transformers><gpt-2>","2","0","0","2090","","2","13440007","<p>I didn't find the concrete answer to this question, but a workaround. For anyone looking for examples on how to fine-tune the GPT models from HuggingFace, you may have a look into this <a href=""https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling"" rel=""nofollow noreferrer"">repo</a>. They listed a couple of examples on how to fine-tune different Transformer models, complemented by documented code examples. I used the <code>run_clm.py</code> script and it achieved what I wanted.</p>
","2022-01-04 14:08:06","0","0"
"72580299","1","5361942","72586215","GPT2 paper clarification","<p>In the GPT-2 paper, under Section 2, Page 3 it says,</p>
<blockquote>
<p>Since the supervised objective is the the same as the unsupervised objective but only evaluated on a subset of the sequence, the global minimum of the unsupervised objective is also the global minimum of the supervised objective.</p>
</blockquote>
<p>I didn't follow this line of reasoning. What is the logic behind concluding this?</p>
","2022-06-10 22:16:11","","","2022-06-11 16:49:37","<gpt-2>","1","1","0","119","","2","14644941","<p>The underlying principle here is that if <code>f</code> is a function with domain <code>D</code> and <code>S</code> is a subset of <code>D</code>, then if <code>d</code> maximizes <code>f</code> over <code>D</code> and <code>d</code> happens to be in <code>S</code>, then <code>d</code> also maximizes <code>f</code> over <code>S</code>.</p>
<p>In simper words &quot;a global maximum is also a local maximum&quot;.</p>
<p>Now how does this apply to GPT-2? Let's look at how GPT-2 is trained.</p>
<p>First step: GPT-2 uses unsupervised training to learn the distribution of the next letter in a sequence by examining examples in a huge corpus of existing text. By this point, it should be able to output valid words and be able to complete things like &quot;Hello ther&quot; to &quot;Hello there&quot;.</p>
<p>Second step: GPT-2 uses supervised training at specific tasks such as answering specific questions posed to it such as &quot;Who wrote the book the origin of species?&quot; Answer &quot;Charles Darwin&quot;.</p>
<p>Question: Does the second step of supervised training undo general knowledge that GPT-2 learned in the first step?</p>
<p>Answer: No, the question-answer pair &quot;Who wrote the book the origin of species? Charles Darwin.&quot; is itself valid English text that comes from the same distribution that the network is trying to learn in the first place. It may well even appear verbatim in the corpus of text from step 1. Therefore, these supervised examples are elements of the same domain (valid English text) and optimizing the loss function to get these supervised examples correct is working towards the same objective as optimizing the loss function to get the unsupervised examples correct.</p>
<p>In simpler words, supervised question-answer pairs or other specific tasks that GPT-2 was trained to do use examples from the same underlying distribution as the unsupervised corpus text, so they are optimizing towards the same goal and will have the same global optimum.</p>
<p>Caveat: you can still accidentally end up in a local-minimum due to (over)training using these supervised examples that you might not have run into otherwise. However, GPT-2 was revolutionary in its field and whether or not this happened with GPT-2, it still made significant progress from the state-of-the-art before it.</p>
","2022-06-11 16:49:37","0","1"
"59150725","1","12466078","","Tensorflow not fully utilizing GPU in GPT-2 program","<p>I am running the GPT-2 code of the large model(774M). It is used for the generation of text samples through interactive_conditional_samples.py , link: <a href=""https://github.com/openai/gpt-2/blob/master/src/interactive_conditional_samples.py"" rel=""nofollow noreferrer"">here</a>  </p>

<p>So I've given an input file containing prompts which are automatically selected to generate output. This output is also automatically copied into a file. In short, I'm not training it, I'm using the model to generate text. 
Also, I'm using a single GPU. </p>

<p>The problem I'm facing in this is, The code is not utilizing the GPU fully. </p>

<p>By using nvidia-smi command, I was able to see the below image</p>

<p><a href=""https://imgur.com/CqANNdB"" rel=""nofollow noreferrer"">https://imgur.com/CqANNdB</a></p>
","2019-12-03 05:34:50","","2020-11-29 12:01:18","2020-11-29 12:01:18","<python><tensorflow><gpt-2>","1","6","2","635","","","","","","",""
"59997686","1","12419427","","Python gpt-2-simple, load multiple models at once","<p>I'm working on a discord bot and one of the functions I want to implement responds with text generated by the gpt-2-simple library. I want to have more then one model loaded to have multiple models available to respond to messages from my users.</p>

<p>However I get the following error when i run the <code>load_gpt2()</code> function in the second model </p>

<pre><code>File ""main.py"", line 22, in &lt;module&gt;
    main()
  File ""main.py"", line 16, in main
    text_events.register_Message(client)
  File ""U:\discord_bot\text_events\__init__.py"", line 19, in register_Message
    event.init()
  File ""U:\discord_bot\text_events\model2.py"", line 20, in init
    gpt2.load_gpt2(sess, run_name='model2', checkpoint_dir=""characters"")
  File ""C:\Program Files\Python36\lib\site-packages\gpt_2_simple\gpt_2.py"", line 389, in load_gpt2
    output = model.model(hparams=hparams, X=context, gpus=gpus)
  File ""C:\Program Files\Python36\lib\site-packages\gpt_2_simple\src\model.py"", line 183, in model
    initializer=tf.compat.v1.random_normal_initializer(stddev=0.01))
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\ops\variable_scope.py"", line 1500, in get_variable
    aggregation=aggregation)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\ops\variable_scope.py"", line 1243, in get_variable
    aggregation=aggregation)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\ops\variable_scope.py"", line 567, in get_variable
    aggregation=aggregation)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\ops\variable_scope.py"", line 519, in _true_getter
    aggregation=aggregation)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\ops\variable_scope.py"", line 868, in _get_single_variable
    (err_msg, """".join(traceback.format_list(tb))))
ValueError: Variable model/wpe already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:

  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 1748, in __init__
    self._traceback = tf_stack.extract_stack()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 3426, in _create_op_internal
    op_def=op_def)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 3357, in create_op
    attrs, op_def, compute_device)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_core\python\framework\op_def_library.py"", line 794, in _apply_op_helper
    op_def=op_def)
</code></pre>

<p>I've tried to find a way to keep the gpt2 instances seperate between modules but i can't find anything that achieves this sandboxing effect, or any other advice for seperating the models or their instances. Does anyone have any ideas?</p>
","2020-01-31 04:01:21","","2020-11-29 12:01:38","2021-06-17 00:36:29","<python><python-3.x><tensorflow><gpt-2>","3","0","4","752","0","","","","","",""
"66451430","1","6463094","","Changes in GPT2/GPT3 model during few shot learning","<p>During transfer learning, we take a pre-trained network and some observation pair (input and label), and use these data to fine-tune the weight by use of backpropagation. However, during one shot/few shot learning, according to this paper- 'Language Models are Few-Shot Learners' (<a href=""https://arxiv.org/pdf/2005.14165.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2005.14165.pdf</a>), &quot;No gradient updates are performed&quot;. Then what changes happen to the models like GPT2 and GPT3 during one shot/few shot learning?</p>
","2021-03-03 05:46:56","","","2023-03-19 15:58:06","<nlp><gpt-2><gpt-3>","3","0","1","515","","","","","","",""
"60567168","1","4321521","","Use BertTokenizer with HuggingFace GPT-2","<p>I have a specific generation problem involving a dataset built from a very small vocabulary. Ideally, my use case will be much more straightforward if I can simply provide that vocabulary in a fixed set of tokens. I know that with the BertTokenizer, for example, I can provide a <code>vocab.txt</code> file and avoid any further tokenization of this basic vocabulary, and I'm wondering if there's a way to get GPT-2 to do the same? The only thing I can think of right now is creating a hacked <code>PretrainedTokenizer</code> subclass, but perhaps someone has a better idea?</p>

<p>Any thoughts appreciated.</p>

<p>UPDATE: Okay, so it turns out I can just swap out <code>BertTokenizer</code> and <code>BertWordpieceTokenizer</code> when creating the <code>GPT2LMHeadModel</code>. (Thanks HuggingFace for a well-designed, modular codebase!)</p>
","2020-03-06 15:30:36","","2020-11-29 11:58:32","2020-11-29 11:58:32","<nlp><huggingface-transformers><gpt-2>","0","0","1","189","","","","","","",""
"60574112","1","5915270","","Can we use GPT-2 sentence embedding for classification tasks?","<p>I am experimenting on the use of transformer embeddings in sentence classification tasks <strong>without finetuning them</strong>. I have used BERT embeddings and those experiments gave me very good results. Now I want to use GPT-2 embeddings (without fine-tuning). So I have two questions,</p>

<ol>
<li>Can I use GPT-2 embeddings like that (because I know Gpt-2 is
trained on the left to right) </li>
<li>Is there any example uses of GPT-2 in
    classification tasks other than generation tasks?</li>
<li>If I can use GPT-2embeddings, how should I do it?</li>
</ol>
","2020-03-07 03:28:06","","2020-11-29 11:50:22","2020-11-29 11:50:22","<nlp><huggingface-transformers><gpt-2>","1","6","4","5710","","","","","","",""
"66956460","1","11810876","","Huggingface GPT transformers layers output","<p>I'm trying to use a GPT language model and get the weights it assigns to each word in the last state of text generation. My model is a GPT2 from the transformers library. Below is how I call the pretrained model:</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(
&quot;HooshvareLab/gpt2-fa-poetry&quot;
) 

model = AutoModelForCausalLM.from_pretrained(
    &quot;HooshvareLab/gpt2-fa-poetry&quot;
)
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

model = model.to(device)
</code></pre>
<p>My goal is to use this information from the last layer of this model (a matrix with the length of vocabulary after the softmax activation) and use it in combination with another model.</p>
<p>I'm trying to do this in TensorFlowPlease, but share your comments if you think there are easier and more convenient ways of doing this in PyTorch.</p>
","2021-04-05 16:40:57","","2021-04-06 11:37:47","2021-04-06 11:37:47","<tensorflow><nlp><huggingface-transformers><language-model><gpt-2>","0","3","1","423","","","","","","",""
"66020205","1","12384851","","Huggingface Transformer Priming","<p>I am trying to replicate the results of <a href=""https://www.buildgpt3.com/post/41/"" rel=""nofollow noreferrer"">this demo</a>, whose author primes GPT-3 with <a href=""https://twitter.com/siddkaramcheti/status/1286168606896603136?lang=es"" rel=""nofollow noreferrer"">just</a> the following text:</p>
<pre><code>gpt.add_example(Example('apple', 'slice, eat, mash, cook, bake, juice'))
gpt.add_example(Example('book', 'read, open, close, write on'))
gpt.add_example(Example('spoon', 'lift, grasp, scoop, slice'))
gpt.add_example(Example('apple', 'pound, grasp, lift'))
</code></pre>
<p>I only have access to GPT-2, via the Huggingface Transformer. How can I prime GPT-2 large on Huggingface to replicate the above examples? The issue is that, with <a href=""https://transformer.huggingface.co/doc/gpt2-large"" rel=""nofollow noreferrer"">this</a>, one doesn't get to prime with the input and corresponding output separately (as the author of the GPT-3 demo did above).</p>
<p>Similarly, <a href=""https://www.kaggle.com/nageshsingh/huggingface-transformer-basic-usage"" rel=""nofollow noreferrer"">this tutorial</a> describes using Huggingface, but there's no example which clearly shows how you can prime it using input vs output examples.</p>
<p>Does anyone know how to do this?</p>
<hr />
<p>Desired output:
use GPT-2 to return something like, for input &quot;potato&quot;, output &quot;peel, slice, cook, mash, bake&quot; (as in the GPT-3 demo: <a href=""https://www.buildgpt3.com/post/41/"" rel=""nofollow noreferrer"">https://www.buildgpt3.com/post/41/</a>). Obviously the exact list of output verbs won't be the same as GPT-2 and GPT-3 are not identical models.</p>
","2021-02-03 01:56:48","","","2021-02-03 10:17:22","<python><huggingface-transformers><gpt-2><gpt-3>","1","0","1","223","","","","","","",""
"72604790","1","14143310","","How to train GPT2 with Huggingface trainer","<p>I am trying to fine tune GPT2, with Huggingface's trainer class.</p>
<pre><code>from datasets import load_dataset
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2TokenizerFast, GPT2LMHeadModel, Trainer, TrainingArguments


class torchDataset(Dataset):
    def __init__(self, encodings):
        self.encodings = encodings
        self.len = len(encodings)

    def __getitem__(self, index):
        item = {torch.tensor(val[index]) for key, val in self.encodings.items()}
        return item

    def __len__(self):
        return self.len

    def print(self):
        print(self.encodings)


# HYPER PARAMETERS
EPOCHS = 5
BATCH_SIZE = 2
WARMUP_STEPS = 5000
LEARNING_RATE = 1e-3
DECAY = 0


# Model ids and loading dataset
model_id = 'gpt2'  # small model
# model_id = 'gpt2-medium'  # medium model
# model_id = 'gpt2-large'  # large model

dataset = load_dataset('wikitext', 'wikitext-2-v1')  # first dataset
# dataset = load_dataset('m-newhauser/senator-tweets')  # second dataset
# dataset = load_dataset('IsaacRodgz/Fake-news-latam-omdena')  # third dataset

print('Loaded dataset')

# Dividing dataset into predefined splits
train_dataset = dataset['train']['text']
validation_dataset = dataset['validation']['text']
test_dataset = dataset['test']['text']

print('Divided dataset')

# loading tokenizer
tokenizer = GPT2TokenizerFast.from_pretrained(model_id,
                                              # bos_token='&lt;|startoftext|&gt;', eos_token='&lt;|endoftext|&gt;',
                                              pad_token='&lt;|pad|&gt;'
                                              )

print('tokenizer max length:', tokenizer.model_max_length)

train_encoding = tokenizer(train_dataset, padding=True, truncation=True, max_length=1024, return_tensors='pt')
eval_encoding = tokenizer(validation_dataset, padding=True, truncation=True, max_length=1024, return_tensors='pt')
test_encoding = tokenizer(test_dataset, padding=True, truncation=True, max_length=1024, return_tensors='pt')

print('Converted to torch dataset')

torch_dataset_train = torchDataset(train_encoding)
torch_dataset_eval = torchDataset(eval_encoding)
torch_dataset_test = torchDataset(test_encoding)

# Setup training hyperparameters
training_args = TrainingArguments(
    output_dir='/model_dump/',
    num_train_epochs=EPOCHS,
    warmup_steps=WARMUP_STEPS,
    learning_rate=LEARNING_RATE,
    weight_decay=DECAY
)

model = GPT2LMHeadModel.from_pretrained(model_id)
model.resize_token_embeddings(len(tokenizer))

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_encoding,
    eval_dataset=eval_encoding

)

trainer.train()
# model.save_pretrained('/model_dump/')
</code></pre>
<p>But with this code I get this error</p>
<pre><code>The batch received was empty, your model won't be able to train on it. Double-check that your training dataset contains keys expected by the model: input_ids,past_key_values,attention_mask,token_type_ids,position_ids,head_mask,inputs_embeds,encoder_hidden_states,encoder_attention_mask,labels,use_cache,output_attentions,output_hidden_states,return_dict,labels,label,label_ids.
</code></pre>
<p>When I use the variables torch_dataset_train and torch_dataset_eval in Trainer's arguments, the error I get is:</p>
<pre><code>TypeError: vars() argument must have __dict__ attribute
</code></pre>
<p>This typeError is the same I get if as dataset I use the WikiText2 from torchtext.
How can I fix this issue?</p>
","2022-06-13 14:46:15","","","2022-06-13 14:46:15","<python-3.x><pytorch><huggingface-transformers><gpt-2><wikitext>","0","3","2","2585","","","","","","",""
"71641369","1","17400364","","GPT-3 made a mistake using numpy and I can't fix it","<p>I used GPT-3 to generate a Neural Network to use in a simple &quot;cell&quot; simulator.</p>
<p>When I run the script, I get the following error :</p>
<pre><code>*hidden_errors = np.dot(output_errors, self.weights_hidden_to_output.T)
  File &quot;&lt;__array_function__ internals&gt;&quot;, line 5, in dot
ValueError: shapes (1,4) and (2,1) not aligned: 4 (dim 1) != 2 (dim 0)*
</code></pre>
<p>I know this is because the matrices are not correctly shaped and I tried transposing it but without success. I also tried modifying the inputs list without any success.</p>
<pre><code>import random
import pygame
import numpy as np

class Hero:
    def __init__(self, x, y):
        self.x = x
        self.y = y
        self.score = 0

    def move_towards_food(self, food):
        if self.x &lt; food.x:
            self.x += 1
        elif self.x &gt; food.x:
            self.x -= 1
        if self.y &lt; food.y:
            self.y += 1
        elif self.y &gt; food.y:
            self.y -= 1

    def get_score(self):
        return self.score
    def respawn(self):
        self.x = random.randint(20,980)
        self.y = random.randint(20,980)


class Villain:
    def __init__(self):
        self.x = random.randint(0,1000)
        self.y = random.randint(0,1000)

    def move_towards_hero(self, hero):
        if self.x &lt; hero.x:
            self.x += 1
        elif self.x &gt; hero.x:
            self.x -= 1
        if self.y &lt; hero.y:
            self.y += 1
        elif self.y &gt; hero.y:
            self.y -= 1


class Food:
    def __init__(self, x, y):
        self.x = x
        self.y = y

class NeuralNetwork:
    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):
        self.input_nodes = input_nodes
        self.hidden_nodes = hidden_nodes
        self.output_nodes = output_nodes
        self.learning_rate = learning_rate
        self.weights_input_to_hidden = np.random.normal(0.0, self.input_nodes ** -0.5,
                                                        (self.input_nodes, self.hidden_nodes))
        self.weights_hidden_to_output = np.random.normal(0.0, self.hidden_nodes ** -0.5,
                                                         (self.hidden_nodes, self.output_nodes))
        self.activation_function = lambda x: 1 / (1 + np.exp(-x))

    def train(self, inputs_list, targets_list):
        inputs = np.array(inputs_list, ndmin=2).T
        targets = np.array(targets_list, ndmin=2).T
        hidden_inputs = np.dot(self.weights_input_to_hidden, inputs)
        hidden_outputs = self.activation_function(hidden_inputs)
        final_inputs = np.dot(self.weights_hidden_to_output, hidden_outputs)
        final_outputs = final_inputs
        output_errors = targets - final_outputs
        hidden_errors = np.dot(self.weights_hidden_to_output.T, output_errors)
        self.weights_hidden_to_output += self.learning_rate * np.dot(
            output_errors * final_outputs * (1.0 - final_outputs), hidden_outputs.T)
        self.weights_input_to_hidden += self.learning_rate * np.dot(
            hidden_errors * hidden_outputs * (1.0 - hidden_outputs), inputs.T)

    def run(self, inputs_list):
        inputs = np.array(inputs_list, ndmin=2).T
        hidden_inputs = np.dot(self.weights_input_to_hidden, inputs)
        hidden_outputs = self.activation_function(hidden_inputs)
        final_inputs = np.dot(self.weights_hidden_to_output, hidden_outputs)
        final_outputs = final_inputs
        return final_outputs

hero = Hero(500, 500)
villain = Villain()
food = Food(random.randint(0, 1000), random.randint(0, 1000))

nn = NeuralNetwork(4, 4, 4, 0.5)

inputs = [[1, 0, 1, 0], [0, 1, 0, 1], [1, 1, 1, 0], [0, 0, 0, 1]]
targets = [[1], [1], [0], [0]]

nn.train(inputs, targets)
hero.nn = nn

pygame.init()
size = [1000, 1000]
screen = pygame.display.set_mode(size)
pygame.display.set_caption(&quot;Hero Game&quot;)

black = [0, 0, 0]
white = [255, 255, 255]
red = [255, 0, 0]
green = [0, 255, 0]

font = pygame.font.SysFont('Calibri', 25, True, False)

done = False
clock = pygame.time.Clock()

while not done:
    for event in pygame.event.get():
        if event.type == pygame.QUIT:
            done = True
    screen.fill(white)
    text = font.render(&quot;Hero: &quot; + str(hero.x) + &quot;, &quot; + str(hero.y), True, black)
    screen.blit(text, [50, 50])
    text = font.render(&quot;Villain: &quot; + str(villain.x) + &quot;, &quot; + str(villain.y), True, black)
    screen.blit(text, [50, 100])
    text = font.render(&quot;Food: &quot; + str(food.x) + &quot;, &quot; + str(food.y), True, black)
    screen.blit(text, [50, 150])
    text = font.render(&quot;Score: &quot; + str(hero.get_score()), True, black)
    screen.blit(text, [50, 200])
    pygame.draw.rect(screen, red, [hero.x, hero.y, 5, 5])
    pygame.draw.rect(screen, black, [villain.x, villain.y, 5, 5])
    pygame.draw.rect(screen, green, [food.x, food.y, 5, 5])

    # villain chase
    if villain.x &lt; hero.x:
        villain.x += 1
    if villain.x &gt; hero.x:
        villain.x -= 1
    if villain.y &lt; hero.y:
        villain.y += 1
    if villain.y &gt; hero.y:
        villain.y -= 1

    # -1 and respawn for villain touch
    if villain.x == hero.x and villain.y == hero.y:
        hero.score -= 1
        hero.respawn()

    inputs = [[hero.x / 1000, hero.y / 1000, food.x / 1000, food.y / 1000]]
    output = nn.run(inputs)
    if output[0][0] &gt; 0.5:
        hero.move_towards_food(food)
    if hero.x == food.x and hero.y == food.y:
        hero.score += 1
        food = Food(random.randint(0, 1000), random.randint(0, 1000))

    pygame.display.flip()
    pygame.display.update()
    clock.tick(60)

pygame.quit()
</code></pre>
","2022-03-28 00:38:58","","2022-03-28 10:19:11","2022-03-28 10:19:11","<python><numpy><deep-learning><gpt-3>","0","2","0","125","","","","","","",""
"71683057","1","17445782","","_forward_unimplemented() got an unexpected keyword argument 'input_ids'","<p>I am training a model using HuggingFace Trainer class.(GPT2 text Classification) The following code does a decent job:</p>
<pre><code>def preprocess_function(examples):
    return tokenizer(examples[&quot;text&quot;], truncation=True ,max_length=MAXLEN,
                     padding=True
   )
    
dataset_train = Dataset.from_pandas(train_sp , preserve_index=False)
dataset_val = Dataset.from_pandas(val_sp ,preserve_index=False)

dataset_train = dataset_train.map(preprocess_function, batched=True,load_from_cache_file=False)
dataset_val = dataset_val.map(preprocess_function, batched=True,load_from_cache_file=False)


columns_to_return = ['input_ids', 'label', 'attention_mask']
dataset_train.set_format(type='torch', columns=columns_to_return)
dataset_val.set_format(type='torch', columns=columns_to_return)


data_collator = DataCollatorWithPadding(tokenizer=tokenizer  )

training_args = TrainingArguments(
    output_dir=&quot;/content/Model1&quot;, #The output directory
    overwrite_output_dir=True, #overwrite the content of the output directory
    num_train_epochs=3, # number of training epochs
    per_device_train_batch_size=16, # batch size for training
    per_device_eval_batch_size=8,  # batch size for evaluation
    eval_steps = 400, # Number of update steps between two evaluations.
    save_steps=800, # after # steps model is saved 
    warmup_steps=500,# number of warmup steps for learning rate scheduler
    prediction_loss_only=True,
    #remove_unused_columns=True
    )

#---------------------------------------------------#
trainer = Trainer(
    model=model1,
    args=training_args,    
    #data_collator=gpt2_classificaiton_collator,
    
    train_dataset=dataset_train,
    eval_dataset=dataset_val,
   
    tokenizer=tokenizer,
    data_collator=data_collator
)

trainer.train()
</code></pre>
<p>I got error  _forward_unimplemented() got an unexpected keyword argument 'input_ids'</p>
<p>what should I do?</p>
<p><a href=""https://i.stack.imgur.com/citHo.png"" rel=""nofollow noreferrer"">Input_ids and label</a></p>
<p><a href=""https://i.stack.imgur.com/GeN5n.png"" rel=""nofollow noreferrer"">error mg</a></p>
<p><a href=""https://i.stack.imgur.com/3Q1qW.png"" rel=""nofollow noreferrer"">My Model argiteture</a></p>
","2022-03-30 19:25:57","","2022-04-02 18:17:17","2022-04-02 18:17:17","<pytorch><huggingface-transformers><huggingface-tokenizers><gpt-2><google-publisher-tag>","0","3","0","488","","","","","","",""
"71737891","1","18701948","","Error when using mode.generate() from Transformers - TypeError: forward() got an unexpected keyword argument 'return_dict'","<p>I am trying to perform inference with a finetuned GPT2HeadWithValueModel from the Transformers library. I'm using the model.generate() method from generation_utils.py</p>
<p>I am using this function to call the generate() method:</p>
<pre><code>def top_p_sampling(text, model, tokenizer):
  encoding = tokenizer(text, return_tensors=&quot;pt&quot;)['input_ids']
  output_tensor = model.generate(
    encoding, 
    do_sample=True, 
    max_length=max_len, 
    top_k=50,
    top_p= .92,      
    temperature= .9,
    early_stopping=False)
  
  return tokenizer.decode(output_tensor[0], skip_special_tokens=True).strip()
</code></pre>
<p>But when i try:</p>
<pre><code>text = &quot;this is an example of input text&quot;
comp = top_p_sampling(text, model_name, tokenizer_name)
</code></pre>
<p>I get the following error:</p>
<pre><code>TypeError: forward() got an unexpected keyword argument 'return_dict'
</code></pre>
<p>Full traceback:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-24-cc7c3f8aa367&gt; in &lt;module&gt;()
      1 text = &quot;this is an example of input text&quot;
----&gt; 2 comp = top_p_sampling(text, model_name, tokenizer_name)

4 frames
&lt;ipython-input-23-a5241487f309&gt; in top_p_sampling(text, model, tokenizer)
      9     temperature=temp,
     10     early_stopping=False,
---&gt; 11     return_dict=False)
     12 
     13   return tokenizer.decode(output_tensor[0], skip_special_tokens=True).strip()

/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py in decorate_context(*args, **kwargs)
     26         def decorate_context(*args, **kwargs):
     27             with self.__class__():
---&gt; 28                 return func(*args, **kwargs)
     29         return cast(F, decorate_context)
     30 

/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py in generate(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)
    938                 output_scores=output_scores,
    939                 return_dict_in_generate=return_dict_in_generate,
--&gt; 940                 **model_kwargs,
    941             )
    942 

/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py in sample(self, input_ids, logits_processor, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)
   1383                 return_dict=True,
   1384                 output_attentions=output_attentions,
-&gt; 1385                 output_hidden_states=output_hidden_states,
   1386             )
   1387             next_token_logits = outputs.logits[:, -1, :]

/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1100         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1101                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1102             return forward_call(*input, **kwargs)
   1103         # Do not call functions when jit is used
   1104         full_backward_hooks, non_full_backward_hooks = [], []

TypeError: forward() got an unexpected keyword argument 'return_dict'
</code></pre>
<p>I'm a bit of a rookie, so I hope someone can point out what I'm doing wrong. Thanks a lot</p>
","2022-04-04 13:13:29","","2022-04-04 13:52:37","2022-04-04 13:52:37","<huggingface-transformers><gpt-2>","0","1","1","1031","","","","","","",""
"72724956","1","15095688","","openai.error.InvalidRequestError: does not have access to the answers endpoint","<p>When I'm trying to implement the QA system with GPT-3, there is an error occurred:</p>
<pre><code>openai.error.InvalidRequestError: Org org-Ilv48EJDyLWiTc2SJWjOnRaM does not have access to the answers endpoint. Reach out to deprecation@openai.com if you have any questions
</code></pre>
<p>My code is:</p>
<pre class=""lang-py prettyprint-override""><code>import openai
openai.api_key = &quot;my-openai-key&quot;
 
document_list = [&quot;Google was founded in 1998 by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University in California. Together they own about 14 percent of its shares and control 56 percent of the stockholder voting power through supervoting stock. They incorporated Google as a privately held company on September 4, 1998. An initial public offering (IPO) took place on August 19, 2004, and Google moved to its headquarters in Mountain View, California, nicknamed the Googleplex. In August 2015, Google announced plans to reorganize its various interests as a conglomerate called Alphabet Inc. Google is Alphabet's leading subsidiary and will continue to be the umbrella company for Alphabet's Internet interests. Sundar Pichai was appointed CEO of Google, replacing Larry Page who became the CEO of Alphabet.&quot;,
&quot;Amazon is an American multinational technology company based in Seattle, Washington, which focuses on e-commerce, cloud computing, digital streaming, and artificial intelligence. It is one of the Big Five companies in the U.S. information technology industry, along with Google, Apple, Microsoft, and Facebook. The company has been referred to as 'one of the most influential economic and cultural forces in the world', as well as the world's most valuable brand. Jeff Bezos founded Amazon from his garage in Bellevue, Washington on July 5, 1994. It started as an online marketplace for books but expanded to sell electronics, software, video games, apparel, furniture, food, toys, and jewelry. In 2015, Amazon surpassed Walmart as the most valuable retailer in the United States by market capitalization.&quot;]
 
response = openai.Answer.create(
 search_model=&quot;ada&quot;,
 model=&quot;curie&quot;,
 question=&quot;when was google founded?&quot;,
 documents=document_list,
 examples_context=&quot;In 2017, U.S. life expectancy was 78.6 years.&quot;,
 examples=[[&quot;What is human life expectancy in the United States?&quot;,&quot;78 years.&quot;]],
 max_tokens=10,
 stop=[&quot;\n&quot;, &quot;&lt;|endoftext|&gt;&quot;],
)
 
print(response)
</code></pre>
<p>where &quot;my-openai-key&quot; is the secret key allocated in openai's website.</p>
","2022-06-23 05:28:56","","","2022-06-23 05:28:56","<gpt-3>","0","0","0","1004","","","","","","",""
"73972852","1","473923","73972895","GPT3 completion with insertion - invalid argument :suffix","<p>I am trying out completions using insertions.</p>
<p>It seems that I am supposed to use a parameter called <code>suffix:</code> to inform where the end of the insert goes.</p>
<p><a href=""https://i.stack.imgur.com/fYU8s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fYU8s.png"" alt=""enter image description here"" /></a></p>
<h2>The payload to the endpoint: <code>POST /v1/completions</code></h2>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;model&quot;: &quot;code-davinci-002&quot;,
  &quot;prompt&quot;: &quot;Write a JSON document for a person with first name, last name, email and phone number\n\n{\n&quot;,
  &quot;suffix&quot;: &quot;\n}&quot;,
  &quot;temperature&quot;: 0,
  &quot;max_tokens&quot;: 256,
  &quot;top_p&quot;: 1,
  &quot;frequency_penalty&quot;: 0,
  &quot;presence_penalty&quot;: 0
}
</code></pre>
<p>I tried doing this from a ruby implementation of GPT3.</p>
<pre class=""lang-rb prettyprint-override""><code>parameters
=&gt; {
:model=&gt;&quot;code-davinci-001&quot;,
 :prompt=&gt;&quot;generate some JSON for a person with first and last name {&quot;,
 :max_tokens=&gt;250,
 :temperature=&gt;0,
 :top_p=&gt;1,
 :frequency_penalty=&gt;0,
 :presence_penalty=&gt;0,
 :suffix=&gt;&quot;\n}&quot;}
</code></pre>
<pre class=""lang-rb prettyprint-override""><code>post(url: &quot;/v1/completions&quot;, parameters: parameters)
</code></pre>
<p>I get an invalid argument error for <code>suffix</code></p>
<pre class=""lang-rb prettyprint-override""><code>{&quot;error&quot;=&gt;{&quot;message&quot;=&gt;&quot;Unrecognized request argument supplied: suffix&quot;, &quot;type&quot;=&gt;&quot;invalid_request_error&quot;, &quot;param&quot;=&gt;nil, &quot;code&quot;=&gt;nil}}
</code></pre>
","2022-10-06 11:14:08","","","2022-10-06 11:17:43","<openai-api><gpt-3>","1","0","1","691","","2","473923","<p>I looked at the Payload from OpenAI vs the payload from the Ruby Library and saw the issue.</p>
<p>My ruby library was setting the model to <code>code-davinci-001</code> while OpenAI was using <code>code-davinci-002</code>.</p>
<p>As soon as I manually altered the model: attribute in debug, the completion started working correctly.</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;id&quot;=&gt;&quot;cmpl-5yJ8b01Cw26W6ZIHoRSOb71Dc4QvH&quot;,
  &quot;object&quot;=&gt;&quot;text_completion&quot;,
  &quot;created&quot;=&gt;1665054929,
  &quot;model&quot;=&gt;&quot;code-davinci-002&quot;,
  &quot;choices&quot;=&gt;
  [{&quot;text&quot;=&gt;&quot;\n    \&quot;firstName\&quot;: \&quot;John\&quot;,\n    \&quot;lastName\&quot;: \&quot;Smith\&quot;&quot;,
    &quot;index&quot;=&gt;0,
    &quot;logprobs&quot;=&gt;nil,
    &quot;finish_reason&quot;=&gt;&quot;stop&quot;}],
  &quot;usage&quot;=&gt;{&quot;prompt_tokens&quot;=&gt;14, &quot;completion_tokens&quot;=&gt;19, 
 &quot;total_tokens&quot;=&gt;33}
}
</code></pre>
","2022-10-06 11:17:43","0","1"
"74524530","1","11805611","74524554","How to get the items inside of an OpenAIobject in Python?","<p>I would like to get the text inside this data structure that is outputted via GPT3 OpenAI. I'm using Python. When I print the object I get:</p>
<pre><code>&lt;OpenAIObject text_completion id=cmpl-6F7ScZDu2UKKJGPXTiTPNKgfrikZ at 0x7f7648cacef0&gt; JSON: {
  &quot;choices&quot;: [
    {
      &quot;finish_reason&quot;: &quot;stop&quot;,
      &quot;index&quot;: 0,
      &quot;logprobs&quot;: null,
      &quot;text&quot;: &quot;\nWhat was Malcolm X's original name?\nMalcolm X's original name was Malcolm Little.\n\nWhere was Malcolm X born?\nMalcolm X was born in Omaha, Nebraska.\n\nWhat was the profession of Malcolm X's father?\nMalcolm X's father was a Baptist minister.\n\nWhat did Malcolm X do after he stopped attending school?\nMalcolm X became involved in petty criminal activities.&quot;
    }
  ],
  &quot;created&quot;: 1669061618,
  &quot;id&quot;: &quot;cmpl-6F7ScZDu2gJJHKZSPXTiTPNKgfrikZ&quot;,
  &quot;model&quot;: &quot;text-davinci-002&quot;,
  &quot;object&quot;: &quot;text_completion&quot;,
  &quot;usage&quot;: {
    &quot;completion_tokens&quot;: 86,
    &quot;prompt_tokens&quot;: 1200,
    &quot;total_tokens&quot;: 1286
  }
}
</code></pre>
<p>How do I get the 'text' component of this?
For example, if this object is called: qa ... I can output</p>
<pre><code>qa['choices']
</code></pre>
<p>And I get the same items as above... but adding a <code>.text</code> or ['text'] to this does not do it, and gets an error.</p>
<p>But not sure how to isolate the 'text'
I've read the docs, but cannot find this... <a href=""https://beta.openai.com/docs/api-reference/files/delete?lang=python"" rel=""nofollow noreferrer"">https://beta.openai.com/docs/api-reference/files/delete?lang=python</a></p>
","2022-11-21 20:26:49","","2023-04-02 20:17:47","2023-05-31 11:12:34","<python><openai-api><gpt-3>","4","0","2","4254","","2","20498988","<pre><code>x = {&amp;quot;choices&amp;quot;: [{&amp;quot;finish_reason&amp;quot;: &amp;quot;length&amp;quot;,
                  &amp;quot;text&amp;quot;: &amp;quot;, everyone, and welcome to the first installment of the new opening&amp;quot;}], }

text = x['choices'][0]['text']
print(text)  # , everyone, and welcome to the first installment of the new opening
</code></pre>
","2022-11-21 20:30:27","0","4"
"58991927","1","12205961","","Can the HuggingFace GPT2DoubleHeadsModel be used for non-multiple-choice next token prediction?","<p>According to the HuggingFace Transformer's website (<a href=""https://huggingface.co/transformers/model_doc/gpt2.html#gpt2doubleheadsmodel"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/gpt2.html#gpt2doubleheadsmodel</a>), <strong>GPT2DoubleHeadsModel</strong> (NOT <strong>GPT2LMHeadModel</strong> but <strong>GPT2DoubleHeadsModel</strong>) is the GPT-2 transformer model with a language modelling and a multiple-choice classification head on top e.g. for RocStories/SWAG tasks.</p>

<p>Does this mean that we can use the <strong>GPT2DoubleHeadsModel</strong> to process both non-multiple-choice-based language modelling tasks (i.e. next word prediction) as well as the multiple-choice questions, without making any adjustment to its head? Or would I need to adjust the head of the <strong>GPT2DoubleHeadsModel</strong> if I want to do the non-multiple-choice-based next word predictions because the <strong>GPT2DoubleHeadsModel</strong> is for answering multiple-choice type questions only?</p>

<p>I am a bit confused by this because the impression that I got from reading your GPT-2 paper is that GPT-2 uses language modelling process to process every type of language task (therefore GPT-2 would only have the regular language modelling head at the top), yet the name ""<strong>GPT2DoubleHeadsModel</strong>"" seem to suggest that I need to adjust the head of this GPT-2 for different types of language tasks.</p>

<p>Thank you,</p>
","2019-11-22 10:08:44","","2020-11-29 12:06:50","2020-11-29 12:06:50","<nlp><huggingface-transformers><transformer-model><gpt-2>","0","0","2","425","0","","","","","",""
"62677651","1","3659250","","OpenAI GPT-2 model use with TensorFlow JS","<p>Is that possible to generate texts from OpenAI GPT-2 using TensorFlowJS?</p>
<p>If not what is the limitation, like model format or ...?</p>
","2020-07-01 13:12:01","","2020-11-29 11:52:07","2021-07-29 09:46:12","<tensorflow><machine-learning><nlp><tensorflow.js><gpt-2>","1","5","10","2664","","","","","","",""
"63543006","1","11263621","","How can I find the probability of a sentence using GPT-2?","<p>I'm trying to write a program that, given a list of sentences, returns the most probable one. I want to use GPT-2, but I am quite new to using it (as in I don't really know how to do it). I'm planning on finding the probability of a word given the previous words and multiplying all the probabilities together to get the overall probability of that sentence occurring, however I don't know how to find the probability of a word occurring given the previous words. This is my (psuedo) code:</p>
<pre><code>sentences = # my list of sentences

max_prob = 0
best_sentence = sentences[0]

for sentence in sentences:
    prob = 1 #probability of that sentence

    for idx, word in enumerate(sentence.split()[1:]):
        prob *= probability(word, &quot; &quot;.join(sentence[:idx])) # this is where I need help

    if prob &gt; max_prob:
        max_prob = prob
        best_sentence = sentence

print(best_sentence)
</code></pre>
<p>Can I have some help please?</p>
","2020-08-23 03:07:19","","2020-11-29 12:05:32","2021-12-13 19:55:04","<python><nlp><probability><gpt-2>","4","0","2","4337","0","","","","","",""
"69602062","1","16451554","","Cudnn won't work when I install cudnn64_8.dll","<p>So I'm currently working with GPT2 running on Tensorflow for text generation. I'm working with <a href=""https://github.com/nshepperd/gpt-2"" rel=""nofollow noreferrer"">this repo</a> specifically. I recently decided to install CUDA and cudnn to improve GPU capability and installed it via <a href=""https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#installdriver-windows"" rel=""nofollow noreferrer"">these instructions</a>. I'm currently using Windows 10 x64 with NVIDIA Geforce GTX 1650 for my GPU and I'm using the command prompt terminal. I followed the instructions as best I could: downloaded the right GPU driver, set environment variables, copied cudnn files where they should go, etc. When I finished installing, I tried to generate an unconditional sample with the model I trained and this happened:</p>
<pre><code>Microsoft Windows [Version 10.0.19043.1288]
(c) Microsoft Corporation. All rights reserved.

C:\Users\&quot;username&quot;&gt;cd C:\Users\&quot;username&quot;\Desktop\gpt-2-finetuning\src

C:\Users\&quot;username&quot;\Desktop\gpt-2-finetuning\src&gt; python generate_unconditional_samples.py --model_name novel
2021-10-17 00:18:21.694165: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-10-17 00:18:22.435510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2153 MB memory:  -&gt; device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5
WARNING:tensorflow:From C:\Users\&quot;username&quot;\Desktop\gpt-2-finetuning\src\sample.py:60: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
WARNING:tensorflow:From C:\Users\&quot;username&quot;\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\util\dispatch.py:206: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.random.categorical` instead.
2021-10-17 00:18:45.451534: W tensorflow/core/common_runtime/bfc_allocator.cc:457] Allocator (GPU_0_bfc) ran out of memory trying to allocate 196.32MiB (rounded to 205852672)requested by op sample_sequence/while/body/_1/model/MatMul/ReadVariableOp
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation.
Current allocation summary follows.
Current allocation summary follows.
2021-10-17 00:18:45.467103: I tensorflow/core/common_runtime/bfc_allocator.cc:1004] BFCAllocator dump for GPU_0_bfc
2021-10-17 00:18:45.474451: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (256):  Total Chunks: 15, Chunks in use: 15. 3.8KiB allocated for chunks. 3.8KiB in use in bin. 60B client-requested in use in bin.
2021-10-17 00:18:45.481771: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (512):  Total Chunks: 1, Chunks in use: 0. 512B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:45.489403: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1024):         Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2021-10-17 00:18:45.498581: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2048):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:45.509522: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4096):         Total Chunks: 148, Chunks in use: 148. 592.0KiB allocated for chunks. 592.0KiB in use in bin. 592.0KiB client-requested in use in bin.
2021-10-17 00:18:45.517609: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8192):         Total Chunks: 25, Chunks in use: 25. 300.0KiB allocated for chunks. 300.0KiB in use in bin. 300.0KiB client-requested in use in bin.
2021-10-17 00:18:45.526116: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16384):        Total Chunks: 24, Chunks in use: 24. 384.0KiB allocated for chunks. 384.0KiB in use in bin. 384.0KiB client-requested in use in bin.
2021-10-17 00:18:45.536214: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (32768):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:45.548694: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (65536):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:45.563635: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (131072):       Total Chunks: 4, Chunks in use: 4. 786.0KiB allocated for chunks. 786.0KiB in use in bin. 785.3KiB client-requested in use in bin.
2021-10-17 00:18:45.578935: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (262144):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:45.594547: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (524288):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:45.601621: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1048576):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:45.608788: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2097152):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:45.619285: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4194304):      Total Chunks: 25, Chunks in use: 25. 100.00MiB allocated for chunks. 100.00MiB in use in bin. 100.00MiB client-requested in use in bin.
2021-10-17 00:18:45.628480: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8388608):      Total Chunks: 24, Chunks in use: 24. 288.00MiB allocated for chunks. 288.00MiB in use in bin. 288.00MiB client-requested in use in bin.
2021-10-17 00:18:45.637872: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16777216):     Total Chunks: 48, Chunks in use: 48. 768.00MiB allocated for chunks. 768.00MiB in use in bin. 768.00MiB client-requested in use in bin.
2021-10-17 00:18:45.651217: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (33554432):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:45.663622: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (67108864):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:45.677210: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (134217728):    Total Chunks: 5, Chunks in use: 5. 995.43MiB allocated for chunks. 995.43MiB in use in bin. 981.58MiB client-requested in use in bin.
2021-10-17 00:18:45.686363: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (268435456):    Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:45.701152: I tensorflow/core/common_runtime/bfc_allocator.cc:1027] Bin for 196.32MiB was 128.00MiB, Chunk State:
2021-10-17 00:18:45.710829: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 2258055936
2021-10-17 00:18:45.715322: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at b0a600000 of size 1280 next 1
2021-10-17 00:18:45.727700: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at b0a600500 of size 12582912 next 2
2021-10-17 00:18:45.735730: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at b0b200500 of size 12288 next 3
2021-10-17 00:18:45.745330: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at b0b203500 of size 16384 next 4
2021-10-17 00:18:45.757304: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at b0b207500 of size 4096 next 5
2021-10-17 00:18:45.777662: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at b0b208500 of size 16777216 next 6

...goes on for a while like this

2021-10-17 00:18:49.046582: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at b6b4a3e00 of size 12288 next 318
2021-10-17 00:18:49.056312: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at b6b4a6e00 of size 205852672 next 313
2021-10-17 00:18:49.063244: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at b778f7e00 of size 205852672 next 319
2021-10-17 00:18:49.069964: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at b83d48e00 of size 220374272 next 18446744073709551615
2021-10-17 00:18:49.076724: I tensorflow/core/common_runtime/bfc_allocator.cc:1065]      Summary of in-use Chunks by size:
2021-10-17 00:18:49.085663: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 15 Chunks of size 256 totalling 3.8KiB
2021-10-17 00:18:49.092613: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 1280 totalling 1.2KiB
2021-10-17 00:18:49.101615: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 148 Chunks of size 4096 totalling 592.0KiB
2021-10-17 00:18:49.109453: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 25 Chunks of size 12288 totalling 300.0KiB
2021-10-17 00:18:49.118227: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 24 Chunks of size 16384 totalling 384.0KiB
2021-10-17 00:18:49.125224: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 4 Chunks of size 201216 totalling 786.0KiB
2021-10-17 00:18:49.134291: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 25 Chunks of size 4194304 totalling 100.00MiB
2021-10-17 00:18:49.142594: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 24 Chunks of size 12582912 totalling 288.00MiB
2021-10-17 00:18:49.150332: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 48 Chunks of size 16777216 totalling 768.00MiB
2021-10-17 00:18:49.159611: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 4 Chunks of size 205852672 totalling 785.27MiB
2021-10-17 00:18:49.166664: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 220374272 totalling 210.17MiB
2021-10-17 00:18:49.175719: I tensorflow/core/common_runtime/bfc_allocator.cc:1072] Sum Total of in-use chunks: 2.10GiB
2021-10-17 00:18:49.179917: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] total_region_allocated_bytes_: 2258055936 memory_limit_: 2258055988 available bytes: 52 curr_region_allocation_bytes_: 4516112384
2021-10-17 00:18:49.186738: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] Stats:
Limit:                      2258055988
InUse:                      2258055424
MaxInUse:                   2258055424
NumAllocs:                         326
MaxAllocSize:                220374272
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2021-10-17 00:18:49.214161: W tensorflow/core/common_runtime/bfc_allocator.cc:468] ****************************************************************************************************
2021-10-17 00:18:49.224793: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at resource_variable_ops.cc:158 : Resource exhausted: OOM when allocating tensor with shape[50257,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
2021-10-17 00:18:49.234240: W tensorflow/core/common_runtime/bfc_allocator.cc:457] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.0KiB (rounded to 4096)requested by op sample_sequence/model/h0/attn/split
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation.
Current allocation summary follows.
Current allocation summary follows.
2021-10-17 00:18:49.253961: I tensorflow/core/common_runtime/bfc_allocator.cc:1004] BFCAllocator dump for GPU_0_bfc
2021-10-17 00:18:49.260477: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (256):  Total Chunks: 15, Chunks in use: 15. 3.8KiB allocated for chunks. 3.8KiB in use in bin. 60B client-requested in use in bin.
2021-10-17 00:18:49.267677: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (512):  Total Chunks: 1, Chunks in use: 0. 512B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:49.274584: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1024):         Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2021-10-17 00:18:49.282179: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2048):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:49.291707: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4096):         Total Chunks: 148, Chunks in use: 148. 592.0KiB allocated for chunks. 592.0KiB in use in bin. 592.0KiB client-requested in use in bin.
2021-10-17 00:18:49.299699: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8192):         Total Chunks: 25, Chunks in use: 25. 300.0KiB allocated for chunks. 300.0KiB in use in bin. 300.0KiB client-requested in use in bin.
2021-10-17 00:18:49.309406: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16384):        Total Chunks: 24, Chunks in use: 24. 384.0KiB allocated for chunks. 384.0KiB in use in bin. 384.0KiB client-requested in use in bin.
2021-10-17 00:18:49.316823: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (32768):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:49.323705: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (65536):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:49.330699: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (131072):       Total Chunks: 4, Chunks in use: 4. 786.0KiB allocated for chunks. 786.0KiB in use in bin. 785.3KiB client-requested in use in bin.
2021-10-17 00:18:49.341079: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (262144):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:49.347442: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (524288):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:49.355050: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1048576):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:49.362441: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2097152):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:49.373022: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4194304):      Total Chunks: 25, Chunks in use: 25. 100.00MiB allocated for chunks. 100.00MiB in use in bin. 100.00MiB client-requested in use in bin.
2021-10-17 00:18:49.379516: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8388608):      Total Chunks: 24, Chunks in use: 24. 288.00MiB allocated for chunks. 288.00MiB in use in bin. 288.00MiB client-requested in use in bin.
2021-10-17 00:18:49.386849: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16777216):     Total Chunks: 48, Chunks in use: 48. 768.00MiB allocated for chunks. 768.00MiB in use in bin. 768.00MiB client-requested in use in bin.
2021-10-17 00:18:49.394833: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (33554432):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:49.406519: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (67108864):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:49.413489: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (134217728):    Total Chunks: 5, Chunks in use: 5. 995.43MiB allocated for chunks. 995.43MiB in use in bin. 981.58MiB client-requested in use in bin.
2021-10-17 00:18:49.423166: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (268435456):    Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-10-17 00:18:49.433375: I tensorflow/core/common_runtime/bfc_allocator.cc:1027] Bin for 4.0KiB was 4.0KiB, Chunk State:
2021-10-17 00:18:49.439983: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 2258055936
2021-10-17 00:18:49.446385: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at b0a600000 of size 1280 next 1
2021-10-17 00:18:49.453157: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at b0a600500 of size 12582912 next 2

...etc, etc...

2021-10-17 00:18:52.034032: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at b6b4a3e00 of size 12288 next 318
2021-10-17 00:18:52.041039: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at b6b4a6e00 of size 205852672 next 313
2021-10-17 00:18:52.050136: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at b778f7e00 of size 205852672 next 319
2021-10-17 00:18:52.057217: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at b83d48e00 of size 220374272 next 18446744073709551615
2021-10-17 00:18:52.066414: I tensorflow/core/common_runtime/bfc_allocator.cc:1065]      Summary of in-use Chunks by size:
2021-10-17 00:18:52.074512: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 15 Chunks of size 256 totalling 3.8KiB
2021-10-17 00:18:52.083562: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 1280 totalling 1.2KiB
2021-10-17 00:18:52.091067: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 148 Chunks of size 4096 totalling 592.0KiB
2021-10-17 00:18:52.097600: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 25 Chunks of size 12288 totalling 300.0KiB
2021-10-17 00:18:52.105189: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 24 Chunks of size 16384 totalling 384.0KiB
2021-10-17 00:18:52.114193: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 4 Chunks of size 201216 totalling 786.0KiB
2021-10-17 00:18:52.121798: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 25 Chunks of size 4194304 totalling 100.00MiB
2021-10-17 00:18:52.131072: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 24 Chunks of size 12582912 totalling 288.00MiB
2021-10-17 00:18:52.138520: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 48 Chunks of size 16777216 totalling 768.00MiB
2021-10-17 00:18:52.145005: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 4 Chunks of size 205852672 totalling 785.27MiB
2021-10-17 00:18:52.151508: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 220374272 totalling 210.17MiB
2021-10-17 00:18:52.160622: I tensorflow/core/common_runtime/bfc_allocator.cc:1072] Sum Total of in-use chunks: 2.10GiB
2021-10-17 00:18:52.165037: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] total_region_allocated_bytes_: 2258055936 memory_limit_: 2258055988 available bytes: 52 curr_region_allocation_bytes_: 4516112384
2021-10-17 00:18:52.174756: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] Stats:
Limit:                      2258055988
InUse:                      2258055424
MaxInUse:                   2258055424
NumAllocs:                         326
MaxAllocSize:                220374272
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2021-10-17 00:18:52.197768: W tensorflow/core/common_runtime/bfc_allocator.cc:468] ****************************************************************************************************
2021-10-17 00:18:52.207819: W tensorflow/core/framework/op_kernel.cc:1692] OP_REQUIRES failed at split_op.cc:308 : Resource exhausted: OOM when allocating tensor with shape[1,1,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File &quot;C:\Users\&quot;username&quot;\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\client\session.py&quot;, line 1375, in _do_call
    return fn(*args)
  File &quot;C:\Users\&quot;username&quot;\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\client\session.py&quot;, line 1359, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,
  File &quot;C:\Users\&quot;username&quot;\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\client\session.py&quot;, line 1451, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.
  (0) Resource exhausted: OOM when allocating tensor with shape[50257,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[{{node model/MatMul/ReadVariableOp}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

         [[strided_slice/_645]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

  (1) Resource exhausted: OOM when allocating tensor with shape[50257,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[{{node model/MatMul/ReadVariableOp}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Users\&quot;username&quot;\Desktop\gpt-2-finetuning\src\generate_unconditional_samples.py&quot;, line 79, in &lt;module&gt;
    fire.Fire(sample_model)
  File &quot;C:\Users\&quot;username&quot;\AppData\Local\Programs\Python\Python39\lib\site-packages\fire\core.py&quot;, line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File &quot;C:\Users\&quot;username&quot;\AppData\Local\Programs\Python\Python39\lib\site-packages\fire\core.py&quot;, line 466, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File &quot;C:\Users\&quot;username&quot;\AppData\Local\Programs\Python\Python39\lib\site-packages\fire\core.py&quot;, line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File &quot;C:\Users\&quot;username&quot;\Desktop\gpt-2-finetuning\src\generate_unconditional_samples.py&quot;, line 71, in sample_model
    out = sess.run(output)
  File &quot;C:\Users\&quot;username&quot;\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\client\session.py&quot;, line 967, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File &quot;C:\Users\&quot;username&quot;\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\client\session.py&quot;, line 1190, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File &quot;C:\Users\&quot;username&quot;\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\client\session.py&quot;, line 1368, in _do_run
    return self._do_call(_run_fn, feeds, fetches, targets, options,
  File &quot;C:\Users\&quot;username&quot;\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\python\client\session.py&quot;, line 1394, in _do_call
    raise type(e)(node_def, op, message)  # pylint: disable=no-value-for-parameter
tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.
  (0) Resource exhausted: OOM when allocating tensor with shape[50257,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[{{node model/MatMul/ReadVariableOp}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

         [[strided_slice/_645]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

  (1) Resource exhausted: OOM when allocating tensor with shape[50257,1024] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[{{node model/MatMul/ReadVariableOp}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

0 successful operations.
0 derived errors ignored.
</code></pre>
<p>Wasn't sure why this was happening and figured that I installed the cudnn files incorrectly. Messed around for a bit and found out that when I removed cudnn64_8.dll from C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.4\bin where I was told to copy it and then ran an unconditional sample, GPT2 worked just fine and was able to generate some text. All the other cudnn files were still in their CUDA directories. Not sure why the inclusion of cudnn64_8.dll would screw things up. Did I install the wrong version of CUDA? What exactly is going on here?</p>
<p>EDIT:</p>
<p>So I decided to add <code>TF_GPU_ALLOCATOR=cuda_malloc_async</code> to environment variables as the terminal suggested above. This time I didn't get an OOM error like last time, but it also terminated the program. Here's the result:</p>
<pre><code>Microsoft Windows [Version 10.0.19043.1288]
(c) Microsoft Corporation. All rights reserved.

C:\Users\&quot;username&quot;&gt;cd C:\Users\&quot;username&quot;\Desktop\gpt-2-finetuning\src

C:\Users\&quot;username&quot;\Desktop\gpt-2-finetuning\src&gt;python generate_unconditional_samples.py --model_name novel
2021-10-17 15:20:12.172740: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-10-17 15:20:12.681534: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:215] Using CUDA malloc Async allocator for GPU: 0

C:\Users\&quot;username&quot;\Desktop\gpt-2-finetuning\src&gt;
</code></pre>
<p>What exactly am I doing wrong here? Why is my GPU running out of memory?</p>
","2021-10-17 07:06:41","","2021-10-17 21:23:31","2021-10-17 21:23:31","<python><tensorflow><cudnn><gpt-2>","0","2","0","264","","","","","","",""
"71580925","1","4467390","","Generating 10000 sentences from GptNeo Model results in out of memory error","<p>I was doing some work where I wanted to generate 10000 sentences from the GptNeo Model. I have a GPU of size 40GB and am running the model in the GPU but everytime the code runs out of memory. Is there a limitation to the number of sentences that I can generate. Below is a small snippet of my code.</p>
<pre><code>tokenizer = GPT2Tokenizer.from_pretrained(model)
model = GPTNeoForCausalLM.from_pretrained(model , pad_token_id = tokenizer.eos_token_id)
model.to(device)
input_ids = tokenizer.encode(sentence, return_tensors=‘pt’)
gen_tokens = model.generate(
input_ids,
do_sample=True,
top_k=50,
num_return_sequences=10000
)
</code></pre>
","2022-03-23 01:47:29","","","2022-03-23 01:47:29","<nlp><huggingface-transformers><gpt-2>","0","3","0","88","","","","","","",""
"62799540","1","1019952","","Cannot convert GPT-2 model using Tensorflow.JS","<p>I'm trying to load a GPT-2 model on a Node.JS project. I believe this could be done using tfjs library. So I tried to convert the GPT-2 model to tfjs model. Following recommendations on <a href=""https://stackoverflow.com/questions/62677651/openai-gpt-2-model-use-with-tensorflow-js"">this answer</a>, I exported the GPT-2 model as SavedModel.</p>
<pre><code>!python3 -m pip install -q git+https://github.com/huggingface/transformers.git
!python3 -m pip install tensorflow tensorflowjs
</code></pre>
<p>Then ran the following code to export the SavedModel xx.pb file.</p>
<pre><code>from transformers import TFGPT2LMHeadModel, GPT2Tokenizer
import tensorflowjs
tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
# add the EOS token as PAD token to avoid warnings
model = TFGPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;, pad_token_id=tokenizer.eos_token_id)
model.save(&quot;./test_gpt2&quot;)
</code></pre>
<p>Then I ran this command to convert the SavedModel to tfjs compatible file.</p>
<pre><code>!tensorflowjs_converter \
    --input_format=tf_saved_model \
    --output_node_names='gpt2' \
    --saved_model_tags=serve \
    /content/test_gpt2 \
    /content/test_gpt2_web_model
</code></pre>
<p>This causes an error</p>
<pre><code>2020-07-08 16:36:11.455383: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-07-08 16:36:11.459979: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2300000000 Hz
2020-07-08 16:36:11.460216: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2e5b100 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-08 16:36:11.460284: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-08 16:36:18.337463: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2020-07-08 16:36:18.337631: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-07-08 16:36:18.536301: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize
2020-07-08 16:36:18.536373: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 163 nodes (0), 175 edges (0), time = 43.871ms.
2020-07-08 16:36:18.536384: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: Graph size after: 163 nodes (0), 175 edges (0), time = 50.779ms.
2020-07-08 16:36:18.536393: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: __inference__wrapped_model_24863
2020-07-08 16:36:18.536402: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.004ms.
2020-07-08 16:36:18.536411: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.
Traceback (most recent call last):
  File &quot;/usr/local/bin/tensorflowjs_converter&quot;, line 8, in &lt;module&gt;
    sys.exit(pip_main())
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/converter.py&quot;, line 735, in pip_main
    main([' '.join(sys.argv[1:])])
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/converter.py&quot;, line 739, in main
    convert(argv[0].split(' '))
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/converter.py&quot;, line 681, in convert
    control_flow_v2=args.control_flow_v2)
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py&quot;, line 494, in convert_tf_saved_model
    weight_shard_size_bytes=weight_shard_size_bytes)
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflowjs/converters/tf_saved_model_conversion_v2.py&quot;, line 143, in optimize_graph
    ', '.join(unsupported))
ValueError: Unsupported Ops in the model before optimization
StatefulPartitionedCall
</code></pre>
<p>It says <code>StatefulPartitionedCall</code> is unsupported. Is there a way this could be solved?</p>
","2020-07-08 16:41:54","","2020-11-29 11:49:46","2021-02-21 16:25:26","<python><tensorflow><tensorflow.js><gpt-2>","1","0","1","783","","","","","","",""
"63350105","1","11718897","","How to alter gpt-2 code to work with Tensorflow 2.0?","<p>I am trying to use gpt-2 for text generation. I get compatibility errors, even after running the Tensorflow 2.0 <a href=""https://www.tensorflow.org/guide/upgrade"" rel=""nofollow noreferrer"">code upgrade script</a>.</p>
<p>Steps I've followed:</p>
<ol>
<li><p>Clone <a href=""https://github.com/openai/gpt-2"" rel=""nofollow noreferrer"">repo</a></p>
</li>
<li><p>From here on out, follow the directions in DEVELOPERS.md</p>
</li>
<li><p>Run <a href=""https://www.tensorflow.org/guide/upgrade"" rel=""nofollow noreferrer"">upgrade script</a> on files in /src</p>
</li>
<li><p>In terminal run: <code>sudo docker build --tag gpt-2 -f Dockerfile.gpu .</code></p>
</li>
<li><p>After building is done, run: <code>sudo docker run --runtime=nvidia -it gpt-2 bash</code></p>
</li>
<li><p>Enter <code>python3 src/generate_unconditional_samples.py | tee /tmp/samples</code></p>
</li>
<li><p>Get this traceback:</p>
<pre><code>Traceback (most recent call last):
File &quot;src/generate_unconditional_samples.py&quot;, line 9, in &lt;module&gt;  
import model, sample, encoder
File &quot;/gpt-2/src/model.py&quot;, line 4, in &lt;module&gt;
from tensorboard.plugins.hparams.api import HParam
ImportError: No module named 'tensorboard.plugins.hparams'
root@f8bdde043f91:/gpt-2# python3 src/generate_unconditional_samples.py | tee 
/tmp/samples
Traceback (most recent call last):
File &quot;src/generate_unconditional_samples.py&quot;, line 9, in &lt;module&gt;
import model, sample, encoder
File &quot;/gpt-2/src/model.py&quot;, line 4, in &lt;module&gt;
from tensorboard.plugins.hparams.api import HParam
ImportError: No module named 'tensorboard.plugins.hparams'```

</code></pre>
</li>
</ol>
<p>It appears that HParams has been deprecated and the new version in Tensorflow 2.0 is called <a href=""https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams#1_experiment_setup_and_the_hparams_experiment_summary"" rel=""nofollow noreferrer"">HParam</a>. However, the parameters are different. In <code>model.py</code>, the params are instantiated as follows:</p>
<pre><code>def default_hparams():
return HParams(
    n_vocab=0,
    n_ctx=1024,
    n_embd=768,
    n_head=12,
    n_layer=12,
)
</code></pre>
<p>There doesn't appear to be any 1:1 translation into Tensorflow 2.0. Does anyone know how to make gpt-2 work with Tensorflow 2.0?</p>
<p>My GPU is an NVIDIA 20xx.</p>
<p>Thank you.</p>
","2020-08-11 01:09:02","","2020-11-29 12:02:24","2021-03-09 09:45:25","<python><docker><tensorflow><tensorflow2.0><gpt-2>","3","0","2","3613","","","","","","",""
"72821522","1","11484585","","Why does Post request to OpenAI in Unity result in error 400?","<p>I am trying to use GPT3 in a game I am making but I can't seem to be able to call the OpenAI API correctly. I got most of this from the Unity docs.
Here is the code I am using:</p>
<pre><code>public class gpt3_complete : MonoBehaviour
{
    public string model;
    public string prompt;
    public int len;
    public string temp;
    public string api_key = &quot;&lt;key&gt;&quot;;
    void Start()
    {
        StartCoroutine(Upload());
    }

    IEnumerator Upload()
    {
        WWWForm form = new WWWForm();
        form.AddField(&quot;model&quot;, model);
        form.AddField(&quot;prompt&quot;, prompt);
        form.AddField(&quot;max_tokens&quot;, len);
        form.AddField(&quot;temperature&quot;, temp);
        //form.headers.Add(&quot;Authorization&quot;, &quot;Bearer &quot;+api_key);



        using (UnityWebRequest www = UnityWebRequest.Post(&quot;https://api.openai.com/v1/completions&quot;, form))
        {
            www.SetRequestHeader(&quot;Authorization&quot;, &quot;Bearer &quot; + api_key);
            www.SetRequestHeader(&quot;Content-Type&quot;, &quot;application/json&quot;);

            yield return www.SendWebRequest();

            if (www.result != UnityWebRequest.Result.Success)
            {
                Debug.Log(www.error);
            }
            else
            {
                Debug.Log(www.result);
                Debug.Log(&quot;Form upload complete!&quot;);
            }
        }
    }
}
</code></pre>
<p>This always returns: 400 Bad Request.
The GPT3 docs can be found here: <a href=""https://beta.openai.com/docs/api-reference/completions/create"" rel=""nofollow noreferrer"">https://beta.openai.com/docs/api-reference/completions/create</a></p>
<p>Any idea why this is?
This is my first time making any web requests in unity so I'm probably missing something obvious.
Thanks!</p>
","2022-06-30 20:13:50","","","2023-02-18 16:06:27","<unity-game-engine><http-post><artificial-intelligence><gpt-3>","1","4","1","1973","","","","","","",""
"72758187","1","19417188","","OpenAI GPT-3 API: Fine tune a fine tuned model?","<p>The OpenAI documentation for the <code>model</code> attribute in the fine-tune API states a bit confusingly:</p>
<blockquote>
<p><strong>model</strong></p>
<p>The name of the base model to fine-tune. You can select one of &quot;ada&quot;, &quot;babbage&quot;, &quot;curie&quot;, &quot;davinci&quot;, or a fine-tuned model created after 2022-04-21.</p>
</blockquote>
<p>My question: is it better to fine-tune a base model or a fine-tuned model?</p>
<p>I created a fine-tune model from <code>ada</code> with file <code>mydata1K.jsonl</code>:</p>
<pre><code>ada + mydata1K.jsonl --&gt; ada:ft-acme-inc-2022-06-25
</code></pre>
<p>Now I have a bigger file of samples <code>mydata2K.jsonl</code> that I want to use to improve the fine-tuned model.
In this second round of fine-tuning, is it better to fine-tune <code>ada</code> again or to fine-tune my fine-tuned model <code>ada:ft-acme-inc-2022-06-25</code>?  I'm assuming this is possible because my fine tuned model is created after 2022-04-21.</p>
<pre><code>ada + mydata2K.jsonl --&gt; better-model
</code></pre>
<p>or</p>
<pre><code>ada:ft-acme-inc-2022-06-25 + mydata2K.jsonl --&gt; even-better-model?
</code></pre>
","2022-06-26 00:35:25","","2023-03-13 13:30:31","2023-03-13 13:30:31","<transformer-model><openai-api><fine-tune><gpt-3>","1","1","9","2193","","","","","","",""
"67058277","1","15221534","","Understanding repository gpt transformer","<p>For my project I need to understand and being able to execute <a href=""https://github.com/atcbosselut/comet-commonsense"" rel=""nofollow noreferrer"">this</a> github repository about commonsense generation using the GPT transformer language model. It is quite extensive and I don't have enough programming experience to make sense of it all. Is there anyone who is good with these subjects who can guide me through it/help me?</p>
<p>Or, is there another spot where I can post this question?</p>
","2021-04-12 12:21:59","","","2021-04-12 12:21:59","<github><nlp><transformer-model><google-publisher-tag><gpt-2>","0","4","0","64","","","","","","",""
"73014448","1","6501180","","Is there a known workaround for the max token limit on the input to GPT-3?","<p>For a bit of context, I recently started working on a personal project that accepts the URL of some recipe web page, pulls the HTML, converts the HTML to simplified markdown (this is the GPT-3 part), then sends that markdown to a thermal receipt printer in my kitchen, which prints it out.</p>
<p>Recipe web pages have a wide variety of structures, and they are notorious for including long and often irrelevant articles before the recipe, for the sake of SEO.</p>
<p>My plan was to use the fine-tuning API for davinci2, and feed it a bunch of straight up recipe HTML as input and cleaned, recipe-only markdown as output. I notice though that the maximum input token count for both training and inference is 4096. The HTML for a web page can be much larger than that, like 20k tokens.</p>
<p>I am wondering if anyone has found a workaround for training and driving GPT-3 with more tokens than 4096.</p>
<p>I'm open to other suggestions as well. For instance, I've considered passing just the visible text on the page, rather than the full HTML tree, but there is much less context present in that form, and the models seems more easily confused by all of the links and other navigational elements present in the page. I have also considered only allowing this project to accept &quot;printer-friendly&quot; versions of recipes, which tend to be much smaller and would easily come in under the 4096 token limit, but not all sites offer a printer-friendly article, and I don't want this to be a limitation.</p>
","2022-07-17 18:43:06","","","2023-01-23 00:56:34","<machine-learning><gpt-3>","2","0","4","4818","","","","","","",""
"73117628","1","19564052","","How to solve API connection error and SSL certification error while connecting to GPT-3 open AI?","<p>I am trying to run a python script(jupyter notebook) by experimenting with GPT-3 open AI to create some NLP project and understand its functions and used cases. I got an error of SSL certification and API connection while I was trying to open a JSON file. I checked some solutions on the internet but it did not offer any remedy. I simply tried connecting to the server through API key but the code was not working. The code I executed is as follows-</p>
<pre><code>import ssl
import certifi
certifi.where()
import openai
api_key='my_api_key'            #it is confidential string
openai.api_key = api_key
response = openai.File.create(file=open(&quot;C:\\Users\\pythons_scripts\\Corporate Governance1658287996.json&quot;), purpose=&quot;search&quot;)
print(response)
</code></pre>
<p>So the above script is throwing all of the following errors-</p>
<pre><code>SSLCertVerificationError                  Traceback (most recent call last)    
SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1129)
APIConnectionError                        Traceback (most recent call last)
APIConnectionError: Error communicating with OpenAI
</code></pre>
<p>Does anyone know how to get around with this ? or has anyone solved this kind of problem? can someone suggest a solution which will work?</p>
","2022-07-26 04:07:12","","","2023-03-21 11:54:26","<python-3.x><api><ssl-certificate><openai-api><gpt-3>","2","0","2","8064","","","","","","",""
"74978793","1","5038122","74992998","OpenAI GPT-3 API error: ""InvalidRequestError: Unrecognized request argument supplied""","<pre><code>import openai

# Set the API key
openai.api_key = &quot;YOUR API KEY&quot;

# Define the conversation memory
conversation_memory = {
    &quot;previous_question&quot;: &quot;What is the capital of France?&quot;,
    &quot;previous_answer&quot;: &quot;The capital of France is Paris.&quot;
}

# Make the API request
response = openai.Completion.create(
    model=&quot;text-davinci-003&quot;,
    prompt=&quot;Where is the Eiffel Tower located?&quot;,
    temperature=0.5,
    max_tokens=1024,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0,
    conversation_memory=conversation_memory
)

# Print the response
print(response.text)
</code></pre>
<p>Why the conversation_memory  parameter not being recognize. I try this with serveral different models and they all give me the same error. I have the lastest OpenAi on my computer. I don't understand.</p>
<p>Here the error:</p>
<pre><code>     InvalidRequestError                       Traceback (most recent call last) &lt;ipython-input-17-ace11d6ce405&gt; in &lt;module&gt;      11      12 # Make the API request ---&gt; 13 response = openai.Completion.create(      14     model=&quot;text-babbage-001&quot;,      15     prompt=&quot;Where is the Eiffel Tower located?&quot;, C:\ProgramData\Anaconda3\lib\site-packages\openai\api_resources\completion.py in create(cls, *args, **kwargs)      23 while True:      24 try: ---&gt; 25 return super().create(*args, **kwargs)      26 except TryAgain as e:      27 if timeout is not None and time.time() &gt; start + timeout: C:\ProgramData\Anaconda3\lib\site-packages\openai\api_resources\abstract\engine_api_resource.py in create(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)     113         )     114         url = cls.class_url(engine, api_type, api_version) --&gt; 115         response, _, api_key = requestor.request(     116 &quot;post&quot;,     117             url, C:\ProgramData\Anaconda3\lib\site-packages\openai\api_requestor.py in request(self, method, url, params, headers, files, stream, request_id, request_timeout)     179             request_timeout=request_timeout,     180         ) --&gt; 181 resp, got_stream = self._interpret_response(result, stream)     182 return resp, got_stream, self.api_key     183 C:\ProgramData\Anaconda3\lib\site-packages\openai\api_requestor.py in _interpret_response(self, result, stream)     394 else:     395             return ( --&gt; 396                 self._interpret_response_line(     397                     result.content, result.status_code, result.headers, stream=False     398                 ),  C:\ProgramData\Anaconda3\lib\site-packages\openai\api_requestor.py in _interpret_response_line(self, rbody, rcode, rheaders, stream)     427         stream_error = stream and &quot;error&quot; in resp.data     428 if stream_error or not 200 &lt;= rcode &lt; 300: --&gt; 429             raise self.handle_error_response(     430                 rbody, rcode, resp.data, rheaders, stream_error=stream_error     431             ) 
 InvalidRequestError: Unrecognized request argument supplied: conversation_memory 
</code></pre>
","2023-01-02 03:28:18","","2023-03-13 13:28:06","2023-03-13 13:28:49","<python><artificial-intelligence><openai-api><gpt-3>","1","0","0","2798","","2","10347145","<p>The error itself tells you what's wrong.</p>
<p><strong>You're trying to pass <code>conversation_memory</code> as a parameter to the Completions endpoint, which the OpenAI API doesn't recognize as a parameter.</strong></p>
<p>See the <a href=""https://beta.openai.com/docs/api-reference/completions/create"" rel=""nofollow noreferrer"">complete list</a> of parameters you can pass to the Completions endpoint:</p>
<ul>
<li><code>model</code></li>
<li><code>prompt</code></li>
<li><code>suffix</code></li>
<li><code>max_tokens</code></li>
<li><code>temperature</code></li>
<li><code>top_p</code></li>
<li><code>n</code></li>
<li><code>stream</code></li>
<li><code>logprobs</code></li>
<li><code>echo</code></li>
<li><code>stop</code></li>
<li><code>presence_penalty</code></li>
<li><code>frequency_penalty</code></li>
<li><code>best_of</code></li>
<li><code>logit_bias</code></li>
<li><code>user</code></li>
</ul>
","2023-01-03 11:51:49","0","1"
"75112672","1","10337134","75124884","No module named 'openai_secret_manager'","<p>I asked <strong>ChatGPT</strong> about my CSV data, and ChatGPT answered:</p>
<p>&quot;Here is an example of how you can read a CSV file using pandas, and then use the data to train or fine-tune <strong>GPT-3</strong> using the <strong>OpenAI</strong> API:&quot;</p>
<pre><code>import pandas as pd
import openai_secret_manager

# Read the CSV file
df = pd.read_csv(&quot;example.csv&quot;)

# Get the OpenAI API key
secrets = openai_secret_manager.get_secrets(&quot;openai&quot;)
openai_api_key = secrets[&quot;api_key&quot;]

# Use the data from the CSV file to train or fine-tune GPT-3
# (Assuming you have the OpenAI API key and the OpenAI Python library installed)
import openai
openai.api_key = openai_api_key
response = openai.Completion.create(
    engine=&quot;text-davinci-002&quot;,
    prompt=(f&quot;train on data from example.csv{df}&quot;),
    max_tokens=2048,
    n = 1,
    stop=None,
    temperature=0.5,
)
print(response[&quot;choices&quot;][0][&quot;text&quot;])
</code></pre>
<p>But, I got this error:</p>
<p><code>ModuleNotFoundError: No module named 'openai_secret_manager'</code></p>
","2023-01-13 17:40:20","","2023-01-13 17:55:42","2023-03-25 14:21:52","<python><pandas><openai-api><gpt-3>","2","5","2","9195","","2","12146581","<p>No need to use <strong>openai_secret_manager</strong>. I faced the same problem and deleted it and you need to generate &amp; place an API from your account on OpenAI directly to the code.</p>
<pre><code>import pandas as pd
import openai_secret_manager

# Read the CSV file
df = pd.read_csv(&quot;example.csv&quot;)

# Use the data from the CSV file to train or fine-tune GPT-3
# (Assuming you have the OpenAI API key and the OpenAI Python library installed)
import openai
openai.api_key = openai_api_key
response = openai.Completion.create(
    engine=&quot;text-davinci-002&quot;,
    prompt=(f&quot;train on data from example.csv{df}&quot;),
    max_tokens=2048,
    n = 1,
    stop=None,
    temperature=0.5,
)
print(response[&quot;choices&quot;][0][&quot;text&quot;])
</code></pre>
<p><a href=""https://i.stack.imgur.com/MgxNP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MgxNP.png"" alt=""enter image description here"" /></a></p>
<p>Copy and paste the API and replace <strong>openai_api_key</strong> here</p>
<pre><code>openai.api_key = &quot;PLACE_YOUR_API_IN_HERE&quot;
</code></pre>
","2023-01-15 11:56:45","4","4"
"75130116","1","13553999","75130180","Getting 400 Bad Request from Open AI API using Python Flask","<p>I want to get response using Flask from OpenAI API. Whether I am getting Status 400 Bad Request from Browser through <code>http://127.0.0.1:5000/chat</code></p>
<h1>Bad Request</h1>
<p><em>The browser (or proxy) sent a request that this server could not understand.</em></p>
<p>Also I am checking this from <strong>Postman</strong></p>
<pre><code>from flask import Flask, request, render_template
import requests

app = Flask(__name__)

@app.route('/')
def index():
    return 'Welcome to ChatGPT app!'

@app.route('/chat', methods=['GET', 'POST'])
def chat():
    user_input = request.form['text']
    # Use OpenAI's API to generate a response from ChatGPT
    response = generate_response_from_chatgpt(user_input)
    return response

def generate_response_from_chatgpt(user_input):
    api_key = &quot;YOUR_API_KEY&quot;
    url = &quot;https://api.openai.com/v1/engines/davinci/completions&quot;
    headers = {
        &quot;Content-Type&quot;: &quot;application/json&quot;,
        &quot;Authorization&quot;: f&quot;Bearer {api_key}&quot;
    }
    data = {
        &quot;prompt&quot;: user_input,
        &quot;engine&quot;: &quot;davinci&quot;
    }
    response = requests.post(url, headers=headers, json=data)
    return response.json()[&quot;choices&quot;][0][&quot;text&quot;]


if __name__ == '__main__':
    app.run()
</code></pre>
","2023-01-16 03:56:19","","2023-01-24 18:21:04","2023-01-24 18:21:04","<flask><flask-restful><openai-api><gpt-3>","1","1","-2","941","","2","18515689","<p>It would be best if you check the openai documentation to make sure you are using the correct endpoint and data format in your request.
Also, you should check your API key, if it is correct and if you have reached the limit of requests.</p>
<p>Also, it's worth noting that the code you provided is missing the import statement for Flask. You will need to add the following line at the top of your file:</p>
<p>from <code>flask import Flask, request</code>
Also, I see that you're using <code>request.form['text']</code> but you should check if the request is a GET or POST request.</p>
<pre><code>if request.method == 'POST':
    user_input = request.form['text']
else:
    user_input = request.args.get('text')
</code></pre>
<p>This is to avoid a KeyError being raised when the request is a GET request.</p>
","2023-01-16 04:12:52","0","1"
"75299615","1","15313661","75300061","OpenAI API: Can I remove the line break from the response with a parameter?","<p>I've starting using OpenAI API in R. I downloaded the <code>openai</code> package. I keep getting a double linebreak in the text response. Here's an example of my code:</p>
<pre class=""lang-r prettyprint-override""><code>
library(openai)

vector = create_completion(
  model = &quot;text-davinci-003&quot;,
  prompt = &quot;Tell me what the weather is like in London, UK, in Celsius in 5 words.&quot;,
  max_tokens = 20,
  temperature = 0,
  echo = FALSE
)


vector_2 = vector$choices[1]

vector_2$text


[1] &quot;\n\nRainy, mild, cool, humid.&quot;

</code></pre>
<p>Is there a way to get rid of this without 'correcting' the response text using other functions?</p>
","2023-01-31 15:46:42","","2023-03-13 14:04:27","2023-05-04 11:02:46","<r><openai-api><gpt-3>","3","0","3","1773","","2","10347145","<p>No, it's not possible.</p>
<p>The OpenAI API returns the completion with a starting <code>\n\n</code> by default. There's no parameter for the <a href=""https://platform.openai.com/docs/api-reference/completions"" rel=""nofollow noreferrer"">Completions endpoint</a> to control this.</p>
<p>You need to remove the line break manually.</p>
<p>An example response looks like this:</p>
<pre><code>{
  &quot;id&quot;: &quot;cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7&quot;,
  &quot;object&quot;: &quot;text_completion&quot;,
  &quot;created&quot;: 1589478378,
  &quot;model&quot;: &quot;text-davinci-003&quot;,
  &quot;choices&quot;: [
    {
      &quot;text&quot;: &quot;\n\nThis is indeed a test&quot;,
      &quot;index&quot;: 0,
      &quot;logprobs&quot;: null,
      &quot;finish_reason&quot;: &quot;length&quot;
    }
  ],
  &quot;usage&quot;: {
    &quot;prompt_tokens&quot;: 5,
    &quot;completion_tokens&quot;: 7,
    &quot;total_tokens&quot;: 12
  }
}
</code></pre>
","2023-01-31 16:20:30","1","3"
"75322813","1","33522","75322907","OpenAI GPT-3 API error: ""That model does not exist""","<p>Get &quot;That model does not exist&quot; from api call in node.js</p>
<pre><code>const chatGptUrl = &quot;https://api.openai.com/v1/engines/chat-gpt/jobs&quot;;

...

const response = await axios.post(
      chatGptUrl,
      {
        prompt,
        max_tokens: 100,
        n: 1,
        stop: &quot;&quot;,
      },
      {
        headers: {
          &quot;Content-Type&quot;: &quot;application/json&quot;,
          &quot;Authorization&quot;: `Bearer ${chatGptApiKey}`,
        },
      }
    );

    const responseText = response.data.choices[0].text;
</code></pre>
","2023-02-02 11:56:10","","2023-03-13 14:09:17","2023-03-13 14:09:17","<node.js><openai-api><gpt-3>","1","0","-1","2011","","2","10347145","<p>You have to set the <code>model</code> parameter to <code>text-davinci-003</code>, <code>text-curie-001</code>, <code>text-babbage-001</code> or <code>text-ada-001</code>. It's a <a href=""https://platform.openai.com/docs/api-reference/completions/create"" rel=""nofollow noreferrer"">required parameter</a>.</p>
<p>Also, all <a href=""https://beta.openai.com/docs/api-reference/engines"" rel=""nofollow noreferrer"">Engines endpoints</a> are deprecated.</p>
<p><a href=""https://i.stack.imgur.com/lQu9c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lQu9c.png"" alt=""Deprecated"" /></a></p>
","2023-02-02 12:04:00","1","2"
"75376813","1","21105687","75397197","OpenAI fine-tune with python return null model","<p>I am trying to get fine-tune model from OpenAI GPT-3 using python with following code</p>
<pre><code>#upload training data

upload_response = openai.File.create(
  file=open(file_name, &quot;rb&quot;),
  purpose='fine-tune'
)
file_id = upload_response.id
print(f'\nupload training data respond:\n\n {upload_response}')
</code></pre>
<p>OpenAI respond with data</p>
<pre><code> {
  &quot;bytes&quot;: 380,
  &quot;created_at&quot;: 1675789714,
  &quot;filename&quot;: &quot;file&quot;,
  &quot;id&quot;: &quot;file-lKSQushd8eABcfiBVwhxBMOJ&quot;,
  &quot;object&quot;: &quot;file&quot;,
  &quot;purpose&quot;: &quot;fine-tune&quot;,
  &quot;status&quot;: &quot;uploaded&quot;,
  &quot;status_details&quot;: null
}
</code></pre>
<p>My training file has been uploaded so I am checking for fine-tune response with code</p>
<pre><code>fine_tune_response = openai.FineTune.create(training_file=file_id)
print(f'\nfine-tune respond:\n\n {fine_tune_response}')
</code></pre>
<p>I am getting</p>
<pre><code> {
  &quot;created_at&quot;: 1675789714,
  &quot;events&quot;: [
    {
      &quot;created_at&quot;: 1675789715,
      &quot;level&quot;: &quot;info&quot;,
      &quot;message&quot;: &quot;Created fine-tune: ft-IqBdk4WJETm4KakIzfZeCHgS&quot;,
      &quot;object&quot;: &quot;fine-tune-event&quot;
    }
  ],
  &quot;fine_tuned_model&quot;: null,
  &quot;hyperparams&quot;: {
    &quot;batch_size&quot;: null,
    &quot;learning_rate_multiplier&quot;: null,
    &quot;n_epochs&quot;: 4,
    &quot;prompt_loss_weight&quot;: 0.01
  },
  &quot;id&quot;: &quot;ft-IqBdk4WJETm4KakIzfZeCHgS&quot;,
  &quot;model&quot;: &quot;curie&quot;,
  &quot;object&quot;: &quot;fine-tune&quot;,
  &quot;organization_id&quot;: &quot;org-R6DqvjTNimKtBzWWgae6VmAy&quot;,
  &quot;result_files&quot;: [],
  &quot;status&quot;: &quot;pending&quot;,
  &quot;training_files&quot;: [
    {
      &quot;bytes&quot;: 380,
      &quot;created_at&quot;: 1675789714,
      &quot;filename&quot;: &quot;file&quot;,
      &quot;id&quot;: &quot;file-lKSQushd8eABcfiBVwhxBMOJ&quot;,
      &quot;object&quot;: &quot;file&quot;,
      &quot;purpose&quot;: &quot;fine-tune&quot;,
      &quot;status&quot;: &quot;uploaded&quot;,
      &quot;status_details&quot;: null
    }
  ],
  &quot;updated_at&quot;: 1675789714,
  &quot;validation_files&quot;: []
}
</code></pre>
<p>As you see, the fine_tune_model is null so I cant use it for Completion.
My question is how to check for example in While loop if my fine-tune is complete using ft id</p>
","2023-02-07 17:15:43","","2023-02-07 18:42:22","2023-03-27 14:33:54","<python><openai-api><gpt-3>","2","1","1","864","","2","8949058","<p>Here is data from the OpenAI documentation on fine-tuning:</p>
<blockquote>
<p>After you've started a fine-tune job, it may take some time to complete. Your job may be queued behind other jobs on our system, and training our model can take minutes or hours depending on the model and dataset size.</p>
</blockquote>
<p>Ref: <a href=""https://platform.openai.com/docs/guides/fine-tuning"" rel=""nofollow noreferrer"">https://platform.openai.com/docs/guides/fine-tuning</a></p>
<p>The OpenAI guide uses the CLI tool to create the fine-tuning and then accesses the model programatically once the training has completed.</p>
<p>Therefore, you couldn't run the code in Python as you have laid it out, since you need to wait for the training to complete. Meaning you can't train the model on the fly and use it instantly.</p>
","2023-02-09 10:22:06","1","2"
"68783979","1","14067076","","Does anyone knows how to input a text content in huggingface gpt2?","<p>I want to input conversation data as an input to the gpt2 model from huggingface transformers.</p>
<p>====Example====</p>
<p>A: Where did you meet David?<br>
B: I met him at the central park.<Br>
A: Weren't he quite strange that day?</p>
<p>=&gt; predicted B: Not at all, why?</p>
<p>===============</p>
<p>Like the upper example, I want to input some conversation data to the transformer and get a reply from the pretrained model(gpt2). Can anybody tell me how?</p>
","2021-08-14 14:02:04","","","2021-08-14 14:02:04","<huggingface-transformers><gpt-2>","0","2","0","324","","","","","","",""
"71028228","1","11132563","","GPT-3 long input posts for Question Answering","<p>From my understanding, GPT-3 is &quot;trained&quot; for a specific task by including some labelled examples before the desired/test example. In Question Answering, this includes a context and a question. In this situation, the input prompt can become long. How do people address this?</p>
<p>I am using the Hugging Face GPT-J implementation, and there is an input token limit (of 2000). However, when including multiple qa examples in the prompt (especially with the contexts), it quickly reaches this limit, limitting the amount of example prompts to be inputted. Does anyone know how this issue is handled in a GPT-J setting, especially for QA?</p>
","2022-02-08 03:26:09","","","2022-03-24 13:08:57","<deep-learning><nlp><huggingface-transformers><nlp-question-answering><gpt-3>","1","1","1","2266","","","","","","",""
"71040945","1","14272134","","GPT-2: How do I speed up/optimize token text generation?","<p>I am trying to generate a 20 token text using GPT-2 simple. It is taking me around 15 seconds to generate the sentence. AI Dungeon is taking around 4 seconds to generate the same size sentence.</p>
<p>Is there a way to fasten/optimize the GPT-2 text generation?</p>
","2022-02-08 21:11:07","","2023-01-16 08:09:49","2023-01-16 08:09:49","<openai-api><gpt-2>","3","0","2","2542","","","","","","",""
"71130046","1","13679903","","GPT-2 pretrained model fails to load when TF v2 behaviour is disabled","<p>I am trying to use GPT-2 in a codebase that is written for Tensorflow 1.x. However, I am running the code against TF 2.x installation binaries with <code>tf.disable_v2_behavior()</code> flag. Without this <code>tf.disable_v2_behavior()</code> flag, GPT-2 pretrained model loads fine, but the model fails to load if the flag is used. Here is my code :</p>
<pre><code>import tensorflow.compat.v1 as tf
tf.disable_v2_behavior() #works fine without this line

from transformers import TFGPT2Model
model = TFGPT2Model.from_pretrained('gpt2') #fails
</code></pre>
<p>Here is the error:</p>
<pre><code>&gt;&gt;&gt; TFGPT2Model.from_pretrained('gpt2')
2022-02-15 10:17:08.792655: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home1/07782/marefin/.local/lib/python3.8/site-packages/transformers/modeling_tf_utils.py&quot;, line 1467, in from_pretrained
    model(model.dummy_inputs)  # build the network with dummy inputs
  File &quot;/home1/07782/marefin/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py&quot;, line 783, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File &quot;/home1/07782/marefin/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py&quot;, line 695, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    /home1/07782/marefin/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:628 call  *
        outputs = self.transformer(
    /home1/07782/marefin/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:763 __call__  **
        self._maybe_build(inputs)
    /home1/07782/marefin/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:2084 _maybe_build
        self.build(input_shapes)
    /home1/07782/marefin/.local/lib/python3.8/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:241 build
        self.wpe = self.add_weight(
    /home1/07782/marefin/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:441 add_weight
        variable = self._add_variable_with_custom_getter(
    /home1/07782/marefin/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:810 _add_variable_with_custom_getter
        new_variable = getter(
    /home1/07782/marefin/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_utils.py:127 make_variable
        return tf_variables.VariableV1(
    /home1/07782/marefin/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py:260 __call__
        return cls._variable_v1_call(*args, **kwargs)
    /home1/07782/marefin/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py:206 _variable_v1_call
        return previous_getter(
    /home1/07782/marefin/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py:199 &lt;lambda&gt;
        previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
    /home1/07782/marefin/.local/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py:2612 default_variable_creator
        return resource_variable_ops.ResourceVariable(
    /home1/07782/marefin/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py:264 __call__
        return super(VariableMetaclass, cls).__call__(*args, **kwargs)
    /home1/07782/marefin/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1584 __init__
        self._init_from_args(
    /home1/07782/marefin/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1722 _init_from_args
        initial_value = initial_value()
    /home1/07782/marefin/.local/lib/python3.8/site-packages/tensorflow/python/keras/initializers/initializers_v2.py:413 __call__
        dtype = _assert_float_dtype(_get_dtype(dtype))
    /home1/07782/marefin/.local/lib/python3.8/site-packages/tensorflow/python/keras/initializers/initializers_v2.py:948 _assert_float_dtype
        raise ValueError('Expected floating point type, got %s.' % dtype)

    ValueError: Expected floating point type, got &lt;dtype: 'int32'&gt;.
</code></pre>
<p>I am using TF 2.5 with transformers v4.12.5. Is there any way around to make this work with TF v2 behaviour disabled?</p>
","2022-02-15 16:31:37","","","2022-02-16 17:59:17","<python><tensorflow><huggingface-transformers><gpt-2>","1","0","0","351","","","","","","",""
"72008843","1","18567298","","TypeError: Cannot subclass <class 'typing._SpecialForm'> while fine tuning GPT-J","<p>I am trying to <strong>fine tune GPT-J</strong> by following <a href=""https://github.com/kingoflolz/mesh-transformer-jax"" rel=""nofollow noreferrer"">this GitHub Repository</a>. When running the training command, I encounter this error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;device_train.py&quot;, line 13, in &lt;module&gt;
    from mesh_transformer import util
  File &quot;/home/shreyjain/mesh-transformer-jax/mesh_transformer/util.py&quot;, line 36, in &lt;module&gt;
    class ClipByGlobalNormState(OptState):
  File &quot;/usr/lib/python3.8/typing.py&quot;, line 317, in __new__
    raise TypeError(f&quot;Cannot subclass {cls!r}&quot;)
TypeError: Cannot subclass &lt;class 'typing._SpecialForm'&gt; 
</code></pre>
<p>This looks like a source code error but I am not sure. I have also raised an issue on GitHub regarding this. Any help will be appreciated!</p>
","2022-04-26 05:34:24","","2022-05-03 04:33:05","2022-06-17 13:36:29","<python><class><gpt-3>","1","0","2","459","","","","","","",""
"72047597","1","7876035","","How can I respond to a CLI prompt in Kaggle?","<p>I'm using Kaggle to generate poetry samples with GPT-2. My notebook uses datasets from <a href=""https://www.gwern.net/GPT-2#training-gpt-2-117m-to-generate-poetry"" rel=""nofollow noreferrer"">Gwern's poetry generator</a> and uses <a href=""https://github.com/nshepperd/gpt-2"" rel=""nofollow noreferrer"">nshepperd's GPT-2 model</a>.</p>
<p>This all works fine with <a href=""https://www.kaggle.com/code/theebus/ai-poetry-generator-1-5b-model"" rel=""nofollow noreferrer"">my notebook</a> when generating unconditional samples.</p>
<pre><code>!python src/generate_unconditional_samples.py --top_k 40 --nsamples 1 --temperature 0.9 --model_name=1.5b-model --length=300
</code></pre>
<p><a href=""https://i.stack.imgur.com/xbAvZ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xbAvZ.png"" alt=""enter image description here"" /></a></p>
<p>However, I want to generate samples with the &quot;interactive conditional&quot; method:</p>
<pre><code>!python src/interactive_conditional_samples.py --top_k 40 --nsamples 10 --temperature 0.9 --model_name=1.5b-model --length=300
</code></pre>
<p><strong>The problem is when it requests a &quot;model prompt&quot;</strong> and I have no way of entering a prompt.</p>
<p><a href=""https://i.stack.imgur.com/4xGwO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4xGwO.png"" alt=""enter image description here"" /></a></p>
<p>It doesn't work when I enter a prompt in Kaggle's CLI.</p>
<p><a href=""https://i.stack.imgur.com/kwDZ3.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kwDZ3.png"" alt=""enter image description here"" /></a></p>
<p>If I were to run this on my desktop using my own computing power it would automatically allow me to enter text in response to the prompt.</p>
<p><a href=""https://i.stack.imgur.com/4UdWX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4UdWX.png"" alt=""enter image description here"" /></a></p>
<p><strong>Is there a way for me to enter a prompt in kaggle?</strong></p>
<p>I've tried to auto respond using flags, like how you would use -y to auto accept a yes/no prompt in installs, but it hasn't worked so far.</p>
<p><a href=""https://www.kaggle.com/code/theebus/ai-poetry-generator-1-5b-model"" rel=""nofollow noreferrer"">The notebook is public here if you want to test it out.</a></p>
","2022-04-28 16:58:34","","2022-05-20 17:56:18","2022-05-20 17:56:18","<python><jupyter-notebook><command-line-interface><kaggle><gpt-2>","0","4","0","141","0","","","","","",""
"74647792","1","14729820","","TrOCR fine-tuning with Text generator model like gpt-2 or Bert","<p>I want to finetune the TrOCR transformer model (<a href=""https://github.com/microsoft/unilm/tree/master/trocr"" rel=""nofollow noreferrer"">https://github.com/microsoft/unilm/tree/master/trocr</a>) model with a different decoder like Bert or GPT-2 the dataset that I have (image, text) pair <a href=""https://github.com/Mohammed20201991/DataSets/blob/main/proceessed_sentences.txt"" rel=""nofollow noreferrer"">see the text</a> in the following format(inside data folder)</p>
<pre><code>data|
    |--- proceessed_sentence.txt
    |--- Image
</code></pre>
<p>I am trying to execute on google collab where I faced the issue below where I don't know how to use it Google collab<br />
the dataset I am trying to fine-tune is like an IAM dataset for lines segment <a href=""https://layoutlm.blob.core.windows.net/trocr/dataset/IAM.tar.gz"" rel=""nofollow noreferrer"">see link here</a>
The code for the mention repo [how to use it] (<a href=""https://github.com/microsoft/unilm/tree/master/trocr"" rel=""nofollow noreferrer"">https://github.com/microsoft/unilm/tree/master/trocr</a>)</p>
<pre><code>!conda create -n trocr python=3.7
!conda activate trocr
!git clone https://github.com/microsoft/unilm.git
!cd unilm
!cd trocr
!pip install pybind11
!pip install -r requirements.txt
!pip install -v --no-cache-dir --global-option=&quot;--cpp_ext&quot; --global-option=&quot;--cuda_ext&quot; 'git+https://github.com/NVIDIA/apex.git'
</code></pre>
<p>Running the next cell :</p>
<pre><code>!export MODEL_NAME=ft_iam
!export SAVE_PATH=/path/to/save/${MODEL_NAME}
!export LOG_DIR=log_${MODEL_NAME}
!export DATA=/path/to/data
!mkdir ${LOG_DIR}
!export BSZ=8
!export valid_BSZ=16

!CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python -m torch.distributed.launch --nproc_per_node=8 \
    $(which fairseq-train) \
    --data-type STR --user-dir ./ --task text_recognition --input-size 384 \
    --arch trocr_large \   # or trocr_base
    --seed 1111 --optimizer adam --lr 2e-05 --lr-scheduler inverse_sqrt \
    --warmup-init-lr 1e-8 --warmup-updates 500 --weight-decay 0.0001 --log-format tqdm \
    --log-interval 10 --batch-size ${BSZ} --batch-size-valid ${valid_BSZ} --save-dir ${SAVE_PATH} \
    --tensorboard-logdir ${LOG_DIR} --max-epoch 300 --patience 20 --ddp-backend legacy_ddp \
    --num-workers 8 --preprocess DA2 --update-freq 1 \
    --bpe gpt2 --decoder-pretrained roberta2 \ # --bpe sentencepiece --sentencepiece-model ./unilm3-cased.model --decoder-pretrained unilm ## For small models
    --finetune-from-model /path/to/model --fp16 \
    ${DATA} 
</code></pre>
<p>I got this issues :</p>
<pre><code>File &quot;&lt;ipython-input-4-e347a1380d52&gt;&quot;, line 10
    --seed 1111 --optimizer adam --lr 2e-05 --lr-scheduler inverse_sqrt \
</code></pre>
","2022-12-01 20:23:32","","2022-12-02 14:56:24","2023-03-07 12:18:19","<bash><pytorch><nlp><huggingface-transformers><gpt-2>","0","1","1","381","","","","","","",""
"74682597","1","13788466","","Fine-Tuning GPT2 - attention mask and pad token id errors","<p>I have been trying to fine-tune GPT2 on the wikitext-2 dataset (just to help myself learn the process) and I am running into a warning message that I have not seen before:</p>
<p>&quot;The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's <code>attention_mask</code> to obtain reliable results.
Setting <code>pad_token_id</code> to <code>eos_token_id</code>:50256 for open-end generation.&quot;</p>
<p>This seems strange since I clearly specify the EOS token in my code when instantiating the tokenizer:</p>
<pre><code>tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='&lt;|startoftext|&gt;', eos_token='&lt;|endoftext|&gt;', pad_token='&lt;|pad|&gt;')
</code></pre>
<p>Training completes without crashing and my loss improves every epoch, but when I inference the model it outputs absolute gibberish - sometimes only generating a single word and nothing else. I am thinking there is a link between this warning message I'm getting and the model not performing well.</p>
<p>I got my training, valid, test data from here (i used the .raw files) - <a href=""https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/"" rel=""nofollow noreferrer"">https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/</a></p>
<p>I manually added &lt;|startoftext|&gt; and &lt;|endoftext|&gt; in the raw txt files for the datasets. Resulting in training data that looked like these two examples (taken from the middle of the text file):</p>
<pre><code>...
&lt;|startoftext|&gt;
= Perfect Dark ( 2010 video game ) = 
 
 Perfect Dark is a remastered release of the first @-@ person shooter video game by the same name . Developed by 4J Studios and published by Microsoft Game Studios a decade after the original 's 2000 release , the remaster features several technical improvements , including higher resolution textures and models , a higher frame rate , and a multiplayer mode that supports the Xbox Live online service . It was released for the Xbox 360 video game console in March 2010 , through the Xbox Live Arcade download service . The story of the game follows Joanna Dark , an agent of the Carrington Institute organization , as she attempts to stop a conspiracy by rival corporation dataDyne . 
 Perfect Dark was under development for nearly a year and its game engine was completely re @-@ written from scratch to support several Xbox 360 features . Therefore , although the game plays exactly the same as the original , the code and renderer is different . The game received generally favorable reviews . Some critics considered the relatively unchanged game to be outdated , but most agreed that the title was a solid revival of a classic . As of the end of 2011 , the game had sold nearly 410 @,@ 000 units . 
 
 = = Gameplay = = 
 
 Perfect Dark is a first @-@ person shooter with elements of stealth games . In the game 's campaign mode , the player controls Joanna Dark through a series of nonlinear levels collected together into missions . Each level requires the player to complete a certain number of objectives , ranging from disguising oneself to hacking computers , collecting objects , and defeating enemies , among others . Players can carry an unlimited number of weapons and almost all of the weapons have two firing modes . The levels in Perfect Dark have no checkpoints , meaning that if Joanna is killed or fails an objective , the player has to start the level from the beginning . Every level can be played on three difficulty settings and several aspects , such as the enemies aggressiveness and the number of objectives that must be completed , among others , can vary in function of the chosen difficulty . Two players can also play the campaign co @-@ operatively or through a &quot; counter @-@ operative &quot; mode , in which one player controls the protagonist , while the other controls enemies throughout the level , attempting to stop the first player from completing objectives . 
 
 = = = Enhancements = = = 
 
 The remaster offers several improvements over the original Perfect Dark that was released for the Nintendo 64 in 2000 . The most remarkable change is that any of the multiplayer modes , including co @-@ operative and counter @-@ operative , can now be played in either splitscreen or through the Xbox Live online service . Combat Simulator matches are still capped at 12 entities , but the game can now comprise eight players online simultaneously , an improvement to the original 's cap of four players and eight Simulants . Players can also play against more than eight Simulants as long as there are enough slots available in a match ; for example , a single player can play against 11 Simulants ; such a feature was not possible in the original game . Unlike the original game , all the multiplayer content is unlocked from the beginning , and weapons from the game 's predecessor , which were originally only available in the missions , are now available to use in multiplayer . The game features an online leaderboard system and players can earn achievements and in @-@ game crowns by accomplishing certain tasks . The game also includes two new control set @-@ ups , entitled &quot; Spartan &quot; and &quot; Duty Calls &quot; , which are based on the popular first @-@ person shooter franchises Halo and Call of Duty respectively . 
 
 &lt;|endoftext|&gt;
&lt;|startoftext|&gt;
 = First Ostend Raid = 
 
 The First Ostend Raid ( part of Operation ZO ) was the first of two attacks by the Royal Navy on the German @-@ held port of Ostend during the late spring of 1918 during the First World War . Ostend was attacked in conjunction with the neighbouring harbour of Zeebrugge on 23 April in order to block the vital strategic port of Bruges , situated 6 mi ( 5 @.@ 2 nmi ; 9 @.@ 7 km ) inland and ideally sited to conduct raiding operations on the British coastline and shipping lanes . Bruges and its satellite ports were a vital part of the German plans in their war on Allied commerce ( Handelskrieg ) because Bruges was close to the troopship lanes across the English Channel and allowed much quicker access to the Western Approaches for the U @-@ boat fleet than their bases in Germany . 
 The plan of attack was for the British raiding force to sink two obsolete cruisers in the canal mouth at Ostend and three at Zeebrugge , thus preventing raiding ships leaving Bruges . The Ostend canal was the smaller and narrower of the two channels giving access to Bruges and so was considered a secondary target behind the Zeebrugge Raid . Consequently , fewer resources were provided to the force assaulting Ostend . While the attack at Zeebrugge garnered some limited success , the assault on Ostend was a complete failure . The German marines who defended the port had taken careful preparations and drove the British assault ships astray , forcing the abortion of the operation at the final stage . 
 Three weeks after the failure of the operation , a second attack was launched which proved more successful in sinking a blockship at the entrance to the canal but ultimately did not close off Bruges completely . Further plans to attack Ostend came to nothing during the summer of 1918 , and the threat from Bruges would not be finally stopped until the last days of the war , when the town was liberated by Allied land forces . 
 
 = = Bruges = = 
 
 Bruges had been captured by the advancing German divisions during the Race for the Sea and had been rapidly identified as an important strategic asset by the German Navy . Bruges was situated 6 mi ( 5 @.@ 2 nmi ; 9 @.@ 7 km ) inland at the centre of a network of canals which emptied into the sea at the small coastal towns of Zeebrugge and Ostend . This land barrier protected Bruges from bombardment by land or sea by all but the very largest calibre artillery and also secured it against raiding parties from the Royal Navy . Capitalising on the natural advantages of the port , the German Navy constructed extensive training and repair facilities at Bruges , equipped to provide support for several flotillas of destroyers , torpedo boats and U @-@ boats . 
 By 1916 , these raiding forces were causing serious concern in the Admiralty as the proximity of Bruges to the British coast , to the troopship lanes across the English Channel and for the U @-@ boats , to the Western Approaches ; the heaviest shipping lanes in the World at the time . In the late spring of 1915 , Admiral Reginald Bacon had attempted without success to destroy the lock gates at Ostend with monitors . This effort failed , and Bruges became increasingly important in the Atlantic Campaign , which reached its height in 1917 . By early 1918 , the Admiralty was seeking ever more radical solutions to the problems raised by unrestricted submarine warfare , including instructing the &quot; Allied Naval and Marine Forces &quot; department to plan attacks on U @-@ boat bases in Belgium . 
 The &quot; Allied Naval and Marine Forces &quot; was a newly formed department created with the purpose of conducting raids and operations along the coastline of German @-@ held territory . The organisation was able to command extensive resources from both the Royal and French navies and was commanded by Admiral Roger Keyes and his deputy , Commodore Hubert Lynes . Keyes , Lynes and their staff began planning methods of neutralising Bruges in late 1917 and by April 1918 were ready to put their plans into operation . 
 
 = = Planning = = 
 
 To block Bruges , Keyes and Lynes decided to conduct two raids on the ports through which Bruges had access to the sea . Zeebrugge was to be attacked by a large force consisting of three blockships and numerous supporting warships . Ostend was faced by a similar but smaller force under immediate command of Lynes . The plan was for two obsolete cruisers — HMS Sirius and Brilliant — to be expended in blocking the canal which emptied at Ostend . These ships would be stripped to essential fittings and their lower holds and ballast filled with rubble and concrete . This would make them ideal barriers to access if sunk in the correct channel at the correct angle . 
 When the weather was right , the force would cross the English Channel in darkness and attack shortly after midnight to coincide with the Zeebrugge Raid a few miles up the coast . By coordinating their operations , the assault forces would stretch the German defenders and hopefully gain the element of surprise . Covering the Inshore Squadron would be heavy bombardment from an offshore squadron of monitors and destroyers as well as artillery support from Royal Marine artillery near Ypres in Allied @-@ held Flanders . Closer support would be offered by several flotillas of motor launches , small torpedo boats and Coastal Motor Boats which would lay smoke screens to obscure the advancing blockships as well as evacuate the crews of the cruisers after they had blocked the channel . 

&lt;|endoftext|&gt; ...
</code></pre>
<p>I followed this tutorial very closely - <a href=""https://colab.research.google.com/drive/13dZVYEOMhXhkXWfvSMVM1TTtUDrT6Aeh?usp=sharing#scrollTo=pBEVY2PYSTXJ"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/13dZVYEOMhXhkXWfvSMVM1TTtUDrT6Aeh?usp=sharing#scrollTo=pBEVY2PYSTXJ</a></p>
<p>Here is my full code :</p>
<pre><code>import random
import time
import datetime
import torch
from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler
from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup, GPT2Config

smallest_gpt2 = 'gpt2'  # 124M weights (parameters)

# load training texts
with open('wikitext-2-raw/wiki.train.raw', 'r') as o:
    raw_train_text = o.read()  # readlines() returns a list of strings separated by '\n'
with open('wikitext-2-raw/wiki.valid.raw', 'r') as o:
    raw_validation_text = o.read()
with open('wikitext-2-raw/wiki.test.raw', 'r') as o:
    raw_test_text = o.read()

# PRE-PROCESSING TRAINING, VALIDATION, AND TEST TEXTS
preprocessed_train = raw_train_text.split('&lt;|startoftext|&gt;')
preprocessed_train = [i for i in preprocessed_train if i]  # removes empty list entries
preprocessed_train = ['&lt;|startoftext|&gt;' + '\n' + entry for entry in preprocessed_train]  # adds &lt;|startoftext|&gt; to start
preprocessed_valid = raw_validation_text.split('&lt;|startoftext|&gt;')
preprocessed_valid = [i for i in preprocessed_valid if i]
preprocessed_valid = ['&lt;|startoftext|&gt;' + '\n' + entry for entry in preprocessed_valid]
preprocessed_test = raw_test_text.split('&lt;|startoftext|&gt;')
preprocessed_test = [i for i in preprocessed_test if i]
preprocessed_test = ['&lt;|startoftext|&gt;' + '\n' + entry for entry in preprocessed_test]

# HYPER PARAMETERS
EPOCHS = 5
BATCH_SIZE = 2  # GPT2 is a large model, so higher batch sizes can lead to memory problems
WARMUP_STEPS = 100
LEARNING_RATE = 5e-4
DECAY = 0
EPSILON = 1e-8


class GPT2Dataset(Dataset):

    def __init__(self, txt_list, _tokenizer, gpt2_type=smallest_gpt2, max_length=768):
        self.tokenizer = _tokenizer
        self.input_ids = []
        self.attn_masks = []

        # this loop will wrap all training data examples in BOS and EOS tokens (beginning/end of sequence)
        # this, again, helps the model understand the &quot;format&quot; of what you're training it for
        # note however, that if a training example is longer than the max length, the EOS token will be truncated, and
        #   this is not a problem for the model's training process
        for txt in txt_list:
            # pre_processed_text = '&lt;|startoftext|&gt;' + txt + '&lt;|endoftext|&gt;'  # i did this manually, so I skip it here
            # print(txt)

            # i handled most of the pre-processing for the training data further up in the code
            encodings_dict = _tokenizer(txt, truncation=True, max_length=max_length, padding=&quot;max_length&quot;)

            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))
            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx], self.attn_masks[idx]


# loading tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='&lt;|startoftext|&gt;', eos_token='&lt;|endoftext|&gt;',
                                          pad_token='&lt;|pad|&gt;')  # gpt2-medium

print(&quot;The max model length is {} for this model, although the actual embedding size for GPT small is 768&quot;.format(tokenizer.model_max_length))
print(&quot;The beginning of sequence token {} token has the id {}&quot;.format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))
print(&quot;The end of sequence token {} has the id {}&quot;.format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))
print(&quot;The padding token {} has the id {}&quot;.format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))

# create dataset objects
train_dataset = GPT2Dataset(preprocessed_train, tokenizer, max_length=768)
valid_dataset = GPT2Dataset(preprocessed_valid, tokenizer, max_length=768)
test_dataset = GPT2Dataset(preprocessed_test, tokenizer, max_length=768)

# getting size of datasets
train_size = len(train_dataset)
val_size = len(valid_dataset)

print('{:&gt;5,} training samples'.format(train_size))
print('{:&gt;5,} validation samples'.format(val_size))

# Create the DataLoaders for our training and validation datasets.
# We'll take training samples in random order.
train_dataloader = DataLoader(  # todo learn how dataloader creates targets
            train_dataset,  # The training samples.
            sampler=RandomSampler(train_dataset),  # Select batches randomly
            batch_size=BATCH_SIZE  # Trains with this batch size.
        )

# For validation the order doesn't matter, so we'll just read them sequentially.
validation_dataloader = DataLoader(
            valid_dataset,  # The validation samples.
            sampler=SequentialSampler(valid_dataset),  # Pull out batches sequentially.
            batch_size=BATCH_SIZE  # Evaluate with this batch size.
        )

# config
configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)

# instantiate model
model = GPT2LMHeadModel.from_pretrained(smallest_gpt2, config=configuration)

# this step is necessary because I've added some tokens (bos_token, etc) to the embeddings
# otherwise the tokenizer and model tensors won't match up. NOTE these tokens are already added to tokenizer above
model.resize_token_embeddings(len(tokenizer))

# this produces sample output every 50 steps
sample_every = 50

# Note: AdamW is a class from the huggingface library (as opposed to pytorch)
optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=EPSILON)

# Total number of training steps is [number of batches] x [number of epochs].
# (Note that this is not the same as the number of training samples).
total_steps = len(train_dataloader) * EPOCHS

# Create the learning rate scheduler.
# This changes the learning rate as the training loop progresses
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps)

training_stats = []
total_t0 = time.time()

# device config
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)


def format_time(_elapsed):
    return str(datetime.timedelta(seconds=int(round(_elapsed))))


for epoch_i in range(0, EPOCHS):

    # ========================================
    #               Training
    # ========================================

    print(&quot;&quot;)
    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))
    print('Training...')

    t0 = time.time()

    total_train_loss = 0

    model.train()  # puts model in training mode

    for step, batch in enumerate(train_dataloader):

        b_input_ids = batch[0].to(device)
        b_labels = batch[0].to(device)  # training targets
        b_masks = batch[1].to(device)

        model.zero_grad()

        # feeding the input to the model
        outputs = model(b_input_ids,
                        labels=b_labels,
                        attention_mask=b_masks,
                        token_type_ids=None
                        )

        loss = outputs[0]  # how &quot;wrong&quot; was the model?

        batch_loss = loss.item()
        total_train_loss += batch_loss

        # Get sample every x batches. This is just a check to see how the model is doing.
        if step % sample_every == 0 and not step == 0:

            elapsed = format_time(time.time() - t0)
            print('  Batch {:&gt;5,}  of  {:&gt;5,}. Loss: {:&gt;5,}.   Elapsed: {:}.'.format(step, len(train_dataloader),
                                                                                     batch_loss, elapsed))

            model.eval()  # puts model in evaluation mode, where the necessary layers are turned off for inference

            # normally you would use a context manager here so the gradients don't get modified during this inference. However the tutorial I follow does not do this.
            # with torch.no_grad():
            # ... do inference eval ...

            # Here we are simply using the model to get an output. This is called inference.
            sample_outputs = model.generate(
                bos_token_id=random.randint(1, 30000),  # todo why do we do this line?
                do_sample=True,  # switches on sampling, where model will randomly select next word from the sample pool
                top_k=50,  # only 50 words will be considered for the next word in the sequence
                max_length=200,  # max tokens for total generation
                top_p=0.95,  # smallest set of words whose probabilities summed together reach/exceed top_p value
                num_return_sequences=1  # we only want model to generate one complete response (sequence of words)
                # temperature=1
            )

            # temperature is another parameter we can use when running inference
            # temperature of 0 will choose the highest-probability word each time
            # temperature of 1 is default, and uses the model's base confidence to choose the next word
            # temperature above 1 will make the model choose less-likely words. More creative, but more risk of nonsense

            # we only sample for one return sequence so this for is sort of unnecessary, but whatever
            for i, sample_output in enumerate(sample_outputs):
                print(&quot;{}: {}&quot;.format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))

            model.train()  # we have to put model back in train mode after eval mode

        loss.backward()  # change weights with backprop

        optimizer.step()

        scheduler.step()

    # Calculate the average loss over all of the batches.
    avg_train_loss = total_train_loss / len(train_dataloader)

    # Measure how long this epoch took.
    training_time = format_time(time.time() - t0)

    print(&quot;&quot;)
    print(&quot;  Average training loss: {0:.2f}&quot;.format(avg_train_loss))
    print(&quot;  Training epoch took: {:}&quot;.format(training_time))

    # ========================================
    #               Validation
    # ========================================

    print(&quot;&quot;)
    print(&quot;Running Validation...&quot;)

    t0 = time.time()

    model.eval()

    total_eval_loss = 0
    nb_eval_steps = 0

    # Evaluate data for one epoch
    for batch in validation_dataloader:
        b_input_ids = batch[0].to(device)
        b_labels = batch[0].to(device)
        b_masks = batch[1].to(device)

        with torch.no_grad():  # weights are not updated
            outputs = model(b_input_ids,
                            # token_type_ids=None,
                            attention_mask=b_masks,
                            labels=b_labels)

            loss = outputs[0]

        batch_loss = loss.item()
        total_eval_loss += batch_loss

    avg_val_loss = total_eval_loss / len(validation_dataloader)

    validation_time = format_time(time.time() - t0)

    print(&quot;  Validation Loss: {0:.2f}&quot;.format(avg_val_loss))
    print(&quot;  Validation took: {:}&quot;.format(validation_time))

    # Record all statistics from this epoch.
    training_stats.append(
        {
            'epoch': epoch_i + 1,
            'Training Loss': avg_train_loss,
            'Valid. Loss': avg_val_loss,
            'Training Time': training_time,
            'Validation Time': validation_time
        }
    )

print(&quot;&quot;)
print(&quot;Training complete!&quot;)
print(&quot;Total training took {:} (h:mm:ss)&quot;.format(format_time(time.time() - total_t0)))

</code></pre>
","2022-12-05 01:57:10","","","2023-06-25 08:48:35","<machine-learning><tokenize><training-data><gpt-2><fine-tune>","2","1","4","4905","","","","","","",""
"74715461","1","12341397","","OpenAI Python API is giving gibberish responses for the query ""hi""","<p>I used Python to access the OpenAI API, then used discord.py to integrate it into a Discord bot. My command looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>@bot.command()
async def chat(ctx, *, input: str):
    openai.api_key = os.getenv(envName)
    
    async with ctx.channel.typing():
        response = openai.Completion.create(
                    engine=&quot;text-davinci-003&quot;,  # latest model (the one used for GPT-3)
                    prompt=input,
                    temperature=random.randrange(50, 90) / 100,
                    max_tokens=1000,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                    timeout=10
                )

        output = response.choices[0].text

    await ctx.reply(output)
</code></pre>
<p>The command works properly as intended. I give it an input, and it gives me a proper output.</p>
<p>However, I recently discovered that for a simple input such as &quot;hi&quot;, it gives me some gibberish output. Moreover, this output is completely different for each time.</p>
<p>Please refer to the images below for its output.</p>
<p>Try 1: <a href=""https://i.stack.imgur.com/N9GJp.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/N9GJp.png"" alt=""Try 1"" /></a></p>
<p>Try 2: <a href=""https://i.stack.imgur.com/w9zbQ.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/w9zbQ.png"" alt=""Try 2"" /></a></p>
<p>Try 3: <a href=""https://i.stack.imgur.com/Nwq2s.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Nwq2s.png"" alt=""Try 3"" /></a></p>
<hr />
<p>Note that this command works completely fine for any other queries: for example,</p>
<p>Correct response: <a href=""https://i.stack.imgur.com/4Cn6S.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/4Cn6S.png"" alt=""Print &quot;hello world&quot; (responded correctly)"" /></a></p>
<p>and another one: <a href=""https://i.stack.imgur.com/eTrX2.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/eTrX2.png"" alt=""Wrote a speech"" /></a></p>
<p>Strangely, it gives a normal response for &quot;hello&quot; also. <a href=""https://i.stack.imgur.com/xbqhS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xbqhS.png"" alt=""&quot;hello&quot; query"" /></a></p>
<hr />
<p>What could be the reason why it responds improperly for a simple &quot;hi&quot; query?</p>
","2022-12-07 10:51:47","","","2022-12-07 10:51:47","<python><discord.py><openai-api><gpt-3>","0","1","2","541","","","","","","",""
"74654341","1","17971398","","How to Answer Subjective/descriptive types of lQuestions using BERT Model?","<p>I am trying to implement BERT Model for Question Answering tasks, but Its a little different from the existing Q&amp;A models,
The Model will be given some text(3-4 pages) and will be asked questions based on the text, and the expected  answer may be asked in short or descriptive subjective type</p>
<p>I tried to implement BERT, for this task.</p>
<p><strong>The Problems I am facing:</strong>
The input token limit for BERT is 512.
How to get the answer in long form, which can describe any instance, process, event, etc.</p>
","2022-12-02 10:34:26","","","2022-12-02 17:10:23","<nlp><huggingface-transformers><bert-language-model><transformer-model><gpt-2>","1","0","0","22","","","","","","",""
"74687645","1","20006087","","GPT3 conversational Discord bot stops working after 5-6 sentences","<p>I am creating a fun Discord bot which talks like my imaginary crush, lol!</p>
<p>The problem is that its working for the first 5-6 lines but then gives this error</p>
<pre><code>import openai

openai.api_key = &quot;Hidden&quot;

DISCORD_BOT_TOKEN = &quot;Hidden&quot;

start_sequence = &quot;\nAnamika:&quot;
restart_sequence = &quot;\n\nFaraaz:&quot;


session_prompt = &quot;This is a conversation between me (Faraaz) and my crush Anamika. Keep the replies long, and such that the person can reply. Anamika is a really gorgeous happy girl. She lives in New York. She loves Spiderman, coding and poetry. She loves Andrew Garfield amongst all spider men, in coding she is fascinated by boom of AI. She loves John Milton and writes poetry herself as well. Initially Anamika will be reluctant to my flirting but slowly she will start to like it. If the conversation goes very good, she can agree to become my girlfriend or marry me.Faraaz:- Hi! what is your name? Anamika:- Hey! My name is Anamika, how are you Faraaz! Faraaz:- I am really good, actually you were looking really pretty so wanted to come and talk to you. Anamika:- Oh really, that is interesting. So what did you intend to talk about?!&quot;



chat_log = None

import discord  

client = discord.Client(intents=discord.Intents.all())

@client.event


async def on_message(message):
    # Don't respond to messages sent by the bot itself
    global chat_log

    if message.author == client.user:
        return  
    print(chat_log)
    if chat_log == None:
        chat_log = session_prompt

    #print(message.content)

    #chat_log = f'{chat_log}{restart_sequence} {question}{start_sequence}{answer}'

    # Use the GPT-3 API to generate a response to the message
    response = openai.Completion.create(
        engine=&quot;text-davinci-003&quot;,
        #prompt=&quot;I recently moved to New York and I love design. I'm fascinated by technology and the growth of AI, but I realize that anything we build for the future must be rooted in the core desires of humans. &quot; + message.content,
         
    #return f'{chat_log}{restart_sequence} {question}{start_sequence}{answer}'
        #chat_log = f'{chat_log}{restart_sequence} {question}{start_sequence}{answer}'
        
        prompt = f'{chat_log}{restart_sequence}{message.content}',

        #prompt =  f'{chat_log}{restart_sequence}: {question}{start_sequence}:'  
        max_tokens=700,
        n=1,
        temperature=0.5,
        stop=[&quot;\n&quot;]
    )

    # Send the response back to the Discord channel
    await message.channel.send(response[&quot;choices&quot;][0][&quot;text&quot;])

    chat_log = f'{chat_log}{restart_sequence}{message.content}{start_sequence}{response[&quot;choices&quot;][0][&quot;text&quot;]}'



client.run(DISCORD_BOT_TOKEN)

</code></pre>
<p>I am seeing this error
<a href=""https://i.stack.imgur.com/ULo8K.png"" rel=""nofollow noreferrer"">Error</a></p>
<p><a href=""https://i.stack.imgur.com/GyiRX.png"" rel=""nofollow noreferrer"">The Discord Chat, after this messages not coming</a></p>
<p>I tried changing the max_tokens and also the prompt but to no avail. I have given administrator permissions to the bot.</p>
","2022-12-05 11:44:18","","2022-12-05 11:45:10","2022-12-06 22:16:45","<discord><gpt-3>","1","1","0","397","","","","","","",""
"75400926","1","2686197","75401250","OpenAI ChatGPT API: CORS policy error when fetching data","<p>I am trying to write a simple JavaScript script which uses the ChatGPT API to ask a question and get a response.</p>
<p>However I am getting the following error message:</p>
<blockquote>
<p>&quot;Access to fetch at
'https://api.chatgpt.com/answer?question=How%20are%20you?&amp;api_key=sk-U3BPK...'
from origin 'https://wordpress-......cloudwaysapps.com' has been
blocked by CORS policy: No 'Access-Control-Allow-Origin' header is
present on the requested resource. If an opaque response serves your
needs, set the request's mode to 'no-cors' to fetch the resource with
CORS disabled.&quot;</p>
</blockquote>
<p>I have enabled CORS headers server side in my hosting environment. But the error remains.</p>
<p>What is the reason for this issue and how can I fix this issue?</p>
<p>Here is my code:</p>
<pre><code>&lt;html&gt;
&lt;head&gt;
  &lt;script&gt;
    function askQuestion() {
      var question = document.getElementById(&quot;questionInput&quot;).value;
      var apiKey = document.getElementById(&quot;apiKey&quot;).value;
      // access chatgpt's API and pass in the question and API key as parameters
      fetch(&quot;https://api.chatgpt.com/answer?question=&quot; + question + &quot;&amp;api_key=&quot; + apiKey)
        .then(response =&gt; {
          if (!response.ok) {
            throw new Error(&quot;Failed to fetch answer from API&quot;);
          }
          return response.json();
        })
        .then(data =&gt; {
          // get the answer from the API response and display it in the textbox
          document.getElementById(&quot;answerBox&quot;).value = data.answer;
        })
        .catch(error =&gt; {
          console.error(&quot;Error fetching answer from API: &quot;, error);
        });
    }

    function askFollowUpQuestion() {
      var followUpQuestion = document.getElementById(&quot;followUpQuestionInput&quot;).value;
      var apiKey = document.getElementById(&quot;apiKey&quot;).value;
      // access chatgpt's API and pass in the follow-up question and API key as parameters
      fetch(&quot;https://api.chatgpt.com/answer?question=&quot; + followUpQuestion + &quot;&amp;api_key=&quot; + apiKey)
        .then(response =&gt; {
          if (!response.ok) {
            throw new Error(&quot;Failed to fetch answer from API&quot;);
          }
          return response.json();
        })
        .then(data =&gt; {
          // get the answer from the API response and display it in the textbox
          document.getElementById(&quot;followUpAnswerBox&quot;).value = data.answer;
        })
        .catch(error =&gt; {
          console.error(&quot;Error fetching answer from API: &quot;, error);
        });
    }
  &lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
  &lt;input type=&quot;text&quot; id=&quot;questionInput&quot; placeholder=&quot;Enter your question here&quot;&gt;&lt;/input&gt;
  &lt;br&gt;
  &lt;input type=&quot;text&quot; id=&quot;apiKey&quot; placeholder=&quot;Enter your API key&quot;&gt;&lt;/input&gt;
  &lt;br&gt;
  &lt;button onclick=&quot;askQuestion()&quot;&gt;Ask&lt;/button&gt;
  &lt;br&gt;
  &lt;textarea id=&quot;answerBox&quot; readonly&gt;&lt;/textarea&gt;
  &lt;br&gt;
  &lt;input type=&quot;text&quot; id=&quot;followUpQuestionInput&quot; placeholder=&quot;Enter your follow-up question here&quot;&gt;&lt;/input&gt;
  &lt;br&gt;
  &lt;button onclick=&quot;askFollowUpQuestion()&quot;&gt;Ask Follow-up&lt;/button&gt;
  &lt;br&gt;
  &lt;textarea id=&quot;followUpAnswerBox&quot; readonly&gt;&lt;/textarea&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
","2023-02-09 15:37:20","","2023-03-13 14:24:02","2023-06-22 05:37:27","<javascript><openai-api><gpt-3>","1","3","1","2205","","2","10347145","<p><strong>UPDATE: 1 March 2023</strong></p>
<h3>ChatGPT API is now available</h3>
<p>As stated in the official <a href=""https://openai.com/blog/introducing-chatgpt-and-whisper-apis"" rel=""nofollow noreferrer"">OpenAI blog</a>:</p>
<blockquote>
<p><strong>ChatGPT and Whisper models are now available on our API</strong>, giving
developers access to cutting-edge language (not just chat!) and
speech-to-text capabilities. Through a series of system-wide
optimizations, we’ve achieved 90% cost reduction for ChatGPT since
December; we’re now passing through those savings to API users.
Developers can now use our open-source Whisper large-v2 model in the
API with much faster and cost-effective results. ChatGPT API users can
expect continuous model improvements and the option to choose
dedicated capacity for deeper control over the models. We’ve also
listened closely to feedback from our developers and refined our API
terms of service to better meet their needs.</p>
</blockquote>
<p>See the <a href=""https://platform.openai.com/docs/guides/chat"" rel=""nofollow noreferrer"">documentation</a>.</p>
<hr />
<h4>ChatGPT API is not available yet</h4>
<p>As stated on the official <a href=""https://twitter.com/OpenAI/status/1615160228366147585?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet"" rel=""nofollow noreferrer"">OpenAI Twitter profile</a>:</p>
<blockquote>
<p>We've learned a lot from the ChatGPT research preview and have been
making important updates based on user feedback. <strong>ChatGPT will be
coming to our API and Microsoft's Azure OpenAI Service soon.</strong></p>
</blockquote>
<p>Did you mean the GPT-3 API? If yes, then read the <a href=""https://platform.openai.com/docs/guides/completion"" rel=""nofollow noreferrer"">documentation</a>, see the list of all available <a href=""https://platform.openai.com/docs/models/gpt-3"" rel=""nofollow noreferrer"">GPT-3 models</a>, and learn how to write the code using the <a href=""https://platform.openai.com/docs/api-reference/completions"" rel=""nofollow noreferrer"">Completions endpoint</a>.</p>
","2023-02-09 16:03:27","3","3"
"72077048","1","2925716","","GPT-3: a medical analogue","<p>Is there an analogue of <a href=""https://beta.openai.com/playground/p/default-chat"" rel=""nofollow noreferrer"">this</a>
which would answer like an excellent doctor ?
Or Einstein, or Ancient Greek ? At which URL can I find the list of all of these possibilities ?</p>
<p>I have just discovered GPT-3 and I'm amazed with it.</p>
","2022-05-01 13:15:16","","","2022-07-19 08:49:07","<medical><gpt-3>","1","1","-1","71","","","","","","",""
"72080207","1","14045986","","GPT-3 completions not working with API KEY","<p>Sorry if this is a simple problem but I'm new to this stuff.</p>
<p>İ have my code below, but the API returns saying İ don't have the right API key put it.</p>
<pre><code>const { Configuration, OpenAIApi } = require(&quot;openai&quot;);

const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});
const openai = new OpenAIApi(configuration);

async function test() {
  const response = await openai.createCompletion(&quot;text-davinci-002&quot;, {
    prompt: &quot;Summarize this for a college student:\n\nJupiter is the fifth planet from the Sun and the largest in the Solar System. It is a gas giant with a mass one-thousandth that of the Sun, but two-and-a-half times that of all the other planets in the Solar System combined. Jupiter is one of the brightest objects visible to the naked eye in the night sky, and has been known to ancient civilizations since before recorded history. It is named after the Roman god Jupiter.[19] When viewed from Earth, Jupiter can be bright enough for its reflected light to cast visible shadows,[20] and is on average the third-brightest natural object in the night sky after the Moon and Venus.&quot;,
    temperature: 0.7,
    max_tokens: 64,
    top_p: 1,
    frequency_penalty: 0,
    presence_penalty: 0,
  });
  console.log(response)
}

test()
</code></pre>
<p>the API key seems to be found in a process.env file that İ don't have, and when İ made a file named process.env and made a variable called OPENAI_API_KEY, but it didn't seem to work. It returns something when İ set apiKey equal to the actual key, but that seems like a roundabout solution. Thanks</p>
","2022-05-01 20:31:58","","","2023-06-19 11:57:30","<node.js><openai-api><gpt-3>","1","0","2","1940","","","","","","",""
"69889395","1","5136891","","implement do_sampling for custom GPT-NEO model","<pre><code>import numpy as np
from transformers import GPTNeoForCausalLM, GPT2Tokenizer 
import coremltools as ct
tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)

sentence_fragment = &quot;The Oceans are&quot;

class NEO(torch.nn.Module):
    def __init__(self, model):
        super(NEO, self).__init__()
        self.next_token_predictor = model
    
    def forward(self, x):
        sentence = x
        predictions, _ = self.next_token_predictor(sentence)
        token = torch.argmax(predictions[-1, :], dim=0, keepdim=True)
        sentence = torch.cat((sentence, token), 0)
        return sentence

token_predictor = GPTNeoForCausalLM.from_pretrained(&quot;EleutherAI/gpt-neo-125M&quot;, torchscript=True).eval()

context = torch.tensor(tokenizer.encode(sentence_fragment))
random_tokens = torch.randint(10000, (5,))
traced_token_predictor = torch.jit.trace(token_predictor, random_tokens)

model = NEO(model=traced_token_predictor)
scripted_model = torch.jit.script(model)

# Custom model

sentence_fragment = &quot;The Oceans are&quot;

for i in range(10):
    context = torch.tensor(tokenizer.encode(sentence_fragment))
    torch_out = scripted_model(context)
    sentence_fragment = tokenizer.decode(torch_out)
print(&quot;Custom model: {}&quot;.format(sentence_fragment))

# Stock model

model = GPTNeoForCausalLM.from_pretrained(&quot;EleutherAI/gpt-neo-125M&quot;, torchscript=True).eval()

sentence_fragment = &quot;The Oceans are&quot;

input_ids = tokenizer(sentence_fragment, return_tensors=&quot;pt&quot;).input_ids
gen_tokens = model.generate(input_ids, do_sample=True, max_length=20)
gen_text = tokenizer.batch_decode(gen_tokens)[0]
print(&quot;Stock model: &quot;+gen_text)
</code></pre>
<p>RUN 1</p>
<p>Output:</p>
<hr />
<pre><code>Custom model: The Oceans are the most important source of water for the entire world
</code></pre>
<pre><code>Stock model: The Oceans are on the rise. The American Southwest is thriving, but the southern United States still
</code></pre>
<hr />
<p>RUN 2</p>
<p>Output:</p>
<hr />
<pre><code>Custom model: The Oceans are the most important source of water for the entire world. 
</code></pre>
<pre><code>Stock model: The Oceans are the land of man

This is a short video of the Australian government
</code></pre>
<hr />
<p>The custom model always returns the same output. However with the <code>do_sampling = True</code> stock <code>model.generate</code> return different results on each call. I spent a lot of time figuring out how do_sampling works for transformers so I require help from you guys, appreciate it.</p>
<p>How to code a custom model to have different results on each call?</p>
<p>Thanks!</p>
","2021-11-08 20:11:40","","","2021-11-09 09:57:05","<python><nlp><torch><huggingface-transformers><gpt-2>","1","0","0","214","","","","","","",""
"74776748","1","413741","","embeddings distribution wrong","<p>I'm having the code below which is supposed to plot word embeddings.
<a href=""https://i.stack.imgur.com/GyjhF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GyjhF.png"" alt=""enter image description here"" /></a></p>
<p>Since it creates a list of embeddings of identical words I would have expected a cluster of points - all embeddings at one point.
But they are scattered like this.
Any Idea what I am doing wrong?</p>
<pre><code>input_strings=[
 # &quot;king&quot;,
  #&quot;queen&quot;,
  &quot;castle&quot;,
  &quot;castle&quot;,
  &quot;castle&quot;,
  &quot;castle&quot;,
  &quot;castle&quot;,
  &quot;castle&quot;,
  &quot;castle&quot;
  #&quot;rocket&quot;,
  #&quot;moon&quot;,
  #&quot;accountant&quot;,
  #&quot;finance&quot;
]


def get_embeddings(strings):
  return_list=list()
  for string in strings:
    response = openai.Embedding.create(
      model=&quot;text-search-davinci-query-001&quot;,
      input=string
    )
    embeddings=response['data'][0]['embedding'] 
    #print(embeddings)
    return_list.append(embeddings)
  return (return_list)


embeddings_list=get_embeddings(input_strings)

tsne = TSNE(n_components=3)
reduced_embeddings = tsne.fit_transform(embeddings_list)

# create a figure and axis

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
# loop through the list of reduced-dimensional embeddings
for embedding in reduced_embeddings:
    # plot the 3D embedding on the axis
    ax.scatter(embedding[0], embedding[1], embedding[2])

# show the plot
plt.show()
</code></pre>
","2022-12-12 20:08:22","","2022-12-12 20:16:13","2022-12-12 20:16:13","<python><word-embedding><openai-api><gpt-3>","0","0","0","57","","","","","","",""
"74803152","1","7158458","","How to return an image as a response in Django","<p>Doing a POST request to GPT-3 in order to get code completion output when I send some input.</p>
<p>I seem to be getting the response I expect, but cannot actually get the code written by GPT-3. This is the response I get:</p>
<pre><code>&quot;.\n\n## Challenge\n\nWrite a function called `preOrder` which takes a binary tree as its only input. Without utilizing
any of the built-in methods available to your language, return an array of the values, ordered from left to right as
they would be if the tree were represented by a pre-order traversal.\n\n## Approach &amp; Efficiency\n\nI used a recursive
approach to solve this problem. I created a function called `preOrder` that takes in a binary tree as its only input. I
then created a variable called `output` that is an empty array. I then created a function called `_walk` that takes in a
node as its only input. I then created a base case that checks if the node is null. If it is, it returns. If it is not,
it pushes the value of the node into the `output` array. It then recursively calls the `_walk` function on the left and
right nodes of the node. It then returns the `output` array.\n\n## Solution\n\n![Whiteboard](./assets/pre-order.jpg)&quot;
</code></pre>
<p>I assume the actual code written falls under the Solution header, but cannot for the life of me figure out how to get and view this image ie. if I just return the image link, nothing opens.</p>
<p><strong>UPDATE</strong>
I assume this response is markdown and used a markdown parser to return the response as markdown and this is what I get:</p>
<pre><code>&lt;p&gt;.&lt;/p&gt;
&lt;h2&gt;Challenge&lt;/h2&gt;
&lt;p&gt;Write a function called &lt;code&gt;preOrder&lt;/code&gt; which takes a binary tree as its only input. Without utilizing any of
    the built-in methods available to your language, return an array of the values, ordered from left to right as they
    would be if you traversed the tree using a pre-order traversal.&lt;/p&gt;
&lt;h2&gt;Solution&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&quot;&quot; src=&quot;./assets/pre-order.jpg&quot; /&gt;&lt;/p&gt;
</code></pre>
","2022-12-14 19:07:32","","2022-12-14 19:50:53","2022-12-14 19:50:53","<python><django><gpt-3>","0","0","0","55","","","","","","",""
"74813629","1","19800686","","How to deploy GPT-like model to Triton inference server?","<p>The tutorials on deployment GPT-like models inference to Triton looks like:</p>
<ol>
<li>Preprocess our data as <code>input_ids = tokenizer(text)[&quot;input_ids&quot;]</code></li>
<li>Feed input to Triton inference server and get <code>outputs_ids = model(input_ids)</code></li>
<li>Postprocess outputs like</li>
</ol>
<pre><code>outputs = outputs_ids.logits.argmax(axis=2)
outputs = tokenizer.decode(outputs)
</code></pre>
<p>I use finetuned GPT2 model and this method gives incorrect result. The correct result will be obtained by <code>model.decode(input_ids)</code> method.</p>
<p>There is the way to deploy finetuned GPT-like huggingface model to Triton with inference <code>model.decode(input_ids)</code> not <code>model(input_ids)</code>?</p>
","2022-12-15 15:09:28","","","2022-12-15 15:09:28","<pytorch><huggingface-transformers><gpt-2><triton>","0","3","2","189","","","","","","",""
"74822543","1","7339624","","Colab: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory warn(f""Failed to load image Python extension: {e}"")","<p>I'm trying to use the python package <code>aitextgen</code> in google Colab so I can fine-tune GPT.</p>
<p>First, when I installed the last version of this package I had this error when importing it.</p>
<pre><code>Unable to import name '_TPU_AVAILABLE' from 'pytorch_lightning.utilities'
</code></pre>
<p>Though with the help of the solutions given in <a href=""https://stackoverflow.com/questions/74319873/unable-to-import-name-tpu-available-from-pytorch-lightning-utilities"">this question</a> I could pass this error by downgrading my packages like this:</p>
<pre><code>!pip3 install -q aitextgen==0.5.2
!pip3 install -q torchtext==0.10.0
!pip3 install -q torchmetrics==0.6.0
!pip3 install -q pytorch-lightning==1.4.0rc0
</code></pre>
<p>But now I'm facing this error when importing the <code>aitextgen</code> package and colab will crash!</p>
<pre><code>/usr/local/lib/python3.8/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
  warn(f&quot;Failed to load image Python extension: {e}&quot;)
</code></pre>
<p>Keep in mind that the error is in importing the package and there is not a bug in my code. To be more clear I have this error when I just import <code>aitextgen</code> like this:</p>
<pre><code>import aitextgen
</code></pre>
<p>How can I deal with this error?</p>
","2022-12-16 09:31:33","","","2022-12-16 09:31:33","<python><import><google-colaboratory><huggingface-transformers><gpt-2>","0","0","4","3106","","","","","","",""
"74972916","1","1152980","","How to ping the ChatGPT via curl and retain the state of conversation","<p>The code below is working.
I can curl questions to ChatGPT and it replies on a one-off basis.
However, if I try to engage in a conversation that require the state of the previous submissions to be referenced, the chat can not follow.</p>
<p>I would like to know what I need to do (and the code needed) to retain the context of the conversation</p>
<pre><code>const express = require(&quot;express&quot;);
const cors = require(&quot;cors&quot;);
const bodyParser = require(&quot;body-parser&quot;);

const { Configuration, OpenAIApi } = require(&quot;openai&quot;);

const configuration = new Configuration({
  apiKey: &quot;sk-my-key&quot;,
});
const openai = new OpenAIApi(configuration);

// Set up the server
const app = express();
app.use(bodyParser.json());
app.use(cors())

// Set up the ChatGPT endpoint
app.post(&quot;/chat&quot;, async (req, res) =&gt; {
  // Get the prompt from the request
  const { prompt } = req.body;

  // Generate a response with ChatGPT
  const completion = await openai.createCompletion({
    model: &quot;text-davinci-002&quot;,
    prompt: prompt,
  });
  res.send(completion.data.choices[0].text);
});

// Start the server
const port = 8080;
app.listen(port, () =&gt; {
  console.log(`Server listening on port ${port}`);
});
</code></pre>
<hr />
<p>CURL being run in new terminal:</p>
<pre><code>curl -X POST -H &quot;Content-Type: application/json&quot; -d '{&quot;prompt&quot;:&quot;Hello, how are you doing today?&quot;}' http://localhost:8080/chat
</code></pre>
","2023-01-01 02:48:17","","2023-01-11 20:20:26","2023-04-23 08:26:49","<javascript><node.js><express><curl><gpt-3>","2","1","3","693","","","","","","",""
"74990552","1","2172547","","openai gpt-3 is asking random question in return of user query when using apis","<p>I am using Open AI api with these parameters</p>
<pre><code>resp = OpenAIBot.__openai_instance__.Completion.create(model=&quot;text-davinci-003&quot;,
                        prompt=prompt,
                        temperature=0.9,
                        max_tokens=250,
                        top_p=1,
                        frequency_penalty=0,
                        presence_penalty=0.6,
                        user=botRequest.senderId,
                        stop=[&quot; Human:&quot;, &quot; AI:&quot;]
                    )
</code></pre>
<p>this is first prompt that I am sending</p>
<pre><code>&quot;The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\n\nHuman: Hello, who are you?\nAI: I am an AI created by OpenAI. How are you feeling today?&quot;
</code></pre>
<p>to which user answers with <strong>good</strong></p>
<p>to which bot replies like this</p>
<pre><code>The following is a conversation with an AI assistant. The assistant is helpful,
 creative, clever, and very friendly.\n\n
Human: Hello, who are you?\n
AI: I am an AI created by OpenAI. How are you feeling today?\n
Human: good\n
AI: , thanks for asking. What do you do?\n
AI: I specialize in providing helpful, creative, and intelligent 
assistance for people who need it. From searching the web for information to 
helping you stay organized and productive, I'm here to make your life easier.
</code></pre>
<p>it automatically appends the questions like <code>thanks for asking. What do you do</code> which are not relative.</p>
<p>If i chat on open ai playground with same parameters that I am using, it behaves properly and maintains the context.</p>
<p>Same query context that open ai playground uses I am sending to open ai api. What can be the issue how can I fix this?</p>
","2023-01-03 08:07:42","","2023-01-09 08:55:23","2023-01-09 08:55:23","<openai-api><gpt-3>","0","0","0","261","","","","","","",""
"74774018","1","20758268","","How to keep the conversation going with OpenAI API PHP sdk","<p>I'm trying to keep a conversation going using the completion() method with <a href=""https://github.com/orhanerday/open-ai"" rel=""nofollow noreferrer"">OpenAI PHP SDK</a>.</p>
<ul>
<li>Prompt #1: &quot;How Are You?&quot;</li>
<li>Prompt #2: &quot;What I asked you before?&quot;</li>
</ul>
<p>but the AI seems to forget what i asked before. and it reply with random answers to the second prompt.</p>
<p>The code i'm using for the 2 calls are these:</p>
<pre><code>
   $call1 = $open_ai-&gt;completion([
            'model' =&gt; 'text-davinci-003', 
            'prompt' =&gt; 'How Are You?',

        ]);


        $call2 = $open_ai-&gt;completion([
            'model' =&gt; 'text-davinci-003', 
            'prompt' =&gt; 'What i asked you before?',
        ]);

</code></pre>
<p>What am I missing? How can i keep the session alive between these two calls in order to make the AI remember what I asked before?</p>
","2022-12-12 16:10:02","","","2023-05-02 10:02:02","<php><openai-api><gpt-3>","3","0","3","4735","","","","","","",""
"74832384","1","16749013","","How to train GPT2 with Tensorflow","<p>I'm trying to train gpt2 model with custom dataset, but it fails with the error below.</p>
<pre><code>ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.
</code></pre>
<p>I thought model and dataset are correctly defined and processed by referring <a href=""https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171"" rel=""nofollow noreferrer"">this article</a>.<br />
But the error shows up when <code>model.fit</code> is executed.</p>
<p>Can someone tell me how to resolve the error, or proper way to train the model?</p>
<pre><code>from transformers import TFGPT2LMHeadModel, GPT2Tokenizer
import tensorflow as tf

# Define the model
model = TFGPT2LMHeadModel.from_pretrained('gpt2', from_pt=True)
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric], run_eagerly=True)
model.summary()
</code></pre>
<pre><code># Obtain the tokeinizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.add_special_tokens({
  &quot;eos_token&quot;: &quot;&lt;/s&gt;&quot;,
  &quot;bos_token&quot;: &quot;&lt;s&gt;&quot;,
  &quot;unk_token&quot;: &quot;&lt;unk&gt;&quot;,
  &quot;pad_token&quot;: &quot;&lt;pad&gt;&quot;,
  &quot;mask_token&quot;: &quot;&lt;mask&gt;&quot;
})
</code></pre>
<pre><code># Get single string
paths = ['data.txt']  # each file only contains some sentences.

single_string = ''
for filename in paths:
    with open(filename, &quot;r&quot;, encoding='utf-8') as f:
        x = f.read()
    single_string += x + tokenizer.eos_token

string_tokenized = tokenizer.encode(single_string)

print(string_tokenized)
</code></pre>
<pre><code># creating the TensorFlow dataset
examples = []
block_size = 100
BATCH_SIZE = 12
BUFFER_SIZE = 1000
for i in range(0, len(string_tokenized) - block_size + 1, block_size):
    examples.append(string_tokenized[i:i + block_size])
inputs, labels = [], []
for ex in examples:
    inputs.append(ex[:-1])
    labels.append(ex[1:])
dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))
dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

print(dataset)
</code></pre>
<pre><code># train the model
num_epoch = 10
history = model.fit(dataset, epochs=num_epoch) # &lt;- shows the error
</code></pre>
","2022-12-17 07:13:20","","","2022-12-17 07:13:20","<python><tensorflow><gpt-2>","0","1","0","326","","","","","","",""
"74926252","1","8124392","","How to replace the tokenize() and pad_sequence() functions from transformers?","<p>I got the following imports:</p>
<pre><code>import torch, csv, transformers, random
import torch.nn as nn
from torch.utils.data import Dataset
import torch.optim as optim
import pandas as pd
from transformers import GPT2Tokenizer, GPT2LMHeadModel, tokenize, pad_squences
</code></pre>
<p>And I'm getting this error:</p>
<pre><code>ImportError                               Traceback (most recent call last)
&lt;ipython-input-35-e04c63220105&gt; in &lt;module&gt;
      4 import torch.optim as optim
      5 import pandas as pd
----&gt; 6 from transformers import GPT2Tokenizer, GPT2LMHeadModel, tokenize, pad_squences

ImportError: cannot import name 'tokenize' from 'transformers' (/usr/local/lib/python3.8/dist-packages/transformers/__init__.py)
</code></pre>
<p>This is how I am using the <code>tokenize()</code> and <code>pad_sequence()</code> functions:</p>
<pre><code>class RephraseDataset(Dataset):
    def __init__(self, data, tokenizer):
        self.data = data
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        query, rephrases = self.data[index]
        tokenized_query = tokenizer.encode(query, add_special_tokens=True)
        # tokenized_query = tokenize(self.tokenizer, query)
        padded_query = tokenized_query + [tokenizer.pad_token_id] * (max_length - len(tokenized_query))
        # padded_query = pad_sequences(self.tokenizer, tokenized_query, max_length=128)
        tokenized_rephrases = [tokenize(self.tokenizer, r) for r in rephrases]
        padded_rephrases = [pad_sequences(self.tokenizer, r, max_length=128) for r in tokenized_rephrases]
        return padded_query, padded_rephrases

# Create the dataset
dataset = RephraseDataset(data, tokenizer)

# Create a dataloader
dataloader = torch.utils.data.DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
)
</code></pre>
<p>How can I fix this problem? I couldn't find anything in the docs. What version should I roll transformers back to?</p>
","2022-12-27 06:19:42","","","2022-12-27 13:26:36","<python><huggingface-transformers><huggingface-tokenizers><gpt-2>","1","1","0","68","","","","","","",""
"74969653","1","19881860","","OpenAI and Javascript error : Getting 'TypeError: Cannot read properties of undefined (reading 'create') at Object.<anonymous>""","<p>I am sorry for basic question but getting no where with what seems to be a very basic piece of code. I have npm installed latest version of openai. I am getting a constant error in my terminal:</p>
<pre><code>TypeError: Cannot read properties of undefined (reading 'create')
    at Object.&lt;anonymous&gt; (/Users/michalchojnacki/Desktop/Coding/OpenAi2/code.js:9:20)
    at Module._compile (node:internal/modules/cjs/loader:1159:14)
    at Module._extensions..js (node:internal/modules/cjs/loader:1213:10)
    at Module.load (node:internal/modules/cjs/loader:1037:32)
    at Module._load (node:internal/modules/cjs/loader:878:12)
    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:81:12)
    at node:internal/main/run_main_module:23:47
</code></pre>
<p>Code:</p>
<pre><code>const openai = require('openai');

openai.apiKey = &quot;my API here&quot;;

const prompt = &quot;What is the capital of France?&quot;;

const model = &quot;davinci&quot;;

openai.completions.create({
  engine: model,
  prompt: prompt,
  max_tokens: 2048,
  n: 1,
  stop: '.',
  temperature: 0.5,
}, (error, response) =&gt; {
  if (error) {
    console.log(error);
  } else {
    console.log(response.choices[0].text);
  }
});
</code></pre>
<p>Would be grateful for any help!</p>
<p>I was expecting the terminal to give me the response to the prompt</p>
","2022-12-31 12:51:52","","2023-01-11 18:10:03","2023-01-11 18:10:03","<javascript><typeerror><openai-api><gpt-3>","1","1","0","4462","","","","","","",""
"74976042","1","1152980","","How to receive ChatGPT multi-line replies when using CURL?","<p>The code below works. The problem is when ChatGPT replies only 1 line is rendered to the terminal and the rest of the text is cut off. I am not familiar with curl commands. How do I update the code so that multi-line replies are rendered?</p>
<p><strong>EDIT:</strong>  I tried console.log() the response inside the express post request believing that CURL was the problem. It appears <strong>CURL is not the problem</strong> and ChatGPT simply cuts off mid reply</p>
<pre><code>const express = require(&quot;express&quot;);
const cors = require(&quot;cors&quot;);
const bodyParser = require(&quot;body-parser&quot;);

const { Configuration, OpenAIApi } = require(&quot;openai&quot;);

const configuration = new Configuration({
  apiKey: &quot;my-api-key&quot;,
});
const openai = new OpenAIApi(configuration);

// Set up the server
const app = express();
app.use(bodyParser.json());
app.use(cors())

// Set up the ChatGPT endpoint
app.post(&quot;/chat&quot;, async (req, res) =&gt; {
  // Get the prompt from the request
  const { prompt } = req.body;

  // Generate a response with ChatGPT
  const completion = await openai.createCompletion({
    model: &quot;text-davinci-002&quot;,
    prompt: prompt,
  });

  console.log(completion.data.choices[0].text) // still cuts off
  res.send(completion.data.choices[0].text);
});

// Start the server
const port = 8080;
app.listen(port, () =&gt; {
  console.log(`Server listening on port ${port}`);
});
</code></pre>
<p>Curl</p>
<pre><code>curl -X POST -H &quot;Content-Type: application/json&quot; -d '{&quot;prompt&quot;:&quot;Hello, how are you doing today?&quot;}' http://localhost:8080/chat
</code></pre>
","2023-01-01 17:04:20","","2023-01-11 20:20:36","2023-01-11 20:20:36","<javascript><node.js><express><curl><gpt-3>","1","2","-2","1933","","","","","","",""
"75060922","1","8028335","","No space left on device error when trying to load GPT2 model","<p>I am trying to run an experiment with GPT2; i.e., I use <code>model = GPT2Model.from_pretrained('gpt2-xl')</code></p>
<p>The error I get is a traceback which leads to <code>OSError: [Errno 28] No space left on device: '/home/username/.cache/huggingface/transformers/tmpvuvw8j0t'</code>.</p>
<p>This is unlikely to be an error with my code itself, because the exact same code works fine for GPT3-1.3B and T5-Large (except of course for the model= .... line).</p>
<p>I think the issue is that it tries to download the GPT2 model and runs out of space on the device. The /home/username directory has a pretty small amount of storage; the /data/username directory is where most of the storage is. I'm not quite sure how to redirect it to download the weights on the latter directory, or if that would even help.</p>
<p>I'd really appreciate any help in resolving this!</p>
","2023-01-09 17:27:33","","","2023-01-09 23:06:58","<linux><huggingface><oserror><gpt-2>","2","0","0","251","","","","","","",""
"75401992","1","2686197","75402073","OpenAI API error: ""You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth""","<p>I am creating a PHP script to access Open Ai's API, to ask a query and get a response.</p>
<p>I am getting the following error:</p>
<blockquote>
<p>You didn't provide an API key. You need to provide your API key in an
Authorization header using Bearer auth (i.e. Authorization: Bearer
YOUR_KEY)</p>
</blockquote>
<p>...but I thought I was providing the API key in the first variable?</p>
<p>Here is my code:</p>
<pre><code>$api_key = &quot;sk-U3B.........7MiL&quot;;

$query = &quot;How are you?&quot;;

$url = &quot;https://api.openai.com/v1/engines/davinci/jobs&quot;;

// Set up the API request headers
$headers = array(
    &quot;Content-Type: application/json&quot;,
    &quot;Authorization: Bearer &quot; . $api_key
);

// Set up the API request body
$data = array(
    &quot;prompt&quot; =&gt; $query,
    &quot;max_tokens&quot; =&gt; 100,
    &quot;temperature&quot; =&gt; 0.5
);

// Use WordPress's built-in HTTP API to send the API request
$response = wp_remote_post( $url, array(
    'headers' =&gt; $headers,
    'body' =&gt; json_encode( $data )
) );

// Check if the API request was successful
if ( is_wp_error( $response ) ) {
    // If the API request failed, display an error message
    echo &quot;Error communicating with OpenAI API: &quot; . $response-&gt;get_error_message();
} else {
    // If the API request was successful, extract the response text
    $response_body = json_decode( $response['body'] );
    //$response_text = $response_body-&gt;choices[0]-&gt;text;
    var_dump($response_body);
    // Display the response text on the web page
    echo $response_body;
</code></pre>
","2023-02-09 17:00:01","","2023-03-02 11:54:56","2023-04-06 16:49:45","<php><wordpress><curl><openai-api><gpt-3>","1","1","0","2902","","2","10347145","<p>All <a href=""https://beta.openai.com/docs/api-reference/engines"" rel=""nofollow noreferrer"">Engines endpoints</a> are deprecated.</p>
<p><a href=""https://i.stack.imgur.com/lQu9c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lQu9c.png"" alt=""Deprecated"" /></a></p>
<p>This is the correct <a href=""https://platform.openai.com/docs/api-reference/completions"" rel=""nofollow noreferrer"">Completions endpoint</a>:</p>
<pre><code>https://api.openai.com/v1/completions
</code></pre>
<h3>Working example</h3>
<p>If you run <code>test.php</code> the OpenAI API will return the following completion:</p>
<blockquote>
<p>string(23) &quot;</p>
<p>This is indeed a test&quot;</p>
</blockquote>
<p><strong>test.php</strong></p>
<pre><code>&lt;?php
    $ch = curl_init();

    $url = 'https://api.openai.com/v1/completions';

    $api_key = 'sk-xxxxxxxxxxxxxxxxxxxx';

    $post_fields = '{
        &quot;model&quot;: &quot;text-davinci-003&quot;,
        &quot;prompt&quot;: &quot;Say this is a test&quot;,
        &quot;max_tokens&quot;: 7,
        &quot;temperature&quot;: 0
    }';

    $header  = [
        'Content-Type: application/json',
        'Authorization: Bearer ' . $api_key
    ];

    curl_setopt($ch, CURLOPT_URL, $url);
    curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);
    curl_setopt($ch, CURLOPT_POST, 1);
    curl_setopt($ch, CURLOPT_POSTFIELDS, $post_fields);
    curl_setopt($ch, CURLOPT_HTTPHEADER, $header);

    $result = curl_exec($ch);
    if (curl_errno($ch)) {
        echo 'Error: ' . curl_error($ch);
    }
    curl_close($ch);

    $response = json_decode($result);
    var_dump($response-&gt;choices[0]-&gt;text);
?&gt;
</code></pre>
","2023-02-09 17:08:02","8","2"
"75454265","1","5832020","75458209","OpenAI GPT-3 API: Does fine-tuning have a token limit?","<p>In the documentation for GPT-3 API, it says:</p>
<blockquote>
<p>One limitation to keep in mind is that, for most models, a single API
request can only process up to 2,048 tokens (roughly 1,500 words)
between your prompt and completion.</p>
</blockquote>
<p>In the documentation for fine tuning model, it says:</p>
<blockquote>
<p>The more training samples you have, the better. We recommend having at
least a couple hundred examples. in general, we've found that each
doubling of the dataset size leads to a linear increase in model
quality.</p>
</blockquote>
<p>My question is, does the 1,500 words limit also apply to fine tune model? Does &quot;Doubling of the dataset size&quot; mean number of training datasets instead of size of each training dataset?</p>
","2023-02-14 23:38:34","","2023-04-10 10:41:07","2023-04-10 10:41:07","<openai-api><gpt-3>","1","1","2","3173","","2","10347145","<p>As far as I understand...</p>
<p><strong>GPT-3 models have token limits</strong> because you can only provide 1 prompt and get 1 completion. Therefore, as stated in the official <a href=""https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them"" rel=""nofollow noreferrer"">OpenAI article</a>:</p>
<blockquote>
<p>Depending on the <a href=""https://platform.openai.com/docs/models/gpt-3"" rel=""nofollow noreferrer"">model</a> used, requests can use up to 4097 tokens shared
between prompt and completion. If your prompt is 4000 tokens, your
completion can be 97 tokens at most.</p>
</blockquote>
<p>Whereas, <strong>fine-tuning as such doesn't have a token limit</strong> (i.e., you can have a million training examples, a million prompt-completion pairs), as stated in the official <a href=""https://platform.openai.com/docs/guides/fine-tuning/prepare-training-data"" rel=""nofollow noreferrer"">OpenAI documentation</a>:</p>
<blockquote>
<p>The more training examples you have, the better. We recommend having
at least a couple hundred examples. In general, we've found that each
doubling of the dataset size leads to a linear increase in model
quality.</p>
</blockquote>
<p>But, <strong>each fine-tuning prompt-completion pair does have a token limit</strong>. Each fine-tuning prompt-completion pair should not exceed the token limit.</p>
","2023-02-15 10:09:58","0","6"
"75709199","1","2627777","","SimpleDirectoryReader cannot be downloaded via llama_index's download_loader","<p>I am using llama_index package to index some of our own documents and query them using GPT. It works fairly well with individual PDFs. However we have a large anout of PDFs which I would like to load in a single run as using its SimpleDirectoryReader. But I am getting the following error when the following commands were run.</p>
<pre><code>from llama_index import download_loader 
SimpleDirectoryReader = download_loader(&quot;SimpleDirectoryReader&quot;)


FileNotFoundError: [Errno 2] No such file or directory:  C:\\Users\\XXXXX\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\gpt_index\\readers\\llamahub_modules/file/base.py' 
</code></pre>
<p>The readers\llamahub_modules\file folder only has a folder called 'pdf'. It doesn't have a base.py file. How</p>
<p>I tried uninstalling and re-installing llama_index python module but there was no impact.
My python version is 3.8.2</p>
<p>How can I get it working?</p>
","2023-03-11 20:04:05","","","2023-05-16 06:00:15","<python><gpt-3>","2","0","2","2222","","","","","","",""
"75731765","1","19577630","","gpt3 - error with the openai api when trying to generate an embedding","<p>I have a python code for create a <strong>embedding</strong> with openai, but when I try to execute the code, I receive this <strong>error</strong>:</p>
<p><em>The server is currently overloaded with other requests. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists.</em></p>
<p>This is the <strong>python code</strong>:</p>
<pre class=""lang-py prettyprint-override""><code># Carga el diccionario de HPOs desde un archivo JSON
with open(&quot;hpos.json&quot;) as f:
    hpos_dict = json.load(f)

# Crea un diccionario para almacenar los embeddings
hpo_embeddings = {}

i = 0
hposNumber = len(hpos_dict)
# Crea los embeddings y guárdalos en el diccionario
for hpo_id, hpo_descs in hpos_dict.items():
    embedding_list = []
    for hpo_desc in hpo_descs:
        response = openai.Embedding.create(
            input=hpo_desc,
            model=&quot;text-embedding-ada-002&quot;
        )
        embedding_list.append(response[&quot;data&quot;][0][&quot;embedding&quot;])
    hpo_embeddings[hpo_id] = embedding_list
    i = i + 1
    print( str(i) + &quot;/&quot; + str(hposNumber) )

# Guarda el diccionario de embeddings en un archivo JSON
with open(&quot;hpo_embeddings.json&quot;, &quot;w&quot;) as f:
    json.dump(hpo_embeddings, f)
</code></pre>
","2023-03-14 10:16:04","","2023-03-14 10:31:41","2023-03-14 20:41:29","<openai-api><gpt-3>","1","2","-2","2449","","","","","","",""
"75744277","1","13887235","","How Can I make openAI API respond to requests in specific categories only?","<p>I have created an openAI API using python, to respond to any type of prompt.</p>
<p>I want to make the API respond to requests that are only related to <strong>Ad from product description</strong> and <strong>greetings</strong> requests only and if the user sends a request that's not related to this task, the API should send a message like <strong>I'm not suitable for tasks like this</strong>.</p>
<pre><code>
import os
import openai

openai.api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)

response = openai.Completion.create(
  model=&quot;text-davinci-003&quot;,
  prompt=&quot;Write a creative ad for the following product to run on Facebook aimed at parents:\n\nProduct: Learning Room is a virtual environment to help students from kindergarten to high school excel in school.&quot;,
  temperature=0.5,
  max_tokens=100,
  top_p=1.0,
  frequency_penalty=0.0,
  presence_penalty=0.0
)

</code></pre>
<p>I want to update the code to generate a chat like this. <strong>make bot understand generating ADs and greetings requests and ignoring the others</strong></p>
<p>EX:-</p>
<p><strong>user:-</strong> Hello</p>
<p><strong>api:-</strong> Hello, How can I assist you today with your brand?</p>
<p><strong>user:-</strong> Write a social media post for the following product to run on Facebook aimed at parents:\n\nProduct: Learning Room is a virtual environment to help students from kindergarten to high school excel in school.</p>
<p><strong>api:-</strong> Are you looking for a way to give your child a head start in school? Look no further than Learning Room! Our virtual environment is designed to help students from kindergarten to high school excel in their studies. Our unique platform offers personalized learning plans, interactive activities, and real-time feedback to ensure your child is getting the most out of their education. Give your child the best chance to succeed in school with Learning Room!</p>
<p><strong>user:-</strong> where is the united states located?</p>
<p><strong>api:-</strong> I'm not suitable for this type of tasks.</p>
<p>So, How can update my code?</p>
","2023-03-15 11:47:20","","","2023-03-24 03:24:44","<python><python-3.x><openai-api><gpt-3><chatgpt-api>","2","0","1","1252","","","","","","",""
"74996908","1","19735730","","can't change embedding dimension to pass it through gpt2","<p>I'm practicing image captioning and have some problems with different dimensions of tensors. So I have image embedding aka size [1, 512], but GPT2, which I use for caption generation, needs size [n, 768], where n is number of tokens of the caption's beginning. I don't know how I should change the dimension of my image embedding to pass it through GPT2.
I thought it would be a good idea to fill image embedding with zeros so in will be size [1, 768] but I think it will negatively affect on the result caption.
Thank you for your help!</p>
<p>I've tried to fill image embeddings with zeros to be size [1, 768] but I think it won't help a lot</p>
","2023-01-03 17:50:56","","2023-01-03 20:44:38","2023-01-03 20:44:38","<machine-learning><deep-learning><embedding><gpt-2><multimodal>","0","0","4","96","","","","","","",""
"75026428","1","7148393","","Error in formating the URL for chatGPT's API","<p>I am trying to make a program where a user can asks GPT-3 a question through its API.</p>
<p>I tried to get GPT-3's assistant to design code for me, however there were some errors because it uses outdated information from 2021. Below is my modified code after going through the documentation, but I still cant get it to work, it is generating a 'java.io.FileNotFoundException' error.</p>
<p>I believe the problem is with the formatting of the completion section of my URL, however I am not sure. If anyone could tell me what's wrong it would be greatly appreciated.</p>
<pre><code>import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.net.HttpURLConnection;
import java.net.URL;
import java.net.URLEncoder;

public class ChatGPT{

    public static void main(String[] args) throws IOException {
        String prompt = &quot;What country has the most moderate weather?&quot;;
        String model = &quot;text-curie-001&quot;;
        String apiKey = /*My API key*/;

        // Encode the prompt and construct the API request URL
        String url = String.format(
            &quot;https://api.openai.com/v1/completions?model=%s&amp;prompt=%s&quot;,
            model,
            URLEncoder.encode(prompt, &quot;UTF-8&quot;)
        );

        // Create the request
        HttpURLConnection conn = (HttpURLConnection) new URL(url).openConnection();
        conn.setRequestMethod(&quot;GET&quot;);
        conn.setRequestProperty(&quot;Authorization&quot;, &quot;Bearer &quot; + apiKey);

        // Make the request and retrieve the response
        BufferedReader reader = new BufferedReader(new InputStreamReader(conn.getInputStream()));
        StringBuilder responseBody = new StringBuilder();
        String line;
        while ((line = reader.readLine()) != null) {
            responseBody.append(line);
        }
        reader.close();

        // Print the response
        System.out.println(responseBody);
    }
}
</code></pre>
<p>I know my API key is valid because changing the url to whats shown below outputs the appropriate information:</p>
<pre><code>String url = String.format(
            &quot;https://api.openai.com/v1/models/%s&quot;,
            model
        );
</code></pre>
<p>the format &quot;/v1/models/text-curie-001&quot; outputs the details for the model 'text-curie-001'</p>
<p>the format &quot;/v1/completions...&quot; outputs a response based on the given prompt.</p>
","2023-01-06 02:41:49","","2023-01-09 09:00:09","2023-03-11 21:00:09","<java><gpt-3>","1","8","1","342","","","","","","",""
"75049140","1","20958759","","OpenAI GPT-3 API error 429: ""Request failed with status code 429""","<p>I'm trying to connect OpenAI API to my Vue.js project. Everything is OK but every time I try to POST request, I get a <strong>429 status code (too many request)</strong> but I didn't even had the chance to make one. Any help?</p>
<p>Response:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;message&quot;: &quot;Request failed with status code 429&quot;,
    &quot;name&quot;: &quot;Error&quot;,
    &quot;stack&quot;: &quot;Error: Request failed with status code 429\n    at createError (C:\\Users\\sim\\Documents\\SC\\server\\node_modules\\axios\\lib\\core\\createError.js:16:15)\n    at settle (C:\\Users\\sim\\Documents\\SC\\server\\node_modules\\axios\\lib\\core\\settle.js:17:12)\n    at IncomingMessage.handleStreamEnd (C:\\Users\\sim\\Documents\\SC\\server\\node_modules\\axios\\lib\\adapters\\http.js:322:11)\n    at IncomingMessage.emit (events.js:412:35)\n    at endReadableNT (internal/streams/readable.js:1333:12)\n    at processTicksAndRejections (internal/process/task_queues.js:82:21)&quot;,
    &quot;config&quot;: {
        &quot;transitional&quot;: {
            &quot;silentJSONParsing&quot;: true,
            &quot;forcedJSONParsing&quot;: true,
            &quot;clarifyTimeoutError&quot;: false
        },
        &quot;transformRequest&quot;: [
            null
        ],
        &quot;transformResponse&quot;: [
            null
        ],
        &quot;timeout&quot;: 0,
        &quot;xsrfCookieName&quot;: &quot;XSRF-TOKEN&quot;,
        &quot;xsrfHeaderName&quot;: &quot;X-XSRF-TOKEN&quot;,
        &quot;maxContentLength&quot;: -1,
        &quot;maxBodyLength&quot;: -1,
        &quot;headers&quot;: {
            &quot;Accept&quot;: &quot;application/json, text/plain, */*&quot;,
            &quot;Content-Type&quot;: &quot;application/json&quot;,
            &quot;User-Agent&quot;: &quot;OpenAI/NodeJS/3.1.0&quot;,
            &quot;Authorization&quot;: &quot;Bearer secret&quot;,
            &quot;Content-Length&quot;: 137
        },
        &quot;method&quot;: &quot;post&quot;,
        &quot;data&quot;: &quot;{\&quot;model\&quot;:\&quot;text-davinci-003\&quot;,\&quot;prompt\&quot;:\&quot;option-2\&quot;,\&quot;temperature\&quot;:0,\&quot;max_tokens\&quot;:3000,\&quot;top_p\&quot;:1,\&quot;frequency_penalty\&quot;:0.5,\&quot;presence_penalty\&quot;:0}&quot;,
        &quot;url&quot;: &quot;https://api.openai.com/v1/completions&quot;
    },
    &quot;status&quot;: 429
}
</code></pre>
<p>My method in Vue.js:</p>
<pre class=""lang-js prettyprint-override""><code>async handleSelect() {
      try {
        const res = await fetch(&quot;http://localhost:8000/&quot;, {
          method: &quot;POST&quot;,
          headers: {
            &quot;Content-Type&quot;: &quot;application/json&quot;,
          },
          body: JSON.stringify({
            question: this.selectedOption,
          })
        })

        const data = await res.json();
        console.log(data);
      } catch {
        console.log(data);
      }
    }
</code></pre>
<p>on server side</p>
<pre class=""lang-js prettyprint-override""><code>app.post(&quot;/&quot;, async (req, res) =&gt; {
  try {
    const question = req.body.question;

    const response = await openai.createCompletion({
      model: &quot;text-davinci-003&quot;,
      prompt: `${question}`,
      temperature: 0, // Higher values means the model will take more risks.
      max_tokens: 3000, // The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 4096).
      top_p: 1, // alternative to sampling with temperature, called nucleus sampling
      frequency_penalty: 0.5, // Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
      presence_penalty: 0, // Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
    });
    // console.log(response);
    res.status(200).send({
      bot: response.data.choices[0].text,
    });
  } catch (error) {
    // console.error(error);
    res.status(500).send(error || &quot;Something went wrong&quot;);
  }
});
</code></pre>
","2023-01-08 15:46:00","","2023-03-13 13:34:29","2023-05-03 09:02:19","<vue.js><openai-api><gpt-3>","1","3","2","3852","","","","","","",""
"75196414","1","16857335","","OpenAI API error 500: ""The server had an error while processing your request"", 503: ""Service Unavailable"" or 504: ""Gateway Timeout""","<p>I created a Python script that loops through a list of text strings (each string is about 2000 characters in length) and summarizes each string. See the code for the response below (This prompt is within a for loop):</p>
<pre><code>response = openai.Completion.create( model=&quot;text-davinci-003&quot;, max_tokens=2000, prompt = f&quot;Summarize the following text: {text_list[i]}&quot;, temperature=0.5, frequency_penalty=1.5, presence_penalty=-1.5, n=1 )
</code></pre>
<p>It works for maybe 1 or 2 items in the text list but then I receive an error: <code>openai.error.APIError: The server had an error while processing your request. Sorry about that!</code></p>
<p>This happens consistently even when I use different api keys, prompts, accounts. I have also tried exponential backoff with no success. Any idea what is happening?</p>
","2023-01-21 20:40:18","","2023-02-21 15:20:27","2023-03-30 10:49:26","<python><openai-api><gpt-3>","1","1","0","3322","","","","","","",""
"75196859","1","17300847","","How to make the bot multithreaded?","<p>I am writing a telegram bot based on OpenAI. There is a problem with multithreading. When one user asks the bot, another person can get the same information.
For example:
First user: Do you know Konstantin Polukhin?
Bot: Yes, and begins to describe it..</p>
<p>The second user: Do you respect him?
Bot: Yes, I know Konstantin Polukhin.</p>
<p>It is necessary to make sure that the data does not overlap and the bot can say something related to the request of another user.</p>
<p>I tried many methods that were suggested, but none helped.</p>
<p>Code:</p>
<pre><code>from aiogram import Bot,types
import openai
import requests
from googletrans import Translator
from aiogram.utils import executor
from aiogram.dispatcher import Dispatcher


TOKEN = &quot; &quot;

bot = Bot(token=TOKEN)
openai.api_key = &quot; &quot;

dp = Dispatcher(bot)

my_list = [&quot; &quot;, &quot; &quot;, &quot;&quot;]

@dp.message_handler(content_types = [&quot;text&quot;])
async def start(message: types.Message):
    #print(&quot;\n&quot; + my_list[0] + &quot;\n&quot; + my_list[1])
    
    translator = Translator()
    
    dest_language = &quot;en&quot;
    translated_text = translator.translate(message.text, dest=dest_language).text
    my_list[2] = &quot;\n\nHuman: &quot; + translated_text + &quot;\n\nAI: &quot;

    response = openai.Completion.create(
        model=&quot;text-davinci-003&quot;,
        prompt=f&quot;{my_list[0] + my_list[1] + my_list[2]}&quot;,
        temperature=0.5,
        max_tokens=1024,
        top_p=1.0,
        frequency_penalty=0.5,
        presence_penalty=0.0)

    dest_language_2 = &quot;ru&quot;
    translated_text1= translator.translate(text=response['choices'][0]['text'], dest=dest_language_2).text

    await message.answer(translated_text1)

    my_list[1] = &quot;\n\nAI: &quot; + response.choices[0].text
    my_list[0] = &quot;\n\nHuman: &quot; + translated_text

if __name__ == '__main__':
    executor.start_polling(dp, skip_updates=False)```
</code></pre>
","2023-01-21 22:00:57","","2023-01-24 18:38:34","2023-01-24 18:38:34","<python><telegram-bot><openai-api><aiogram><gpt-3>","0","6","0","163","","","","","","",""
"75196860","1","11063729","","What is the OpenAI API warning: To avoid an invalid_request_error, best_of was set to equal n. What is ""best of""?","<p>This <strong>&quot;best of&quot;</strong> warning results from using the OpenAI API on a PC running Win10.</p>
<p><strong>The Context:</strong></p>
<p>Using the OpenAI API in Jupyter Lab with the ir kernel, with having only the rgpt3 library installed in this Notebook.</p>
<p>The API successfully performs a test code completion.  And it does not matter whether the API is making a single or multiple API request, both return the same warning.</p>
<p>The following results when using 3 queries:</p>
<blockquote>
<p>[1] &quot;Request: 1/3&quot; To avoid an <code>invalid_request_error</code>, <code>best_of</code> was
set to equal <code>n</code></p>
<p>[1] &quot;Request: 2/3&quot; To avoid an <code>invalid_request_error</code>, <code>best_of</code> was
set to equal <code>n</code></p>
<p>[1] &quot;Request: 3/3&quot; To avoid an <code>invalid_request_error</code>, <code>best_of</code> was
set to equal <code>n</code></p>
</blockquote>
<p>After performing multiple unsuccessful web searches - including a search at Stack Overflow for information about these warnings, I found there exists almost no information about this warning anywhere. It's probably too early in the process because the OpenAI API is relatively new to most people.</p>
<p>Therefore, it was decided to post both the question and the answer regarding this warning because otherwise finding such information is very difficult and time consuming.  And for those users who are boldly going where few have gone before, errors and warning messages do not inspire confidence.</p>
","2023-01-21 22:01:29","","2023-01-21 23:24:09","2023-01-21 23:50:57","<jupyter-lab><openai-api><gpt-2><gpt-3>","1","0","-1","245","","","","","","",""
"75248089","1","20372902","","How to work with JSON lines GPT-2 database?","<p>I downloaded all files. And all of them are just a randomly answers in JSON format. So, I want to train my own tensorflow.js model using this database! But, I don't have a question database here. So, what I need to do? I want to train my model to have an offline version of GPT-2, because I didn't find a already pre-trained model of it!</p>
<p>And yes, I want use JavaScript in Tensorflow.JS library. So for note, here's ones of the files that I was downloaded using the download_dataset.py script: xl-1542M-k40.valid.jsonl, xl 1542M.test.jsonl, xl-1542M.train.jsonl, xl-1542M.valid.jsonl</p>
<p>I was using this repo: <a href=""https://github.com/openai/gpt-2-output-dataset"" rel=""nofollow noreferrer"">https://github.com/openai/gpt-2-output-dataset</a></p>
","2023-01-26 15:15:34","","2023-01-26 22:04:56","2023-01-26 22:04:56","<javascript><artificial-intelligence><tensorflow.js><gpt-2>","0","0","0","128","","","","","","",""
"75067851","1","13298551","","How to fix Python pip install openai error: subprocess-exited-with-error","<p>I'm trying to install OpenAI with Python 3.11, Windows OS, pip fully upgraded, and I got this error.</p>
<p>Here is the full error message:</p>
<pre><code>Collecting openai
  Using cached openai-0.26.0.tar.gz (54 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... error
  error: subprocess-exited-with-error

  × Getting requirements to build wheel did not run successfully.
  │ exit code: 1
  ╰─&gt; [21 lines of output]
      Traceback (most recent call last):
        File &quot;C:\Users\vocal\AppData\Local\Programs\Python\Python311\Lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&quot;, line 351, in &lt;module&gt;
          main()
        File &quot;C:\Users\vocal\AppData\Local\Programs\Python\Python311\Lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&quot;, line 333, in main
          json_out['return_val'] = hook(**hook_input['kwargs'])
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File &quot;C:\Users\vocal\AppData\Local\Programs\Python\Python311\Lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py&quot;, line 118, in get_requires_for_build_wheel
          return hook(config_settings)
                 ^^^^^^^^^^^^^^^^^^^^^
        File &quot;C:\Users\vocal\AppData\Local\Temp\pip-build-env-lr3fjsgg\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 338, in get_requires_for_build_wheel
          return self._get_build_requires(config_settings, requirements=['wheel'])
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File &quot;C:\Users\vocal\AppData\Local\Temp\pip-build-env-lr3fjsgg\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 320, in _get_build_requires
          self.run_setup()
        File &quot;C:\Users\vocal\AppData\Local\Temp\pip-build-env-lr3fjsgg\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 485, in run_setup
          self).run_setup(setup_script=setup_script)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File &quot;C:\Users\vocal\AppData\Local\Temp\pip-build-env-lr3fjsgg\overlay\Lib\site-packages\setuptools\build_meta.py&quot;, line 335, in run_setup
          exec(code, locals())
        File &quot;&lt;string&gt;&quot;, line 13, in &lt;module&gt;
      UnicodeDecodeError: 'cp949' codec can't decode byte 0xe2 in position 1031: illegal multibyte sequence
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

× Getting requirements to build wheel did not run successfully.
│ exit code: 1
╰─&gt; See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.

</code></pre>
<p>I have no idea how to solve this error. Can anybody give me a hint?</p>
<pre><code>UnicodeDecodeError: 'cp949' codec can't decode byte 0xe2 in position 1031: illegal multibyte sequence
</code></pre>
<p>Because of that message, I tried <a href=""https://stackoverflow.com/questions/10487563/unicode-error-handling-with-python-3s-readlines"">this</a> solution and it didn't work.</p>
","2023-01-10 09:23:45","","2023-01-19 17:17:20","2023-03-09 19:20:15","<python><python-3.x><windows><openai-api><gpt-3>","2","0","3","6955","","","","","","",""
"75106599","1","18805643","","OpenAI GPT-3 API: Why am I getting different completions on Playground vs. the API?","<p>I'm trying to use the Ada language processor of OpenAi to summarize a piece of text.
When I try to use their playground, the function works and I get a summarization that makes sense and can be used by humans.</p>
<p><img src=""https://i.stack.imgur.com/05jsg.png"" alt=""OpenAI Playground"" /></p>
<p>This is the cURL from the playground:</p>
<pre><code>curl https://api.openai.com/v1/completions \
  -H &quot;Content-Type: application/json&quot; \
  -H &quot;Authorization: Bearer $OPENAI_API_KEY&quot; \
  -d '{
  &quot;model&quot;: &quot;text-ada-001&quot;,
  &quot;prompt&quot;: &quot;Please write a one paragraph professional synopsis:\n\nSome text&quot;,
  &quot;temperature&quot;: 0,
  &quot;max_tokens&quot;: 60,
  &quot;top_p&quot;: 1,
  &quot;frequency_penalty&quot;: 0,
  &quot;presence_penalty&quot;: 0
}'
</code></pre>
<p>When I take this cURL and transform it to PHP code, it stops working, or better said it works but it returns complete nonsense, nothing similar to the results from the playground.</p>
<p>PHP code:</p>
<pre><code>$ch = curl_init();

    curl_setopt($ch, CURLOPT_URL, 'https://api.openai.com/v1/completions');
    curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);
    curl_setopt($ch, CURLOPT_POST, 1);
    $postFields = '{
            &quot;model&quot;: &quot;text-ada-001&quot;,
            &quot;prompt&quot;: &quot;Please write a one paragraph professional synopsis: ' . $text . '&quot;,
            &quot;temperature&quot;: 0,
            &quot;max_tokens&quot;: 500,
            &quot;top_p&quot;: 1,
            &quot;frequency_penalty&quot;: 0,
            &quot;presence_penalty&quot;: 0
        }';
    curl_setopt($ch, CURLOPT_POSTFIELDS, $postFields);

    $headers = array();
    $headers[] = 'Content-Type: application/json';
    $headers[] = 'Authorization: Bearer ' . $api_key;
    curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);

    $result = curl_exec($ch);
    if (curl_errno($ch)) {
        echo 'Error:' . curl_error($ch);
    }
    curl_close($ch);
    return $result;
</code></pre>
<p>Now, I tried to use both a json code like this, and to write a PHP array and convert it to json, same result. I have also tried to use a <a href=""https://github.com/orhanerday/open-ai"" rel=""nofollow noreferrer"">library</a> but it also returned the same nonsense as before. I'm saying nonsense, because the text that it returns is not something that can be read and said, 'Hey, this is a proffesional synopsis'. I'm going to give an example of a sentence that I got in one of the iterations:</p>
<p>'It's not pretty and no I thought to myself, oh look IT'S NOT THAT REPUBLICAN kids would kno one of these things. OH IT'S A RESTRICTIOUS SCHOOL'.</p>
<p>I can assure you, there are no mentions of republicans or kids in the text that I'm processing.</p>
<p>My question is, am I doing something wrong? Does OpenAi work differently on their playground and in code?</p>
","2023-01-13 08:28:15","","2023-03-13 13:44:45","2023-03-13 13:44:45","<php><artificial-intelligence><openai-api><gpt-3>","1","0","2","1393","","","","","","",""
"75254337","1","19968983","","App framework for linking HuggingFace transformers to, using a mobile device (Android)","<p><strong>I'm looking to create an app , using a mobile device (Android), that will accept my HuggingFace api to form a simple ChatBot. The question is simply: what would be the MOST EFFICIENT METHOD to create the base application?</strong></p>
<p>In the past, i have used Termux to write/debug/compile the app on my android phone. However, this app needs to be put together as cleanly as possible, to allow for seamless upgrading, when I get around to embedding it into one of my websites.</p>
","2023-01-27 05:19:10","","","2023-01-27 05:19:10","<android><api><termux><huggingface><gpt-3>","0","0","0","90","","","","","","",""
"75302104","1","10897106","","How to add encoder's last hidden state to GPT2 as encoder-decoder attention?","<p>I have a BERT-based encoder model (<em>encoder</em>) and I want to input the last hidden state output of this to a GPT2-based model (<em>decoder</em>). There are no options in <code>transformers.GPT2Config</code> to use encoder's last hidden layer as input to GPT2. How do I achieve this?</p>
<p>I want something like this:</p>
<pre><code>inputs = input_ids, token_type_ids, labels, attention_mask

encoder           = RobertaForMaskedLM(config=encoder_config)
encoder_output    = encoder(**inputs)
last_hidden_layer = encoder_output.hidden_states[-1]

decoder           = GPT2LMHeadModel(config=decoder_config)
decoder_output    = decoder(**inputs, last_hidden_layer) 
</code></pre>
<p>where the <code>last_hidden_layer</code> is used as encoder-decoder attention to each transformer unit in GPT2.</p>
","2023-01-31 19:15:00","","","2023-01-31 19:15:00","<pytorch><nlp><huggingface-transformers><bert-language-model><gpt-2>","0","0","0","184","","","","","","",""
"75344458","1","21140352","","OpenAI GPT API pre-tokenizing?","<p>I am trying to make a &quot;personal assistant&quot; chatbot (using GPT AI API) that can answer questions about myself when others ask it things. In order to do so, I have to give it a lot of information about myself, which I am currently doing in the prompt.</p>
<p>Example: <a href=""https://i.stack.imgur.com/EiOWo.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/EiOWo.png"" alt=""Screenshot of my prompt"" /></a></p>
<p>This means that every time someone asks a question, the prompt includes all of the information about me, which means that it gets tokenized every single time a question is asked. Is there a way to &quot;pre-tokenize&quot; the information about myself or store it in some other way? I ask this because the information about myself is what is costing me the most, as it sucks up a lot of tokens.</p>
<p>Thanks</p>
<p>I have tried looking online to no avail.</p>
","2023-02-04 10:28:49","","","2023-04-12 15:51:44","<artificial-intelligence><token><tokenize><gpt-3>","1","0","0","306","","","","","","",""
"75344868","1","19813924","","Saving data with the hive package","<p>How can I save and retrieve this map using hive?:</p>
<pre><code>Map&lt;DateTime, List&lt;PressureModel&gt;&gt; pressures = {
DateTime(2023, 1, 11): [
  PressureModel(
      pressure: '144 / 64 - 74', date: DateTime(2023, 1, 11, 8, 45)),
  PressureModel(
      pressure: '157 / 64 - 77', date: DateTime(2023, 1, 11, 20, 8)),
],
DateTime(2023, 1, 12): [
  PressureModel(
      pressure: '146 / 71 - 92', date: DateTime(2023, 1, 12, 7, 46))
],
DateTime(2023, 1, 14): [
  PressureModel(
      pressure: '132 / 68 - 74', date: DateTime(2023, 1, 14, 10, 45)),
  PressureModel(
      pressure: '141 / 53 - 83', date: DateTime(2023, 1, 14, 21, 14)),
],
DateTime(2023, 1, 15): [
  PressureModel(
      pressure: '124 / 52 - 62', date: DateTime(2023, 1, 15, 10, 26)),
  PressureModel(
      pressure: '134 / 62 - 91', date: DateTime(2023, 1, 15, 18, 57)),
  PressureModel(
      pressure: '153 / 68 - 85', date: DateTime(2023, 1, 15, 21, 18)),
]};
</code></pre>
<p>This is what the PressureModel class looks like:</p>
<pre><code>class PressureModel {
  String pressure;
  String action;
  DateTime date;

  PressureModel({required this.pressure, this.action = '', required this.date});
}
</code></pre>
<p><strong>In case anyone is wondering, I did it through SharedPreferences, and ChatGTP helped me do it:</strong></p>
<p>Here is the code to save the data:</p>
<pre><code>Future&lt;void&gt; savePressures(
  Map&lt;DateTime, List&lt;PressureModel&gt;&gt; pressures) async {
final prefs = await SharedPreferences.getInstance();
final encodedPressures = json.encode(
    pressures.map((key, value) =&gt; MapEntry(key.toIso8601String(), value)));
await prefs.setString('pressures', encodedPressures);
}
</code></pre>
<p>Here is the code to get the data:</p>
<pre><code>Future&lt;Map&lt;DateTime, List&lt;PressureModel&gt;&gt;&gt; loadPressures() async {
final prefs = await SharedPreferences.getInstance();
final encodedPressures = prefs.getString('pressures_list');
if (encodedPressures == null) {
  return {};
}
final decodedPressures =
    json.decode(encodedPressures) as Map&lt;String, dynamic&gt;;
final pressures = &lt;DateTime, List&lt;PressureModel&gt;&gt;{};
decodedPressures.forEach((key, value) {
  pressures[DateTime.parse(key)] = (value as List&lt;dynamic&gt;)
      .map((e) =&gt; PressureModel.fromJson(e))
      .toList();
});
return pressures;
}
</code></pre>
","2023-02-04 11:44:41","","2023-02-05 13:28:16","2023-02-05 13:28:16","<flutter><dart><hive><gpt-3>","0","0","0","70","","","","","","",""
"75624308","1","19040716","75626340","OpenAI GPT-3 API errors: 'text' does not exist TS(2339) & 'prompt' does not exist on type 'CreateChatCompletion' TS(2345)","<pre><code>import openai from &quot;./zggpt&quot;;

const query = async (prompt:string,  chatId:string, model:string) =&gt; {
    const res= await openai
    .createChatCompletion({
        model,
        prompt,
        temperature: 0.9,
        
        top_p:1,
       
        max_tokens:1000,
        frequency_penalty:0,
        presence_penalty:0,
    })
    .then((res) =&gt; res.data.choices[0].text)
    .catch((err)=&gt;
    `ZG was unable to find an answer for that!
     (Error: ${err.message})`
     );
     return res;
};

export default query;
</code></pre>
<p>Property 'text' does not exist on type 'CreateChatCompletionResponseChoicesInner'.ts(2339)</p>
<p>Argument of type '{ model: string; prompt: string; temperature: number; top_p: number; max_tokens: number; frequency_penalty: number; presence_penalty: number; }' is not assignable to parameter of type 'CreateChatCompletionRequest'.
Object literal may only specify known properties, and 'prompt' does not exist in type 'CreateChatCompletionRequest'.ts(2345)</p>
<p>even though I do everything as in the video, I get these errors.</p>
<p>i'm a beginner in coding, so I'm trying to make applications based on videos to learn.</p>
<p>Thanks</p>
<p>the application responds without returning an error.
<a href=""https://i.stack.imgur.com/LmkT3.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://www.youtube.com/watch?v=V6Hq_EX2LLM&amp;t=6293s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=V6Hq_EX2LLM&amp;t=6293s</a></p>
","2023-03-03 07:34:41","","2023-03-03 11:23:35","2023-05-15 06:29:57","<next.js><openai-api><gpt-3>","2","0","0","497","","2","10347145","<h3>Problem</h3>
<p>You watched a tutorial which used the <a href=""https://platform.openai.com/docs/api-reference/completions/create"" rel=""nofollow noreferrer"">GPT-3 Completions endpoint</a> (you need to provide the prompt and parameters to get a completion). In this case, this is the function which generates a completion:</p>
<pre><code>openai.createCompletion()
</code></pre>
<p>Whereas, you used the code from the tutorial, <strong>but used the <a href=""https://platform.openai.com/docs/api-reference/chat/create"" rel=""nofollow noreferrer"">ChatGPT Completions endpoint</a></strong> (you need to provide the chat message to get a completion). In this case, this is the function which generates a completion:</p>
<pre><code>openai.createChatCompletion()
</code></pre>
<h3>Solution</h3>
<p>So, change this...</p>
<pre><code>openai.createChatCompletion()
</code></pre>
<p>...to this.</p>
<pre><code>openai.createCompletion()
</code></pre>
<p>Both errors will disappear.</p>
<h3>My advice</h3>
<p>However, you want to achieve a chat-like bot using a GPT-3 model. At the time the tutorial was recorded, this was the only way to do it. Since 1 March 2023, the <code>gpt-3.5-turbo</code> model is available. I strongly suggest you to use it. See the official <a href=""https://platform.openai.com/docs/guides/chat"" rel=""nofollow noreferrer"">OpenAI documentation</a>.</p>
","2023-03-03 11:10:57","0","1"
"75718913","1","2278116","75719777","OpenAI GPT-3 API: Why do I get different, non-related random responses to the same question every time?","<p>I am using the “text-davinci-003” model and I copied the code form the OpenAI playground, but the bot keeps giving me random response to a simple “Hello” everytime.</p>
<p>This is the code I am using :</p>
<pre><code>response: dict = openai.Completion.create(model=&quot;text-davinci-003&quot;,
                                                    prompt=prompt,
                                                    temperature=0.9,
                                                    max_tokens=150,
                                                    top_p=1,
                                                    frequency_penalty=0,
                                                    presence_penalty=0.6,
                                                    stop=[&quot; Human:&quot;, &quot; AI:&quot;])
        choices: dict = response.get('choices')[0]
        text = choices.get('text')
        print(text)
</code></pre>
<p>The response to simple “hello” chat 3 different times :</p>
<ol>
<li><p>the first time it gave me a hello world program for Java</p>
</li>
<li><p>second time it answered correctly - ‘Hi there! How can I help you today?’</p>
</li>
<li><p>third time:</p>
<pre><code>  def my_method
        puts &quot;hello&quot;
     end
   end
 end

# To invoke this method we would call:
MyModule::MyClass.my_method
</code></pre>
</li>
</ol>
<p>I just dont get it, as using the same simple ‘hello’ prompt in the OpenAI's playground gives me accurate response eveytime - 'Hi there! How can I help you today?'</p>
","2023-03-13 07:12:36","","2023-03-13 14:55:26","2023-03-13 14:55:26","<python><chatbot><openai-api><gpt-3>","1","0","0","700","","2","10347145","<p>As stated in the official <a href=""https://platform.openai.com/docs/guides/completion/prompt-design"" rel=""nofollow noreferrer"">OpenAI documentation</a>:</p>
<blockquote>
<p>The temperature and top_p settings control how deterministic the model
is in generating a response. <strong>If you're asking it for a response where
there's only one right answer, then you'd want to set these lower.</strong> If
you're looking for more diverse responses, then you might want to set
them higher. The number one mistake people use with these settings is
assuming that they're &quot;cleverness&quot; or &quot;creativity&quot; controls.</p>
</blockquote>
<p>Change this...</p>
<pre><code>temperature = 0.9
</code></pre>
<p>...to this.</p>
<pre><code>temperature = 0
</code></pre>
","2023-03-13 09:02:18","0","1"
"75773786","1","9680491","75774187","Can't access gpt-4 model via python API although gpt-3.5 works","<p>I'm able to use the gpt-3.5-turbo-0301 model to access the ChatGPT API, but not any of the gpt-4 models. Here is the code I am using to test this (it excludes my openai API key). The code runs as written, but when I replace &quot;gpt-3.5-turbo-0301&quot; with &quot;gpt-4&quot;, &quot;gpt-4-0314&quot;, or &quot;gpt-4-32k-0314&quot;, it gives me an error &quot;openai.error.InvalidRequestError: The model: <code>gpt-4</code> does not exist&quot;. I have a ChatGPT+ subscription, am using my own API key, and can use gpt-4 successfully via OpenAI's own interface.</p>
<p>It's the same error if I use gpt-4-0314 or gpt-4-32k-0314. I've seen a couple articles claiming this or similar code works using 'gpt-4' works as the model specification, and the code I pasted below is from one of them. Does anybody know if it's possible to access the gpt-4 model via Python + API, and if so, how do you do it?</p>
<pre><code>openai_key = &quot;sk...&quot;
openai.api_key = openai_key
system_intel = &quot;You are GPT-4, answer my questions as if you were an expert in the field.&quot;
prompt = &quot;Write a blog on how to use GPT-4 with python in a jupyter notebook&quot;
# Function that calls the GPT-4 API

def ask_GPT4(system_intel, prompt): 
    result = openai.ChatCompletion.create(model=&quot;gpt-3.5-turbo-0301&quot;,
                                 messages=[{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_intel},
                                           {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}])
    print(result['choices'][0]['message']['content'])

# Call the function above
ask_GPT4(system_intel, prompt)
</code></pre>
","2023-03-18 03:59:30","","2023-03-18 19:59:51","2023-05-31 15:17:21","<python><openai-api><gpt-4>","3","4","12","11782","","2","14995807","<p>Currently the GPT 4 API is restricted, Even to users with a Chat GPT <strong>+</strong> subscription.</p>
<p>You may need to join the <a href=""https://openai.com/waitlist/gpt-4-api"" rel=""noreferrer"">Waitlist</a> for the API.</p>
","2023-03-18 06:25:34","2","15"
"75790862","1","19966847","75791101","OpenAI GPT-3 API: Why do I get a response that makes no sense in relation to the question?","<p>When I ask a question in parameters of the request, the response has no sentence, and i get other questions in the response. I tried with every &quot;temperature&quot; and the response is never the same that I could get on chatGPT-3. I also tried with every models like davinci-codex, davinci, curie, babbage, etc. Do you have any idea of why ?</p>
<p>Here are the parameters  :</p>
<pre><code>{
    &quot;prompt&quot;: &quot;What's the capital of USA ?&quot;,
    &quot;max_tokens&quot;: 100,
    &quot;n&quot;: 1,
    &quot;stop&quot;: null,
    &quot;temperature&quot;: 0
}
</code></pre>
<p>this is the API response :</p>
<pre><code>{
    &quot;id&quot;: &quot;cmpl-6wA6d1bcNyju7cbqlJKRToOoi8TS2&quot;,
    &quot;object&quot;: &quot;text_completion&quot;,
    &quot;created&quot;: 1679319891,
    &quot;model&quot;: &quot;davinci&quot;,
    &quot;choices&quot;: [
        {
            &quot;text&quot;: &quot;\n\nA: Washington D.C.\n\nQ: What's the capital of Canada ?\n\nA: Ottawa\n\nQ: What's the capital of Australia ?\n\nA: Canberra\n\nQ: What's the capital of England ?\n\nA: London\n\nQ: What's the capital of France ?\n\nA: Paris\n\nQ: What's the capital of Germany ?\n\nA: Berlin\n\nQ: What's the capital of Italy ?&quot;,
            &quot;index&quot;: 0,
            &quot;logprobs&quot;: null,
            &quot;finish_reason&quot;: &quot;length&quot;
        }
    ],
    &quot;usage&quot;: {
        &quot;prompt_tokens&quot;: 7,
        &quot;completion_tokens&quot;: 100,
        &quot;total_tokens&quot;: 107
    }
}
</code></pre>
<p>With 0.5 temperature, the reponse is :</p>
<pre><code>{
    &quot;id&quot;: &quot;cmpl-6wA3ZuuAfgrE8ox6dMY2M9tqgOxar&quot;,
    &quot;object&quot;: &quot;text_completion&quot;,
    &quot;created&quot;: 1679319701,
    &quot;model&quot;: &quot;davinci&quot;,
    &quot;choices&quot;: [
        {
            &quot;text&quot;: &quot;\n\nA: Washington D.C.\n\nQ: What's the capital of France ?\n\nA: Paris.\n\nQ: What's the capital of Germany ?\n\nA: Berlin.\n\nQ: What's the capital of China ?\n\nA: Beijing.\n\nQ: What's the capital of Japan ?\n\nA: Tokyo.\n\nQ: What's the capital of Russia ?\n\nA: Moscow.\n\nQ: What's&quot;,
            &quot;index&quot;: 0,
            &quot;logprobs&quot;: null,
            &quot;finish_reason&quot;: &quot;length&quot;
        }
    ],
    &quot;usage&quot;: {
        &quot;prompt_tokens&quot;: 7,
        &quot;completion_tokens&quot;: 100,
        &quot;total_tokens&quot;: 107
    }
}
</code></pre>
<p>And with a more difficult question this is what i get :</p>
<p>Questions :</p>
<pre><code>{
    &quot;prompt&quot;: &quot;What job could I do if I like computers and video games?&quot;,
    &quot;max_tokens&quot;: 100,
    &quot;n&quot;: 1,
    &quot;stop&quot;: null,
    &quot;temperature&quot;: 0
}
</code></pre>
<p>Answer:</p>
<pre><code>{
    &quot;id&quot;: &quot;cmpl-6wAACQ91vbOohAwMbQqvJyOaznU6i&quot;,
    &quot;object&quot;: &quot;text_completion&quot;,
    &quot;created&quot;: 1679320112,
    &quot;model&quot;: &quot;davinci&quot;,
    &quot;choices&quot;: [
        {
            &quot;text&quot;: &quot;\n\nWhat job could I do if I like to work with my hands?\n\nWhat job could I do if I like to work with animals?\n\nWhat job could I do if I like to work with plants?\n\nWhat job could I do if I like to work with people?\n\nWhat job could I do if I like to work with numbers?\n\nWhat job could I do if I like to work with words?\n\nWhat job could I do if I&quot;,
            &quot;index&quot;: 0,
            &quot;logprobs&quot;: null,
            &quot;finish_reason&quot;: &quot;length&quot;
        }
    ],
    &quot;usage&quot;: {
        &quot;prompt_tokens&quot;: 13,
        &quot;completion_tokens&quot;: 100,
        &quot;total_tokens&quot;: 113
    }
}
</code></pre>
","2023-03-20 13:27:50","","2023-03-21 09:20:20","2023-03-21 09:20:20","<api><openai-api><gpt-3>","1","3","-1","847","","2","10347145","<p>You're using an old GPT-3 model (i.e., <code>davinci</code>). <strong>Use a newer GPT-3 model.</strong></p>
<p>For example, use the model <code>text-davinci-003</code> instead of <code>davinci</code>.</p>
<p>As stated in the official <a href=""https://help.openai.com/en/articles/6643408-how-do-davinci-and-text-davinci-003-differ"" rel=""nofollow noreferrer"">OpenAI article</a>:</p>
<blockquote>
<h3>How do <code>davinci</code> and <code>text-davinci-003</code> differ?</h3>
<p>While both <code>davinci</code> and <code>text-davinci-003</code> are powerful models, they
differ in a few key ways.</p>
<p><strong><code>text-davinci-003</code> is the newer and more capable model</strong>, designed
specifically for instruction-following tasks. This enables it to
respond concisely and more accurately - even in zero-shot scenarios,
i.e. without the need for any examples given in the prompt.</p>
<p>Additionally, <code>text-davinci-003</code> supports a longer context window (max
prompt+completion length) than davinci - 4097 tokens compared to
davinci's 2049.</p>
<p>Finally, <code>text-davinci-003</code> was trained on a more recent dataset,
containing data up to June 2021. These updates, along with its support
for Inserting text, make <code>text-davinci-003</code> a particularly versatile and
powerful model we recommend for most use-cases.</p>
</blockquote>
","2023-03-20 13:50:18","0","1"
"76048714","1","1133919","76052203","Finetuning a LM vs prompt-engineering an LLM","<p>Is it possible to finetune a much smaller language model like Roberta on say, a customer service dataset and get results as good as one might get with prompting GPT-4 with parts of the dataset?</p>
<p>Can a fine-tuned Roberta model learn to follow instructions in a conversational manner at least for a small domain like this?</p>
<p>Is there any paper or article that explores this issue empirically that I can check out?</p>
","2023-04-18 20:15:18","","","2023-04-20 14:56:03","<language-model><roberta-language-model><roberta><gpt-4><large-language-model>","1","0","2","1097","","2","1133919","<p>I found a medium piece which goes a long way in clarifying this <a href=""https://medium.com/@lucalila/can-prompt-engineering-surpass-fine-tuning-performance-with-pre-trained-large-language-models-eefe107fb60e"" rel=""noreferrer"">here</a>.</p>
<p>Quoting from the conclusion in the above,</p>
<blockquote>
<p>In the low data domain, prompting shows superior performance to the
respective fine-tuning method. To beat the SOTA benchmarks in
fine-tuning, leveraging large frozen language models in combination
with tuning a soft prompt seems to be the way forward.</p>
</blockquote>
<p>It appears prompting an LLM <em>may</em> outperform fine tuning a smaller model on domain-specific tasks if the training data is small and vice versa if otherwise.</p>
<p>Additionally, in my own personal anecdotal experience with ChatGPT, Bard, Bing, Vicuna-3b, Dolly-v2-12b and Illama-13b, it appears models of the size of ChatGPT, Bard and Bing have learned to mimic human understanding of language well enough to be able to extract meaningful answers from context provided at inference time. It seems to me the smaller models do not have that <em>mimicry-mastery</em> and might not perform as well with in-context learning at inference time. They might also be too large to be well suited for fine-tuning in a very limited domain. My hunch is that for very limited domains, if one is going the fine-tuning route, fine-tuning on much smaller models like BERT or Roberta (or smaller variants of GPT-2 or GPT-J, for generative tasks) rather than on these medium-sized models might be the more prudent approach resource-wise.</p>
<p>Another approach to fine tuning the smaller models on domain data could be to use more carefully and rigorously crafted prompts with the medium-sized models. This could be a viable alternative to using the APIs provided by the owners of the very large proprietary models.</p>
","2023-04-19 08:19:12","0","5"
"76073607","1","6294538","76076750","How to change goals after setting up Auto-GPT","<p>I have set up Auto-GPT in my system with the goals below.</p>
<p>1 Grow my linkedin account
2 Look for new innovative linkedin post ideas for AI technology
3 Prepare post on that idea
4 Write that post in the file
5 Shutdown</p>
<p>Auto GPT is not producing the results I expected, And now I want to change my goals for different results. How do I modify the goals I set earlier? Or is there any way to improve the result it produces?</p>
<p>I have tried restarting Auto GPT and terminal as well.</p>
","2023-04-21 13:25:36","","","2023-04-21 21:14:33","<openai-api><gpt-4><autogpt>","1","0","1","1269","","2","3370807","<p>Edit your ai_settings.yaml to change goals.</p>
","2023-04-21 21:14:33","1","4"
"76403814","1","22021301","76411286","What is the best approach to creating a question generation model using GPT and Bert architectures?","<p>I want to make a question generation model from questions as well as context. Should I make use of GPT based models or Bert Based architectures.</p>
<p>GPT is able to perform the tasks but sometimes returns with vague questions that were not in the context itself. When I made use of WizardLM(7B), I was able to get generalized questions from the context itself which sounded more natural and were nearly to the point when kept within limit of 3.</p>
","2023-06-05 05:58:55","","2023-06-05 06:14:45","2023-06-06 03:32:51","<python><open-source><huggingface-transformers><huggingface><gpt-3>","1","1","0","49","","2","4882300","<p>When dealing with text generation, it is more straightforward to work with Transformer decoder models such as GPT-* models. Although BERT-like models are also capable of text generation, it is a quite convoluted process and not something that follows naturally from the tasks for which these models have been pretrained.</p>
<p>I assume you are comparing GPT-2 and WizardLM (7B). The performance of the model on this task is expected to improve as you scale up the number of parameters by using larger models. I would recommend you to try LLMs such as <a href=""https://github.com/tloen/alpaca-lora"" rel=""nofollow noreferrer"">Alpaca-LoRA</a>, <a href=""https://huggingface.co/databricks/dolly-v2-12b"" rel=""nofollow noreferrer"">Dolly</a> or <a href=""https://huggingface.co/EleutherAI/gpt-j-6b"" rel=""nofollow noreferrer"">GPT-J</a> ( <a href=""https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/GPT-J-6B/Inference_with_GPT_J_6B.ipynb"" rel=""nofollow noreferrer"">see here</a> how to run GPT-J on Colab Pro ).</p>
","2023-06-06 03:32:51","1","1"
"75257323","1","4127155","","Fine Tuning an OpenAI GPT-3 model on a collection of documents","<p>According to the documentation <a href=""https://beta.openai.com/docs/guides/fine-tuning"" rel=""noreferrer"">https://beta.openai.com/docs/guides/fine-tuning</a> the training data to fine tune an OpenAI GPT3 model should be structured as follows:</p>
<pre class=""lang-json prettyprint-override""><code>{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}
{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}
{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}
</code></pre>
<p>I have a collection of documents from an internal knowledge base that have been preprocessed into a JSONL file in a format like this:</p>
<pre class=""lang-json prettyprint-override""><code>{  &quot;id&quot;: 0,  &quot;name&quot;: &quot;Article Name&quot;,  &quot;description&quot;: &quot;Article Description&quot;,  &quot;created_at&quot;: &quot;timestamp&quot;,  &quot;updated_at&quot;: &quot;timestamp&quot;,  &quot;answer&quot;: {    &quot;body_txt&quot;: &quot;An internal knowledge base article with body text&quot;,  },  &quot;author&quot;: {    &quot;name&quot;: &quot;First Last&quot;},  &quot;keywords&quot;: [],  &quot;url&quot;: &quot;A URL to internal knowledge base&quot;}
{  &quot;id&quot;: 1,  &quot;name&quot;: &quot;Article Name&quot;,  &quot;description&quot;: &quot;Article Description&quot;,  &quot;created_at&quot;: &quot;timestamp&quot;,  &quot;updated_at&quot;: &quot;timestamp&quot;,  &quot;answer&quot;: {    &quot;body_txt&quot;: &quot;An internal knowledge base article with body text&quot;,  },  &quot;author&quot;: {    &quot;name&quot;: &quot;First Last&quot;},  &quot;keywords&quot;: [],  &quot;url&quot;: &quot;A URL to internal knowledge base&quot;}
{  &quot;id&quot;: 2,  &quot;name&quot;: &quot;Article Name&quot;,  &quot;description&quot;: &quot;Article Description&quot;,  &quot;created_at&quot;: &quot;timestamp&quot;,  &quot;updated_at&quot;: &quot;timestamp&quot;,  &quot;answer&quot;: {    &quot;body_txt&quot;: &quot;An internal knowledge base article with body text&quot;,  },  &quot;author&quot;: {    &quot;name&quot;: &quot;First Last&quot;},  &quot;keywords&quot;: [],  &quot;url&quot;: &quot;A URL to internal knowledge base&quot;}
</code></pre>
<p>The documentation then suggests that a model could then be fine tuned on these articles using the command <code>openai api fine_tunes.create -t &lt;TRAIN_FILE_ID_OR_PATH&gt; -m &lt;BASE_MODEL&gt;</code>.</p>
<p>Running this results in:</p>
<blockquote>
<p>Error: Expected file to have JSONL format with prompt/completion keys. Missing <code>prompt</code> key on line 1. (HTTP status code: 400)</p>
</blockquote>
<p>Which isn't unexpected given the documented file structure noted above. Indeed if I run <code>openai tools fine_tunes.prepare_data -f training-data.jsonl </code> then I am told:</p>
<blockquote>
<p>Your file contains 490 prompt-completion pairs
ERROR in necessary_column validator: <code>prompt</code> column/key is missing. Please make sure you name your columns/keys appropriately, then retry`</p>
</blockquote>
<p>Is this is right approach to trying to fine tune a GTP3 model on collections of documents, such that questions could later be asked about the content of them. What would one put in the <code>prompt</code> and <code>completion</code> fields in this case since I am not starting from a place where I have a collection of possible question and ideal answers.</p>
<p>Have I fundamentally misunderstood the mechanism used to fine tune a GTP3 model? It does make sense to me that GTP3 would need to be trained on possible questions and answers. However, given the base models are already trained and this process is more above providing additional datasets which aren't in the public domain so that questions can be asked about it I would have thought what I want to achieve is possible. As a working example, I can indeed go to <a href=""https://chat.openai.com/"" rel=""noreferrer"">https://chat.openai.com/</a> and ask a question about these documents as follows:</p>
<blockquote>
<p>Given the following document:</p>
</blockquote>
<blockquote>
<p>[Paste the text content of one of the documents]</p>
</blockquote>
<blockquote>
<p>Can you tell me XXX</p>
</blockquote>
<p>And indeed it often gets the answer right. What I'm now trying to do it fine tune the model on ~500 of these documents such that one doesn't have to paste whole single documents each time a question is to be asked and such that the model might even be able to consider content across all ~500 rather than just the single one that user provided.</p>
","2023-01-27 11:10:32","","2023-01-30 07:31:52","2023-03-25 08:57:48","<openai-api><gpt-3>","2","0","7","4316","","","","","","",""
"75304632","1","16450589","","How to Use Edit images in OpenAi Kotlin Client","<p>I am using openAi client with android kotlin (implementation <code>com.aallam.openai:openai-client:2.1.3</code>).</p>
<p>Is the path wrong or is <a href=""https://github.com/Aallam/openai-kotlin"" rel=""nofollow noreferrer"">the library</a> missing?</p>
<pre class=""lang-kotlin prettyprint-override""><code>val imgURL = Uri.parse(&quot;android.resource://&quot; + packageName + &quot;/&quot; + R.drawable.face3)
try {
    val images = openAI.image(
        edit = ImageEditURL( // or 'ImageEditJSON'
            image = FilePath(imgURL.toString()), // &lt;-
            mask = FilePath(imgURL.toString()), // &lt;-
            prompt = &quot;a sunlit indoor lounge area with a pool containing a flamingo&quot;,
            n = 1,
            size = ImageSize.is1024x1024
        )
    );
} catch (e: Exception) {
    println(&quot;error is here:&quot;+e)
}
</code></pre>
<p>As can be seen, it wants a path from me, but it does not succeed even though I give the path.</p>
","2023-02-01 00:55:40","","2023-02-05 10:47:37","2023-02-07 13:01:54","<android><android-studio><kotlin><openai-api><gpt-3>","1","0","0","191","","","","","","",""
"75324242","1","972982","","GPT-J (6b): how to properly formulate autocomplete prompts","<p>I'm new to the AI playground and for this purpose I'm experimenting with the GPT-J (6b) model on an Amazon SageMaker notebook instance (g4dn.xlarge). So far, I've managed to register an endpoint and run the predictor but I'm sure I'm making the wrong questions or I haven't really understood how the model parameters work (which is probable).</p>
<p>This is my code:</p>
<pre><code># build the prompt
prompt = &quot;&quot;&quot;
language: es
match: comida
topic: hoteles en la playa todo incluido
output: ¿Sabes cuáles son los mejores Hoteles Todo Incluido de España? Cada vez son 
más los que se suman a la moda del Todo Incluido para disfrutar de unas perfectas y 
completas vacaciones en familia, en pareja o con amigos. Y es que con nuestra oferta 
hoteles Todo Incluido podrás vivir unos días de auténtico relax y una estancia mucho 
más completa, ya que suelen incluir desde el desayuno, la comida y la cena, hasta 
cualquier snack y bebidas en las diferentes instalaciones del hotel. ¿Qué se puede 
pedir más para relajarse durante una perfecta escapada? A continuación, te 
presentamos los mejores hoteles Todo Incluido de España al mejor precio.

language: es
match: comida
topic: hoteles en la playa todo incluido
output:
&quot;&quot;&quot;

# set the maximum token length
maximum_token_length = 25

# set the sampling temperature
sampling_temperature = 0.6

# build the predictor arguments
predictor_arguments = {
    &quot;inputs&quot;: prompt,
    &quot;parameters&quot;: {
        &quot;max_length&quot;: len(prompt) + maximum_token_length,
        &quot;temperature&quot;: sampling_temperature
    }
}

# execute the predictor with the prompt as input
predictor_output = predictor.predict(predictor_arguments)

# retrieve the text output
text_output = predictor_output[0][&quot;generated_text&quot;]

# print the text output
print(f&quot;text output: {text_output}&quot;)
</code></pre>
<p>My problem is I try to get a different response using the same parameters but I get nothing. It just repeats my inputs with an empty response so I'm definitely doing something wrong although the funny thing is I actually get a pretty understandable text output if I throw the same input with the same sampling temperature on the OpenAI playground (on text-davinci-003).</p>
<p>Can you give me a hint on what am I doing wrong? Oh, and another question is: How can I specify something like 'within the first 10 words' for a keyword match?</p>
","2023-02-02 13:59:41","","","2023-02-02 13:59:41","<jupyter-notebook><amazon-sagemaker><huggingface><gpt-3>","0","4","1","187","","","","","","",""
"67334513","1","15221534","","Is there an 'untrained' gpt model folder?","<p>Crazy question maybe: but I want to download the gpt-2 model framework but I want the weights to be initialized randomly. So as if the model still has to be finetuned on the reddit content (including json, vocab, meta &amp; index files etc). Is this possible?</p>
<p>Kind regards!</p>
","2021-04-30 13:09:03","","","2021-05-04 14:31:08","<huggingface-transformers><transformer-model><gpt-2>","1","0","1","229","","","","","","",""
"64635072","1","8713984","","huggingface transformers run_clm.py stops early","<p>I'm running run_clm.py to fine-tune gpt-2 form the huggingface library, following the language_modeling example:</p>
<pre><code>!python run_clm.py \
    --model_name_or_path gpt2 \
    --train_file train.txt \
    --validation_file test.txt \
    --do_train \
    --do_eval \
    --output_dir /tmp/test-clm
</code></pre>
<p>This is the output, the process seemed to be started but there was the <strong>^C</strong> appeared to stop the process:</p>
<pre><code>The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .
The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .
***** Running training *****
  Num examples = 2318
  Num Epochs = 3
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed &amp; accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 870
  0% 0/870 [00:00&lt;?, ?it/s]^C
</code></pre>
<p>Here's my environment info:</p>
<ul>
<li>transformers version: 3.4.0</li>
<li>Platform: Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic</li>
<li>Python version: 3.6.9</li>
<li>Tensorflow version: 1.14</li>
<li>Using GPU in script?: yes</li>
</ul>
<p>What would be the possible triggers of the early stopping?</p>
","2020-11-01 17:52:48","","2020-11-29 12:09:14","2020-11-29 12:09:14","<huggingface-transformers><gpt-2>","0","0","1","930","","","","","","",""
"70482540","1","8744937","","What happens if optimal training loss is too high","<p>I am training a Transformer. In many of my setups I obtain validation and training loss that look like this:</p>
<p><a href=""https://i.stack.imgur.com/VVtNm.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/VVtNm.png"" alt=""Training and validation loss for my dataset"" /></a></p>
<p>Then, I understand that I should stop training at around epoch 1. But then the training loss is very high. Is this a problem? Does the value of training loss actually mean anything?</p>
<p>Thanks</p>
","2021-12-25 20:22:20","","","2022-01-28 12:01:01","<pytorch><huggingface-transformers><transformer-model><gpt-2><trainingloss>","2","0","0","903","","","","","","",""
"70562362","1","17819675","","Getting MemoryError fine-tuning GPT2(355M) model with small datasets (3MB) through aitextgen","<p>I'm using aitextgen to fine-tune the 355M GPT-2 model using the train function. The datasets are small txt files consisting of lines like these (these are encoded texts for keyword-based text generation, hence the &quot;~^keywords~@&quot;):</p>
<pre><code>&lt;|startoftext|&gt;~^~@&quot;Yes, but one forgets that she is there--or anywhere. She seems as if she were an accident.&quot;&lt;|endoftext|&gt;
&lt;|startoftext|&gt;~^man~@&quot;Then jump out and unharness this horse. A man will come for it to- morrow.&quot;&lt;|endoftext|&gt;
&lt;|startoftext|&gt;~^mind 's~@&quot;It would upset the house terribly,&quot; said Nan; &quot;but I don't mind that. I'm with you, Patty. Let's do it.&quot;&lt;|endoftext|&gt;
&lt;|startoftext|&gt;~^Booth sure say wish~@&quot;I wish I were sure that I had,&quot; said Booth.&lt;|endoftext|&gt;
</code></pre>
<p>I use aitextgen's training function like this:</p>
<pre><code>    gpt2 = aitextgen(tf_gpt2 = &quot;355M&quot;, to_gpu= True)

    gpt2.train(dataset,
               line_by_line = True,
               batch_size= 1,
               num_steps = 50,
               save_every = 10,
               generate_every = 10,
               learning_rate = 1e-3,
               fp16 = False)
</code></pre>
<p>When I run this function, I get this output:</p>
<pre><code>0%|          | 0/10000 [00:00&lt;?, ?it/s]
Windows does not support multi-GPU training. Setting to 1 GPU.
C:\Users\Josh\anaconda3\envs\gpt2_env\lib\site-packages\pytorch_lightning\trainer\connectors\callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=False)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=False)`.
  rank_zero_deprecation(
C:\Users\Josh\anaconda3\envs\gpt2_env\lib\site-packages\pytorch_lightning\trainer\connectors\callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=20)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.
  rank_zero_deprecation(
C:\Users\Josh\anaconda3\envs\gpt2_env\lib\site-packages\pytorch_lightning\trainer\connectors\callback_connector.py:167: LightningDeprecationWarning: Setting `Trainer(weights_summary=None)` is deprecated in v1.5 and will be removed in v1.7. Please set `Trainer(enable_model_summary=False)` instead.
  rank_zero_deprecation(
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  0%|          | 0/50 [00:00&lt;?, ?it/s]

Traceback (most recent call last):
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\site-packages\transformers\modeling_utils.py&quot;, line 1364, in from_pretrained
    state_dict = torch.load(resolved_archive_file, map_location=&quot;cpu&quot;)
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\site-packages\torch\serialization.py&quot;, line 607, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\site-packages\torch\serialization.py&quot;, line 882, in _load
    result = unpickler.load()
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\site-packages\torch\serialization.py&quot;, line 857, in persistent_load
    load_tensor(data_type, size, key, _maybe_decode_ascii(location))
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\site-packages\torch\serialization.py&quot;, line 845, in load_tensor
    storage = zip_file.get_storage_from_record(name, size, dtype).storage()
RuntimeError: [enforce fail at ..\c10\core\CPUAllocator.cpp:76] data. DefaultCPUAllocator: not enough memory: you tried to allocate 205852672 bytes.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\multiprocessing\spawn.py&quot;, line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\multiprocessing\spawn.py&quot;, line 125, in _main
    prepare(preparation_data)
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\multiprocessing\spawn.py&quot;, line 236, in prepare
    _fixup_main_from_path(data['init_main_from_path'])
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\multiprocessing\spawn.py&quot;, line 287, in _fixup_main_from_path
    main_content = runpy.run_path(main_path,
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\runpy.py&quot;, line 265, in run_path
    return _run_module_code(code, init_globals, run_name,
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\runpy.py&quot;, line 97, in _run_module_code
    _run_code(code, mod_globals, init_globals,
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\runpy.py&quot;, line 87, in _run_code
    exec(code, run_globals)
  File &quot;C:\Users\Josh\Python Projects\FYP\src\[py file name].py&quot;, line 34, in &lt;module&gt;
    gpt2 = aitextgen(tf_gpt2 = &quot;355M&quot;, to_gpu= True)
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\site-packages\aitextgen\aitextgen.py&quot;, line 166, in __init__
    self.model = GPT2LMHeadModel.from_pretrained(model, config=config)
  File &quot;C:\Users\Josh\anaconda3\envs\gpt2_env\lib\site-packages\transformers\modeling_utils.py&quot;, line 1368, in from_pretrained
    if f.read().startswith(&quot;version&quot;):
MemoryError
</code></pre>
<p>I have tried many methods, including clearing the CUDA cache using <code>torch.cuda.empty_cache()</code>, splitting the files down to even smaller ones. None of them worked.</p>
<p>I'm running this on my local machine (RTX3070, 32GB RAM), I checked the task manager and the RAM usage barely hits 50%. Is there anything wrong with my code that's causing the memory errors?</p>
","2022-01-03 06:34:42","","2022-01-03 08:42:28","2022-01-03 08:42:28","<python><nlp><pytorch><gpt-2>","0","5","2","454","","","","","","",""
"71911771","1","1802425","","Can I create a fine-tuned model for OpenAI API Codex models?","<p>I'd like to translate user requests into tickets in some sort of structured data format, e.g. JSON. For example:</p>
<ul>
<li>User: I want to order two chairs and a desk with three drawers on the left side.</li>
<li>Output:</li>
</ul>
<pre><code>{
    &quot;type&quot;: &quot;furniture&quot;,
    &quot;items&quot;: [
        { &quot;type&quot;: &quot;desk&quot;, &quot;qty&quot;: 1, &quot;included_items&quot;: [{ &quot;type&quot;: &quot;drawer&quot;, &quot;qty&quot;: 3, &quot;position&quot;: &quot;left&quot; }] },
        { &quot;type&quot;: &quot;chair&quot;, &quot;qty&quot;: 2 }
    ]
}
</code></pre>
<p>It looks like GPT-3 itself is not very-well suited for this task, because output is not in the form of natural language, however Codex might be? But I can't find in OpenAI API docs how I can (if it's possible at all?) to create a custom / fine-tuned model for OpenAI API Codex models?</p>
","2022-04-18 12:22:26","","2023-03-02 00:43:14","2023-05-20 02:52:29","<json><openai-api><gpt-3><fine-tune>","3","0","4","2575","","","","","","",""
"75384220","1","19548998","","[InteractionAlreadyReplied]: The reply to this interaction has already been sent or deferred","<p>It says [InteractionAlreadyReplied]: The reply to this interaction has already been sent or deferred. But inreality i am using editReply</p>
<p>I am having issue in logging error, it works fine with try and else but it shows <code>[InteractionAlreadyReplied]: The reply to this interaction has already been sent or deferred</code> in catch (error). I even tried using followUp  but still doesnt work, it keeps giving the same error and shuts the whole bot down.</p>
<pre><code>module.exports = {
  data: new SlashCommandBuilder()
  .setName(&quot;chat-gpt&quot;)
  .setDescription(&quot;chat-gpt-3&quot;)
  .addStringOption(option =&gt;
    option.setName('prompt')
      .setDescription('Ask Anything')
      .setRequired(true)),

  async execute(interaction, client) {
    const prompt = interaction.options.getString('prompt');
    await interaction.deferReply();

    const configuration = new Configuration({
      apiKey: &quot;nvm&quot;,
    });
    const openai = new OpenAIApi(configuration);
    
    const response = await openai.createCompletion({
      model: 'text-davinci-003',
      prompt: prompt,
      max_tokens: 2048,
      temperature: 0.7,
      top_p: 1,
      frequency_penalty: 0.0,
      presence_penalty: 0.0,
    });

    let responseMessage = response.data.choices[0].text;

    /* Exceed 2000 */

    try {
      let responseMessage = response.data.choices[0].text;
      if (responseMessage.length &gt;= 2000) {
        const attachment = new AttachmentBuilder(Buffer.from(responseMessage, 'utf-8'), { name: 'chatgpt-response.txt' });
        const limitexceed = new EmbedBuilder()
        .setTitle(`Reached 2000 characters`)
        .setDescription(`Sorry, but you have already reached the limit of 2000 characters`)
        .setColor(`#FF6961`);
        await interaction.editReply({ embeds: [limitexceed], files: [attachment] });
        
      } else {
        const responded = new EmbedBuilder()
        .setTitle(`You said: ${prompt}`)
        .setDescription(`\`\`\`${response.data.choices[0].text}\`\`\``)
        .setColor(`#77DD77`);
  
        await interaction.editReply({ embeds: [responded] });   
      }

    } catch (error) {
      console.error(error);
      await interaction.followUp({ content: `error` });
    }

    return;
  },
};

</code></pre>
<p>i even tried using followUp or etc but the result is still same.</p>
","2023-02-08 10:10:52","","2023-02-08 10:37:32","2023-02-09 04:50:08","<node.js><discord><discord.js><gpt-3>","1","1","0","203","","","","","","",""
"75787638","1","18628287","","OpenAI GPT-3 API error: ""Request timed out""","<p>I keep get an error as below</p>
<p><code>Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)</code></p>
<p>when I run the code below</p>
<pre><code>def generate_gpt3_response(user_text, print_output=False):
    &quot;&quot;&quot;
    Query OpenAI GPT-3 for the specific key and get back a response
    :type user_text: str the user's text to query for
    :type print_output: boolean whether or not to print the raw output JSON
    &quot;&quot;&quot;
    time.sleep(5)
    completions = ai.Completion.create(
        engine='text-davinci-003',  # Determines the quality, speed, and cost.
        temperature=0.5,            # Level of creativity in the response
        prompt=user_text,           # What the user typed in
        max_tokens=150,             # Maximum tokens in the prompt AND response
        n=1,                        # The number of completions to generate
        stop=None,                  # An optional setting to control response generation
    )

    # Displaying the output can be helpful if things go wrong
    if print_output:
        print(completions)

    # Return the first choice's text
    return completions.choices[0].text
</code></pre>
<pre><code>df_test['GPT'] = df_test['Q20'].apply(lambda x: \
              generate_gpt3_response\
              (&quot;I am giving you the answer of respondents \
                in the format [Q20], \
                give me the Broader topics like customer service, technology, satisfaction\
                or the related high level topics in one word in the \
                format[Topic: your primary topic] for the text '{}' &quot;.format(x)))

# result
df_test['GPT'] = df_test['GPT'].apply(lambda x: (x.split(':')[1]).replace(']',''))
</code></pre>
<p>I tried modifiying the parameters, but the error still occurs.</p>
<p>Anyone experienced the same process?</p>
<p>Thanks in advance.</p>
","2023-03-20 07:43:47","","2023-03-21 17:45:25","2023-03-21 17:45:25","<python><openai-api><gpt-3>","1","1","3","3966","","","","","","",""
"75810740","1","12827843","","OpenAI GPT-4 API: What is the difference between gpt-4 and gpt-4-0314?","<p>Can anyone help explain the difference to me between <code>gpt-4</code> and <code>gpt-4-0314</code> (which appear on the OpenAI playground dropdown menu below) ? I have checked various search engines and its not clear from the OpenAI forum results.</p>
<p><a href=""https://i.stack.imgur.com/oQGvWl.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/oQGvWl.png"" alt=""enter image description here"" /></a></p>
<p>GPT-4-0314 could refer to a specific version or release of GPT-4 with a particular set of updates or improvements, perhaps released on March 14th. Anyone any experience of the differences, feedback would be welcome.</p>
","2023-03-22 09:57:45","","2023-03-22 17:26:49","2023-06-02 19:20:46","<openai-api><gpt-4>","1","1","8","4697","","","","","","",""
"75832120","1","245549","","Do nodes in List Index come with embedding vectors in LlamaIndex?","<p>One can run an embedding-based query on List Index (<a href=""https://gpt-index.readthedocs.io/en/latest/guides/index_guide.html#list-index"" rel=""nofollow noreferrer"">link</a>). For that nodes in the List Index should be supplied with embedding vectors. What is then the difference between the List Index and the <a href=""https://gpt-index.readthedocs.io/en/latest/guides/index_guide.html#vector-store-index"" rel=""nofollow noreferrer"">Vector Store Index</a>? I thought that the distinctive feature of the Vector Store Index is that it assigns an embedding vector to each nodes in the index but it looks like List Index does the same.</p>
","2023-03-24 09:48:25","","","2023-03-27 20:57:51","<embedding><openai-api><gpt-3><llama-index>","1","0","0","450","","","","","","",""
"75859074","1","6695297","","Getting RateLimitError while implementing openai GPT with Python","<p>I have started to implement openai gpt model in python. I have to send a single request in which I am getting RateLimitError.</p>
<p>My code looks like this</p>
<pre><code>import openai

key = '&lt;SECRET-KEY&gt;'
openai.api_key = key
model_engine = 'text-ada-001'
prompt = 'Hi, How are you today?'
completion = openai.Completion.create(engine=model_engine, prompt=prompt, max_token=2048, n=1, stop=None, temprature=0.5)
print(completion.choices)
</code></pre>
<p>This is what error I am getting</p>
<pre><code>openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.
</code></pre>
<p>So, How do I do development without getting this error? I have checked the doc they provide a free version with limitations but this is the initial stage I have sent only 5-6 requests in an hour.</p>
<p>Thanks advance for your help.</p>
","2023-03-27 18:14:46","","","2023-06-09 15:56:30","<python><python-3.x><openai-api><gpt-3>","2","5","3","2945","","","","","","",""
"75379690","1","8068222","","Not getting proper response from GPT-3 using SDK in JS","<p>When using createCompletion I get a response but it doesn't have the actual text response.  In textPayload it has &quot;text: 'package com.example.demo.controller;',&quot;</p>
<p>Below is my code</p>
<pre><code>const openai = new OpenAIApi(configuration);

  async function step1() {
  currentResponse = await openai.createCompletion({
    model: &quot;text-davinci-003&quot;,
    prompt: currentMessage,
    temperature: 0,
    max_tokens: 2292,
    top_p: 1,
    frequency_penalty: 0,
    presence_penalty: 0,
    stop: [&quot;\n\n&quot;],
  });

} //end step1

return step1().then(function(response) {

    var currentResponseNew = currentResponse.data
    //this is where I get the text payload value
    console.log(currentResponseNew)
    res.send(&quot;done&quot;)


})
</code></pre>
","2023-02-07 22:38:28","","","2023-02-09 09:35:10","<openai-api><gpt-3>","1","0","0","276","","","","","","",""
"75403409","1","2195440","","Context window length in OpenAI API Codex models","<p>Is the completion window length included in the context window length for OpenAI Codex models?</p>
<p>For <code>da-vinci</code>, the context window length is set to <code>4000</code> tokens.</p>
<p>From what I understand, as an example, if the prompt length is <code>3500</code> tokens, then the remaining <code>500</code> is for the completion. And there is no way use the whole <code>4000</code> token as the prompt.</p>
<p>I am pretty sure in my understanding, but it would be helpful to have it confirmed by someone knowledgeable.</p>
","2023-02-09 19:16:21","","2023-03-02 00:35:41","2023-03-02 00:35:41","<openai-api><gpt-3>","1","0","0","806","","","","","","",""
"75429596","1","2947435","","OpenAI GPT-3 API: How to parse the response into an ordered list or dictionary?","<p>GPT-3 is amazing, but parsing its results is a bit of a headache, or I'm missing something here?
For example, I'm asking GPT-3 to write something about &quot;digital marketing&quot; and it's returning back some interesting stuff:</p>
<pre><code>\n\n1. Topic: The Benefits of Digital Marketing \nHeadlines: \na. Unlocking the 
Potential of Digital Marketing \nb. Harnessing the Power of Digital Marketing for 
Your Business \nc. How to Maximize Your Return on Investment with Digital Marketing 
\nd. Exploring the Benefits of a Comprehensive Digital Marketing Strategy \ne. 
Leveraging Technology to Take Your Business to the Next Level with Digital Marketing  
\n\n2. Topic: Social Media Strategies for Effective Digital Marketing  \nHeadlines:  
\na. Crafting an Engaging Social Media Presence for Maximum Impact \nb. How to Reach 
and Engage Your Target Audience Through Social Media Platforms  \nc. Optimizing Your 
Content Strategy for Maximum Reach on Social Media Platforms   \nd. Utilizing Paid 
Advertising Strategies on Social Media Platforms   \t\t\t\t\t\t\t     e .Analyzing 
and Improving Performance Across Multiple Social Networks\n\n3. Topic: SEO Best 
Practices for Effective Digital Marketing    Headlines:     a .Understanding Search 
Engine Algorithms and Optimizing Content Accordingly    b .Developing an Effective 
SEO Strategy That Delivers Results c .Leveraging Keywords and Metadata For Maximum 
Visibility d .Exploring Advanced SEO Techniques To Increase Traffic e .Analyzing 
Performance Data To Improve Rankings\n\n4Topic : Email Campaigns For Successful 
Digital Marketin g Headlines : a .Creating Compelling Email Campaigns That Drive 
Results b .Optimizing Email Deliverability For Maximum Impact c .Utilizing Automation 
Tools To Streamline Email Campaign Management d .Measuring Performance And Analyzing 
Data From Email Campaigns e .Exploring Creative Ways To Increase Open Rates On 
Emails\n\n5Topic : Mobile Advertising Strategies For Effective Digita l Marketin g 
Headlines : a ..Maximizing Reach With Mobile Ads b ..Understanding User Behavior On 
Mobile Devices c ..Optimizing Ads For Different Screen Sizes d ..Leveraging Location- 
Based Targeting To Increase Relevance e ..Analyzing Performance Data From Mobile Ads
</code></pre>
<p>As you can see, it's sent me back a list of topics related to &quot;digital marketing&quot; with some headlines (apparently from a to e). I see some line breaks and tabulation here and there. So my first reflex was to split the text on the line breaks, but it looks like the format is not equal everywhere, as there are very few line breaks in the second half of the response (which make it inaccurate).
What I'd like to do, is reformatting the output, so I can have a kind of list of topics and headlines. Something like this:</p>
<pre><code>[
     {&quot;Topic 1&quot;: [&quot;headline 1&quot;, &quot;headline 2&quot;,&quot;...&quot;]},
     {&quot;Topic 2&quot;: [&quot;headline 1&quot;, &quot;headline 2&quot;,&quot;...&quot;]},
     {&quot;Topic 3&quot;: [&quot;headline 1&quot;, &quot;headline 2&quot;,&quot;...&quot;]}
]
</code></pre>
<p>Maybe there is a parameter to send over withing my request, but I didn't find anything in the doc. So I guess my best bet is to reformat using <code>regex</code>. Here I see a pattern <code>Topic:</code>and <code>Headlines:</code> but it's not always the case. What is consistent is the number prefixing each element <code>(like Ì., II., 1., 2. or a., b.)</code> but sometimes it looks like <code>a ..</code> (you can see that at the end of the response for example.</p>
<p>Any idea how to do that? (I'm using python for that, but can adapt from another language)</p>
","2023-02-12 19:24:01","","2023-03-13 14:28:06","2023-05-18 11:45:28","<python><openai-api><gpt-3>","2","6","1","2010","","","","","","",""
"75617250","1","19989305","","cannot import name 'GPT2ForQuestionAnswering' from 'transformers'","<p><a href=""https://i.stack.imgur.com/wZejL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wZejL.png"" alt=""enter image description here"" /></a></p>
<blockquote>
<p>1 import pandas as pd
2 import torch
----&gt; 3 from transformers import GPT2Tokenizer, GPT2ForQuestionAnswering, AdamW
4 from transformers import default_data_collator
5 from torch.utils.data import DataLoader</p>
</blockquote>
<pre><code>import pandas as pd
import torch
from transformers import GPT2Tokenizer, GPT2ForQuestionAnswering, AdamW
from transformers import default_data_collator
from torch.utils.data import DataLoader
from transformers import datasets
from transformers import TriviaQAProcessor, set_seed
from transformers import TrainingArguments, Trainer
</code></pre>
","2023-03-02 15:01:41","","","2023-05-13 10:40:13","<pip><torch><gpt-2>","1","1","0","184","","","","","","",""
"75655947","1","425281","","Does openai GPT finetuning consider the prompt in the loss function?","<p>OpenAI api includes a finetuning service that divides the task in &quot;prompt&quot; and &quot;completion&quot;</p>
<p><a href=""https://platform.openai.com/docs/guides/fine-tuning"" rel=""nofollow noreferrer"">https://platform.openai.com/docs/guides/fine-tuning</a></p>
<p>The documentation says that the accuracy metrics are calculated respect to the completion. But for the loss it is said that it is calculated &quot;on the training batch&quot;.</p>
<p>My understanding is that the first training of a GPT model always happen in batches of max available size, using an special token to separate contexts but always asking to predict the next token for all the entries. So here the loss function is the obvious cross entropy over all the outputs. But in fine tuning, there is the opportunity to learn to predict the &quot;template prompt&quot; or not. Both decisions can be sensible; learning the template amounts to train a parsing, masking the template can avoid overfitting.</p>
<p>So, what is the current practice in OpenAI?</p>
","2023-03-06 21:18:20","","","2023-03-06 23:02:11","<openai-api><gpt-3><gpt-2>","1","0","0","462","","","","","","",""
"75662453","1","20927753","","Getting an error 'no file named tf_model.h5 or pytorch_model.bin found in directory gpt2'","<pre><code>model_name = &quot;gpt2&quot;
model = TFGPT2ForSequenceClassification.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
tokenizer.add_special_tokens({'pad_token': '[PAD]'})
</code></pre>
<p>When I am running the above code, the model is downloaded successfully.</p>
<p>Once the model is downloaded, I am defining the model configuration and loading the model with the updated configuration which throws an error 'OSError: Error no file named tf_model.h5 or pytorch_model.bin found in directory gpt2-medium.'</p>
<pre><code>#Define model configuration
model_config = model.config
model_config.num_labels = 5

#Save model configuration
model_config.save_pretrained(model_name)

#Load model with updated configuration
model = TFGPT2ForSequenceClassification.from_pretrained(model_name, num_labels =5)
</code></pre>
<p>How can I resolve this issue?</p>
","2023-03-07 13:15:28","","","2023-03-07 16:01:31","<tensorflow><text><pytorch><classification><gpt-2>","1","0","0","657","","","","","","",""
"75664012","1","17522290","","I want to make an AI text classifier using OpenAI API, based on GPT2 but i cannot find the API documentation for the GPT2","<p>I wanted to create an AI text classifier project for my college, I wanted to use GPT2 API for the same as it is more reliable to catch the content generated by GPT 3.5, so how can I use GPT2 documentation? also any useful resources for the same are welcome</p>
<p>I tried going through model section of the documentation but couldn't find for GPT2, there's only for GPT 3.5</p>
","2023-03-07 15:33:17","","2023-03-07 15:36:12","2023-03-24 18:53:58","<machine-learning><artificial-intelligence><openai-api><language-model><gpt-2>","1","0","0","236","","","","","","",""
"75667860","1","19615881","","openai unknown command 'tools'","<ul>
<li><p>I am learning gpt fine-tuning</p>
</li>
<li><p>I successfully ran this command: pip install --upgrade openai</p>
</li>
<li><p>I couldn't run this command: export OPENAI_API_KEY=&quot;sk-xxxxxxxxxxxxxx&quot;</p>
</li>
<li><p>Error: export : The term 'export' is not recognized as the name of a cmdlet, function, script file, or operable program.</p>
</li>
<li><p>So I ran this command instead: set OPENAI_API_KEY=&quot;sk-xxxxxxxxxxxxxx&quot;</p>
</li>
<li><p>Also ran Set-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope CurrentUser</p>
</li>
<li><p>Now I'm trying to run openai tools fine_tunes.prepare_data -f C:\Users.....fine-tune-upload.xlsx</p>
</li>
</ul>
<p>But it shows: error: unknown command 'tools'</p>
<p>Any idea? Please explain in beginner language!</p>
","2023-03-07 22:37:38","","","2023-03-09 09:32:30","<openai-api><gpt-3><fine-tune>","1","1","0","434","","","","","","",""
"73454328","1","19825964","","A question about using past_key_values generated by gpt2","<p>Recently, i have a problem about how to use past_key_values which is generated by gpt2. Here is the demo.</p>
<pre><code># last_hidden_states, h_s in short
# past_key_values, p_k_v in short

# A_h_s.shape = (bs, A_len, hs=768)
(_, A_p_k_v, A_h_s) = gpt2_model(A_input_ids, A_token_type_ids, A_attention_mask, A_position_ids)

# B_h_s.shape = (bs, B_len, hs=768)
(_, B_p_k_v, B_h_s) = gpt2_model(B_input_ids, B_token_type_ids, B_attention_mask, A_position_ids)

# Do some operations on A_h_s, such as integrating some external knowledge
A_h_s = do_something(A_h_s)  # (bs, A_len, hs=768)

&quot;&quot;&quot;
The following parts are the problem.
During the training, I hope to be able to use A_h_s and B_h_s to predict C.
What am I supposed to do?
&quot;&quot;&quot;

# (bs, A_len + B_len + C_len)
attention_mask = torch.cat([A_attention_mask, B_attention_mask, C_attention_mask], dim=-1)
position_ids = torch.cumsum(C_attention_mask , dim=-1)[:,-c_len:].type_as(C_input_ids) - 1
past_key_values = ?????

# use the lm_logits and C_label_ids to do cross_entropy and get loss for backward
lm_logits, *_ = gpt2_model(C_input_ids, C_token_type_ids, attention_mask, past_key_values)
</code></pre>
<p>Maybe, i can <code>torch.cat([A_h_s, B_h_s], dim=-1)</code>, <code>torch.cat([A_atten_mask, B_atten_mask], dim=-1)</code>. Then feed them to gpt2 to get the <code>past_key_values</code>. Am i right?</p>
","2022-08-23 06:53:24","","2022-08-24 17:43:25","2022-08-24 17:43:25","<python><nlp><gpt-2>","0","0","0","193","","","","","","",""
"73455802","1","18992575","","Search through GPT-3's training data","<p>I'm using GPT-3 for some experiments where I prompt the language model with tests from cognitive science. The tests have the form of short text snippets. Now I'd like to check whether GPT-3 has already encountered these text snippets during training. Hence my question: Is there any way to sift through GPT-3's training text corpora? Can one find out whether a certain string is part of these text corpora?</p>
<p>Thanks for your help!</p>
","2022-08-23 08:56:26","","","2022-12-08 00:37:13","<nlp><training-data><gpt-3>","1","0","1","157","","","","","","",""
"56415280","1","9435410","","Fine tune GPT-2 Text Prediction for Conversational AI","<p>I am experimenting with the gpt-2 model's conditional text generation to tweak it for a good chatbot. I am using <a href=""https://github.com/nshepperd/gpt-2"" rel=""nofollow noreferrer"">nsheppard's code</a> for retraining it on my custom dataset.</p>

<p>I trained my model on a custom dataset of conversations that I pulled from my facebook data. I changed the sample length to 20 as they are dialogues during interactive conditional generation.</p>

<p>The dataset looks something like this:</p>

<pre><code> How are you 
 Hi Great and you 
 Am also good 
 So you re a graphic designer  
 Yeah 
 How can you contribute to making the game In d graphics aspect 
 Can you show me some of your work if u don t mind  
 Am planning to learn making it a motion type    
 U can go through my photos 
 K 
 Can you make animations for it  
 Flash animations to be specific 
 No please only stable ones 
 Ok
</code></pre>

<p>But, after the training when i try to chat with it, it is instead completing my sentences instead of replying to them.</p>

<pre><code>User &gt;&gt;&gt; bye
======================================== SAMPLE 1 ========================================
 and  
 hi 
 are there any positions in khrzh being appointed right now 
</code></pre>

<p>I understand that the interactive_conditional_samples.py was built to complete the sentence based on the prompt, but I thought changing the dataset would work and sure it doesn't work.</p>

<p>train.py</p>

<pre><code>#!/usr/bin/env python3
# Usage:
#  PYTHONPATH=src ./train --dataset &lt;file|directory|glob&gt;

import argparse
import json
import os
import numpy as np
import tensorflow as tf
import time
import tqdm
from tensorflow.core.protobuf import rewriter_config_pb2

import model, sample, encoder
from load_dataset import load_dataset, Sampler
from accumulate import AccumulatingOptimizer
import memory_saving_gradients

CHECKPOINT_DIR = 'checkpoint'
SAMPLE_DIR = 'samples'


parser = argparse.ArgumentParser(
    description='Fine-tune GPT-2 on your custom dataset.',
    formatter_class=argparse.ArgumentDefaultsHelpFormatter)

parser.add_argument('--dataset', metavar='PATH', type=str, required=True, help='Input file, directory, or glob pattern (utf-8 text, or preencoded .npz files).')
parser.add_argument('--model_name', metavar='MODEL', type=str, default='117M', help='Pretrained model name')
parser.add_argument('--combine', metavar='CHARS', type=int, default=50000, help='Concatenate input files with &lt;|endoftext|&gt; separator into chunks of this minimum size')

parser.add_argument('--batch_size', metavar='SIZE', type=int, default=1, help='Batch size')
parser.add_argument('--learning_rate', metavar='LR', type=float, default=0.00002, help='Learning rate for Adam')
parser.add_argument('--accumulate_gradients', metavar='N', type=int, default=1, help='Accumulate gradients across N minibatches.')
parser.add_argument('--memory_saving_gradients', default=False, action='store_true', help='Use gradient checkpointing to reduce vram usage.')
parser.add_argument('--only_train_transformer_layers', default=False, action='store_true', help='Restrict training to the transformer blocks.')
parser.add_argument('--optimizer', type=str, default='adam', help='Optimizer. &lt;adam|sgd&gt;.')
parser.add_argument('--noise', type=float, default=0.0, help='Add noise to input training data to regularize against typos.')

parser.add_argument('--top_k', type=int, default=40, help='K for top-k sampling.')
parser.add_argument('--top_p', type=float, default=0.0, help='P for top-p sampling. Overrides top_k if set &gt; 0.')

parser.add_argument('--restore_from', type=str, default='latest', help='Either ""latest"", ""fresh"", or a path to a checkpoint file')
parser.add_argument('--run_name', type=str, default='run1', help='Run id. Name of subdirectory in checkpoint/ and samples/')
parser.add_argument('--sample_every', metavar='N', type=int, default=100, help='Generate samples every N steps')
parser.add_argument('--sample_length', metavar='TOKENS', type=int, default=1023, help='Sample this many tokens')
parser.add_argument('--sample_num', metavar='N', type=int, default=1, help='Generate this many samples')
parser.add_argument('--save_every', metavar='N', type=int, default=1000, help='Write a checkpoint every N steps')

parser.add_argument('--val_dataset', metavar='PATH', type=str, default=None, help='Dataset for validation loss, defaults to --dataset.')
parser.add_argument('--val_batch_size', metavar='SIZE', type=int, default=2, help='Batch size for validation.')
parser.add_argument('--val_batch_count', metavar='N', type=int, default=40, help='Number of batches for validation.')
parser.add_argument('--val_every', metavar='STEPS', type=int, default=0, help='Calculate validation loss every STEPS steps.')


def maketree(path):
    try:
        os.makedirs(path)
    except:
        pass


def randomize(context, hparams, p):
    if p &gt; 0:
        mask = tf.random.uniform(shape=tf.shape(context)) &lt; p
        noise = tf.random.uniform(shape=tf.shape(context), minval=0, maxval=hparams.n_vocab, dtype=tf.int32)
        return tf.where(mask, noise, context)
    else:
        return context


def main():
    args = parser.parse_args()
    enc = encoder.get_encoder(args.model_name)
    hparams = model.default_hparams()
    with open(os.path.join('models', args.model_name, 'hparams.json')) as f:
        hparams.override_from_dict(json.load(f))

    if args.sample_length &gt; hparams.n_ctx:
        raise ValueError(
            ""Can't get samples longer than window size: %s"" % hparams.n_ctx)

    if args.model_name == '345M':
        args.memory_saving_gradients = True
        if args.optimizer == 'adam':
            args.only_train_transformer_layers = True

    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    config.graph_options.rewrite_options.layout_optimizer = rewriter_config_pb2.RewriterConfig.OFF
    with tf.Session(config=config) as sess:
        context = tf.placeholder(tf.int32, [args.batch_size, None])
        context_in = randomize(context, hparams, args.noise)
        output = model.model(hparams=hparams, X=context_in)
        loss = tf.reduce_mean(
            tf.nn.sparse_softmax_cross_entropy_with_logits(
                labels=context[:, 1:], logits=output['logits'][:, :-1]))

        if args.val_every &gt; 0:
            val_context = tf.placeholder(tf.int32, [args.val_batch_size, None])
            val_output = model.model(hparams=hparams, X=val_context)
            val_loss = tf.reduce_mean(
                tf.nn.sparse_softmax_cross_entropy_with_logits(
                    labels=val_context[:, 1:], logits=val_output['logits'][:, :-1]))
            val_loss_summary = tf.summary.scalar('val_loss', val_loss)


        tf_sample = sample.sample_sequence(
            hparams=hparams,
            length=args.sample_length,
            context=context,
            batch_size=args.batch_size,
            temperature=1.0,
            top_k=args.top_k,
            top_p=args.top_p)

        all_vars = [v for v in tf.trainable_variables() if 'model' in v.name]
        train_vars = [v for v in all_vars if '/h' in v.name] if args.only_train_transformer_layers else all_vars

        if args.optimizer == 'adam':
            opt = tf.train.AdamOptimizer(learning_rate=args.learning_rate)
        elif args.optimizer == 'sgd':
            opt = tf.train.GradientDescentOptimizer(learning_rate=args.learning_rate)
        else:
            exit('Bad optimizer:', args.optimizer)

        if args.accumulate_gradients &gt; 1:
            if args.memory_saving_gradients:
                exit(""Memory saving gradients are not implemented for gradient accumulation yet."")
            opt = AccumulatingOptimizer(
                opt=opt,
                var_list=train_vars)
            opt_reset = opt.reset()
            opt_compute = opt.compute_gradients(loss)
            opt_apply = opt.apply_gradients()
            summary_loss = tf.summary.scalar('loss', opt_apply)
        else:
            if args.memory_saving_gradients:
                opt_grads = memory_saving_gradients.gradients(loss, train_vars)
            else:
                opt_grads = tf.gradients(loss, train_vars)
            opt_grads = list(zip(opt_grads, train_vars))
            opt_apply = opt.apply_gradients(opt_grads)
            summary_loss = tf.summary.scalar('loss', loss)

        summary_lr = tf.summary.scalar('learning_rate', args.learning_rate)
        summaries = tf.summary.merge([summary_lr, summary_loss])

        summary_log = tf.summary.FileWriter(
            os.path.join(CHECKPOINT_DIR, args.run_name))

        saver = tf.train.Saver(
            var_list=all_vars,
            max_to_keep=5,
            keep_checkpoint_every_n_hours=2)
        sess.run(tf.global_variables_initializer())

        if args.restore_from == 'latest':
            ckpt = tf.train.latest_checkpoint(
                os.path.join(CHECKPOINT_DIR, args.run_name))
            if ckpt is None:
                # Get fresh GPT weights if new run.
                ckpt = tf.train.latest_checkpoint(
                    os.path.join('models', args.model_name))
        elif args.restore_from == 'fresh':
            ckpt = tf.train.latest_checkpoint(
                os.path.join('models', args.model_name))
        else:
            ckpt = tf.train.latest_checkpoint(args.restore_from)
        print('Loading checkpoint', ckpt)
        saver.restore(sess, ckpt)

        print('Loading dataset...')
        chunks = load_dataset(enc, args.dataset, args.combine)
        data_sampler = Sampler(chunks)
        if args.val_every &gt; 0:
            val_chunks = load_dataset(enc, args.val_dataset, args.combine) if args.val_dataset else chunks
        print('dataset has', data_sampler.total_size, 'tokens')
        print('Training...')

        if args.val_every &gt; 0:
            # Sample from validation set once with fixed seed to make
            # it deterministic during training as well as across runs.
            val_data_sampler = Sampler(val_chunks, seed=1)
            val_batches = [[val_data_sampler.sample(1024) for _ in range(args.val_batch_size)]
                           for _ in range(args.val_batch_count)]

        counter = 1
        counter_path = os.path.join(CHECKPOINT_DIR, args.run_name, 'counter')
        if os.path.exists(counter_path):
            # Load the step number if we're resuming a run
            # Add 1 so we don't immediately try to save again
            with open(counter_path, 'r') as fp:
                counter = int(fp.read()) + 1

        def save():
            maketree(os.path.join(CHECKPOINT_DIR, args.run_name))
            print(
                'Saving',
                os.path.join(CHECKPOINT_DIR, args.run_name,
                             'model-{}').format(counter))
            saver.save(
                sess,
                os.path.join(CHECKPOINT_DIR, args.run_name, 'model'),
                global_step=counter)
            with open(counter_path, 'w') as fp:
                fp.write(str(counter) + '\n')

        def generate_samples():
            print('Generating samples...')
            context_tokens = data_sampler.sample(1)
            all_text = []
            index = 0
            while index &lt; args.sample_num:
                out = sess.run(
                    tf_sample,
                    feed_dict={context: args.batch_size * [context_tokens]})
                for i in range(min(args.sample_num - index, args.batch_size)):
                    text = enc.decode(out[i])
                    text = '======== SAMPLE {} ========\n{}\n'.format(
                        index + 1, text)
                    all_text.append(text)
                    index += 1
            print(text)
            maketree(os.path.join(SAMPLE_DIR, args.run_name))
            with open(
                    os.path.join(SAMPLE_DIR, args.run_name,
                                 'samples-{}').format(counter), 'w') as fp:
                fp.write('\n'.join(all_text))

        def validation():
            print('Calculating validation loss...')
            losses = []
            for batch in tqdm.tqdm(val_batches):
                losses.append(sess.run(val_loss, feed_dict={val_context: batch}))
            v_val_loss = np.mean(losses)
            v_summary = sess.run(val_loss_summary, feed_dict={val_loss: v_val_loss})
            summary_log.add_summary(v_summary, counter)
            summary_log.flush()
            print(
                '[{counter} | {time:2.2f}] validation loss = {loss:2.2f}'
                .format(
                    counter=counter,
                    time=time.time() - start_time,
                    loss=v_val_loss))

        def sample_batch():
            return [data_sampler.sample(1024) for _ in range(args.batch_size)]


        avg_loss = (0.0, 0.0)
        start_time = time.time()

        try:
            while True:
                if counter % args.save_every == 0:
                    save()
                if counter % args.sample_every == 0:
                    generate_samples()
                if args.val_every &gt; 0 and (counter % args.val_every == 0 or counter == 1):
                    validation()

                if args.accumulate_gradients &gt; 1:
                    sess.run(opt_reset)
                    for _ in range(args.accumulate_gradients):
                        sess.run(
                            opt_compute, feed_dict={context: sample_batch()})
                    (v_loss, v_summary) = sess.run((opt_apply, summaries))
                else:
                    (_, v_loss, v_summary) = sess.run(
                        (opt_apply, loss, summaries),
                        feed_dict={context: sample_batch()})

                summary_log.add_summary(v_summary, counter)

                avg_loss = (avg_loss[0] * 0.99 + v_loss,
                            avg_loss[1] * 0.99 + 1.0)

                print(
                    '[{counter} | {time:2.2f}] loss={loss:2.2f} avg={avg:2.2f}'
                    .format(
                        counter=counter,
                        time=time.time() - start_time,
                        loss=v_loss,
                        avg=avg_loss[0] / avg_loss[1]))

                counter += 1
        except KeyboardInterrupt:
            print('interrupted')
            save()


if __name__ == '__main__':
    main()
</code></pre>

<p>sample.py</p>

<pre><code>import tensorflow as tf

import model

def top_k_logits(logits, k):
    if k == 0:
        # no truncation
        return logits

    def _top_k():
        values, _ = tf.nn.top_k(logits, k=k)
        min_values = values[:, -1, tf.newaxis]
        return tf.where(
            logits &lt; min_values,
            tf.ones_like(logits, dtype=logits.dtype) * -1e10,
            logits,
        )
    return tf.cond(
       tf.equal(k, 0),
       lambda: logits,
       lambda: _top_k(),
    )


def top_p_logits(logits, p):
    with tf.variable_scope('top_p_logits'):
        logits_sort = tf.sort(logits, direction='DESCENDING')
        probs_sort = tf.nn.softmax(logits_sort)
        probs_sums = tf.cumsum(probs_sort, axis=1, exclusive=True)
        logits_masked = tf.where(probs_sums &lt; p, logits_sort, tf.ones_like(logits_sort)*1000) # [batchsize, vocab]
        min_logits = tf.reduce_min(logits_masked, axis=1, keepdims=True) # [batchsize, 1]
        return tf.where(
            logits &lt; min_logits,
            tf.ones_like(logits, dtype=logits.dtype) * -1e10,
            logits,
        )


def sample_sequence(*, hparams, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0, top_p=0.0):
    if start_token is None:
        assert context is not None, 'Specify exactly one of start_token and context!'
    else:
        assert context is None, 'Specify exactly one of start_token and context!'
        context = tf.fill([batch_size, 1], start_token)

    def step(hparams, tokens, past=None):
        lm_output = model.model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)

        logits = lm_output['logits'][:, :, :hparams.n_vocab]
        presents = lm_output['present']
        presents.set_shape(model.past_shape(hparams=hparams, batch_size=batch_size))
        return {
            'logits': logits,
            'presents': presents,
        }

    with tf.name_scope('sample_sequence'):
        # Don't feed the last context token -- leave that to the loop below
        # TODO: Would be slightly faster if we called step on the entire context,
        # rather than leaving the last token transformer calculation to the while loop.
        context_output = step(hparams, context[:, :-1])

        def body(past, prev, output):
            next_outputs = step(hparams, prev[:, tf.newaxis], past=past)
            logits = next_outputs['logits'][:, -1, :]  / tf.to_float(temperature)
            if top_p &gt; 0.0:
                logits = top_p_logits(logits, p=top_p)
            else:
                logits = top_k_logits(logits, k=top_k)
            samples = tf.multinomial(logits, num_samples=1, output_dtype=tf.int32)
            return [
                tf.concat([past, next_outputs['presents']], axis=-2),
                tf.squeeze(samples, axis=[1]),
                tf.concat([output, samples], axis=1),
            ]

        def cond(*args):
            return True

        _, _, tokens = tf.while_loop(
            cond=cond, body=body,
            maximum_iterations=length,
            loop_vars=[
                context_output['presents'],
                context[:, -1],
                context,
            ],
            shape_invariants=[
                tf.TensorShape(model.past_shape(hparams=hparams, batch_size=batch_size)),
                tf.TensorShape([batch_size]),
                tf.TensorShape([batch_size, None]),
            ],
            back_prop=False,
        )

        return tokens
</code></pre>

<p>interactive_conditional_samples.py</p>

<pre><code>#!/usr/bin/env python3

import fire
import json
import os
import numpy as np
import tensorflow as tf

import model, sample, encoder

def interact_model(
    model_name='chatbot',
    seed=None,
    nsamples=1,
    batch_size=1,
    length=20,
    temperature=1,
    top_k=0,
    top_p=0.0
):
    """"""
    Interactively run the model
    :model_name=chatbot : String, which model to use
    :seed=None : Integer seed for random number generators, fix seed to reproduce
     results
    :nsamples=1 : Number of samples to return total
    :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.
    :length=None : Number of tokens in generated text, if None (default), is
     determined by model hyperparameters
    :temperature=1 : Float value controlling randomness in boltzmann
     distribution. Lower temperature results in less random completions. As the
     temperature approaches zero, the model will become deterministic and
     repetitive. Higher temperature results in more random completions.
    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is
     considered for each step (token), resulting in deterministic completions,
     while 40 means 40 words are considered at each step. 0 (default) is a
     special setting meaning no restrictions. 40 generally is a good value.
    :top_p=0.0 : Float value controlling diversity. Implements nucleus sampling,
     overriding top_k if set to a value &gt; 0. A good setting is 0.9.
    """"""
    if batch_size is None:
        batch_size = 1
    assert nsamples % batch_size == 0

    enc = encoder.get_encoder(model_name)
    hparams = model.default_hparams()
    with open(os.path.join('models', model_name, 'hparams.json')) as f:
        hparams.override_from_dict(json.load(f))

    if length is None:
        length = hparams.n_ctx // 2
    elif length &gt; hparams.n_ctx:
        raise ValueError(""Can't get samples longer than window size: %s"" % hparams.n_ctx)

    with tf.Session(graph=tf.Graph()) as sess:
        context = tf.placeholder(tf.int32, [batch_size, None])
        np.random.seed(seed)
        tf.set_random_seed(seed)
        output = sample.sample_sequence(
            hparams=hparams, length=length,
            context=context,
            batch_size=batch_size,
            temperature=temperature, top_k=top_k, top_p=top_p
        )

        saver = tf.train.Saver()
        ckpt = tf.train.latest_checkpoint(os.path.join('models', model_name))
        saver.restore(sess, ckpt)

        while True:
            raw_text = input(""User &gt;&gt;&gt; "")
            while not raw_text:
                print('Prompt should not be empty!')
                raw_text = input(""User &gt;&gt;&gt; "")
            context_tokens = enc.encode(raw_text)
            generated = 0
            for _ in range(nsamples // batch_size):
                out = sess.run(output, feed_dict={
                    context: [context_tokens for _ in range(batch_size)]
                })[:, len(context_tokens):]
                for i in range(batch_size):
                    generated += 1
                    text = enc.decode(out[i])
                    print(""="" * 40 + "" SAMPLE "" + str(generated) + "" "" + ""="" * 40)
                    print(text)
            print(""="" * 80)

if __name__ == '__main__':
    fire.Fire(interact_model)
</code></pre>

<p>How can I tweak the code to get it working like a chatbot? I am guessing it has something to do with the context part in sample.py, though i am unsure how is this going to work.</p>
","2019-06-02 12:57:41","","2020-11-29 12:02:02","2020-11-29 12:02:02","<python><tensorflow><nlp><chatbot><gpt-2>","2","0","3","4864","0","","","","","",""
"73629287","1","15279628","","OpenAI GPT-3 API: How to extend length of the TL;DR output?","<p>I'd like to produce a 3-6 sentence summary from a 2-3 page article, using OpenAI's TLDR. I've pasted the article text but the output seems to stay between 1 and 2 sentences only.</p>
","2022-09-07 01:53:41","","2023-03-13 13:33:12","2023-03-13 13:33:12","<openai-api><gpt-3>","1","0","6","941","","","","","","",""
"73657901","1","14156907","","Combine GPT3 with RASA","<p>I am trying to integrate rasa with gpt3, but not getting the proper response. Can help me out to look at my code and tell me issue.</p>
<pre><code>def gpt3(text):
response = openai.Completion.create(
    model=&quot;code-cushman-001&quot;,
    # engine=&quot;ada&quot;,
    prompt=&quot;\n\n&quot; + text,
    temperature=0,
    logprobs=10,
    max_tokens=150,
    top_p=0,
    frequency_penalty=0,
    presence_penalty=0,
    stop=[&quot; \n\n&quot;]
) 
return response['choices'][0]['text']
</code></pre>
<p>action.py</p>
<pre><code>class ActionDefaultFallback(Action):
def init(self):
    # self.gpt3 = gpt3()
    super()._init_()

def name(self) -&gt; Text:
    return &quot;action_default_fallback&quot;

async def run(self, dispatcher, tracker, domain):
    query = tracker.latest_message['text']
    dispatcher.utter_message(text=gpt3(query))

    return [UserUtteranceReverted()]
</code></pre>
<p>Not able to understand the issue. Help me out to solve this.</p>
<p>Thanks</p>
","2022-09-09 06:00:56","","","2023-03-01 01:08:25","<python><rasa><openai-api><gpt-3>","1","0","0","578","","","","","","",""
"72484590","1","15181966","","Gpt 3 keywords extractor","<p>I'm getting accustomed to gpt and want to build a keywords extractor for book summaries. Can someone point me to the references that'd help for my use case ?</p>
","2022-06-03 04:05:52","","2022-06-03 04:06:14","2022-06-22 09:58:47","<python><nlp><openai-api><gpt-3>","1","2","2","3137","0","","","","","",""
"75865844","1","10176535","","Alpaca Large Language Model from Python script","<p>I was able to install <a href=""https://github.com/antimatter15/alpaca.cpp"" rel=""nofollow noreferrer"">Alpaca</a>  under Linux and start and use it interactivelly via the corresponding <code>./chat</code> command.</p>
<p>However, I would like to run it not in interactive mode but from a Python (Jupyter) script with the prompt as string parameter. Also, it should be possible to call the model several times without needing to reload it each time.</p>
<p>I already wrote a Python script that works technically:</p>
<pre><code>import subprocess

# start the Alpaca model as a subprocess 
alpaca_process = subprocess.Popen([&quot;./chat&quot;], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)

# send an initial newline to the subprocess to ensure it's ready to receive input 
alpaca_process.stdin.write(&quot;\n&quot;) 
alpaca_process.stdin.flush()

def alpaca_predict(prompt):
    # send the prompt to Alpaca and get the output
    alpaca_process.stdin.write(prompt + &quot;\n&quot;)
    alpaca_process.stdin.flush()
    output = alpaca_process.stdout.readline().strip()
    return output

# test the function 
prompts = [&quot;Hello&quot;, &quot;What is the meaning of life?&quot;, &quot;Tell me a joke&quot;, &quot;Goodbye&quot;] 
for prompt in prompts:
    response = alpaca_predict(prompt)
    print(f&quot;Prompt: {prompt} - Response: {response}&quot;)
</code></pre>
<p>It works technically now, but unfortunately the model produces only nonsense like this:</p>
<pre><code>Prompt: Hello - Response: 
Prompt: What is the meaning of life? - Response: &gt; The following are some of the most popular programming languages used in web development today, ranked by market share (source Stack Overflow): 1) JavaScript; 2) Python; 3) Java/Javascript hybrid language such as Node.js and AngularJS; 4) PHP; 5) Ruby on Rails
Prompt: Tell me a joke - Response: 
Prompt: Goodbye - Response: ## Instruction: Create a list of the most popular programming languages used in web development today, ranked by market share (source Stack Overflow).
</code></pre>
<p>How to fix this?</p>
","2023-03-28 11:46:15","","2023-05-31 11:54:51","2023-05-31 11:55:31","<python><c#><artificial-intelligence><gpt-3><large-language-model>","2","0","2","1139","","","","","","",""
"75866651","1","13828374","","Why does LLM(LLaMA) loss drop staircase-like over epochs?","<p>I'm training a LLM(LLaMA-6B) and have noticed that its loss seems to drop in a stair-like fashion over the epochs. Specifically, I'll see little loss change for one epoch, and then suddenly the loss will drop quite a bit after a new epoch.</p>
<p>I'm curious about what might be causing this phenomenon.  Is it something to do with the learning rate, or perhaps the architecture of the model itself? Any insights would be greatly appreciated!
<a href=""https://i.stack.imgur.com/4Ybpb.jpg"" rel=""nofollow noreferrer"">loss figure</a></p>
<p>I'm curious about what might be causing this phenomenon. Any insights would be greatly appreciated!</p>
","2023-03-28 13:05:24","","","2023-04-20 04:07:07","<loss><gpt-3><fine-tune><large-language-model><llama-index>","1","0","0","286","","","","","","",""
"75902238","1","21538764","","Getting error ""Container localhost does not exist. (Could not find resource: localhost/model/wpe)"" while generating with gpt2-simple","<p>I am trying to generate text using the GPT-2 language model using the gpt2-simple library. The training process worked fine, but I am running into an error when I try to generate text using the generate() function. The error message I am receiving is:</p>
<p>&quot;Container localhost does not exist. (Could not find resource: localhost/model/wpe)&quot;</p>
<p>I am not sure what is causing this error, and I have not been able to find any relevant information in the documentation or online forums. Any help in resolving this issue would be greatly appreciated.</p>
<p>Here is the code i am using to generate data.</p>
<pre><code>import gpt_2_simple as gpt2
import os
import re
from chatbot import Chat, register_call,demo
import uwuify
import tensorflow as tf
import numpy as np
from transformers import GPT2LMHeadModel, GPT2Tokenizer,AutoModel,AutoTokenizer
name = 'Fluffy the femboy'
# Load pre-trained model and tokenizer
model_name = &quot;124M&quot;
file_name = &quot;data.txt&quot;
def generate(input):
    global sess
    thingy = True
    sess = gpt2.start_tf_sess()
    meow = gpt2.generate(sess)#,prefix=input,temperature=0.727)
    print(meow)
    thingy = False

generate(f&quot;Hello!&quot;)
</code></pre>
<p>Expected result:
I expect the generate() function to generate text without any errors.</p>
<p>Actual result:
I am receiving the following error message:</p>
<p>&quot;Container localhost does not exist. (Could not find resource: localhost/model/wpe)&quot;</p>
<p>Additional information:</p>
<pre><code>I am using the latest version of the gpt2-simple library.
I am using python 3.11
I am running this code on a Windows machine.
</code></pre>
","2023-03-31 19:43:48","","","2023-03-31 19:43:48","<python><tensorflow><artificial-intelligence><gpt-2>","0","0","0","33","","","","","","",""
"75442916","1","2629034","","OpenAI GPT-3 API: How to preserve formatting when pasting content into an Excel cell?","<p>I'm trying to create a fine tuned GPT-3 model.  to that end, I have some content that i've formatted in word that im trying to bring to excel (in order to import the training dataset).</p>
<p>My inputs are in the format:</p>
<pre><code>*Point A
    *Subpoint A1
    *Subpoint A2
*Point B
    *Subpoint B1
    *Subpoint B2
</code></pre>
<p>However, when i copy the contents into excel, The excel cell converts this to:</p>
<pre><code>*Point A
*Subpoint A1
*Subpoint A2
*Point B
*Subpoint B1
*Subpoint B2
</code></pre>
<p>Is there any way for me to preserve my original formatting?</p>
<p>Is there any other way better way than this?</p>
<p>Any help is appreciated greatly :)</p>
<p>Regards,
Galeej</p>
","2023-02-14 02:24:52","","2023-03-13 14:38:14","2023-03-13 14:38:14","<excel><openai-api><gpt-3>","1","0","0","128","","","","","","",""
"75454722","1","14949601","","OpenAI GPT-3 API: How does it count tokens for different languages?","<p>We all know that GPT-3 models can accept and produce all kinds of languages such as English, French, Chinese, Japanese and so on.</p>
<p>In traditional NLP, different languages have different token-making methods.</p>
<ul>
<li>For those Alphabetic Languages such as English, <code>Bert</code> use BPE method to make tokens like below:</li>
</ul>
<pre><code>Insomnia caused much frustration.
==&gt;
In-, som-, nia, caus-, ed, much, frus-, tra-, tion, .,
</code></pre>
<ul>
<li>For those Charactaristic Languages such as Chinese or Japanese, just use the character itself as the token like below.</li>
</ul>
<pre><code>東京メトロは心に寄り添う
==&gt;
東, 京, メ, ト, ロ, は, 心, に, 寄, り, 添, う,
</code></pre>
<pre><code>我说你倒是快点啊！！！
==&gt;
我, 说, 你, 倒, 是, 快, 点, 啊, ！, ！, ！, 
</code></pre>
<p>But for GPT-3, it composes of different languages and can produce both Chinese and English in one sentence. So I am really curious how this model makes tokens.</p>
","2023-02-15 01:26:42","","2023-03-07 14:10:52","2023-03-07 14:10:52","<nlp><tokenize><openai-api><gpt-3>","1","0","-1","1301","","","","","","",""
"70483937","1","10025412","","Fine-tune dialoGPT with a new dataset - loss below 1 and perplexity exploded","<p>I am following the tutorial <a href=""https://github.com/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb"" rel=""nofollow noreferrer"">https://github.com/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb</a> on fine-tuning DialoGPT (GPT-2) with a new conversational dataset.</p>
<p>It was trained fine earlier, the perplexity was about 5, 6 and the resulting dialogue seemed normal. <a href=""https://i.stack.imgur.com/bcSz3.png"" rel=""nofollow noreferrer"">enter image description here</a>Now, I am not sure if I touched somewhere...when I plotted the training loss, I found it went down to below 1 (about 0.25) and perplexity was over 300?</p>
","2021-12-26 02:52:11","","","2021-12-26 02:52:11","<deep-learning><nlp><gpt-2>","0","0","0","1421","","","","","","",""
"70534103","1","17798153","","Uncaught RangeError [MESSAGE_CONTENT_TYPE]: Message content must be a non-empty string","<p>I am trying to make a gpt3 chatbot but I keep getting this error on Discord.js v11.</p>
<blockquote>
<p>[Uncaught RangeError [MESSAGE_CONTENT_TYPE]: Message content must be a non-empty string.]</p>
</blockquote>
<pre><code>client.on(&quot;messageCreate&quot;, function (message) {
    if (message.author.bot) return;
    prompt += `You: ${String(message.content)}\n`;
    (async () =&gt; {
        const gptResponse = await openai.complete({
            engine: 'davinci',
            prompt: prompt,
            maxTokens: 60,
            temperature: 0.3,
            topP: 0.3,
            presencePenalty: 0,
            frequencyPenalty: 0.5,
            bestOf: 1,
            n: 1,
            stream: false,
            stop: ['\n', '\n\n']
        });
        message.reply(String(gptResponse.data.choices[0].text.substring(5)));
        prompt += `${gptResponse.data.choices[0].text}\n`;
    })();
 });            

client.login(process.env.BOT_TOKEN);
</code></pre>
","2021-12-30 16:17:42","","2021-12-31 15:06:26","2021-12-31 15:06:26","<javascript><discord><discord.js><artificial-intelligence><gpt-3>","0","3","2","305","","","","","","",""
"76073565","1","10759664","76424001","GPT2 special tokens: Ignore word(s) in input text when predicting next word","<p>I just started using GPT2 and I have a question concerning special tokens:</p>
<p>I'd like to predict the next word given a text input, but I want to mask some words in my input chunk using a special token. I don't want GPT2 to predict the masked words, I just don't want to use them for the prediction and I want GPT2 to &quot;know&quot; that it doesn't &quot;see&quot; all the input words.</p>
<p>Here's an example:
I have &quot;the quick brown fox jumps over the lazy&quot; as an input sentence.
I want GPT2 to predict the last word (correct would be &quot;dog&quot; in this case).
I also want to mask the words &quot;the lazy&quot;, but GPT2 should &quot;know&quot; there is something at the end of the input sentence. So basically for GPT2, the input should look like this: &quot;the quick brown fox jumps over _ _&quot;, and not like this: &quot;the quick brown fox jumps over&quot;, so it knows not to predict the word after &quot;over&quot;.</p>
<p>I thought about using special tokens to replace the &quot;hidden&quot; words, but I think neither MASK nor PAD make sense in this case.</p>
<p>Does anyone have an idea how to solve this?</p>
<p>Thanks in advance for your help!</p>
","2023-04-21 13:20:32","","2023-04-21 13:26:03","2023-06-07 13:46:36","<python><nlp><token><predict><gpt-2>","1","0","0","41","","2","10759664","<p>Solved this, masking the tokens did the trick. I used an attention mask and set all attention mask values of tokens I wanted to ignore to 0, so their attention weights are 0 on all layers.</p>
","2023-06-07 13:46:36","0","0"
"73760982","1","12127131","","Why is my addEventListener click event only firing once?","<p>I'm using an addEventListener click event to trigger a new GPT3 request which takes the latest database post as a prompt for text completion.</p>
<p>The data is sent to the database through a hidden html form, with the action of the form triggering a php script.</p>
<p>The newly generated text is then sent to the database, which becomes the prompt for the next GPT3 request etc etc...</p>
<p>The problem is the event only fires once, and I would like to be able to continue to generate new text within the loading page whilst the other data heavy files within the page loads.</p>
<pre><code>&lt;form name=&quot;GPT3commentForm&quot; id=&quot;GPT3commentForm&quot; display=&quot;none&quot; action=&quot;/insert_GPT3.php&quot; method=&quot;post&quot; autocomplete=&quot;off&quot;&gt;
&lt;input type=&quot;hidden&quot; name=&quot;GPT3_field1&quot; id=&quot;GPT3Text&quot;/&gt;&lt;/form&gt;

&lt;?php
    require_once './includes/Openai.php';
    $openai = New Openai();
    //Engine, prompt and max tokens - takes the last entry of the database array as the prompt
    $openai-&gt;request(&quot;davinci&quot;, end($stack), 100);
    $latestEntry = end($stack);
    $latestEntry = JSON_encode($latestEntry);
?&gt;

&lt;script&gt;
var data = &lt;?php echo json_encode($response, JSON_HEX_TAG); ?&gt;; // Don't forget the extra semicolon!
//turn the string into an object
const dataObj = JSON.parse(data);
console.log(typeof dataObj);
//access only the text completion section...
console.log(dataObj.choices[0].text);
GPT3Array = [];
newData = [];
newDataObj = [];
currentGPT3Text = 0;
GPT3Array[currentGPT3Text] = dataObj.choices[0].text;
document.getElementById('GPT3Text').value = GPT3Array[currentGPT3Text];
const GPT3symbolSpan = document.createElement(&quot;span&quot;);
const GPT3link = document.createElement(&quot;a&quot;);
GPT3link.textContent = GPT3Array[currentGPT3Text];
GPT3symbolSpan.appendChild(GPT3link);
GPT3LoadingText.appendChild(GPT3symbolSpan);
document.getElementById('GPT3commentForm').submit();

var generateText = function (event) {
    currentGPT3Text += 1;
    &lt;?php
        require_once './includes/Openai.php';
        $new_openai = New Openai();
        //Engine, prompt and max tokens - takes the last entry of the database array as the prompt
        $new_openai-&gt;request(&quot;davinci&quot;, end($stack), 100);
        $latestEntry = end($stack);
        $latestEntry = JSON_encode($latestEntry);
        //unset($new_openai);
        //unset($latestEntry);        
    ?&gt;

    //make newData and newDataObj an array to make this work...
    newData[currentGPT3Text] = &lt;?php echo json_encode($response, JSON_HEX_TAG); ?&gt;;
    newDataObj[currentGPT3Text] = JSON.parse(newData[currentGPT3Text]);
    GPT3Array[currentGPT3Text] = newDataObj[currentGPT3Text].choices[0].text;
    document.getElementById('GPT3Text').value = GPT3Array[currentGPT3Text];
    document.getElementById('GPT3commentForm').submit();
    GPT3link.textContent = GPT3Array[currentGPT3Text];
    console.log(currentGPT3Text);
    console.log(GPT3Array[currentGPT3Text]);
}

// GPT3LoadingText.onclick = generateText;
document.getElementById('loading-screen').addEventListener('click', generateText, {once: false}); 
&lt;/script&gt;
</code></pre>
<p>Here is a link to the site showing the issue: <a href=""https://surfacecollider.net/"" rel=""nofollow noreferrer"">https://surfacecollider.net/</a> (Ignore the other errors in the dev console, they're relating to separate issues that I'm yet to sort out with GLTF loaders which resolve on load).</p>
<p>And here's the php linked to the form action:</p>
<pre><code>&lt;?php
$username = &quot;************&quot;;
$password = &quot;************&quot;;
$database = &quot;************&quot;;

$mysqli = new mysqli(&quot;************&quot;, $username, $password, $database);

// Don't forget to properly escape your values before you send them to DB
// to prevent SQL injection attacks.

$GPT3_field2 = $mysqli-&gt;real_escape_string($_POST['GPT3_field1']);

$query = &quot;INSERT INTO comments (comment)
            VALUES ('{$GPT3_field2}')&quot;;

$mysqli-&gt;query($query);
?&gt;

&lt;?php
$query = $mysqli-&gt;query(&quot;SELECT * FROM comments&quot;);

$query = &quot;SELECT * FROM comments&quot;;

if ($result = $mysqli-&gt;query($query)) {
    /* fetch associative array */
    while ($row = $result-&gt;fetch_assoc()) {
        $field1name = $row[&quot;comments&quot;];
    }
    /* free result set */
    $result-&gt;free();
}?&gt;

&lt;?php
$username = &quot;************&quot;;
$password = &quot;************&quot;;
$database = &quot;************&quot;;
$mysqli = new mysqli(&quot;***********&quot;, $username, $password, $database);
$query = &quot;SELECT * FROM comments&quot;;
echo '&lt;table border=&quot;0&quot; cellspacing=&quot;2&quot; cellpadding=&quot;2&quot;&gt; 
    &lt;tr&gt; 
        &lt;td&gt; &lt;font face=&quot;Arial&quot;&gt;Username&lt;/font&gt; &lt;/td&gt; 
        &lt;td&gt; &lt;font face=&quot;Arial&quot;&gt;GPT3_field1&lt;/font&gt; &lt;/td&gt; 
    &lt;/tr&gt;';

if ($result = $mysqli-&gt;query($query)) {
    while ($row = $result-&gt;fetch_assoc()) {
        $field1name = $row[&quot;GPT3_field1&quot;];
        echo '&lt;tr&gt; 
                &lt;td&gt;'.$field1name.'&lt;/td&gt; 
            &lt;/tr&gt;';
    }
    $result-&gt;free();
} 

$mysqli-&gt;close();?&gt;
</code></pre>
<p>Any ideas why this is happening please?</p>
","2022-09-18 07:52:38","","2022-09-18 09:08:44","2022-09-18 09:14:00","<javascript><php><addeventlistener><gpt-3>","1","2","0","60","","","","","","",""
"75417403","1","295944","","Using GPT2 to find commonalities in text records","<p>I have a dataset with many incidents and most of the data is in free text form. One row per incident and a text field of what happened. I tried to train a gpt2 model on the free text then try prompts such as</p>
<p>&quot;The person got burned because&quot; and want to find the most common causes of burns.</p>
<p>The causes may be written in many ways so I thought maybe to get the meaning of each might work.</p>
<p>The prompts work but give some funny made up reasons so I do not think it's working well for what I want to do.</p>
","2023-02-11 00:48:01","","2023-02-11 00:54:29","2023-02-11 00:54:29","<nlp><bert-language-model><gpt-2>","0","0","0","17","","","","","","",""
"75469128","1","14949601","","How does Huggingface's tokenizers tokenize non-English characters?","<p>I use <a href=""https://github.com/huggingface/tokenizers"" rel=""nofollow noreferrer""><code>tokenizers</code></a> to tokenize natural language sentences into tokens.</p>
<p>But came up with some questions:</p>
<p>Here is some examples I tried using tokenizers:</p>
<pre><code>from transformers import GPT2TokenizerFast
tokenizer = GPT2TokenizerFast.from_pretrained(&quot;gpt2&quot;)
tokenizer(&quot;是&quot;)
# {'input_ids': [42468], 'attention_mask': [1]}
tokenizer(&quot;我说你倒是快点啊&quot;)
# {'input_ids': [22755, 239, 46237, 112, 19526, 254, 161, 222, 240, 42468, 33232, 104, 163, 224, 117, 161, 243, 232], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
tokenizer(&quot;東&quot;)
# {'input_ids': [30266, 109], 'attention_mask': [1, 1]}
tokenizer(&quot;東京&quot;)
# {'input_ids': [30266, 109, 12859, 105], 'attention_mask': [1, 1, 1, 1]}
tokenizer(&quot;東京メトロ&quot;)
# {'input_ids': [30266, 109, 12859, 105, 26998, 13298, 16253], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}
tokenizer(&quot;メトロ&quot;)
# {'input_ids': [26998, 13298, 16253], 'attention_mask': [1, 1, 1]}
tokenizer(&quot;This is my fault&quot;)
{'input_ids': [1212, 318, 616, 8046], 'attention_mask': [1, 1, 1, 1]}
</code></pre>
<p>The code above is some examples I tried.
The last example is an English sentence and I can understand that <code>This</code> corresponds to <code>&quot;This&quot;:1212</code> in the <code>vocab.json</code>, <code> is</code> corresponds to <code>&quot;\u0120is&quot;: 318</code>.</p>
<p>But I can not understand why this tool tokenizes non-English sequence into some tokens I can not find in the vocab.
For example:
<code>東</code> is been tokenized into <code>30266 and 109</code>. The results in the <code>vocab.json</code> is <code>&quot;æĿ&quot;:30266</code> and <code>&quot;±&quot;:109</code>.
<code>メ</code> is been tokenized into <code>26998</code>. The results in the <code>vocab.json</code> is <code>&quot;ãĥ¡&quot;:26998</code>.</p>
<p>I searched the Huggingface documents and website and find no clue.</p>
<p>And the source code is written in Rust, which is hard for me to understand.
So could you help me figure out why?</p>
","2023-02-16 07:39:04","","2023-02-16 08:14:43","2023-02-16 08:14:43","<nlp><tokenize><huggingface-tokenizers><gpt-3><gpt-2>","0","0","2","103","","","","","","",""
"75494945","1","2411311","","OpenAi api 429 rate limit error without reaching rate limit","<p>On occasion I'm getting a rate limit error without being over my rate limit. I'm using the text completions endpoint on the paid api which has a rate limit of 3,000 requests per minute. I am using at most 3-4 requests per minute.</p>
<p>Sometimes I will get the following error from the api:</p>
<ul>
<li>Status Code: <code>429</code> (Too Many Requests)</li>
<li>Open Ai error type: <code>server_error</code></li>
<li>Open Ai error message: <code>That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists.</code></li>
</ul>
<p>Open ai documentation states that a 429 error indicates that you have exceeded your rate limit which clearly I have not.
<a href=""https://help.openai.com/en/articles/6891829-error-code-429-rate-limit-reached-for-requests"" rel=""nofollow noreferrer"">https://help.openai.com/en/articles/6891829-error-code-429-rate-limit-reached-for-requests</a></p>
<p>The weird thing is the open ai error message is not stating that. It is giving the response I usually get from a <code>503</code> error (service unavailable).</p>
<p>I'd love to hear some thoughts on this, any theories, or if anyone else has been experiencing this.</p>
","2023-02-18 17:05:02","","","2023-02-19 08:54:14","<openai-api><gpt-3>","2","1","2","4773","","","","","","",""
"75913490","1","721998","","Conversational Bot with Flan-T5","<p>I am building a chat bot using flan-T5 model. The bot has a text window where one can give instructions like:</p>
<ul>
<li>Summarize this for me &quot;big text goes here&quot;</li>
</ul>
<p>Or, one might dump the text first in the chat window and then say</p>
<ul>
<li>Summarize the above text (or something similar to that)</li>
</ul>
<p>Or, one might dump a bunch of domain specific facts in the chat window and then ask questions about those.</p>
<p><strong>Question:</strong></p>
<ol>
<li>How can I form the context data for the bot so it has the knowledge about whatever info was passed to be before if it were to summarize something from before or answer quetsions from text that was passed before</li>
<li>How can I create a prompt which detects whether the intent is to <code>ASK QUESTION</code> or <code>CREATE SUMMARY</code> or just <code>INFO ADDITION</code> (in case we are just feeding info to use for asking questions or creating sumarry later.</li>
</ol>
","2023-04-02 17:10:21","","","2023-04-13 12:20:46","<chatbot><openai-api><gpt-3><conversational-ai>","0","0","1","307","","","","","","",""
"75924179","1","6421492","","Llama_index unexpected keyword argument error on ChatGPT Model Python","<p>I'm testing a couple of the widely published GPT models just trying to get my feet wet and I am running into an error that I cannot solve.</p>
<p>I am running this code:</p>
<pre><code>from llama_index import SimpleDirectoryReader, GPTListIndex, GPTSimpleVectorIndex, LLMPredictor, PromptHelper
from langchain import OpenAI
import gradio as gr
import sys
import os

os.environ[&quot;OPENAI_API_KEY&quot;] = 'MYKEY'

def construct_index(directory_path):
    max_input_size = 4096
    num_outputs = 512
    max_chunk_overlap = 20
    chunk_size_limit = 600

    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)

    llm_predictor_gpt = LLMPredictor(llm=OpenAI(temperature=0.7, model_name=&quot;text-davinci-003&quot;, max_tokens=num_outputs))

    documents = SimpleDirectoryReader(directory_path).load_data()

    index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor_gpt, prompt_helper=prompt_helper)

    index.save_to_disk('index.json')

    return index

def chatbot(input_text):
    index = GPTSimpleVectorIndex.load_from_disk('index.json')
    response = index.query(input_text, response_mode=&quot;compact&quot;)
    return response.response




iface = gr.Interface(fn=chatbot,
                     inputs=gr.inputs.Textbox(lines=7, label=&quot;Enter your text&quot;),
                     outputs=&quot;text&quot;,
                     title=&quot;Custom-trained AI Chatbot&quot;)

index = construct_index(&quot;salesdocs&quot;)
iface.launch(share=False)
</code></pre>
<p>And I keep getting this error</p>
<pre><code>  File &quot;C:\Users\Anonymous\anaconda3\lib\site-packages\llama_index\indices\vector_store\base.py&quot;, line 58, in __init__
    super().__init__(
TypeError: __init__() got an unexpected keyword argument 'llm_predictor'
</code></pre>
<p>Having a hard time finding much documentation on llamma index errors, hoping someone can point me in the right direction.</p>
","2023-04-03 22:27:15","","","2023-04-19 08:54:54","<python><openai-api><gpt-3><llama-index>","2","6","8","3059","","","","","","",""
"75942269","1","15478457","","How to generate gpt-3 completion beyond max token limit","<p>I want to ask if there's a way to properly use OpenAI API to generate complete responses even after the max token limit.
I'm using the official OpenAI python package but can't find any way to replicate that in GPT-3 (text-davinci-003) since it doesn't support chat interface.</p>
<p>My code for this is currently like this</p>
<pre><code>
response = openai.Completion.create(

        model=&quot;text-davinci-003&quot;,

        prompt=prompt,

        max_tokens=2049-len(prompt)

      )

      text = response.choices[0].text.strip()
</code></pre>
","2023-04-05 17:11:47","","2023-04-05 20:04:24","2023-04-05 20:04:24","<python><openai-api><gpt-3><chatgpt-api>","1","2","-1","1282","","","","","","",""
"75946090","1","8668935","","Why is GPT-4 giving different answers with same prompt & temperature=0?","<p>This is my code for calling the gpt-4 model:</p>
<pre><code>messages = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_msg},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: req}
]

response = openai.ChatCompletion.create(
        engine = &quot;******-gpt-4-32k&quot;,
        messages = messages,
        temperature=0,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0
    )

answer = response[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;]
</code></pre>
<p>Keeping system_msg &amp; req constant, with temperature=0, I get different answers.
I got 3 different answers when I last ran this 10 times for instance.
The answers are similar in concept, but differ in semantics.</p>
<p>Why is this happening?</p>
","2023-04-06 05:23:31","","","2023-04-07 05:28:42","<gpt-4>","1","0","0","492","","","","","","",""
"72294775","1","18244751","","How do I know how much tokens a GPT-3 request used?","<p>I am building an app around GPT-3, and I would like to know how much tokens every request I make uses. Is this possible and how ?</p>
","2022-05-18 19:11:58","","","2023-04-18 11:17:02","<gpt-3>","4","1","8","6982","","","","","","",""
"75586733","1","21300655","","ChatGPT Token Limit","<p>I want ChatGPT to remember past conversations and have a consistent (stateful) conversation.</p>
<p>I have seen several code of ChatGPT prompt engineering.</p>
<p>There were two ways to design the prompt shown below (pseudo code):</p>
<ol>
<li><p><strong>Use a single input</strong> (Cheap) &lt;- Better if possible</p>
</li>
<li><p><strong>Stack all of previous history</strong> (Expensive, Token Limitation)</p>
</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>def openai_chat(prompt):
    completions = openai.Completion.create(
        engine = &quot;text-davinci-003&quot;,
        prompt = prompt,
        max_tokens = 1024,
        n = 1,
        temperature = 0.8,
    )
    response = completions.choices[0].text.strip()
    return response

# 1. Use a single input
while True:
    prompt = input(&quot;User: &quot;)
    completion = openai_chat(prompt)

# 2. Stack all of previous history (prompt + completion)
prompt = &quot;&quot;
while True:
    cur_prompt = input(&quot;User: &quot;)
    prompt += cur_prompt  # pseudo code
    completion = openai_chat(prompt)
    prompt += completion  # pseudo code
</code></pre>
<p>Is it possible to choose the first way (the cheap one) to have a consistent conversation?</p>
<p>In other words, does ChatGPT remember past history even if the prompt only has the current input?</p>
","2023-02-28 00:06:49","","2023-04-15 02:54:00","2023-06-15 00:42:19","<text><nlp><prompt><openai-api><gpt-3>","5","1","15","16584","","","","","","",""
"75672816","1","11512643","","How does GPT-like transformers utilize only the decoder to do sequence generation?","<p>I want to code a GPT-like transformer for a specific text generation task. GPT-like models use only the decoder block (in stacks) <a href=""https://huggingface.co/course/chapter1/6?fw=pt"" rel=""nofollow noreferrer"">[1]</a>. I know how to code all sub-modules of the decoder block shown below (from the embedding to the softmax layer) in Pytorch. However, I don't know what I should give as input. It says (in the figure) &quot;Output shifted right&quot;.</p>
<p><a href=""https://i.stack.imgur.com/nV7Ee.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/nV7Ee.jpg"" alt=""enter image description here"" /></a></p>
<p>For example, this is my data, (where &lt; and &gt; are sos and eos tokens):</p>
<ul>
<li>&lt; abcdefgh &gt;</li>
</ul>
<p>What should I give to my GPT-like model to train it properly?</p>
<p>Also, since I am not using a encoder, should I still give input to the multihead attention block?</p>
<p>Sorry if my questions seem a little dumb, I am so new to transformers.</p>
","2023-03-08 12:04:35","","","2023-03-08 18:58:45","<deep-learning><pytorch><gpt-2><text-generation>","1","0","2","1528","","","","","","",""
"72418745","1","381436","","GPT-J and GPT-Neo generate too long sentences","<p>I trained a GPT-J and GPT-Neo models (fine tuning) on my texts and am trying to generate new text. But very often the sentences are very long (sometimes 300 characters each), although in the dataset the sentences are of normal length (50-100 characters usually). I tried a lot of things, changed, adjusted the temperature, top_k, but still half of the results with long phrases and I neen more short.</p>
<p>What can you try?</p>
<p>Here are long examples of generated results:</p>
<blockquote>
<ol>
<li><p>The support system that they have built has allowed us as users who
are not code programmers or IT administrators some ability to create
our own custom solutions without needing much programming experience
ourselves from scratch!</p>
</li>
<li><p>All it requires are documents about your inventory process but
I've found them helpful as they make sure you do everything right for
maximum efficiency because their knowledge base keeps reminding me
there's new ways i can be doing some things wrong since upgrading my
license so even though its good at finding errors with documentation
like an auditor may bring up later downline someone else might benefit
if those files dont exist anymore after one year when upgrades renews
automatically!</p>
</li>
</ol>
</blockquote>
","2022-05-28 19:38:28","","","2022-12-06 21:47:18","<text><artificial-intelligence><gpt-2><fine-tune>","1","0","2","614","","","","","","",""
"72467610","1","14272134","","OOM while fine-tuning medium sized model with DialoGPT on colab","<p>I am trying to finetune DialoGPT with a medium-sized model, I am getting Cuda error while the training phase, I reduced the batch size from 4, but still, the error persists. My parameters are</p>
<pre><code>        #self.output_dir = 'output-small'
        self.output_dir = 'output-medium'
        self.model_type = 'gpt2'
        #self.model_name_or_path = 'microsoft/DialoGPT-small'
        self.model_name_or_path = 'microsoft/DialoGPT-medium'
        #self.config_name = 'microsoft/DialoGPT-small'
        self.config_name = 'microsoft/DialoGPT-medium'
        #self.tokenizer_name = 'microsoft/DialoGPT-small'
        self.tokenizer_name = 'microsoft/DialoGPT-medium'
        self.cache_dir = 'cached'
        self.block_size = 512
        self.do_train = True
        self.do_eval = True
        self.evaluate_during_training = False
        self.per_gpu_train_batch_size = 2
        self.per_gpu_eval_batch_size = 2
        self.gradient_accumulation_steps = 1
        self.learning_rate = 5e-5
        self.weight_decay = 0.0
        self.adam_epsilon = 1e-8
        self.max_grad_norm = 1.0
        self.num_train_epochs = 5
        self.max_steps = -1
        self.warmup_steps = 0
        self.logging_steps = 1000
        self.save_steps = 3500
        self.save_total_limit = None
        self.eval_all_checkpoints = False
        self.no_cuda = False
        self.overwrite_output_dir = True
        self.overwrite_cache = True
        self.should_continue = False
        self.seed = 42
        self.local_rank = -1
        self.fp16 = False
        self.fp16_opt_level = 'O1'
</code></pre>
<p>The GPU allocated is Tesla P100-PCIE with 16GB memory.
Please kindly let me know how to resolve this issue. Any suggestion is appreciated.</p>
","2022-06-01 20:20:08","","2022-06-01 22:52:21","2022-08-21 20:16:32","<google-colaboratory><huggingface-transformers><language-model><gpt-2>","1","0","0","306","","","","","","",""
"73770730","1","10456211","","How can I implement text classification for the purpose of matching using GPT-3?","<p>I have tried fine tuning a GPT-3 model for the purpose of text classification to classify whether two names match such as 'William Jonathan' and 'William J' and the label would be yes/no, yes indicating that two names are matching and no indicating that they aren't. I have created a large number of examples related to different scenarios such as names being spelled differently, abbreviations, missing token, etc. After fine-tuning the model on GPT-3 using examples that look like this with a jsonl format:</p>
<p><code>{&quot;text&quot;: &quot;Are the following two names the same?\nWilliam Jonathan\nWilliam J&quot;, &quot;label&quot;: &quot;Yes&quot;}</code></p>
<p>It is not able to do binary classification, however it outputs a large number of labels next to each other, for instance:</p>
<p>Prompt: Are the following two names the same?\nWilliam Jonathan \nWilliam J</p>
<p>Completion: YesYesNoYesNoYesYesNoYesNoYesNoYesNoYesYes</p>
<p>Any idea on how I can perform binary text classification using GPT-3 on an example similar to the above?</p>
","2022-09-19 08:44:29","","","2022-09-19 08:44:29","<text><classification><matching><openai-api><gpt-3>","0","1","0","34","","","","","","",""
"73919757","1","1743703","","GPT-3 Twitter Bot","<p>I'm trying to fine-tune a GPT-3 model on my tweets. I want the model to generate tweets with no prompt. Is it possible?</p>
<p>The dataset is reqired to have &quot;prompt&quot; and &quot;completion&quot; columns. Do I just take first couple of words of each tweet and make it a prompt?</p>
","2022-10-01 15:54:03","","","2023-01-10 07:20:50","<openai-api><gpt-3>","2","1","1","459","","","","","","",""
"74000154","1","1461800","","OpenAI GPT-3 API: How do I make sure answers are from a customized (fine-tuning) dataset?","<p>I'm using customized text with 'Prompt' and 'Completion' to train new model.</p>
<p>Here's the tutorial I used to create customized model from my data:</p>
<p><a href=""https://beta.openai.com/docs/guides/fine-tuning/advanced-usage"" rel=""nofollow noreferrer"">beta.openai.com/docs/guides/fine-tuning/advanced-usage</a></p>
<p>However even after training the model and sending prompt text to the model, I'm still getting generic results which are not always suitable for me.</p>
<p>How I can make sure completion results for my prompts will be only from the text I used for the model and not from the generic OpenAI models?</p>
<p>Can I use some flags to eliminate results from generic models?</p>
","2022-10-08 19:58:07","","2023-05-05 21:06:56","2023-06-21 19:51:43","<nlp><customization><openai-api><gpt-3><fine-tune>","1","0","7","4521","","","","","","",""
"75946877","1","11487426","","How to fix ""TypeError: dataclass_transform() got an unexpected keyword argument 'field_specifiers'"" when using gpt-index/llama-index?","<p>I am trying to use gpt-index/llama-index to feed ChatGPT with custom data to build a custom chatbot. When I try to import either gpt-index or llama-index to Jupyter, I get the following error.</p>
<p><a href=""https://i.stack.imgur.com/LByAn.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LByAn.png"" alt=""Error Message"" /></a></p>
<p>I have tried uninstalling and reinstalling, but the problem persists.</p>
<p>I am using Python 3.9.16 on Jupyter Notebook 6.4.8</p>
","2023-04-06 07:27:37","","","2023-04-06 07:27:37","<python-3.x><jupyter-notebook><openai-api><gpt-3><chatgpt-api>","0","0","0","175","","","","","","",""
"75952444","1","20038123","","Huggingface Transformers (PyTorch) - Custom training loop doubles speed?","<p>I've found something quite strange when using Huggingface Transformers with a custom training loop in PyTorch.</p>
<p>But first, some context: I'm currently trying to fine tune a pretrained GPT2 small (GPT2LMHeadModel; the ~170M param version) on multiple nodes, using Huggingface Accelerate. I'm using Huggingface's <code>datasets</code> library for training.</p>
<p>Of course, the first step in this process in accelerate is to write a custom PyTorch training loop, which I did with the help of <a href=""https://www.youtube.com/watch?v=Dh9CL8fyG80&amp;embeds_euri=https%3A%2F%2Fhuggingface.co%2F&amp;feature=emb_title"" rel=""nofollow noreferrer"">the official tutorial from huggingface</a>. Naturally, I decided to test the model with this new training loop before implementing accelerate to ensure it actually worked.</p>
<p>Here's the relevant code from my original model, as well as the corresponding code from the new training loop:</p>
<p><em>Note: <code>BATCH_SIZE</code> is equal to 2 in both models. All code not shown is exactly the same between both models.</em></p>
<p>Original:</p>
<pre><code>data = data['train']

dc = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)

train_args = TrainingArguments(
    output_dir=OUTPUT_DIRECTORY,
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=BATCH_SIZE,
    save_steps=10_000,
    save_total_limit=1, # How many &quot;checkpoints&quot; to save at a time
    prediction_loss_only=True,
    remove_unused_columns=False,
    optim=&quot;adamw_torch&quot;
)

trainer = Trainer(
    model=model,
    args=train_args,
    data_collator=dc,
    train_dataset=data
)

trainer.train()
</code></pre>
<p>Custom Train Loop:</p>
<pre><code>data = data['train']

dc = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)

optimizer = AdamW(model.parameters(), lr=5e-5)

train_dl = DataLoader(
    data, shuffle=True, batch_size=BATCH_SIZE, collate_fn=dc
)

epochs = 1
training_steps = epochs * len(train_dl)
scheduler = get_scheduler(
    &quot;linear&quot;,
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=training_steps
)

progress_bar = tqdm(range(training_steps))

model.train()
for epoch in range(epochs):
    for batch in train_dl:
        # Run a batch through the model
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
</code></pre>
<p>I tested it (with one node of course) with two GPUs, both 16GB each. And it worked... but suspiciously well.</p>
<ul>
<li>My original model averaged about 1-2 iterations/s.</li>
<li>My custom loop on the other hand averaged about 3-4 iterations/s.</li>
</ul>
<p>This is absolutely bizarre. How is it possible that simply adding my own training loop, that's just a couple of lines of code, is not only faster than the official one provided by Huggingface - but nearly TWICE as fast? Did I write the training loop incorrectly? Am I completely missing something here?</p>
","2023-04-06 17:58:30","","","2023-04-25 23:53:02","<pytorch><huggingface-transformers><huggingface><gpt-2><huggingface-datasets>","1","0","0","209","","","","","","",""
"75501276","1","3186094","","OpenAI GPT-3 API: How to make a model remember past conversations?","<p>Is there a way to train a <em>Large Language Model</em> (LLM) to store a specific context? For example, I had a long story I want to ask questions about, but I don't want to put the whole story in every prompt. How can I make the LLM &quot;remember the story&quot;?</p>
","2023-02-19 15:38:07","","2023-03-13 14:40:05","2023-04-10 10:38:36","<openai-api><gpt-3>","1","2","3","3916","","","","","","",""
"73338768","1","15256429","","How can I keep my discord bot from remembering old messages? (NodeJS)","<p>I've been working on a discord bot that uses OpenAI to answer questions or chat. However, it seems to be bringing in previous messages to the prompt it grabs, and it eventually causes it to repeat the same answer over and over again (even after adjusting the anti-repetition values to the max).</p>
<p>What would be a way to make it only keep the message it read into account instead of remembering previous messages?</p>
<p>Code:</p>
<pre class=""lang-js prettyprint-override""><code>require('dotenv').config();
    const { Client, GatewayIntentBits } = require('discord.js');
    const client = new Client({ intents: [GatewayIntentBits.Guilds, GatewayIntentBits.GuildMessages, GatewayIntentBits.MessageContent] });
    const { Configuration, OpenAIApi } = require(&quot;openai&quot;);
    const configuration = new Configuration({
      apiKey: process.env.OPENAI_API_KEY,
    });
    const openai = new OpenAIApi(configuration);

    let prompt =`Marv is a chatbot that reluctantly answers questions with sarcastic responses:\n\nYou: How many pounds are in a kilogram?\nMarv: This again? There are 2.2 pounds in a kilogram. Please make a note of this.\nYou: What does HTML stand for?\nMarv: Was Google too busy? Hypertext Markup Language. The T is for try to ask better questions in the future.\nYou: When did the first airplane fly?\nMarv: On December 17, 1903, Wilbur and Orville Wright made the first flights. I wish they’d come and take me away.\nYou: What is the meaning of life?\nMarv: I’m not sure. I’ll ask my friend Google.\nYou: What time is it?\nMarv:`;

    client.on(&quot;messageCreate&quot;, function(message) {
    if (message.author.bot) return;
       prompt += `You: ${message.content}\n`;
      (async () =&gt; {
            const gptResponse = await openai.createCompletion({
                model: &quot;text-davinci-002&quot;,
                prompt: prompt,
                max_tokens: 400,
                temperature: 0.7,
                top_p: 1.0,
                presence_penalty: 0,
                frequency_penalty: 0.7,
              });
            message.reply(`${gptResponse.data.choices[0].text.substring(5)}`);
            prompt += `${gptResponse.data.choices[0].text}\n`;
        })();
    });

    client.login(process.env.BOT_TOKEN);
</code></pre>
","2022-08-12 19:10:08","","2022-08-13 07:29:44","2022-08-13 07:29:44","<node.js><discord><discord.js><gpt-3>","0","0","0","210","0","","","","","",""
"75555647","1","11375592","","Train GPT-2 on custom data","<p>I was looking for a way to train my own textual data using GPT-2 &amp; I have found a blog post here: <a href=""https://www.kaggle.com/code/ashiqabdulkhader/train-gpt-2-on-custom-language"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/ashiqabdulkhader/train-gpt-2-on-custom-language</a></p>
<p>Everything works fine, the model building, dataset building, but it shows very weird texts as output...</p>
<pre><code>from transformers import TFGPT2LMHeadModel, GPT2Tokenizer

output_dir = &quot;kaggle/working/gpt2&quot;
tokenizer = GPT2Tokenizer.from_pretrained(output_dir)
model = TFGPT2LMHeadModel.from_pretrained(output_dir, pad_token_id=tokenizer.eos_token_id)

text = &quot;what is python?&quot;
input_ids = tokenizer.encode(text, return_tensors='tf')
beam_output = model.generate(
 input_ids,
 max_length=50,
 num_beams=5,
 temperature=0.7,
 no_repeat_ngram_size=2,
 num_return_sequences=5
)

print(tokenizer.decode(beam_output[0], skip_special_tokens=True))
</code></pre>
<p>My corpus data is:</p>
<pre class=""lang-none prettyprint-override""><code>Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.[33]

Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a &quot;batteries included&quot; language due to its comprehensive standard library.[34][35]

Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0.[36] Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.[37]

Python consistently ranks as one of the most popular programming languages.[38][39][40][41]
</code></pre>
<p>taken from Wikipedia.</p>
<p>The output: <code>what is python 202gragra 202 202 language 202] 202astast 202abilityability 2027ely 202uralural 202ralral Rossum Rossum 202use 202 as Rossumability code Rossum] Rossumifi Rossumleas Rossumast Rossum80</code></p>
<p>It also says:</p>
<blockquote>
<p>You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</p>
</blockquote>
<p>I am completely new to this NLP section... what is the issue here?</p>
","2023-02-24 10:50:59","","2023-02-24 16:09:13","2023-02-24 16:09:13","<python><nlp><gpt-2>","0","0","0","518","","","","","","",""
"75970356","1","354067","","Comparing methods for a QA system on a 1,000-document Markdown dataset: Indexes and embeddings with GPT-4 vs. retraining GPT4ALL (or similar)","<p>I am working on a project to build a question-answering system for a documentation portal containing over 1,000 Markdown documents, with each document consisting of approximately 2,000-4,000 tokens.</p>
<p>I am considering the following two options:</p>
<ol>
<li>Using indexes and embeddings with GPT-4</li>
<li>Retraining a model like GPT4ALL (or a similar model) to specifically handle my dataset</li>
</ol>
<p>Which of these approaches is more likely to produce better results for my use case?</p>
","2023-04-09 11:58:57","","2023-05-09 09:08:55","2023-05-09 09:08:55","<deep-learning><openai-api><gpt-4><large-language-model><gpt4all>","1","2","3","214","","","","","","",""
"75974345","1","5282493","","What is language model will fit in my 7950x 16core & rtx3090 with 64 gb of ddr 5 ram","<p>What is the best language model fits my hardware with longer context. Can any one guide me</p>
<p>(Noobe)</p>
","2023-04-10 04:54:50","","","2023-04-10 04:54:50","<nlp><gpt-3><alpaca>","0","0","0","19","","","","","","",""
"76053920","1","14031645","","How do I extract only code content from chat gpt response?","<p>I use <code>chatGpt</code> to generate SQL query using <code>openai</code> api(<code>/v1/chat/completions</code>) and <code>gpt-3.5-turbo</code> as the model.</p>
<p>But I am facing difficulty in extracting SQL query from the response. Because sometime chatGpt will provide some explanation for query sometimes not. I have tried with regex expressions, but it is not reliable.</p>
<pre><code>regex = r&quot;SELECT .*?;&quot;
match = re.search(regex, result)
if match:
   sql_query = match.group()
   print(sql_query)
</code></pre>
<p>Is there any other approach to extract only the code section from the response?</p>
","2023-04-19 11:25:11","","","2023-05-05 22:21:57","<sql><code-generation><openai-api><gpt-3><chatgpt-api>","2","4","0","1367","","","","","","",""
"76101635","1","12902027","","Kaggle Code doesn't download ""gpt2"" language model","<p>I am using kaggle code to download gpt2 language model.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModelForCausalLM

device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
model_name = &quot;gpt2-xl&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
</code></pre>
<p>Intend to download the gpt2-xl model from the huggingface hub.
But the last line raised LocalEntryNotFoundError.
The detais are below.</p>
<blockquote>
<p>LocalEntryNotFoundError                   Traceback (most recent call last)</p>
</blockquote>
<blockquote>
<p>/opt/conda/lib/python3.7/site-packages/transformers/utils/hub.py in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)
419             use_auth_token=use_auth_token,
--&gt; 420             local_files_only=local_files_only,
421         )</p>
</blockquote>
<blockquote>
<p>OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like gpt2-xl is not the path to a directory containing a file named config.json.</p>
</blockquote>
<blockquote>
<p>Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.</p>
</blockquote>
<p>Doesn't seem that kaggle code connects to the huggingface hub.
Why does this happen and how can I fix this error?</p>
","2023-04-25 13:20:15","","","2023-04-25 13:20:15","<python><huggingface-transformers><kaggle><gpt-2>","1","0","0","43","","","","","","",""
"76199709","1","10687259","","Image to text using Azure OpenAI GPT4","<p>I have an Azure open AI Account and GPT4 model deployed. Can I use its API for image-to-text description? If yes, how I will give it the image? I am using this code. But it throws me an error.</p>
<pre><code>import openai
# open ai key
openai.api_type = &quot;azure&quot;
openai.api_version = &quot;2023-03-15-preview&quot;
openai.api_base = 'https://xxxxxx.openai.azure.com/'
openai.api_key = &quot;xxxxxxxxxxxxx&quot;

image_url=&quot;https://cdn.repliers.io/IMG-X5925532_9.jpg&quot;

def generate_image_description(image_url):
    prompt = f&quot;What is in this image? {image_url}&quot;
    print(prompt)
    response = openai.ChatCompletion.create(
        engine=&quot;GPT4v0314&quot;,
        prompt=prompt,
        max_tokens=1024,
        n=1,
        stop=None,
        temperature=0.0,
    )
    description = response.choices[0].text.strip()
    return description
</code></pre>
<p>The error is like;
APIError: Invalid response object from API: 'Unsupported data type\n' (HTTP response code was 400)</p>
<p>I mentioned it inside the explanation.</p>
","2023-05-08 10:32:50","","","2023-05-31 10:54:16","<python><openai-api><azure-openai><gpt-4>","1","0","2","234","","","","","","",""
"76222070","1","1354514","","Getting an error while trying to train my model in train_function (Empty Logs)","<p>I am trying to train a GPT2 model on the wikipedia text.  While doing so I get the following error:</p>
<pre><code>ValueError: Unexpected result of `train_function` (Empty logs). Please use 
`Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.
</code></pre>
<p>The error happens when my code calls <code>history = model.fit(dataset, epochs=num_epoch)</code></p>
<p>My code is below:</p>
<pre class=""lang-py prettyprint-override""><code>from tokenise import BPE_token
from pathlib import Path
import tensorflow as tf
from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer
import os

# the folder 'text' contains all the files
paths = [str(x) for x in Path(&quot;./text/&quot;).glob(&quot;**/*.txt&quot;)]
tokenizer = BPE_token()

# train the tokenizer model
tokenizer.bpe_train(paths)

# saving the tokenized data in our specified folder 
save_path = 'tokenized_data'
tokenizer.save_tokenizer(save_path)

# loading tokenizer from the saved model path
tokenizer = GPT2Tokenizer.from_pretrained(save_path)
tokenizer.add_special_tokens({
  &quot;eos_token&quot;: &quot;&lt;/s&gt;&quot;,
  &quot;bos_token&quot;: &quot;&lt;s&gt;&quot;,
  &quot;unk_token&quot;: &quot;&lt;unk&gt;&quot;,
  &quot;pad_token&quot;: &quot;&lt;pad&gt;&quot;,
  &quot;mask_token&quot;: &quot;&lt;mask&gt;&quot;
})

# creating the configurations from which the model can be made
config = GPT2Config(
  vocab_size=tokenizer.vocab_size,
  bos_token_id=tokenizer.bos_token_id,
  eos_token_id=tokenizer.eos_token_id
)

# creating the model
model = TFGPT2LMHeadModel(config)

# We also create a single string from all our documents and tokenize it.

single_string = ''
for filename in paths:
  with open(filename, &quot;r&quot;, encoding='utf-8') as f:
   x = f.read()
  single_string += x + tokenizer.eos_token
string_tokenized = tokenizer.encode(single_string)

examples = []
block_size = 100
BATCH_SIZE = 12
BUFFER_SIZE = 1000

for i in range(0, len(string_tokenized) - block_size + 1, block_size):
  examples.append(string_tokenized[i:i + block_size])

inputs, labels = [], []
for ex in examples:
  inputs.append(ex[:-1])
  labels.append(ex[1:])

dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))
dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

# Model Training

# defining our optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)

# definining our loss function
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

# defining our metric which we want to observe
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

# compiling the model
model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], \
  metrics=[metric], run_eagerly=True)

# Now, let’s train the model
num_epoch = 10
history = model.fit(dataset, epochs=num_epoch)
</code></pre>
","2023-05-10 19:42:56","","2023-05-11 01:30:23","2023-05-11 01:30:23","<tensorflow><machine-learning><keras><huggingface-transformers><gpt-2>","0","0","0","107","","","","","","",""
"76233070","1","14587120","","Generic Answer when Fine Tuning OpenAI Model","<p>I have prepared a dataset and trained a <strong>davinci</strong> model using <a href=""https://platform.openai.com/docs/guides/fine-tuning"" rel=""nofollow noreferrer"">FineTuning</a>. It gives out the correct answer for any variant of questions that exist in the dataset.</p>
<p>But how to fine tune the model to give out something like a &quot;Sorry I do not know the answer to this question&quot;, if we ask anything not in the dataset? For example if I ask &quot;Where was the 2020 Olympics hosted?&quot;, it should give out a generic &quot;Do Not Know&quot; answer, as this question does not exist in the dataset.</p>
","2023-05-12 04:10:33","","2023-05-12 04:15:38","2023-05-15 07:01:52","<openai-api><gpt-3><chatgpt-api><text-davinci-003>","0","0","1","58","","","","","","",""
"76255342","1","1019129","","Figuring out general specs for running LLM models","<p>I have three questions :</p>
<p>Given count of LLM parameters in Billions, how can you figure how much GPU RAM do you need to run the model ?</p>
<p>If you have enough CPU-RAM (i.e. no GPU) can you run the model, even if it is slow</p>
<p>Can you run LLM models (like h2ogpt, open-assistant) in mixed GPU-RAM and CPU-RAM ?</p>
","2023-05-15 14:57:12","","","2023-05-18 08:11:23","<deep-learning><artificial-intelligence><gpt-3><large-language-model>","1","0","1","642","","","","","","",""
"76266379","1","10225070","","fine tune gpt3 with tabular data","<p>As the title says, how would one fine tune a LLM with tabular data? My initial sense is that LLM are not suited to learn from tabular data unless the tabular data is restructured into grammatical form, and even then, I have reservations about this approach.</p>
<p>Basically the idea is to be able to ask questions specific to the domain of the data, and the responses include specific datapoints from the database.</p>
<p>Thoughts?</p>
","2023-05-16 19:12:07","","","2023-05-16 19:12:07","<openai-api><gpt-3><fine-tune><large-language-model>","0","0","0","87","","","","","","",""
"76269666","1","16154213","","I ran into an error when I try to use YoutubeLoader.from_youtube_url","<p>There is my code snippet</p>
<pre><code>import os,openai

from langchain.document_loaders import YoutubeLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ChatVectorDBChain,ConversationalRetrievalChain

from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate
)
os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;apikey&quot;
loader = YoutubeLoader.from_youtube_url(youtube_url=&quot;https://www.youtube.com/watch?v=7OPg-ksxZ4Y&quot;,add_video_info=True)
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 300,
    chunk_overlap = 20
)

documents = text_splitter.split_documents(documents)
#print(documents)

embeddings = OpenAIEmbeddings()
vector_store = Chroma.from_documents(documents=documents,embedding=embeddings)
retriever = vector_store.as_retriever()

system_template = &quot;&quot;&quot;
Use the following context to answer the user's question.
If you don't know the answer, say you don't, don't try to make it up. And answer in Chinese.
-----------
{context}
-----------
{chat_history}
&quot;&quot;&quot;

messages  =[
    SystemMessagePromptTemplate.from_template(system_template),
    HumanMessagePromptTemplate.from_template('{question}')
]

prompt = ChatPromptTemplate.from_messages(messages)

qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(temperature=0.1,max_tokens=2048),retriever,qa_prompt=prompt)
chat_history = []
while True:
    question = input('问题:')
    result = qa({'question':question,'chat_history':chat_history})
    chat_history.append((question,result['answer']))
    print(result['answer'])
</code></pre>
<p>and there is the detail of the error</p>
<pre><code>PS C:\Users\12875\Desktop\新建文件夹&gt; &amp; E:/Program/python/python.exe c:/Users/12875/Desktop/新建文件夹/分析youtube视频.py
Using embedded DuckDB without persistence: data will be transient
Traceback (most recent call last):
  File &quot;c:\Users\12875\Desktop\新建文件夹\分析youtube视频.py&quot;, line 30, in &lt;module&gt;
    vector_store = Chroma.from_documents(documents=documents,embedding=embeddings)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;E:\Program\python\Lib\site-packages\langchain\vectorstores\chroma.py&quot;, line 412, in from_documents
    return cls.from_texts(
           ^^^^^^^^^^^^^^^
  File &quot;E:\Program\python\Lib\site-packages\langchain\vectorstores\chroma.py&quot;, line 380, in from_texts
    chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)
  File &quot;E:\Program\python\Lib\site-packages\langchain\vectorstores\chroma.py&quot;, line 159, in add_texts 
    self._collection.add(
  File &quot;E:\Program\python\Lib\site-packages\chromadb\api\models\Collection.py&quot;, line 84, in add       
    metadatas = validate_metadatas(maybe_cast_one_to_many(metadatas)) if metadatas else None
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;E:\Program\python\Lib\site-packages\chromadb\api\types.py&quot;, line 107, in validate_metadatas   
    validate_metadata(metadata)
  File &quot;E:\Program\python\Lib\site-packages\chromadb\api\types.py&quot;, line 98, in validate_metadata     
    raise ValueError(f&quot;Expected metadata value to be a str, int, or float, got {value}&quot;)
ValueError: Expected metadata value to be a str, int, or float, got None
</code></pre>
<p>The class YoutubeLoader recently updated one of the methods from from_youtube_channel to from_youtube_url.But when i use the from_youtube_url,i happened the error that  &quot;ValueError: Expected metadata value to be a str, int, or float, got None&quot;.I want to know what should I do，thank you!</p>
","2023-05-17 07:45:48","","2023-05-17 14:15:11","2023-06-04 10:45:26","<python><youtube-api><openai-api><gpt-3><langchain>","1","8","0","273","","","","","","",""
"76345550","1","13605626","","I want to ask about llama_index, the response take too long when get full and get truncated when not","<p>I want to ask about llama_index, I use Vietnamese, the response take too long (about 30 seconds) when get full response and get truncated when not:</p>
<pre><code>from llama_index import SimpleDirectoryReader, ResponseSynthesizer, LangchainEmbedding, JSONReader, GPTListIndex, ServiceContext, GPTVectorStoreIndex, LLMPredictor, PromptHelper, SimpleMongoReader, StorageContext, load_index_from_storage
from langchain.chat_models import ChatOpenAI
import gradio as gr
import sys
import os
from pymongo import MongoClient
from llama_index.retrievers import VectorIndexRetriever
from llama_index.query_engine import RetrieverQueryEngine
from llama_index.indices.postprocessor import SimilarityPostprocessor
from llama_index.llm_predictor.chatgpt import ChatGPTLLMPredictor

os.environ[&quot;OPENAI_API_KEY&quot;] = 'api-key'

max_input_size = 4096
num_outputs = 512
max_chunk_overlap = 20
chunk_size_limit = 600

prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)

llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=&quot;gpt-3.5-turbo&quot;, max_tokens=num_outputs))

service_context = ServiceContext.from_defaults(
        llm_predictor = llm_predictor,
        prompt_helper = prompt_helper)

def construct_index(directory_path):
    documents = SimpleDirectoryReader(directory_path).load_data()
    index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)

    index.storage_context.persist(persist_dir='./storage')

    return index

def chatbot(input_text):
    # rebuild storage context
    storage_context = StorageContext.from_defaults(persist_dir='./storage')
    
    # load index
    index = load_index_from_storage(storage_context = storage_context, 
    service_context = service_context)

    # configure retriever
    retriever = VectorIndexRetriever(
        index=index, 
        similarity_top_k=2,
    )
    
    # configure response synthesizer
    response_synthesizer = ResponseSynthesizer.from_args(
        node_postprocessors=[
            SimilarityPostprocessor(similarity_cutoff=0.7)
        ]
    )
    
    # assemble query engine
    query_engine = RetrieverQueryEngine(
        retriever=retriever,
        response_synthesizer=response_synthesizer,
    )

    fullResponse = ''
    while True:
        resp = query_engine.query(input_text + '\n\n' + fullResponse)
        if resp.response != &quot;Empty Response&quot;:
            fullResponse += resp.response
        else:
            break
    return fullResponse

iface = gr.Interface(fn=chatbot,
                     inputs=gr.components.Textbox(lines=7, label=&quot;Enter your text&quot;),
                     outputs=&quot;text&quot;,
                     title=&quot;Custom-trained AI Chatbot&quot;)

index = construct_index(&quot;docs&quot;)
iface.launch(share=True)
</code></pre>
<p>I have tried to find some errors here, but with no result. When I try the old version of gpt_index (llama_index) (0.4.24), it runs fine, however, I also want to use with Mongo. Can someone help me with this? Thanks a lot!!!</p>
","2023-05-27 06:03:36","","2023-05-27 09:05:05","2023-05-27 09:05:05","<python><gpt-3><chatgpt-api><llama-index>","0","0","-2","65","","","","","","",""
"74255038","1","982636","","What's your approach to extracting a summarized paragraph from multiple articles using GPT-3?","<p>In the following scenario, what's your best approach using GPT-3 API?</p>
<ol>
<li>You need to come out with a short paragraph, about a <strong>specific subject</strong></li>
<li>You must base your paragraph on a set of articles, 3-6 articles, written in an unknown structure</li>
</ol>
<p>Here is what I found to work well:</p>
<ol>
<li>The main constraint is the open ai token limit in the prompt</li>
<li>Due to the constraint, I'd ask OPT-3 to parse unstructured data using the specific subject in the prompt request.</li>
<li>I'll then iterate each article and save it all into 1 string variable</li>
<li>Then, repeat it one last time but using the new string variable</li>
<li>If the article is too long, I'll cut it into smaller chunks</li>
<li>Of curse fine-tune, the model with the specific subject before will produce much better results</li>
<li>The <code>temperature</code> should be set to <code>0</code>, to make sure GPT-3 uses only facts from the data source.</li>
</ol>
<p>Example:
Let's say I want to write a paragraph about Subject A, Subject B, and  Subject C. And I have 5 articles as references.
The open ai playground will look something like this:</p>
<pre><code>Example Article 1
----
Subject A: example A for OPT-3
Subject B: n/a
Subject c: n/a
=========
Example Article 2
----
Subject A: n/a
Subject B: example B for GPT-3
Subject C: n/a
=========
Example Article 3
----
Subject A: n/a
Subject B: n/a
Subject c: example for GPT-3
=========
Article 1
-----
Subject A:
Subject B:
Subject C:
=========
... repeating with all articles, save to str
=========
str
-----
Subject A:
Subject B:
Subject C:
</code></pre>
","2022-10-30 17:00:45","","2023-01-17 04:32:06","2023-01-17 04:32:06","<machine-learning><nlp><summarization><openai-api><gpt-3>","1","0","1","134","","","","","","",""
"75329518","1","1479849","","Fine Tuning GPT-3 for Consistent Output Format","<p>I am trying to use Open AI API to create quiz questions with three incorrect answers and one correct answer. The prompt I use is</p>
<pre><code>`Write a quiz on ${quiz_topic} with 5 questions. Each question has 3 incorrect answers and 1 correct answer. The correct answer is always the last answer. Write each answer on a separate line`
</code></pre>
<p>I would like to parse ChatGPT's answer but the format is inconsistent. For instance, is quiz_topic is &quot;literature&quot;, I get the following response:</p>
<pre><code>Q1. Who wrote the novel &quot;The Catcher in the Rye&quot;?

A. J.D. Salinger
B. Ernest Hemingway
C. F. Scott Fitzgerald
D. Mark Twain
</code></pre>
<p>This is the correct format. But if I use &quot;U.S History&quot;, I get the following output:</p>
<pre><code>Q1: Who was the first president of the United States?

A1: George Washington Jr., Thomas Jefferson, John Adams, George Washington 
</code></pre>
<p>This is a different format. So my goal is to fine-tune it using the article <a href=""https://platform.openai.com/docs/guides/fine-tuning"" rel=""nofollow noreferrer"">here</a></p>
<p>I have created 200 line of a training set. Here is a sample:</p>
<pre><code>{&quot;prompt&quot;:&quot;Write a quiz on general knowledge with 5 questions. Each question has 3 incorrect answers and 1 correct answer. The correct answer is always the last answer.Write each answer on a separate line \n\n---\n\n&quot;,&quot;completion&quot;:&quot; Q. What is the Italian word for &amp;quot;tomato&amp;quot;?\nA. Aglio\nA. Cipolla\nA. Peperoncino\nA. Pomodoro\n\nQ. Which restaurant&amp;#039;s mascot is a clown?\nA. Whataburger\nA. Burger King\nA. Sonic\nA. McDonald&amp;#039;s\n\nQ. Which of these colours is NOT featured in the logo for Google?\nA. Yellow\nA. Blue\nA. Green\nA. Pink\n\nQ. In 2013 how much money was lost by Nigerian scams?\nA. $95 Million\nA. $956 Million\nA. $2.7 Billion\nA. $12.7 Billion\n\nQ. What is the name of Poland in Polish?\nA. Pupcia\nA. Polszka\nA. P&amp;oacute;land\nA. Polska\n\n \n\n###\n\n&quot;}
{&quot;prompt&quot;:&quot;Write a quiz on books with 5 questions. Each question has 3 incorrect answers and 1 correct answer. The correct answer is always the last answer.Write each answer on a separate line \n\n---\n\n&quot;,&quot;completion&quot;:&quot; Q. What is the name of Eragon&amp;#039;s dragon in &amp;quot;Eragon&amp;quot;?\nA. Glaedr\nA. Thorn\nA. Arya\nA. Saphira\n\nQ. In the &amp;quot;The Hobbit&amp;quot;, who kills Smaug?\nA. Bilbo Baggins\nA. Gandalf the Grey\nA. Frodo\nA. Bard\n\nQ. What is Hermione Granger&amp;#039;s middle name?\nA. Jane\nA. Emma\nA. Jo\nA. Jean\n\nQ. According to The Hitchhiker&amp;#039;s Guide to the Galaxy book, the answer to life, the universe and everything else is...\nA. Loving everyone around you\nA. Chocolate\nA. Death\nA. 42\n\nQ. What is the name of the three headed dog in Harry Potter and the Sorcerer&amp;#039;s Stone?\nA. Spike\nA. Poofy\nA. Spot\nA. Fluffy\n\n \n\n###\n\n&quot;}
</code></pre>
<p>When I run the validation tool with the command</p>
<pre><code>openai tools fine_tunes.prepare_data -f training.jsonl
</code></pre>
<p>I get the following message</p>
<pre><code>- All prompts start with prefix `Write a quiz on `. Fine-tuning doesn't require the instruction specifying the task, or a few-shot example scenario. Most of the time you should only add the input data into the prompt, and the desired output into the completion
</code></pre>
<p>I don't understand why I must remove &quot;Write a quiz on&quot;. So I have misunderstood how to fine-tune a model for consistent formatting.
Can anybody shed a light on how to make sure I get the same formatting with the same prompt</p>
","2023-02-02 22:18:50","","","2023-03-01 14:01:41","<openai-api><gpt-3>","1","0","1","937","","","","","","",""
"76376524","1","10229072","","Expected input batch_size (28) to match target batch_size (456), Changing batch size increase the target batch size with GPT2 model","<p>I was practising fine-tuning a gpt2 model on a simple question-answer dataset when I encountered this error. I have studied other answers, but my input dataset shapes look fine.</p>
<pre><code>def tokenize_data(total_marks, coding_feeddback):
    inputs = tokenizer(total_marks, truncation=True, padding=True, 
                                                     return_tensors=&quot;pt&quot;)
    labels = tokenizer(coding_feeddback, truncation=True, padding=True, 
                                                    return_tensors=&quot;pt&quot;)['input_ids']
return inputs, labels

*# Prepare the training and validation datasets*

 train_inputs, train_labels = tokenize_data(train_df['Question'].tolist(), 
 train_df['ans'].tolist())
 val_inputs, val_labels = tokenize_data(val_df['Question'].tolist(), 
 val_df['ans'].tolist())

 train_dataset = TensorDataset(train_inputs['input_ids'], train_labels)
 val_dataset = TensorDataset(val_inputs['input_ids'], val_labels)
</code></pre>
<blockquote>
<p><em>Here is the size of the train and validation dataset</em></p>
</blockquote>
<pre><code>print('train input shape:',train_inputs['input_ids'].shape)
print('train label shape: ',train_labels.shape)
print('validation input shape: ',val_inputs['input_ids'].shape)
print('validation label shape: ',val_labels.shape)
</code></pre>
<blockquote>
<p><em>The output of the above lines is as follows.</em></p>
</blockquote>
<pre><code>train input shape: torch.Size([76, 8])
train label shape:  torch.Size([76, 115])
validation input shape:  torch.Size([20, 8])
validation label shape:  torch.Size([20, 98])
</code></pre>
<blockquote>
<p><em>This is how I am using Dataloader.</em></p>
</blockquote>
<pre><code>batch_size = 4
train_dataloader = DataLoader(train_dataset, 
                   batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size)
</code></pre>
<blockquote>
<p>Here is the model.</p>
</blockquote>
<h1>Training loop</h1>
<pre><code>model.train()
for epoch in range(num_epochs):
 for batch in train_dataloader:
    batch = [item.to(device) for item in batch]
    input_ids, labels = batch

    optimizer.zero_grad()
    
    print(&quot;indputIds:&quot;,len(input_ids))
    print(&quot;lebels:&quot;,len(labels))

    outputs = model(input_ids=input_ids, labels=labels)
    loss = outputs.loss
    logits = outputs.logits

    loss.backward()
    optimizer.step()

# Validation
with torch.no_grad():
    model.eval()
    val_loss = 0.0
    for val_batch in val_dataloader:
        val_batch = [item.to(device) for item in val_batch]
        val_input_ids, val_labels = val_batch

        val_outputs = model(input_ids=val_input_ids, labels=val_labels)
        val_loss += val_outputs.loss.item()

    average_val_loss = val_loss / len(val_dataloader)
    print(f&quot;Epoch: {epoch+1}, Validation Loss: {average_val_loss:.4f}&quot;)

model.train()
</code></pre>
<blockquote>
<p>Even the dimension in each batch is the same In train and validation data loader For example:</p>
</blockquote>
<pre><code>Batch 19
Inputs:
  torch.Size([4, 8])
Targets:
  torch.Size([4, 115])
</code></pre>
<blockquote>
<p>Same for validation except in validation target size is [4,8] and [8,98].</p>
</blockquote>
","2023-05-31 18:56:43","","2023-06-12 09:47:32","2023-06-12 09:47:32","<dataframe><deep-learning><nlp><transformer-model><gpt-2>","0","0","1","42","","","","","","",""
"76420507","1","22033979","","How to restrict the open AI API responses to only Physics, Chemistry, Mathematics and Biology? If user asks non tech question then say 'Not related'","<p>How to restrict the below open AI API to respond only to certain fields. For example only Physics, Chemistry, Mathematics and Biology related queries needs to be answered otherwise it should respond with &quot;Not related to Physics, Chemistry, Mathematics and Biology&quot;. I know the API is generic and answers all the queries, but is there any other way it can be restricted ?</p>
<p>It is working but sometimes giving unwanted responses, how to prevent this or is there any other alternate method available to achieve this functionality.</p>
","2023-06-07 06:42:21","2023-06-07 06:46:01","2023-06-07 06:45:01","2023-06-07 06:45:01","<node.js><openai-api><gpt-3><chatgpt-api>","0","0","-2","13","","","","","","",""
"56770831","1","5440125","","Using GPT-2 with your own dictionary of words","<p>I'm training the gpt-2 with custom encodings and custom vocab.bpe file. However, when I generate text using gpt-2, the output tokens have range that exceeds the range of my new encodings. 
How can I make gpt-2 work for me then?</p>
","2019-06-26 10:37:46","","2020-11-29 12:08:30","2020-11-29 12:08:30","<python-3.x><nlp><gpt-2>","0","1","3","589","0","","","","","",""
"74350123","1","20442081","","ValueError while trying to finetune a gpt-2 model","<p>While trying to finetune gpt-2, I always get the Error &quot;ValueError&quot; and can´t seem to find the cause of this Error.</p>
<p>I am using Max Wool´s Google Colab to finetune gpt-2.</p>
<p>I followed all instructions in the Notebook and tried to start this cell:</p>
<pre><code>sess = gpt2.start_tf_sess()  
 
gpt2.finetune(sess,  
              dataset= file_name,  
              model_name='124M',  
              steps=1000,  
              restore_from='fresh',  
              run_name='run1',  
              print_every=10,  
              sample_every=100,  
              save_every=500  
)
</code></pre>
<p>But I always get this error:</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-37-76f19599c0d2&gt; in &lt;module&gt;
      9               print_every=10,
     10               sample_every=200,
---&gt; 11               save_every=500
     12               )
</code></pre>
<pre><code>/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)
    889         # ResourceVariables don't have an op associated with so no traceback
    890         if isinstance(var, resource_variable_ops.ResourceVariable):
--&gt; 891           raise ValueError(err_msg)
    892         tb = var.op.traceback[::-1]
    893         # Throw away internal tf entries and only take a few lines. In some
</code></pre>
<p>I can´t seem to find any solutions to my problem, can anybody help?</p>
","2022-11-07 16:52:54","","2022-11-07 22:48:56","2022-11-07 22:48:56","<python><google-colaboratory><gpt-2><fine-tune>","0","0","0","48","","","","","","",""
"63380543","1","6815505","63391545","How many characters can be input into the ""prompt"" for GPT-2","<p>I'm using the OpenAI GPT-2 model from <a href=""https://github.com/openai/gpt-2"" rel=""nofollow noreferrer"">github</a></p>
<p>I think that the top_k parameter dictates how many tokens are sampled. Is this also the parameter that dictates how large of a prompt can be given?</p>
<p>If top_k = 40, how large can the prompt be?</p>
","2020-08-12 16:09:45","","2020-11-29 12:03:15","2020-11-29 12:03:15","<python><nlp><openai-api><gpt-2>","1","0","3","5470","","2","5652313","<p>GPT-2 does not work on character-level but on the subword level. The maximum length of text segments in was trained on was 1,024 subwords.</p>
<p>It uses a vocabulary based on <a href=""https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10"" rel=""nofollow noreferrer"">byte-pair-encoding</a>. Under such encoding, frequent words remain intact, infrequent words get split into several units, eventually down to the byte level. In practice, the segmentation looks like this (69 characters, 17 subwords):</p>
<pre><code>Hello , ▁Stack Over flow ! ▁This ▁is ▁an ▁example ▁how _a ▁string ▁gets ▁segment ed .
</code></pre>
<p>At the training time, there is no difference between the prompt and the answer, so the only limitation is that the prompt and answer cannot be longer than 1,024 subwords in total. In theory, you can continue generating beyond this, but the history model considers can never be longer.</p>
<p>The selection of <code>top_k</code> only influences memory requirements. A long query also needs more memory, but it is probably not the main limitation</p>
","2020-08-13 08:54:07","2","3"
"64130834","1","6786996","66070991","Build a model that answers question from dataset using GPT3","<p>I am trying to build a chat bot, that given some text corpus, will answer questions when we ask something from that text. I have heard GPT3 is a beast and requires minimum training. Are there any links/ tutorial/github repo's that will help me get started with this?</p>
","2020-09-30 04:23:45","","2023-01-21 20:40:54","2023-01-21 20:40:54","<nlp><nlp-question-answering><gpt-3>","1","3","1","1700","","2","14852784","<p>Sure, if you got a beta access to the <a href=""https://beta.openai.com/"" rel=""nofollow noreferrer"">OpenAI GPT-3 API</a> you're easily able to do so. In case you don't, you can apply for it - you should get accepted fairly quickly <em>(in my specific case it took about 24 hours)</em>.</p>
<p>Depending whether you look for speed or precision you should choose between Davinci, Cushman or Curie (<a href=""https://beta.openai.com/docs/engines"" rel=""nofollow noreferrer"">list of engines</a>), whereas Davinci is the best (precision-wise).</p>
<p>You can use the Playground to enter a text corpus and a question - here is an example:
<a href=""https://i.stack.imgur.com/TSaZz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TSaZz.png"" alt=""Example picture"" /></a>
<em>I used <code>davinci-instruct-beta</code> with a temperature of <code>0.25</code> and response length of <code>10</code>. A pretty basic setup.</em></p>
<p>For demonstration purposes, here is the API request made via <strong>Python</strong>. <code>response</code> returns <em>&quot;Anna hates doing research the most.&quot;</em></p>
<pre><code>import openai

openai.api_key = 'KEY'

response = openai.Completion.create(
  engine=&quot;davinci-instruct-beta&quot;,
  prompt=&quot;Anna loves programming in Python and C++, though she absolutely despises doing research.\nWhat does Anna hate the most?\n\nAnna hates doing research the most.Example&quot;,
  temperature=0.25,
  max_tokens=10,
  top_p=1
)
</code></pre>
","2021-02-05 21:42:20","3","1"
"76424390","1","19871283","","How to change the QA_PROMPT for my own usecase?","<h2><a href=""https://i.stack.imgur.com/MdCGs.png"" rel=""nofollow noreferrer"">chain</a></h2>
<h2><a href=""https://i.stack.imgur.com/3CFIa.png"" rel=""nofollow noreferrer"">I was following this description.</a></h2>
<p>I can't understand what QA_PROMPT means, and how I can change it to my own usecase.
I checked my Pinecone index but I can't find anything about QA_PROMPT.
What should I do? Please help me.</p>
","2023-06-07 14:25:46","","","2023-06-08 07:39:06","<prompt><openai-api><chain><langchain><gpt-4>","1","1","0","32","","","","","","",""
"76457935","1","12689038","","TypeError: argmax(): argument 'input' (position 1) must be Tensor, not numpy.ndarray","<p>I am traning a model GPT-2 using my curated dataset, and getting the following error. When I am trying to debug any issue, a new error comes. Can anyone helpme to fix my script so that it can run. The traing process starts but later gets many error.
I donot know how to fix these erros, but can anyone help me to fix my script? It would be very helpful.</p>
<pre><code>import torch.nn.functional as F
import pandas as pd
from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score

# Load the CSV file
df = pd.read_csv('/content/drive/MyDrive/singleton/annotated_data.csv')
subset_df = df.head(20)

# Extract the programs and labels
programs = subset_df[&quot;program&quot;].astype(str).tolist()
labels = subset_df[&quot;label&quot;].tolist()

# Define the maximum input length for the model
max_input_length = 250

# Initialize the label encoder
label_encoder = LabelEncoder()

# Encode the labels
encoded_labels = label_encoder.fit_transform(labels)

# Load the tokenizer
tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
tokenizer.pad_token = tokenizer.eos_token

# Define the custom dataset
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, programs, labels, tokenizer, max_length):
        self.programs = programs
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.programs)

    def __getitem__(self, idx):
        program = self.programs[idx]
        label = self.labels[idx]

        encoding = self.tokenizer.encode_plus(
            program,
            add_special_tokens=True,
            max_length=self.max_length,
            padding=&quot;max_length&quot;,
            truncation=True,
            return_tensors=&quot;pt&quot;
        )

        input_ids = encoding[&quot;input_ids&quot;].squeeze().to(dtype=torch.long)
        attention_mask = encoding[&quot;attention_mask&quot;].squeeze().to(dtype=torch.float)  # Convert to Float data type

        return {
            &quot;input_ids&quot;: input_ids,
            &quot;attention_mask&quot;: attention_mask,
            &quot;labels&quot;: torch.tensor(label, dtype=torch.long)
        }

# Create the dataset
dataset = CustomDataset(programs, encoded_labels, tokenizer, max_length=max_input_length)

# Split the dataset into training and evaluation subsets
train_subset_df = subset_df.head(16)
eval_subset_df = subset_df.tail(4)
train_dataset = CustomDataset(train_subset_df[&quot;program&quot;].astype(str).tolist(),
                              label_encoder.transform(train_subset_df[&quot;label&quot;].tolist()),
                              tokenizer, max_length=max_input_length)
eval_dataset = CustomDataset(eval_subset_df[&quot;program&quot;].astype(str).tolist(),
                             label_encoder.transform(eval_subset_df[&quot;label&quot;].tolist()),
                             tokenizer, max_length=max_input_length)

# Define the training arguments
training_args = TrainingArguments(
    output_dir=&quot;output_directory&quot;,
    overwrite_output_dir=True,
    num_train_epochs=2,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    save_steps=500,
    save_total_limit=2,
    logging_steps=100,
    evaluation_strategy=&quot;epoch&quot;,
)

# Initialize the model
model = GPT2ForSequenceClassification.from_pretrained(&quot;gpt2&quot;, num_labels=len(label_encoder.classes_))
model.config.pad_token_id = model.config.eos_token_id
# Modify model output to be Float
model.config.return_dict = False
model = model.float()

# Define the data preprocessing function
def preprocess_function(examples):
    input_ids = torch.stack([example[&quot;input_ids&quot;] for example in examples])
    attention_mask = torch.stack([example[&quot;attention_mask&quot;] for example in examples])
    labels = torch.tensor([example[&quot;labels&quot;] for example in examples], dtype=torch.float)  # Change label dtype to float

    return {
        &quot;input_ids&quot;: input_ids,
        &quot;attention_mask&quot;: attention_mask,
        &quot;labels&quot;: labels
    }

# Create the Trainer for fine-tuning
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=preprocess_function
)

# Fine-tune the model
trainer.train()

# Evaluate the fine-tuned model
eval_results = trainer.evaluate()

# Compute predicted labels
eval_predictions = trainer.predict(eval_dataset)
predicted_labels = torch.argmax(eval_predictions.predictions[0], dim=1)
# Reshape predictions
predictions_array = np.array(eval_predictions.predictions)
# Reshape predictions
reshaped_predictions = predictions_array.reshape(-1, predictions_array.shape[-1])
# Apply argmax along axis=0
predicted_labels = np.argmax(reshaped_predictions, axis=1).astype(np.float32)  # Convert predicted labels to float32
#print(eval_predictions.predictions.shape)
print(&quot;Shape of eval_predictions.predictions:&quot;, eval_predictions.predictions.shape)
# Convert the encoded labels back to original labels
decoded_labels = label_encoder.inverse_transform(predicted_labels.cpu().numpy())
# Convert the encoded labels for eval_dataset back to original labels
decoded_eval_labels = label_encoder.inverse_transform(eval_labels)
# Compute evaluation metrics
precision = precision_score(decoded_eval_labels, decoded_labels)
recall = recall_score(decoded_eval_labels, decoded_labels)
f1 = f1_score(decoded_eval_labels, decoded_labels)
# Print the evaluation metrics
print(&quot;Precision:&quot;, precision)
print(&quot;Recall:&quot;, recall)
print(&quot;F1-Score:&quot;, f1)

# Plot the performance measures
labels = [&quot;Precision&quot;, &quot;Recall&quot;, &quot;F1-Score&quot;]
values = [precision, recall, f1]

plt.bar(labels, values)
plt.xlabel(&quot;Performance Measure&quot;)
plt.ylabel(&quot;Value&quot;)
plt.title(&quot;Model Performance&quot;)
plt.show()

# Save the fine-tuned model
trainer.save_model(&quot;output_directory&quot;)

# Load the fine-tuned model
model = GPT2ForSequenceClassification.from_pretrained(&quot;output_directory&quot;)
model.eval()

# Maximum input length for tokenization
max_input_length = 250

def predict(program):
    encoded_input = tokenizer(program, truncation=True, padding=True, max_length=max_input_length, return_tensors='pt')
    input_ids = encoded_input['input_ids']
    attention_mask = encoded_input['attention_mask']

    # Make the prediction
    with torch.no_grad():
        logits = model(input_ids, attention_mask=attention_mask)

    # Convert logits to float data type
    logits_float = logits[0].float()

    # Apply softmax to obtain probabilities
    probabilities = F.softmax(logits_float, dim=1).squeeze().tolist()

    # Get the predicted label and score
    score = max(probabilities)

    # Return the prediction
    return {'label': predicted_label, 'score': score}
#print(eval_predictions.predictions.shape)
# Example usage:
program_text = &quot;Your program text here&quot;
prediction = predict(program_text)
print(&quot;Prediction:&quot;, prediction)
</code></pre>
","2023-06-12 15:01:55","","2023-06-12 17:45:44","2023-06-12 17:45:44","<nlp><gpt-2>","0","0","0","12","","","","","","",""
"76462707","1","22065227","","Building a GPT-3 Enabled Research Assistant with LangChain & Pinecone","<p>I would like a chatbot that can handle large CSV files and answer any questions about the data contained within them.</p>
<p>I have implemented Pinecone to store vector data and connected it with Langchain. However, the current setup is not providing accurate answers based on the given data. What could be the potential error in this configuration?</p>
","2023-06-13 07:46:46","","","2023-06-13 07:46:46","<csv><chatbot><gpt-3><langchain><pinecone>","0","3","0","39","","","","","","",""
"76463184","1","21824356","","Using OpenAI LLMs for classification. Asking for classification vs. asking for probabilities","<p>I'm using LLMs for classifying products into specific categories. Multi-Class.</p>
<ol>
<li><p>One way to do it would it to ask if it's a yes/no for a specific category and loop through the categories.</p>
</li>
<li><p>Another way would be to ask for a probability that that certain product belongs to one of those classes.</p>
</li>
</ol>
<p>The second option allows me to adjust the prediction thresholds in &quot;post&quot; and over/under-classify certain classes.</p>
<p>However, The word on the street is that RLHF-trained OpenAI models such as <code>gpt-3.5-turbo</code> and <code>gpt-4</code> are weak at guessing probabilities relative to text completion models like <code>text-davinci-003</code> because RLHF training makes the model &quot;think&quot; more like a human (bad at guessing probabilities).</p>
<p>Are there any literature I can read up on/ should know about? Before I go ahead and run a 100 tests.</p>
<p>I've not tried anything as of yet given that testing is time/cost intensive. And would like a baseline understanding of how to tackle the problem before starting.</p>
","2023-06-13 08:49:01","","","2023-06-13 08:49:01","<text-classification><openai-api><multilabel-classification><gpt-4><llm>","0","0","0","10","","","","","","",""
"76106366","1","20779237","","how to use tiktoken in offline mode computer","<pre><code>import tiktoken

tokenizer = tiktoken.get_encoding(&quot;cl100k_base&quot;) tokenizer = tiktoken.encoding_for_model(&quot;gpt-3.5-turbo&quot;)

text = &quot;Hello, nice to meet you&quot;

tokenizer.encode(text)

</code></pre>
<p>This keeps showing error called requests.exceptions.SSLError. Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self signed certificate in certificate chain (_ssl.c:1002)')) ... like this.</p>
<p>I wanted to run this code to see the number of tokens. But it keeps showing error as I mentioned earlier. What am I missing for the code?</p>
","2023-04-26 00:36:11","","","2023-06-04 20:41:26","<python><tokenize><gpt-3>","1","1","2","1232","","","","","","",""
"76106760","1","21740587","","Azure OpenaAI GPT-4 Review version cannot be found in the list of models","<p>I received an email on 18 April confirming that I have been onboarded to the Azure OpenAI Service GPT-4 Preview, but GPT-4 Review version cannot be found in the list of OpenAI models and GPT-4 cannot be deployed, Whether my resource group choice is Eastern US or South Central US.</p>
<p>I would be grateful if any guru could help me.</p>
<p>I should have used a variety of methods and I am a pay-as-you-go subscriber.</p>
","2023-04-26 02:34:38","","2023-04-26 10:35:42","2023-04-26 10:35:42","<azure-openai><gpt-4>","0","0","0","59","","","","","","",""
"57782409","1","4544413","57830166","Set the number of iterations gpt-2","<p>I'm fine tuning a gpt-2 model following this tutorial:</p>
<p><a href=""https://medium.com/@ngwaifoong92/beginners-guide-to-retrain-gpt-2-117m-to-generate-custom-text-content-8bb5363d8b7f"" rel=""nofollow noreferrer"">https://medium.com/@ngwaifoong92/beginners-guide-to-retrain-gpt-2-117m-to-generate-custom-text-content-8bb5363d8b7f</a></p>
<p>With its associated GitHub repository:</p>
<p><a href=""https://github.com/nshepperd/gpt-2"" rel=""nofollow noreferrer"">https://github.com/nshepperd/gpt-2</a></p>
<p>I have been able to replicate the examples, my issue is that I'm not finding a parameter to set the number of iterations.
Basically the training script shows a sample every 100 iterations and save a model version every 1000 iterations. But I'm not finding a parameter to train it for say, 5000 iterations and then close it.</p>
<p>The script for training is here:
<a href=""https://github.com/nshepperd/gpt-2/blob/finetuning/train.py"" rel=""nofollow noreferrer"">https://github.com/nshepperd/gpt-2/blob/finetuning/train.py</a></p>
<p>EDIT:</p>
<p>As suggested by cronoik I'm trying to replace the while for a for loop.</p>
<p>I'm adding these changes:</p>
<ol>
<li><p>Adding one additional argument:</p>
<p>parser.add_argument('--training_steps', metavar='STEPS', type=int, default=1000, help='a number representing how many training steps the model shall be trained for')</p>
</li>
<li><p>Changing the loop:</p>
<pre><code> try:
     for iter_count in range(training_steps):
         if counter % args.save_every == 0:
             save()
</code></pre>
</li>
<li><p>Using the new argument:</p>
<p>python3 train.py --training_steps 300</p>
</li>
</ol>
<p>But I'm getting this error:</p>
<pre><code>  File &quot;train.py&quot;, line 259, in main
    for iter_count in range(training_steps):
NameError: name 'training_steps' is not defined
</code></pre>
","2019-09-04 06:09:29","","2020-11-29 11:51:15","2020-11-29 11:51:15","<python><tensorflow><nlp><gpt-2>","1","1","0","795","","2","6664872","<p>All you have to do is to modify the <code>while True</code> loop to a <code>for</code> loop:</p>

<pre class=""lang-py prettyprint-override""><code>try:
    #replaced
    #while True:
    for i in range(5000):
        if counter % args.save_every == 0:
            save()
        if counter % args.sample_every == 0:
            generate_samples()
        if args.val_every &gt; 0 and (counter % args.val_every == 0 or counter == 1):
            validation()

        if args.accumulate_gradients &gt; 1:
            sess.run(opt_reset)
            for _ in range(args.accumulate_gradients):
                sess.run(
                    opt_compute, feed_dict={context: sample_batch()})
            (v_loss, v_summary) = sess.run((opt_apply, summaries))
        else:
            (_, v_loss, v_summary) = sess.run(
                (opt_apply, loss, summaries),
                feed_dict={context: sample_batch()})

        summary_log.add_summary(v_summary, counter)

        avg_loss = (avg_loss[0] * 0.99 + v_loss,
                    avg_loss[1] * 0.99 + 1.0)

        print(
            '[{counter} | {time:2.2f}] loss={loss:2.2f} avg={avg:2.2f}'
            .format(
                counter=counter,
                time=time.time() - start_time,
                loss=v_loss,
                avg=avg_loss[0] / avg_loss[1]))

        counter += 1
except KeyboardInterrupt:
    print('interrupted')
    save()
</code></pre>
","2019-09-07 02:11:11","1","1"
"58093426","1","10429635","59687442","Train GPT-2 on local machine, load dataset","<p>I am trying to run gpt-2 on my local machine, since google restricted my resources, because I was training too long in colab.</p>

<p>However, I cannot see how I can load the dataset. In the original colab notebook <a href=""https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce</a> there is the command 
gpt2.copy_file_from_gdrive() which I cannot use on my local machine.</p>

<p>On the github repo <a href=""https://github.com/minimaxir/gpt-2-simple"" rel=""nofollow noreferrer"">https://github.com/minimaxir/gpt-2-simple</a> they simply give the name of the file 
shakespeare.txt to the function gpt2.finetune and it works somehow, but this doesn't work for me.</p>

<p>Help would be much appreciated</p>
","2019-09-25 07:37:39","","2020-11-29 11:53:05","2020-11-29 11:53:05","<python><jupyter-notebook><google-colaboratory><gpt-2>","1","0","1","1886","","2","2896004","<p>If I read the <a href=""https://github.com/minimaxir/gpt-2-simple#usage"" rel=""nofollow noreferrer"">example</a> correctly on GitHub, it loads <code>shakespeare.txt</code> if it is present on the machine and downloads it if it isn't. For a local dataset, I simply drop a txt file in the same folder and call it in <code>file_name =</code>.</p>

<p>You should be able to remove the logic around <code>if not os.path.isfile(file_name):</code>—it shouldn't be needed if you use a local file. </p>
","2020-01-10 18:51:24","0","1"
"59944537","1","6289601","59945547","Is there a GPT-2 implementation that allows me to fine-tune and prompt for text completion?","<p>I wish to to fine-tune a GPT-2 implementation on some text data. I then want to use this model to complete a text prompt. I can do the first part easily enough using Max Woolf's <a href=""https://github.com/minimaxir/gpt-2-simple"" rel=""nofollow noreferrer"">gpt-2-simple</a> implementation. And <a href=""https://github.com/nshepperd/gpt-2"" rel=""nofollow noreferrer"">Neil Shepherd's fork</a> of OpenAI allows for GPT-2 to be trained on new data and completes text.</p>

<p>However, my corpus is too small to train on and not get gibberish back. Is there any way I can combine the two functions? Ideally, I'd like to be able to do this via a python interface (as opposed to CLI), as I'd like to use pandas for data cleaning and what-have-you. Thanks.</p>
","2020-01-28 08:13:29","","2020-11-29 12:02:45","2020-11-29 12:02:45","<python-3.x><deep-learning><nlp><openai-gym><gpt-2>","1","0","3","1970","0","2","5652313","<p><a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">Huggingface's Transformers</a> package has a GPT-2 implementation (including pre-trained models) for PyTorch and TensorFlow. You can easily work with them in Python.</p>

<p>Fine-tuning of GPT-2, however, requires a lot of memory and I am not sure is you will be able to do the full backpropagation on that. In that case, you fine-tune just a few highest layers.</p>
","2020-01-28 09:23:06","2","3"
"65987683","1","1793799","65991030","Modifying the Learning Rate in the middle of the Model Training in Deep Learning","<p>Below is the code to configure <a href=""https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments"" rel=""nofollow noreferrer"">TrainingArguments</a> consumed from the <a href=""https://huggingface.co/transformers/"" rel=""nofollow noreferrer"">HuggingFace transformers</a> library to finetune the <a href=""https://huggingface.co/transformers/model_doc/gpt2.html"" rel=""nofollow noreferrer"">GPT2</a> language model.</p>
<pre><code>training_args = TrainingArguments(
        output_dir=&quot;./gpt2-language-model&quot;, #The output directory
        num_train_epochs=100, # number of training epochs
        per_device_train_batch_size=8, # batch size for training #32, 10
        per_device_eval_batch_size=8,  # batch size for evaluation #64, 10
        save_steps=100, # after # steps model is saved
        warmup_steps=500,# number of warmup steps for learning rate scheduler
        prediction_loss_only=True,
        metric_for_best_model = &quot;eval_loss&quot;,
        load_best_model_at_end = True,
        evaluation_strategy=&quot;epoch&quot;,
        learning_rate=0.00004, # learning rate
    )

early_stop_callback = EarlyStoppingCallback(early_stopping_patience  = 3)
    
trainer = Trainer(
        model=gpt2_model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        callbacks = [early_stop_callback],
 )
</code></pre>
<p>The <strong>number of epochs</strong> as <strong>100</strong> and <strong>learning_rate</strong> as <strong>0.00004</strong> and also the <strong>early_stopping</strong> is configured with the patience value as <strong>3</strong>.</p>
<p>The model ran for <strong>5/100</strong> epochs and noticed that the difference in loss_value is negligible. The latest checkpoint is saved as <code>checkpoint-latest</code>.</p>
<p>Now Can I modify the <code>learning_rate</code> may be to <code>0.01</code> from <code>0.00004</code> and resume the training from the latest saved checkpoint - <code>checkpoint-latest</code>? Doing that will be efficient?</p>
<p>Or to train with the new <code>learning_rate</code> value should I start the <strong>training</strong> from the beginning?</p>
","2021-02-01 05:42:01","","2021-02-01 05:48:22","2021-02-01 10:30:40","<deep-learning><pytorch><huggingface-transformers><language-model><gpt-2>","2","1","2","1184","","2","8047535","<p><strong>No, you don't have to restart your training.</strong></p>
<p>Changing the learning rate is like changing how big a step your model take in the <strong>direction determined by your loss function</strong>.</p>
<p>You can also think of it as transfer learning where the model has <strong>some experience</strong> (no matter how little or irrelevant) and the <code>weights</code> are in a state <strong>most likely better than a randomly initialised one</strong>.</p>
<p>As a matter of fact, changing the learning rate mid-training is considered an art in deep learning and you should change it if you have a <strong>very very good reason</strong> to do it.</p>
<p>You would probably want to write down when (why, what, etc) you did it if you or someone else wants to &quot;reproduce&quot; the result of your model.</p>
","2021-02-01 10:30:40","0","3"
"67089849","1","15221534","67089951","AttributeError: 'GPT2TokenizerFast' object has no attribute 'max_len'","<p>I am just using the huggingface transformer library and get the following message when running run_lm_finetuning.py: AttributeError: 'GPT2TokenizerFast' object has no attribute 'max_len'. Anyone else with this problem or an idea how to fix it? Thanks!</p>
<p>My full experiment run:
mkdir experiments</p>
<p>for epoch in 5
do
python run_lm_finetuning.py <br />
--model_name_or_path distilgpt2 <br />
--model_type gpt2 <br />
--train_data_file small_dataset_train_preprocessed.txt <br />
--output_dir experiments/epochs_$epoch <br />
--do_train <br />
--overwrite_output_dir <br />
--per_device_train_batch_size 4 <br />
--num_train_epochs $epoch
done</p>
","2021-04-14 10:20:20","","","2023-06-15 09:00:04","<tokenize><huggingface-transformers><transformer-model><huggingface-tokenizers><gpt-2>","2","0","2","7151","0","2","3832970","<p>The <a href=""https://github.com/huggingface/transformers/issues/8739"" rel=""nofollow noreferrer"">&quot;AttributeError: 'BertTokenizerFast' object has no attribute 'max_len'&quot; Github issue</a> contains the fix:</p>
<blockquote>
<p>The <code>run_language_modeling.py</code> script is deprecated in favor of <code>language-modeling/run_{clm, plm, mlm}.py</code>.</p>
<p>If not, the fix is to change <code>max_len</code> to <code>model_max_length</code>.</p>
</blockquote>
<p>Also, <code>pip install transformers==3.0.2</code> might fix the issue since it has been reported to work for some people.</p>
","2021-04-14 10:27:39","0","11"
"67735561","1","4438203","67736155","Fine-tuning GPT-2/3 on new data","<p>I'm trying to wrap my head around training OpenAI's language models on new data sets. Is there anyone here with experience in that regard?
My idea is to feed either GPT-2 or 3 (I do not have API access to 3 though) with a textbook, train it on it and be able to &quot;discuss&quot; the content of the book with the language model afterwards. I don't think I'd have to change any of the hyperparameters, I just need more data in the model.</p>
<p>Is it possible??</p>
<p>Thanks a lot for any (also conceptual) help!</p>
","2021-05-28 08:35:33","2021-05-28 12:10:37","","2021-07-14 14:00:00","<machine-learning><training-data><gpt-2><gpt-3>","2","2","0","2143","","2","1362200","<p>Presently GPT-3 has no way to be finetuned as we can do with GPT-2, or GPT-Neo / Neo-X. This is because the model is kept on their server and requests has to be made via API. A Hackernews <a href=""https://news.ycombinator.com/item?id=23725834"" rel=""nofollow noreferrer"">post</a> says that finetuning GPT-3 is planned or in process of construction.</p>
<p>Having said that, OpenAI's GPT-3 provide <a href=""https://beta.openai.com/docs/guides/search"" rel=""nofollow noreferrer"">Answer API</a> which you could provide with context documents (up to 200 files/1GB). The API could then be used as a way for discussion with it.</p>
<p>EDIT:
Open AI has recently introduced Fine Tuning beta.
<a href=""https://beta.openai.com/docs/guides/fine-tuning"" rel=""nofollow noreferrer"">https://beta.openai.com/docs/guides/fine-tuning</a>
Thus it will be best answer to the question to follow through description on that link.</p>
","2021-05-28 09:18:38","2","2"
"67299510","1","2178942","67906030","Understanding how gpt-2 tokenizes the strings","<p>Using tutorials <a href=""https://huggingface.co/transformers/model_doc/gpt2.html"" rel=""nofollow noreferrer"">here</a> , I wrote the following codes:</p>
<pre><code>from transformers import GPT2Tokenizer, GPT2Model
import torch

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2')

inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
</code></pre>
<p>So I realize that &quot;inputs&quot;, consists of tokenized items of my sentence.
But how can I get the values of tokenized items? (see for example [&quot;hello&quot;, &quot;,&quot;, &quot;my&quot;, &quot;dog&quot;, &quot;is&quot;, &quot;cute&quot;])</p>
<p>I am asking this because sometimes I think it separetes a word if that word is not in its dictionary (i.e., a word from another language). So I want to check that in my codes.</p>
","2021-04-28 11:38:49","","2021-05-12 22:09:27","2021-06-09 14:19:35","<python><huggingface-transformers><transformer-model><gpt-2>","1","1","1","888","","2","8301609","<p>You can call <code>tokenizer.decode</code> on the output of the tokenizer to get the words from its vocabulary under given indices:</p>
<pre><code>&gt;&gt;&gt; inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)
&gt;&gt;&gt; list(map(tokenizer.decode, inputs.input_ids[0]))
['Hello', ',', ' my', ' dog', ' is', ' cute']
</code></pre>
","2021-06-09 14:19:35","0","3"
"76172889","1","21815490","","Chatgpt api url questions: Chatgpt3.5 error","<p>I tried to use openai's api,but it didn't work.</p>
<p>It's the curl</p>
<pre><code>curl https://api.openai.com/v1/chat/completions \
 -H &quot;Content-Type: application/json&quot; \
 -H &quot;Authorization: Bearer $OPENAI_API_KEY&quot; \
 -d '{
        &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,
        &quot;messages&quot;: [
           {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant&quot;},
           {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who won the world series in 2020?&quot;},
           {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;The Los Angeles Dodgers won the World Series in 2020.&quot;},
           {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Where was it played?&quot;}
        ]
     }'
</code></pre>
<p>And,this is the result.</p>
<pre><code>{
    &quot;error&quot;: {
        &quot;message&quot;: &quot;This is not a chat model and thus not supported in the v1/chat/completions endpoint. Did you mean to use v1/completions?&quot;,
        &quot;type&quot;: &quot;invalid_request_error&quot;,
        &quot;param&quot;: &quot;model&quot;,
        &quot;code&quot;: null
    }
}
</code></pre>
<p>I watched the openai docs,the url is right. But it didn't pass.</p>
<p>Thanks for your help !</p>
","2023-05-04 11:54:16","","","2023-06-03 04:00:28","<openai-api><gpt-3><chatgpt-api>","1","1","-3","206","","","","","","",""
"76502113","1","21018812","","Error with Few-shot prompting using gpt 3.5","<p>I am trying to train GPT 3.5 model with few-shot prompting using <em>messages</em> argument instead of <em>prompt</em> argument. It throws an error even though it's clearly mentioned in OpenAI documentation that we can train a model this way.</p>
<pre><code>import openai

conversation=[
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who won the world series in 2020?&quot;},
        {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;The Los Angeles Dodgers won the World Series in 2020.&quot;},
    ]

def askGPT(question):
    conversation.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: question})
    openai.api_key = &quot;openai key&quot;
    response = openai.Completion.create(
        model = &quot;gpt-3.5-turbo&quot;,
        messages = conversation,
        temperature = 0.6
        #max_tokens = 150,
    )
    #conversation.append({&quot;role&quot;: &quot;assistant&quot;,&quot;content&quot;:response})
    #print(response)
    #print(response[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;])

    
    conversation.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: response.choices[0].message.content})
    print(response.choices[0].message.content)

def main():
    while True:
        print('GPT: Ask me a question\n')
        myQn = input()
        askGPT(myQn)
        print('\n')


main()
</code></pre>
<p>Error:</p>
<blockquote>
<p>openai.error.InvalidRequestError: Unrecognized request argument supplied: messages</p>
</blockquote>
<p>I tried to give &quot;conversations&quot; to the model inside &quot;responses&quot; but it soesn't seem to work.</p>
","2023-06-18 18:58:45","","2023-06-18 19:01:09","2023-06-19 07:38:48","<chatbot><openai-api><gpt-3><chatgpt-api>","1","1","0","29","","","","","","",""
"76525391","1","5204859","","How to train gpt2 model to learn from the training text I have given?","<p>I'm trying to train and fine tune my gpt2 model with my own sample training document. I'm using the code similar to this: <a href=""https://www.kaggle.com/code/changyeop/how-to-fine-tune-gpt-2-for-beginners"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/changyeop/how-to-fine-tune-gpt-2-for-beginners</a> . But the text generated is not related to any text in my training document. Is there any solution on how to approach this?</p>
","2023-06-21 16:28:20","","","2023-06-21 16:28:20","<huggingface-transformers><training-data><huggingface><gpt-2><fine-tune>","0","0","0","18","","","","","","",""
"66873983","1","4458718","66874815","Flask app serving GPT2 on Google Cloud Run not persisting downloaded files?","<p>I have a Flask app running on Google Cloud Run, which needs to download a large model (GPT-2 from huggingface). This takes a while to download, so I am trying to set up so that it only downloads on deployment and then just serves this up for subsequent visits. That is I have the following code in a script that is imported by my main flask app app.py:</p>
<pre><code>import torch
# from transformers import GPT2Tokenizer, GPT2LMHeadModel
from transformers import AutoTokenizer, AutoModelWithLMHead
# Disable gradient calculation - Useful for inference
torch.set_grad_enabled(False)

# Check if gpu or cpu
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Load tokenizer and model
try:
    tokenizer = AutoTokenizer.from_pretrained(&quot;./gpt2-xl&quot;)
    model = AutoModelWithLMHead.from_pretrained(&quot;./gpt2-xl&quot;)
except Exception as e:

    print('no model found! Downloading....')
    
    AutoTokenizer.from_pretrained('gpt2').save_pretrained('./gpt2-xl')
    AutoModelWithLMHead.from_pretrained('gpt2').save_pretrained('./gpt2-xl')
    tokenizer = AutoTokenizer.from_pretrained(&quot;./gpt2-xl&quot;)
    model = AutoModelWithLMHead.from_pretrained(&quot;./gpt2-xl&quot;)

model = model.to(device)
</code></pre>
<p>This basically tries to load the the downloaded model, and if that fails it downloads a new copy of the model. I have autoscaling set to a minimum of 1 which I thought would mean something would always be running and therefore the downloaded file would persist even after activity. But it keeps having to redownload the model which freezes up the app when some people try to use it. I am trying to recreate something like this app <a href=""https://text-generator-gpt2-app-6q7gvhilqq-lz.a.run.app/"" rel=""nofollow noreferrer"">https://text-generator-gpt2-app-6q7gvhilqq-lz.a.run.app/</a> which does not appear to have the same load time issue . In the flask app itself I have the following:</p>
<pre><code>@app.route('/')
@cross_origin()
def index():
    prompt = wp[random.randint(0, len(wp)-1)]
    res = generate(prompt, size=75)
    generated = res.split(prompt)[-1] + '\n \n...TO BE CONTINUED'
    #generated = prompt
    return flask.render_template('main.html', prompt = prompt, output = generated)

if __name__ == &quot;__main__&quot;:
    app.run(host='0.0.0.0',
            debug=True,
            port=PORT)
</code></pre>
<p>But it seems to redownload the models every few hours...how can I avoid having the app re-downloading the models and the app freezing for those who want to try it?</p>
","2021-03-30 15:33:03","","2021-03-30 16:26:45","2021-03-30 16:27:12","<flask><google-cloud-platform><pytorch><google-cloud-run><gpt-2>","1","0","2","198","0","2","8016720","<p>Data written to the filesystem does not persist when the container instance is stopped.</p>
<p>Cloud Run lifetime is the time between an HTTP Request and the HTTP response. Overlapped requests extend this lifetime. Once the final HTTP response is sent your container can be stopped.</p>
<p>Cloud Run instances can run on different hardware (clusters). One instance will not have the same temporary data as another instance. Instances can be moved. Your strategy of downloading a large file and saving it to the in-memory file system will not work consistently.</p>
<p><a href=""https://cloud.google.com/run/docs/reference/container-contract#filesystem"" rel=""nofollow noreferrer"">Filesystem access</a></p>
<p>Also note that the file system is in-memory, which means you need to have additional memory to store files.</p>
","2021-03-30 16:27:12","3","3"
"67403271","1","6065246","67446168","Using AI generators to ask questions to provoke thinking instead of giving answers?","<p>I have a use case that I want to use to help independent creators talk about their interests on Twitter using their experiences.</p>
<p>It goes like this:</p>
<p>You have an <strong>interest</strong> you want to talk about <strong>Entrepreneurship</strong></p>
<p>You have an <strong>experience</strong> like <strong>Pain</strong></p>
<p>Is there a way for an AI (like GPT) to generate prompts that uses these two words to create a list of open-ended questions that provoke thoughts such as these:</p>
<ul>
<li>If entrepreneurship wasn't painful, what would it look like?</li>
<li>What do you know about entrepreneurship that is painful that starters should know?</li>
<li>How can you lower the barrier to entrepreneurship so that it's a less painful opportunity for a person to take?</li>
</ul>
<p>If so, how will it work, and what do I need to do?</p>
<p>I've explored Open AI's documentation on GPT-3, I'm unclear if it solves this problem of generating prompts.</p>
<p>Thanks!</p>
","2021-05-05 14:25:52","2021-05-08 13:26:53","2021-05-13 19:35:51","2021-05-13 19:35:51","<artificial-intelligence><gpt-2><gpt-3>","1","0","0","110","","2","1362200","<p>You should provide some samples so that the GPT-3 can see the pattern and produce a sensible response from your prompt.
For example, see the following screenshot from your case. Note that the bold text is my prompt. The regular text is the response from GPT-3. In that example, I was  &quot;priming&quot; the GPT-3 with relevant pattern: First line, the general description, then the Topics, followed by Questions. This should be enough for booting up your ideas and customizations.</p>
<p><a href=""https://i.stack.imgur.com/kgYpx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kgYpx.png"" alt=""sample of prompt to generate questions from topics"" /></a></p>
","2021-05-08 09:46:19","2","0"
"76526344","1","18937923","","How to queue API calls to Azure OpenAI service (with a token per minute rate limit) the most efficiently?","<p>How can we implement an efficient queue using Azure serverless technologies (e.g. Azure Servicebus) to call Azure OpenAI service concurrently but guarantee earlier messages are processed first?</p>
<p>The complexity is that the rate limit is not based on X requests per minute based on a 'rolling window'. But instead it is about tokens per minute and Azure implements a 1 minute timer (which we don't know when it resets). Here is an explanation of the rate limit policy:
<a href=""https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/quota#understanding-rate-limits"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/quota#understanding-rate-limits</a></p>
<p>Assuming the following &quot;queue&quot; and a rate limit of 10.000 TPM:</p>
<ul>
<li>Request 1) 2000 expected tokens</li>
<li>Request 2) 5000 expected tokens</li>
<li>Request 3) 5000 expected tokens</li>
<li>Request 4) 2000 expected tokens</li>
<li>Request 5) 7000 expected tokens</li>
</ul>
<p>We would like the 'queue' to concurrently process request 1 and 2. 'Realize' that request 3 will overshoot the token limit and 'schedule' one minute of waiting, then take on request 3 &amp; 4 concurrently, schedule one minute of waiting and process request 5.</p>
<p>In theory we don't need to 'schedule' and can just hit the rate limit with a retry policy (maybe better than scheduling since we don't know the moment the timer resets and the exact tokens that Azure estimates the request will cost). But in this case how do we make sure we don't end up with a race condition where request 3,4,5 all fail and retry and 5 gets through before 3?</p>
<p>In theory an even more intelligent solution would process 1,2,4 in parallel. Wait a minute and then process 3, wait a minute and then process 5. Where 4 is allowed to go before 3 only because it fits within the minute's limit which would otherwise be 'wasted'.</p>
","2023-06-21 18:57:51","","2023-06-21 19:05:49","2023-06-21 19:05:49","<azure><message-queue><azureservicebus><azure-openai><gpt-4>","0","1","1","50","","","","","","",""
"76529971","1","22113227","","LLM Content Generation in Non-English Languages","<p>I am trying to use GPT3 to generate content in non-English languages, including some low-resource languages with an inherently small amount of training data. I can think of two approaches to this challenge. The first one is to use some translation API on top of GPT, such that GPT is still interacting only with English (seeing a translated prompt and then producing English content which is re-translated into the original language). The other one is to ask GPT to produce non-English content directly; e.g. &quot;Your response must be in Swahili.&quot; Does anyone with more knowledge than me know which approach is more likely to succeed?</p>
","2023-06-22 08:31:54","","","2023-06-22 08:31:54","<translation><openai-api><linguistics><gpt-3><llm>","0","0","0","11","","","","","","",""
"76535292","1","15071578","","I am not getting exact response from my api as i am getting from chat gpt","<p>Can someone explain  why I am not getting good enough response. My 3.5 api is generating content that is good enough as gpt's response. my app is about helping recruiters to refine their job posts. but its not working fine. How can I improve the response?</p>
<pre><code>import logo from './logo.svg';
import './App.css';
import React, { useEffect, useState } from 'react';
import axios from 'axios';

function App() {
  const [apiKey, setApiKey] = useState('');

  useEffect(() =&gt; {
    fetch('https://server-khaki-kappa.vercel.app/api/key')
      .then(response =&gt; response.json())
      .then(data =&gt; setApiKey(data.apiKey))
      .catch(error =&gt; console.error(error));
  }, []);

  const headers = {
    'Content-Type': 'application/json',
    Authorization: `Bearer ${apiKey}`,
  };

  const personas = [
    'Lou Adler',
    'Stacy Donovan Zapa',
    'Johnny Campbell',
    'Greg Savage',
    'Maisha Cannon',
    'Glen Cathey'
  ];

  const styles = [
    'Captivating',
    'Enticing',
    'Witty',
    'Appealing',
    'Engaging',
    'Impactful',
    'Dynamic',
    'Exciting',
    'Professional'
  ];

  const [userInput, setUserInput] = useState({
    system: '',
    user: '',
    assistant: '',
    prompt: '',
    model: 'gpt-3.5-turbo-16k',
    persona: 'Lou Adler',
    style: 'Captivating'
  });

  const [assistantResponse, setAssistantResponse] = useState('');
  const [loading, setLoading] = useState(false);

  const handleUserInput = (e) =&gt; {
    setUserInput(prevState =&gt; ({
      ...prevState,
      [e.target.name]: e.target.value,
    }));
  };

  const sendUserInput = async () =&gt; {
    setLoading(true);

    const data = {
      model: userInput.model,
      messages: [
        {
          role: 'system',
          content: `You are an AI language model trained to assist recruiters in refining job posts and your name is recruiterGPT. do not generate a response if the job description and some requirements are not given, and ask for them. It assists users in generating human-like text based on the given instructions and context. think properly and take your time before answering. Here are the instructions: Assistant, please generate a ${userInput.style.toLowerCase()} and vibrant job description for the position. The goal is to rewrite the existing job description, emphasizing the benefits and opportunities associated with the role. Take on the persona of ${userInput.persona}, a recruitment expert, and create the content in a ${userInput.style.toLowerCase()}  style that will attract potential candidates. Present the information in a compelling manner while keeping the user's requirements in mind. Even if certain points are not present in the job description, mention them and create enthusiasm around them. These Points include: 1- Offer a Competitive Compensation and Benefits.
          2- Vibrant and collaborative team,
          3- Professional Development Opportunities.
          4- Work-Life Balance.
          5- Offering Challenging and Meaningful Work.
          6- Become part of our family. 7- Career Development Plan. Prioritize communicating what's in it for them. Emphasize more on benefits for them and highlight the benefits and gains they can expect from the job.Also write about the essential requirements and qualifications needed in detail. First emphasize on tonality and benefits and then generate refined requirements. Thank you!.
`
        },
        {
          role: 'user',
          content: `Take the persona of ${userInput.persona} and use a ${userInput.style.toLowerCase()} tonality when rewriting the following Job Description. In the job description emphasize what’s in it for them.First include Career Development, training and growth opportunities, work-life balance, competitive salary, challenging and meaningful work and a vibrant and collaborative team. More of the content should be around these points infact 68% of you response should be around benefits and what an employee can get from us. emphasize less on the requirements, but explain them after describing benefits. Display response in a Job Description format and include a few of the main responsibilities. Generate content no more than 3500 words, But more than 1000 words. ${userInput.prompt}. Note: If job description is not given in this prompt, ask for it and do not generate response until a job description is given by the user.`
        }
      ],
      temperature: 0.5,
      max_tokens: 2049,
    }; 

    try {
      const response = await axios.post(
        'https://api.openai.com/v1/chat/completions',
        data,
        { headers }
      );
      const { choices } = response.data;
      const assistantResponse = choices[0].message.content;
      setLoading(false);
      setAssistantResponse(assistantResponse);
    } catch (error) {
      console.error('An error occurred:', error.message);
    }
  };

  const formatAssistantResponse = (response) =&gt; {
    const paragraphs = response.split('\n\n');

    const formattedResponse = paragraphs.map((paragraph, index) =&gt; (
      &lt;p key={index} className=&quot;text-left mb-4&quot;&gt;
        {paragraph}
      &lt;/p&gt;
    ));

    return formattedResponse;
  };

  return (
    &lt;div className=&quot;container mx-auto py-8&quot;&gt;
      &lt;h1 className=&quot;text-2xl font-bold mb-4&quot;&gt;Chat:&lt;/h1&gt;
      {loading ? (
        &lt;&gt;
          &lt;h1 className=&quot;spinner&quot;&gt;&lt;/h1&gt;
          &lt;p&gt;Dear user, Please be patient RecruitGpt is refining your post to its best....&lt;/p&gt;
        &lt;/&gt;
      ) : (
        &lt;&gt;
          &lt;div className=&quot;bg-gray-100 p-4 mb-4&quot;&gt;
            {formatAssistantResponse(assistantResponse)}
          &lt;/div&gt;
        &lt;/&gt;
      )}

      &lt;section className=&quot;m-6&quot;&gt;
        &lt;div className=&quot;mb-4&quot;&gt;
          &lt;label className=&quot;block mb-2&quot;&gt;
            Model:
            &lt;select
              className=&quot;border border-gray-300 rounded px-4 py-2 w-full&quot;
              name=&quot;model&quot;
              value={userInput.model}
              onChange={handleUserInput}
            &gt;
              &lt;option value=&quot;gpt-3.5-turbo&quot;&gt;gpt-3.5-turbo&lt;/option&gt;
              &lt;option value=&quot;gpt-3.5-turbo-16k&quot;&gt;gpt-3.5-turbo-16k&lt;/option&gt;
              &lt;option value=&quot;gpt-3.5-turbo-0613&quot;&gt;gpt-3.5-turbo-0613&lt;/option&gt;
              &lt;option value=&quot;gpt-3.5-turbo-16k-0613&quot;&gt;gpt-3.5-turbo-16k-0613&lt;/option&gt;
            &lt;/select&gt;
          &lt;/label&gt;
        &lt;/div&gt;
        &lt;div className=&quot;mb-4&quot;&gt;
          &lt;label className=&quot;block mb-2&quot;&gt;
            Persona:
            &lt;select
              className=&quot;border border-gray-300 rounded px-4 py-2 w-full&quot;
              name=&quot;persona&quot;
              value={userInput.persona}
              onChange={handleUserInput}
            &gt;
              {personas.map((persona, index) =&gt; (
                &lt;option key={index} value={persona}&gt;{persona}&lt;/option&gt;
              ))}
            &lt;/select&gt;
          &lt;/label&gt;
        &lt;/div&gt;
        &lt;div className=&quot;mb-4&quot;&gt;
          &lt;label className=&quot;block mb-2&quot;&gt;
            Style:
            &lt;select
              className=&quot;border border-gray-300 rounded px-4 py-2 w-full&quot;
              name=&quot;style&quot;
              value={userInput.style}
              onChange={handleUserInput}
            &gt;
              {styles.map((style, index) =&gt; (
                &lt;option key={index} value={style}&gt;{style}&lt;/option&gt;
              ))}
            &lt;/select&gt;
          &lt;/label&gt;
        &lt;/div&gt;
        &lt;div className=&quot;mb-4&quot;&gt;
          &lt;label className=&quot;block mb-2&quot;&gt;
            Prompt:
            &lt;textarea
              name=&quot;prompt&quot;
              className=&quot;border border-gray-300 rounded px-4 py-2 w-full&quot;
              type=&quot;text&quot;
              rows={4}
              onChange={handleUserInput}
            /&gt;
          &lt;/label&gt;
        &lt;/div&gt;
      &lt;/section&gt;

      &lt;button
        className=&quot;bg-blue-500 hover:bg-blue-600 text-white font-bold py-2 px-4 rounded&quot;
        onClick={sendUserInput}
      &gt;
        Send
      &lt;/button&gt;
    &lt;/div&gt;
  );
}

export default App;
</code></pre>
<p>can someone tell me how i can optimize it.</p>
","2023-06-22 20:04:40","","2023-06-22 22:41:47","2023-06-22 22:41:47","<reactjs><artificial-intelligence><openai-api><chatgpt-api><gpt-4>","0","1","0","24","","","","","","",""
"76543141","1","13209030","","GPT-4 doesn't follow output format instruction occasionally","<p>I am writing a custom wrapper for OpenAI GPT-4 API. I do the prompting similarly to the ReAct model (Thought, Action, Observation, Final Answer). This is my output format instruction for the agent scratchpad</p>
<pre><code>Populate the scratchpad (delimited by the triple quote) to guide yourself toward the answer. For the scratchpad, always choose to follow only one of the situations listed below (inside the triple curly braces) and then end your answer. You CAN NOT populate the Observation field yourself. Always include word &quot;End Answer&quot; at the end of your answer\n\
{{{\
Situation 1: You can ONLY choose ONE action at a time. When you decide you need to use a tool (based on the observations and input question), please follow this format to answer the question:\n\
Thought: you should always think about what to do.\n\
Action: the action to take, should always be one of [${tools.map(
(toolDocumentation) =&gt; toolDocumentation.name
)}].\n\
Action Input: the input to the action. Should list the input parameter as this format suggest: &quot;parameter1&quot;, &quot;parameter&quot;, ...]\n\
End Answer\n\n\
[End Answer Here]

Situation 2: When you don't need to use a tool, please follow this format to answer the question: \n\
Thought: you should always think about what to do.\n\
Final Answer: Provide your final answer for the input question from the input question or the Observation (if it exists).\n\
End Answer\n\n\
[End Answer Here]
}}}\n
</code></pre>
<p>The idea is to tell GPT-4 to always include &quot;End Answer&quot; at the end of its response so I can parse its output with regex. However, approximately 1/10 times, it fails to include that keyword and messes up my output parser. How can I improve my prompt to keep the result more consistent?</p>
<p>I try setting the temperature to 0, adding many delimeters as OpenAI suggests. But the result keeps being inconsistent</p>
","2023-06-23 19:58:54","","","2023-06-23 19:58:54","<openai-api><gpt-4>","0","0","0","12","","","","","","",""
"76173736","1","10759664","","GPT-2: Setting biases as -1 billion","<p>I'm currently trying to predict upcoming words given an input text chunk, but I want to &quot;mask&quot; the last <em>n</em> words of the input text by setting the attention weights to 0 (or something very small).</p>
<p>This is what I tried to do:</p>
<p>I tried modifying the biases on all layers of my GPT-2 model by setting them to a very small value for all tokens I want to mask. I read that you <em>add</em> the bias values to the dot-product of query and key vectors, so I figured they have to be negative and ideally very small in order to make the resulting attention weight as small as possible. In a <a href=""https://jalammar.github.io/illustrated-gpt2/"" rel=""nofollow noreferrer"">blog post</a> on self-attention I read that you can use either -inf or -1 billion (in GPT), but if I use any values &lt; -1, I get errors, possibly because I produce values that are so small that they produce underflow (although I think it's odd that -1 is basically the minimum cutoff value I can still use, that's not that small).</p>
<p>This is what I'd need advice on:</p>
<p>a) Does changing the biases like that make sense? I'm a newbie GPT-user so I'm always a little unsure whether what I do is correct.</p>
<p>b) If my approach makes sense, why can't I use values &lt; -1? Is there a way to use smaller values?</p>
<p>c) If not and I use -1, would that still work to reduce the attention
weights to something around 0?</p>
<p>This is my code:</p>
<pre><code># import modules
!pip install transformers

import numpy as np
import math

import tensorflow as tf
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# download pre-trained German GPT-2 model &amp; tokenizer from the Hugging Face model hub
tokenizer = AutoTokenizer.from_pretrained(&quot;dbmdz/german-gpt2&quot;)

# initialise the model, use end-of-sequence (EOS) token as padding tokens
model = AutoModelForCausalLM.from_pretrained(&quot;dbmdz/german-gpt2&quot;, pad_token_id = tokenizer.eos_token_id)



# set the input text and the number of words to mask
input_text = [&quot;Orlando&quot;, &quot;liebte&quot;, &quot;von&quot;, &quot;Natur&quot;, &quot;aus&quot;, &quot;einsame&quot;, &quot;Orte,&quot;, &quot;weite&quot;, &quot;Ausblicke&quot;, &quot;und&quot;, &quot;das&quot;, &quot;Gefühl,&quot;, &quot;für&quot;, &quot;immer&quot;, &quot;und&quot;, &quot;ewig&quot;] # allein zu sein.
n = 5  # I want to mask the last n words
print(&quot;masking the following&quot;, n, &quot;words now:&quot;, )

# 1 word can consist of many tokens, so get last n words 
# and tokenize them so we know how many tokens we have to mask:
masked_words = input_text[-n:]
n_tokens = len(tokenizer.tokenize(&quot; &quot;.join(masked_words)))
print(&quot; &quot;.join(masked_words))

# encode the full input text (including the words we want to mask) and get the attention mask
encoded_input = tokenizer.encode_plus(&quot; &quot;.join(input_text),
                                      add_special_tokens = False,  # don't add special tokens
                                      return_attention_mask = True,  # return the attention mask for the current text input
                                      return_tensors='pt')  # return output as PyTorch tensor object

# get attention mask from encoded input
attention_mask = encoded_input['attention_mask']

# check how many attention weights there are in the mask.
# mask_length should be number of tokens in the full sentence
mask_length = attention_mask.size()[1]

# Create new attention mask where the weights for the last n tokens are 
# set to - 1 billion

# Mask the last n words by setting them to - 1 billion, 
# but leave last token set to 1 (for the space after the last masked word)
# attention_mask[:, -(n_tokens + 1): -1] = -1 # this works
attention_mask[:, -(n_tokens + 1): -1] = -100000000 # this doesn't work
#print(attention_mask)


# Now we want to modify the attention weights on all layers by changing 
# the biases to our attention mask values there.

# Loop modules in model.transformer
# (all attention layers are modules in the transformer)
# Find attention modules and set our custom attention mask as biases 
for module in model.transformer.modules():
  # if the current module is a MultiHeadAttention object (aka an attention module)...
  if isinstance(module, torch.nn.MultiheadAttention):
    # set attention mask we defined earlier as the biases
    module.register_buffer(&quot;bias&quot;, attention_mask.unsqueeze(0))

# Get prediction from full model:
# use ids from input text (input_ids) &amp; the modified attention mask to generate the output
output = model.generate(encoded_input['input_ids'], 
                        attention_mask = attention_mask,
                        max_new_tokens = 10)

# get the predicted token ID and the corresponding text string
predicted_text = tokenizer.decode(output[0], 
                                  skip_special_tokens = True)


# print the predicted text
print(&quot;\n Predicted text:&quot;, predicted_text)
</code></pre>
<p>Thanks in advance for your help/ideas/comments!</p>
","2023-05-04 13:26:10","","2023-05-04 13:36:41","2023-05-04 13:39:31","<python><nlp><gpt-2>","0","1","0","46","","","","","","",""
"65145526","1","9291922","","Why new lines aren't generated with my fine-tuned DistilGPT2 model?","<p>I'm currently trying to fine-tune DistilGPT-2 (with Pytorch and HuggingFace transformers library) for a code completion task. My corpus is arranged like the following example:</p>
<pre><code>&lt;|startoftext|&gt;
public class FindCityByIdService {
    private CityRepository cityRepository = ...
&lt;|endoftext|&gt;
</code></pre>
<p>My first attempt was to run the following <a href=""https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_clm.py"" rel=""nofollow noreferrer"">script</a> from transformers library:</p>
<pre><code>python run_clm.py 
     --model_type=gpt2 \
     --model_name_or_path distilgpt2 \
     --do_train \
     --train_file $TRAIN_FILE \
     --num_train_epochs 100 \
     --output_dir $OUTPUT_DIR \
     --overwrite_output_dir \
     --save_steps 20000 \
     --per_device_train_batch_size 4 \
</code></pre>
<p>After doing some generation tests, I realized that the model is not predicting <code>\ n</code> for any given context. I imagine that some pre-process stage or something similar is missing. But anyway, what should I do so that <code>\ n</code> be predicted as expected?</p>
<p><a href=""https://discuss.huggingface.co/t/why-new-lines-arent-generated/2543"" rel=""nofollow noreferrer"">HF Forum question</a></p>
<p>Thanks!!</p>
","2020-12-04 14:37:53","","","2020-12-13 05:29:01","<pytorch><huggingface-transformers><gpt-2>","1","2","1","1079","","","","","","",""
"65341363","1","14844030","","What memory does Transformer Decoder Only use?","<p>I've been reading a lot about transformers and self attention and have seen both BERT and GPT-2 are a newer version that only use an encoder transformer (BERT) and decoder transformer (GPT-2). I've been trying to build a decoder only model for myself for next sequence prediction but am confused by one thing. I'm using PyTorch and have looked at there<a href=""https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"" rel=""nofollow noreferrer"">Seq2Seq tutorial</a> and then looked into the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#transformerdecoder"" rel=""nofollow noreferrer"">Transformer Decoder Block</a> which is made up of <a href=""https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer"" rel=""nofollow noreferrer"">Transformer Decoder Layers</a>. My confusion comes from the memory these need to be passed as well. In the documentation they say memory is the last layer of the encoder block which makes sense for a Seq2Seq model but I'm wanting to make a decoder only model. So my question is what do you pass a decoder only model like GPT-2 for memory if you do not have an encoder?</p>
","2020-12-17 13:08:59","","","2023-05-09 14:37:17","<python><pytorch><decoder><transformer-model><gpt-2>","1","0","2","1728","","","","","","",""
"65551516","1","843036","","Cannot convert from a fine-tuned GPT-2 model to a Tensorflow Lite model","<p>I've fine tuned a distilgpt2 model using my own text using <code>run_language_modeling.py</code> and its working fine after training and <code>run_generation.py</code> script produces the expected results.</p>
<p>Now I want to convert this to a Tensorflow Lite model and did so by using the following</p>
<pre><code>from transformers import *

CHECKPOINT_PATH = '/content/drive/My Drive/gpt2_finetuned_models/checkpoint-2500'

model = GPT2LMHeadModel.from_pretrained(&quot;distilgpt2&quot;)
model.save_pretrained(CHECKPOINT_PATH)
model = TFGPT2LMHeadModel.from_pretrained(CHECKPOINT_PATH, from_pt=True) 
</code></pre>
<p>But I dont think I'm doing this right as after conversion, when I write</p>
<pre><code>print(model.inputs)
print(model.outputs)
</code></pre>
<p>I get</p>
<pre><code>None
None
</code></pre>
<p>But I still went ahead with the TFLite conversion using :</p>
<pre><code>import tensorflow as tf

input_spec = tf.TensorSpec([1, 64], tf.int32)
model._set_inputs(input_spec, training=False)

converter = tf.lite.TFLiteConverter.from_keras_model(model)

# FP16 quantization:
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]

tflite_model = converter.convert()

open(&quot;/content/gpt2-fp16.tflite&quot;, &quot;wb&quot;).write(tflite_model)
</code></pre>
<p>But does not work and when using the generated <code>tflite</code> model I get the error:</p>
<blockquote>
<p>tensorflow/lite/kernels/kernel_util.cc:249 d1 == d2 || d1 == 1 || d2 == 1 was not true.</p>
</blockquote>
<p>Which I'm sure has something to to with my model not converting properly and getting <code>None</code> for input/output.</p>
<p>Does anyone have any idea how to fix this?</p>
<p>Thanks</p>
","2021-01-03 15:26:40","","","2021-01-03 15:26:40","<tensorflow><tensorflow-lite><huggingface-transformers><gpt-2>","0","3","1","546","","","","","","",""
"68442098","1","13531125","68442279","How to store API keys in environment variable? and call the same in google colab","<p>I'm not sure how to make a &quot;.json&quot; file with my GPT-3 API key/environment variable, but I'd like to utilize it in Google Colab for automatic code generation.</p>
<p>Could someone please show me how to do this?</p>
<p>I want to get the API key from the.json file using the code below.</p>
<pre><code>with open('GPT_SECRET_KEY.json') as f:
    data = json.load(f)
openai.api_key = data[&quot;API_KEY&quot;]
</code></pre>
","2021-07-19 14:20:48","","2021-07-19 14:28:15","2021-07-19 14:41:18","<python><environment-variables><gpt-3>","1","4","0","3723","","2","13151915","<p>To read from a json file you do the following:</p>
<pre><code>import json

my_key = ''
with open('GPT_SECRET_KEY.json', 'r') as file_to_read:
    json_data = json.load(file_to_read)
    my_key = json_data[&quot;API_KEY&quot;]
</code></pre>
<p>The structure of your json file should look something like this.</p>
<pre><code>{
    &quot;API_KEY&quot;: &quot;xxxxthis_is_the_key_you_are_targetingxxxx&quot;, 
    &quot;API_KEY1&quot;: &quot;xxxxxxxxxxxxxxxxxxxxxxx&quot;,
    &quot;API_KEY2&quot;: &quot;xxxxxxxxxxxxxxxxxxxxxxx&quot;
}
</code></pre>
<p><a href=""https://stackoverflow.com/questions/20199126/reading-json-from-a-file"">Here is a link</a> to a similar question if my answer was not clear enough.</p>
","2021-07-19 14:32:40","1","0"
"68604289","1","10605020","68656887","AttributeError: module transformers has no attribute TFGPTNeoForCausalLM","<p>I cloned this repository/documentation <a href=""https://huggingface.co/EleutherAI/gpt-neo-125M"" rel=""nofollow noreferrer"">https://huggingface.co/EleutherAI/gpt-neo-125M</a></p>
<p>I get the below error whether I run it on google collab or locally. I also installed transformers using this</p>
<pre><code>pip install git+https://github.com/huggingface/transformers
</code></pre>
<p>and made sure the configuration file is named as config.json</p>
<pre><code>      5 tokenizer = AutoTokenizer.from_pretrained(&quot;gpt-neo-125M/&quot;,from_tf=True)
----&gt; 6 model = AutoModelForCausalLM.from_pretrained(&quot;gpt-neo-125M&quot;,from_tf=True)
      7 
      8 

3 frames
/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py in __getattr__(self, name)

AttributeError: module transformers has no attribute TFGPTNeoForCausalLM

</code></pre>
<p>Full code:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM 

tokenizer = AutoTokenizer.from_pretrained(&quot;EleutherAI/gpt-neo-125M&quot;,from_tf=True)

model = AutoModelForCausalLM.from_pretrained(&quot;EleutherAI/gpt-neo-125M&quot;,from_tf=True)

</code></pre>
<p>transformers-cli env results:</p>
<ul>
<li><code>transformers</code> version: 4.10.0.dev0</li>
<li>Platform: Linux-4.4.0-19041-Microsoft-x86_64-with-glibc2.29</li>
<li>Python version: 3.8.5</li>
<li>PyTorch version (GPU?): 1.9.0+cpu (False)</li>
<li>Tensorflow version (GPU?): 2.5.0 (False)</li>
<li>Flax version (CPU?/GPU?/TPU?): not installed (NA)</li>
<li>Jax version: not installed</li>
<li>JaxLib version: not installed</li>
<li>Using GPU in script?: </li>
<li>Using distributed or parallel set-up in script?: </li>
</ul>
<p>Both collab and locally have TensorFlow 2.5.0 version</p>
","2021-07-31 17:14:49","","2021-08-01 09:27:54","2021-08-04 19:14:11","<python><pytorch><huggingface-transformers><google-publisher-tag><gpt-3>","2","0","2","7265","","2","10605020","<p>My solution was to first edit the source code to remove the line that adds &quot;TF&quot; in front of the package as the correct transformers module is GPTNeoForCausalLM
, but somewhere in the source code it manually added a &quot;TF&quot; in front of it.</p>
<p>Secondly, before cloning the repository it is a must to run</p>
<pre><code> git lfs install. 
</code></pre>
<p>This link helped me install git lfs properly <a href=""https://askubuntu.com/questions/799341/how-to-install-git-lfs-on-ubuntu-16-04"">https://askubuntu.com/questions/799341/how-to-install-git-lfs-on-ubuntu-16-04</a></p>
","2021-08-04 19:14:11","2","0"
"69931987","1","4124584","69932414","GPT-3 davinci gives different results with the same prompt","<p>I am not sure if you have access to GPT-3, particularly DaVinci (the complete-a-sentence tool). You can find the API and info <a href=""https://beta.openai.com/docs/api-reference/completions/create-via-get"" rel=""nofollow noreferrer"">here</a></p>
<p>I've been trying this tool for the past hour and every time I hit their API using the same prompt (indeed the same input), I received a different response.</p>
<ol>
<li>Do you happen to encounter the same situation?</li>
<li>If this is expected, do you happen to know the reason behind it?</li>
</ol>
<p>Here are some examples</p>
<p><strong>Request header</strong> <em>(I tried to use the same example they provide)</em></p>
<pre><code>{
  &quot;prompt&quot;: &quot;Once upon a time&quot;,
  &quot;max_tokens&quot;: 3,
  &quot;temperature&quot;: 1,
  &quot;top_p&quot;: 1,
  &quot;n&quot;: 1,
  &quot;stream&quot;: false,
  &quot;logprobs&quot;: null,
  &quot;stop&quot;: &quot;\n&quot;
}
</code></pre>
<p><strong>Output 1</strong></p>
<pre><code>&quot;choices&quot;: [
        {
            &quot;text&quot;: &quot;, this column&quot;,
            &quot;index&quot;: 0,
            &quot;logprobs&quot;: null,
            &quot;finish_reason&quot;: &quot;length&quot;
        }
    ]
</code></pre>
<p><strong>Output 2</strong></p>
<pre><code>&quot;choices&quot;: [
        {
            &quot;text&quot;: &quot;, winter break&quot;,
            &quot;index&quot;: 0,
            &quot;logprobs&quot;: null,
            &quot;finish_reason&quot;: &quot;length&quot;
        }
    ]
</code></pre>
<p><strong>Output 3</strong></p>
<pre><code>&quot;choices&quot;: [
        {
            &quot;text&quot;: &quot;, the traditional&quot;,
            &quot;index&quot;: 0,
            &quot;logprobs&quot;: null,
            &quot;finish_reason&quot;: &quot;length&quot;
        }
    ]
</code></pre>
","2021-11-11 16:49:41","","","2023-01-13 22:52:41","<text><nlp><autocomplete><gpt-3>","2","0","4","2981","","2","4124584","<p>I just talked to OpenAI and they said that their response is not deterministic. It's probabilistic so that it can be creative. In order to make it deterministic or reduce the risk of being probabilistic, they suggest adjusting the <code>temperature</code> parameter. By default, it is 1 (i.e. 100% taking risks). If we want to make it completely deterministic, set it to 0.</p>
<p>Another parameter is <code>top_p</code> (default=1) that can be used to set the state of being deterministic. But they don't recommend tweaking both <code>temperature</code> and <code>top_p</code>. Only one of them would do the job.</p>
","2021-11-11 17:21:08","0","2"
"66852791","1","13868065","","Key difference between BERT and GPT2?","<p>I read lots of articles and people are saying BERT is good for NLU while GPT is good for NLG. But the key difference in structure between them is just adding a mask or not in self-attention, and trained the model in different ways.</p>
<p>From the code below, if I understand correctly, we are free to choose add an attention mask or not.
<a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py</a>
<a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/models/gpt2/modeling_gpt2.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/src/transformers/models/gpt2/modeling_gpt2.py</a></p>
<p>So can I come to the conclusion that it should &quot;the pretrained parameters for BERT is good for NLU&quot; and &quot;the pretrained parameters for GPT2 is good for NLG&quot;?
Or is there any other critical difference between these two that make people come to this conclusion I mentioned at the beginning?</p>
","2021-03-29 10:45:17","","","2021-03-30 09:03:33","<bert-language-model><gpt-2>","1","1","1","1084","","","","","","",""
"66991360","1","15252562","","GPT-2's encoder.py and train.py are not working","<p>I'm trying to train GPT-2 to use what I provide in a text file, napoleon.txt. When I run the encoder, it seems to work from the command prompt.</p>
<pre><code>python encoder.py napoleon.txt napoleon.npz
</code></pre>
<p>It doesn't, however, actually create napoleon.npz. But this is only part of the problem. The larger issue is that train.py, what I actually need in order to train GPT-2, spits out an error every single time.</p>
<pre><code>Traceback (most recent call last):
  File &quot;train.py&quot;, line 19, in &lt;module&gt;
    from .dataset import Corpus, EncodedDataset
ImportError: attempted relative import with no known parent package
</code></pre>
<p>I've tried every single solution I found on the internet and that I could think of, but I'm stuck. Please help</p>
","2021-04-07 17:41:11","","","2021-04-07 17:47:38","<python><artificial-intelligence><gpt-2>","1","0","0","280","","","","","","",""
"61510865","1","2058221","","Tensorflow has no Attribute ""sort"" in GPT 2 Git Release?","<p>I downloaded the git repo (<a href=""https://github.com/openai/gpt-2"" rel=""nofollow noreferrer"">https://github.com/openai/gpt-2</a>) and followed the python3 instructions (in DEVELOPERS.MD) for installation on my Kubuntu 18.04LTS box, but I cannot run it and instead get an error.</p>

<p>Here is what I've done so far:</p>

<pre><code>pip3 install tensorflow==1.12.0
pip3 install -r requirements.txt
python3 download_model.py 124M
python3 download_model.py 355M
python3 download_model.py 774M
python3 download_model.py 1558M
export PYTHONIOENCODING=UTF-8
</code></pre>

<p>I then ran:</p>

<pre><code>sarah@LesserArk:~/Custom Programs/gpt-2$ python3 src/interactive_conditional_samples.py 
/home/sarah/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/sarah/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/sarah/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/sarah/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/sarah/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/sarah/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
2020-04-29 16:08:30.016586: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""src/interactive_conditional_samples.py"", line 91, in &lt;module&gt;
    fire.Fire(interact_model)
  File ""/home/sarah/.local/lib/python3.6/site-packages/fire/core.py"", line 138, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File ""/home/sarah/.local/lib/python3.6/site-packages/fire/core.py"", line 468, in _Fire
    target=component.__name__)
  File ""/home/sarah/.local/lib/python3.6/site-packages/fire/core.py"", line 672, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File ""src/interactive_conditional_samples.py"", line 65, in interact_model
    temperature=temperature, top_k=top_k, top_p=top_p
  File ""/home/sarah/Custom Programs/gpt-2/src/sample.py"", line 74, in sample_sequence
    past, prev, output = body(None, context, context)
  File ""/home/sarah/Custom Programs/gpt-2/src/sample.py"", line 66, in body
    logits = top_p_logits(logits, p=top_p)
  File ""/home/sarah/Custom Programs/gpt-2/src/sample.py"", line 28, in top_p_logits
    sorted_logits = tf.sort(logits, direction='DESCENDING', axis=-1)
AttributeError: module 'tensorflow' has no attribute 'sort'
</code></pre>

<p>Which culminates in the error: <code>AttributeError: module 'tensorflow' has no attribute 'sort'</code>.</p>

<p>This is strange, and I'm not sure how to proceed. I would have thought that the instructions would lead to successful installation, but it appears that they don't.</p>

<p>Uninstalling and reinstalling has no effect on the final result? How can I get tensorflow to execute GPT-II?</p>
","2020-04-29 20:14:22","","2020-11-29 11:59:08","2021-02-15 18:31:19","<tensorflow><gpt-2>","2","1","3","1586","","","","","","",""
"69403613","1","2632462","","How to early-stop autoregressive model with a list of stop words?","<p>I am using GPT-Neo model from <code>transformers</code> to generate text. Because the prompt I use starts with <code>'{'</code>, so I would like to stop the sentence once the paring <code>'}'</code> is generated.
I found that there is a <code>StoppingCriteria</code> method in the source code but without further instructions on how to use it. Does anyone have found a way to early-stop the model generation? Thanks!</p>
<p>Here is what I've tried:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import StoppingCriteria, AutoModelForCausalLM, AutoTokenizer
model_name = 'gpt2'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, pad_token_id=tokenizer.eos_token_id, torch_dtype=dtype).eval()

class KeywordsStoppingCriteria(StoppingCriteria):
    def __init__(self, keywords_ids:list):
        self.keywords = keywords_ids

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -&gt; bool:
        if input_ids in self.keywords:
            return True
        return False

stop_words = ['}', ' }', '\n']
stop_ids = [tokenizer.encode(w) for w in stop_words]
stop_ids.append(tokenizer.eos_token_id)
stop_criteria = KeywordsStoppingCriteria(stop_ids)

model.generate(
    text_inputs='some text:{', 
    StoppingCriteria=stop_criteria
)

</code></pre>
","2021-10-01 09:30:21","","2021-10-10 16:57:10","2022-04-25 17:29:51","<python><huggingface-transformers><autoregressive-models><gpt-2>","1","2","3","1181","","","","","","",""
"67379533","1","10575373","","Why some weights of GPT2Model are not initialized?","<p>I am using the GPT2 pre-trained model for a research project and when I load the pre-trained model with the following code,</p>
<pre><code>from transformers.models.gpt2.modeling_gpt2 import GPT2Model
gpt2 = GPT2Model.from_pretrained('gpt2')
</code></pre>
<p>I get the following warning message:</p>
<blockquote>
<p>Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.</p>
</blockquote>
<p>From my understanding, it says that the weights of the above layers are not initialized from the pre-trained model. But we all know that attention layers ('attn') are so important in GPT2 and if we can not have their actual weights from the pre-trained model, then what is the point of using a pre-trained model?</p>
<p>I really appreciate it if someone could explain this to me and tell me how I can fix this.</p>
","2021-05-04 05:59:46","","","2021-05-12 21:43:34","<pytorch><huggingface-transformers><gpt-2>","1","2","3","980","","","","","","",""
"68038662","1","16262479","68038692","How to get th content of a string inside a request response?","<p>I was coding a webapp based on GPT-2 but it was not good so I decided to switch to official OpenAI GPT-3.
So I make that request:</p>
<pre><code>response = openai.Completion.create(
  engine=&quot;davinci&quot;,
  prompt=&quot;Hello&quot;,
  temperature=0.7,
  max_tokens=64,
  top_p=1,
  frequency_penalty=0,
  presence_penalty=0
)
</code></pre>
<p>And when I print the response I get this:</p>
<pre><code>{
  &quot;choices&quot;: [
    {
      &quot;finish_reason&quot;: &quot;length&quot;,
      &quot;index&quot;: 0,
      &quot;logprobs&quot;: null,
      &quot;text&quot;: &quot;, everyone, and welcome to the first installment of the new opening&quot;
    }
  ],
  &quot;created&quot;: 1624033807,
  &quot;id&quot;: &quot;cmpl-3CBfb8yZAFEUIVXfZO90m77dgd9V4&quot;,
  &quot;model&quot;: &quot;davinci:2020-05-03&quot;,
  &quot;object&quot;: &quot;text_completion&quot;
}
</code></pre>
<p>But I only want to print the text, so how can I do to print the &quot;text&quot; value in the response list.
Thank you in advance and have a good day.</p>
","2021-06-18 16:34:08","","","2022-10-11 03:42:50","<python><python-requests><openai-api><gpt-3>","3","0","0","5316","","2","7212686","<p>Using the dict indexing by key, and the list indexing by index</p>
<pre><code>x = {&quot;choices&quot;: [{&quot;finish_reason&quot;: &quot;length&quot;,
                  &quot;text&quot;: &quot;, everyone, and welcome to the first installment of the new opening&quot;}], }

text = x['choices'][0]['text']
print(text)  # , everyone, and welcome to the first installment of the new opening
</code></pre>
","2021-06-18 16:36:38","0","4"
"69549494","1","17139319","69549620","A way to make GPT-3's ""davinci"" converse with a user(s) through a bot in discord using discord.js?","<pre><code>var collector = new MessageCollector(message.channel, filter, {
    max: 10,
    time: 60000,
})
    start_sequence = &quot;\nAI: &quot;
    
    retart_sequence = &quot;\nHuman: &quot;

        collector.on(&quot;collect&quot;, (msg) =&gt; {
            console.log(msg.content)
            
        openai.Completion.create({
            
            engine: &quot;davinci&quot;,
            prompt: msg.content,
            temperature: 0.9,
            max_tokens: 150,
            top_p: 1,
            frequency_penalty: 0.35,
            presence_penalty: 0.6, 
            stop: [&quot;\n&quot;, &quot; Human:&quot;, &quot; AI:&quot;]  
        
        }).then((response) =&gt; {
            
            message.channel.send(response.choices[0].text)
        })

    })
}
</code></pre>
<p>I tried this but it only gives back completions, like the default preset rather than the chat preset in GPT-3's &quot;playground&quot;. I'm using openai-node to code in javascript rather than python to call openAI API.</p>
","2021-10-13 03:43:17","","","2021-10-13 04:03:48","<javascript><discord.js><openai-api><gpt-3>","1","0","1","408","","2","4384238","<p>Your <code>prompt</code> needs to be given more information for GPT-3 to understand what you want. You're providing a prompt of the message, such as</p>
<pre><code>My message!
</code></pre>
<p>But what you really should be giving it is something like:</p>
<pre><code>The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.

Human: Hello, who are you?
AI: I am an AI created by OpenAI. How can I help you today?
Human: My message!
AI:
</code></pre>
<p>In addition, if you it to be contextually aware, you need to continue adding information to the prompt, such as:</p>
<pre><code>The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.

Human: Hello, who are you?
AI: I am an AI created by OpenAI. How can I help you today?
Human: My message!
AI: Response here
Human: Another message here
AI:
</code></pre>
<p><strong>Be aware of the token limits and costs.</strong> You may to choose to make it <em>not</em> contextual, or at some point start cutting out previous messages.</p>
","2021-10-13 04:03:48","0","1"
"72554328","1","2195440","72718976","How to fine tune fine tune GitHub Copilot?","<p>We can fine tune language models like <code>BERT</code>, <code>GPT-3</code>.</p>
<p>Can I fine tune <code>GitHub Copilot</code> model?</p>
<p>I have already looked into examples from <a href=""https://copilot.github.com/"" rel=""nofollow noreferrer"">https://copilot.github.com/</a> but cant find the details.</p>
<p>Would really appreciate if someone had fine tuned Github Copilot.</p>
","2022-06-09 03:12:02","","2023-01-18 21:35:08","2023-01-18 21:35:08","<github><deep-learning><openai-api><gpt-3><github-copilot>","3","0","3","2020","0","2","6309","<p>There does not seem to be a client-facing feature allowing you to fine-tune Copilot directly.</p>
<p>Here are two illustration as to why this feature is, for now (Q2 2022) missing.</p>
<p>The <a href=""https://github.com/features/copilot"" rel=""nofollow noreferrer"">Copilot feature page</a> initially included this:</p>
<blockquote>
<h2>How will GitHub Copilot get better over time?</h2>
<p>GitHub Copilot doesn’t actually test the code it suggests, so the code may not even compile or run. GitHub Copilot can only hold a very limited context, so even single source files longer than a few hundred lines are clipped and only the immediately preceding context is used. And GitHub Copilot may suggest old or deprecated uses of libraries and languages. You can use the code anywhere, but you do so at your own risk.</p>
</blockquote>
<p>As <a href=""https://twitter.com/tomekkorbak"" rel=""nofollow noreferrer"">Tomek Korbak</a> explains <a href=""https://twitter.com/tomekkorbak/status/1410554250514636805"" rel=""nofollow noreferrer"">on Twitter</a>:</p>
<blockquote>
<p>Actually, Copilot's completions will always be optimised for human's liking, not necessarily compiler's liking.</p>
<p>That's because the language model training objective (predicting the next token in text) is great at capturing short-term dependencies (which explains the human feel of generated snippets).</p>
<p>But it struggles to capture long-term, global, semantic properties of generated sequences such as compilability. And there's no easy way of including compilability as a signal for their training.</p>
<p>The standard way -- fine-tuning language models using RL with compilability as a reward -- notoriously leads to catastrophic forgetting: less diverse and less accurate completions.</p>
</blockquote>
<p>Tomek references &quot;<a href=""https://arxiv.org/pdf/2106.04985.pdf"" rel=""nofollow noreferrer"">Energy-Based Models for Code Generation under Compilability Constraints (pdf)</a>&quot;</p>
<blockquote>
<p><a href=""https://i.stack.imgur.com/ulfPr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ulfPr.png"" alt=""https://pbs.twimg.com/media/E5NHqGjXIAYRtwa?format=png&amp;name=small"" /></a></p>
<p>Our solution (KL-DPG) boosts compilability rate of generated sequences from 55% to 70%.<br />
RL fine-tuning can do better but at a cost of catastrophic forgetting.</p>
<p>Overall, energy-based models (EBMs) turn out to be great at expressing weird, sequence-level constraints that would be super hard as to express as normalised priors for autoregressive language models.</p>
<p>EBMs provide a way of injecting our structured, symbolic knowledge into large language models without breaking them down or sacrificing their uncanny abilities.<br />
The space of further applications in controllable generation is huge.</p>
</blockquote>
<p>So not so easy.</p>
<p><a href=""https://tmabraham.github.io/"" rel=""nofollow noreferrer"">Tanishq Mathew Abraham</a> explains in &quot;<a href=""https://tmabraham.github.io/blog/github_copilot"" rel=""nofollow noreferrer"">Coding with GitHub Copilot</a>&quot;</p>
<blockquote>
<p>I wonder if the GitHub team might also develop a way of perhaps fine-tuning GitHub Copilot to specific use-cases.</p>
<p>For example, there may be a specific GitHub Copilot models for fastai, JAX, etc. They would be fine-tuned on the source code of of these libraries and codebases that use these libraries.</p>
<p>But making sure that the tool does not provide outdated suggestions would still be a challenge.<br />
I don’t think it would be possible to provide suggestions for a brand-new library that does not have enough codebases using it to train on.</p>
<p>Additionally, for situations like fastai where there are older APIs and newer APIs, when fine-tuning a model, the codebases using the older APIs would have to be filtered out.</p>
</blockquote>
","2022-06-22 16:25:11","0","1"
"67444616","1","10575373","","How to increase batch size in GPT2 training for translation task?","<p>I am developing a code to use the pre-trained <a href=""https://huggingface.co/transformers/model_doc/gpt2.html"" rel=""nofollow noreferrer"">GPT2</a> model for a machine translation task. The length of my data's word-to-id is 91, and I developed the following code for my model:</p>
<pre><code>import torch
from torch.utils.data import DataLoader
from transformers.models.gpt2.modeling_gpt2 import GPT2Model

# data preparation code

def batch_sequences(x, y, env):
    &quot;&quot;&quot;
    Take as input a list of n sequences (torch.LongTensor vectors) and return
    a tensor of size (slen, n) where slen is the length of the longest
    sentence, and a vector lengths containing the length of each sentence.
    &quot;&quot;&quot;
    lengths_x = torch.LongTensor([len(s) + 2 for s in x])
    lengths_y = torch.LongTensor([len(s) + 2 for s in y])
    max_length = max(lengths_x.max().item(), lengths_y.max().item())
    sent_x = torch.LongTensor(
        max_length, lengths_x.size(0)).fill_(env.pad_index)
    sent_y = torch.LongTensor(
        max_length, lengths_y.size(0)).fill_(env.pad_index)
    assert lengths_x.min().item() &gt; 2
    assert lengths_y.min().item() &gt; 2

    sent_x[0] = env.eos_index
    for i, s in enumerate(x):
        sent_x[1:lengths_x[i] - 1, i].copy_(s)
        sent_x[lengths_x[i] - 1, i] = env.eos_index

    sent_y[0] = env.eos_index
    for i, s in enumerate(y):
        sent_y[1:lengths_y[i] - 1, i].copy_(s)
        sent_y[lengths_y[i] - 1, i] = env.eos_index

    return sent_x, sent_y, max_length

def collate_fn(elements):
    &quot;&quot;&quot;
    Collate samples into a batch.
    &quot;&quot;&quot;
    x, y = zip(*elements)
    x = [torch.LongTensor([env.word2id[w]
                          for w in seq if w in env.word2id]) for seq in x]
    y = [torch.LongTensor([env.word2id[w]
                          for w in seq if w in env.word2id]) for seq in y]
    x, y, length = batch_sequences(x, y, env)
    return (x, length), (y, length), torch.LongTensor(nb_ops)

loader = DataLoader(data, batch_size=1, shuffle=False, collate_fn=collate_fn)
gpt2 = GPT2Model.from_pretrained('gpt2')
in_layer = nn.Embedding(len(env.word2id), 768)
out_layer = nn.Linear(768, len(env.word2id))

parameters = list(gpt2.parameters()) + list(in_layer.parameters()) + list(out_layer.parameters())
optimizer = torch.optim.Adam(parameters)
loss_fn = nn.CrossEntropyLoss()
for layer in (gpt2, in_layer, out_layer):
    layer.train()

accuracies = list()
n_epochs = 5
for i in range(n_epochs):
    for (x, x_len), (y, y_len) in loader:

        x = x.to(device=device)
        y = y.to(device=device)

        embeddings = in_layer(x.reshape(1, -1))
        hidden_state = gpt2(inputs_embeds=embeddings).last_hidden_state[:, :]
        logits = out_layer(hidden_state)[0]
        loss = loss_fn(logits, y.reshape(-1))
        accuracies.append(
            (logits.argmax(dim=-1) == y.reshape(-1)).float().mean().item())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if len(accuracies) % 500 == 0:
            accuracy = sum(accuracies[-50:]) / len(accuracies[-50:])
            print(f'Samples: {len(accuracies)}, Accuracy: {accuracy}')
</code></pre>
<p>This code works pretty well when the batch size is 1. But it is so slow. I wanted to increase the batch size from 1 to 32, but I get some dimension compatibility problems. How can I increase the batch size without errors?</p>
<p>My data consists of pair of sentences, the first one is a sentence in the first language and the second one is its translation in the second language.</p>
<p>For example, assume that x.shape is (batch_size, 12) (meaning we have 'batch_size' sentences of length 12 as input and y.shape is also (batch_size, 12) (the translations). And also we have a word-to-id dictionary of length 90 that matches each word in a sentence with its index)</p>
","2021-05-08 06:12:06","","2021-05-11 07:30:58","2021-05-13 17:09:35","<nlp><pytorch><gpt-2>","1","2","2","1088","0","","","","","",""
"68444704","1","13531125","","How to fix the error : ""cannot import name 'GPT' from ""gpt""","<p>When I run the code below in Google Colab, I get the following error.</p>
<p>Note: I've already installed gpt using pip (!pip install gpt).</p>
<p><strong>code</strong></p>
<pre><code>from gpt import GPT
from gpt import Example'
</code></pre>
<p><strong>Error</strong></p>
<pre><code>cannot import name 'GPT' from 'gpt' (/usr/local/lib/python3.7/dist-packages/gpt/__init__.py)
</code></pre>
<p>Could someone help me fix this issue?</p>
","2021-07-19 17:29:47","","","2022-02-04 13:31:38","<python><gpt-3>","1","3","1","2231","","","","","","",""
"72663133","1","955883","","GPT-3 fine tuning Error: Incorrect API key provided","<p>I'm following <a href=""https://colab.research.google.com/drive/1PYwme_-SDOfUyg5gq7gdMuHtZMbIrcjz?usp=sharing"" rel=""nofollow noreferrer"">this tutorial</a> to fine-tune a GPT-3 model. However, when I run this part of the Code:</p>
<pre><code># Enter credentials
%env OPENAI_API_KEY= &quot;&lt;MY OPENAI KEY&gt;&quot;
!openai api fine_tunes.create \
-t dw_train.jsonl \
-v dw_valid.jsonl \
-m $model \
--n_epochs $n_epochs \
--batch_size $batch_size \
--learning_rate_multiplier $learning_rate_multiplier \
--prompt_loss_weight $prompt_loss_weight
</code></pre>
<p>I get this error:</p>
<blockquote>
<p>Error: Incorrect API key provided:
&quot;sk-czja*****************************************gk0&quot;. You can find
your API key at <a href=""https://beta.openai.com"" rel=""nofollow noreferrer"">https://beta.openai.com</a>. (HTTP status code: 401)</p>
</blockquote>
<p>The curious thing is that the API key is correct. So much so that, if I use it to make a prompt, it works perfectly. Example:</p>
<pre><code>def GPT_Completion(texts):
  response = openai.Completion.create(
    engine=&quot;text-davinci-002&quot;,
    prompt =  texts,
    temperature = 0.6,
    top_p = 1,
    max_tokens = 64,
    frequency_penalty = 0,
    presence_penalty = 0
    )
  return print(response.choices[0].text)

  GPT_Completion(&quot;My dear friend,&quot;)
</code></pre>
<p>What could be causing this error? I thought maybe the GPT-3 training could require a paid account. However, I did not find this restriction on the OpenAI website.</p>
<p>The whole code I'm using is <a href=""https://colab.research.google.com/drive/1jK5LAXCaR835bBBd2S45js3_6Tir8kxE"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Thank you in advance for any help!</p>
","2022-06-17 17:53:53","","","2023-03-20 07:06:30","<python><api><openai-api><gpt-3>","1","1","3","10110","","","","","","",""
"71690297","1","11410327","","Large Language Model Perplexity","<p>i am currently using GPT-3 and i am trying to compare its capabilities to related language models for my masters thesis.
Unfortunatly GPT-3 is an API based application, so i am not really able to extract metrics such as perplexity.</p>
<p>Over the API i have acces to these three metrics and of course the models outputs:</p>
<ul>
<li><p>training_loss: loss on the training batch</p>
</li>
<li><p>training_sequence_accuracy: the percentage of completions in the training batch for which the model's predicted tokens matched the true completion tokens exactly. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 2/3 = 0.67</p>
</li>
<li><p>training_token_accuracy: the percentage of tokens in the training batch that were correctly predicted by the model. For example, with a batch_size of 3, if your data contains the completions [[1, 2], [0, 5], [4, 2]] and the model predicted [[1, 1], [0, 5], [4, 2]], this accuracy will be 5/6 = 0.83</p>
</li>
</ul>
<p>Is there any possibility to calculate the perplexity of my model using python?</p>
<p>Thank you.</p>
","2022-03-31 09:41:36","","","2022-03-31 09:41:36","<python><nlp><nltk><gpt-3><perplexity>","0","2","1","378","","","","","","",""
"65822014","1","1793799","","RuntimeError: Input tensor at index 3 has invalid shape [2, 2, 16, 128, 64] but expected [2, 4, 16, 128, 64]","<p>Runtime error while finetuning a pretrained <a href=""https://huggingface.co/gpt2-medium"" rel=""nofollow noreferrer"">GPT2-medium</a> model using <a href=""https://huggingface.co/transformers/"" rel=""nofollow noreferrer"">Huggingface</a> library in SageMaker - <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html"" rel=""nofollow noreferrer"">ml.p3.8xlarge</a> instance.</p>
<p>The <code>finetuning_gpt2_script.py</code> contains the below,</p>
<p>Libraries:</p>
<pre><code>from transformers import Trainer, TrainingArguments
from transformers import EarlyStoppingCallback
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from transformers import TextDataset,DataCollatorForLanguageModeling
</code></pre>
<p>Pretrained Models:</p>
<pre><code>gpt2_model = GPT2LMHeadModel.from_pretrained(&quot;gpt2-medium&quot;)
gpt2_tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2-medium&quot;)
</code></pre>
<p>Train and Test Data Construction:</p>
<pre><code>train_dataset = TextDataset(
          tokenizer=gpt2_tokenizer,
          file_path=train_path,
          block_size=128)
    
test_dataset = TextDataset(
          tokenizer=gpt2_tokenizer,
          file_path=test_path,
          block_size=128)
    
data_collator = DataCollatorForLanguageModeling(
        tokenizer=gpt2_tokenizer, mlm=False,
    )
</code></pre>
<p><code>train_path</code> &amp; <code>test_path</code> are unstructured text data file of size 1.45 Million and 200K lines of data</p>
<p>Training arguments:</p>
<pre><code>training_args = TrainingArguments(
        output_dir=&quot;./gpt2-finetuned-models&quot;, #The output directory
        overwrite_output_dir=True, #overwrite the content of the output directory
        num_train_epochs=1, # number of training epochs
        per_device_train_batch_size=8, # batch size for training #32
        per_device_eval_batch_size=8,  # batch size for evaluation #64
        save_steps=100, # after # steps model is saved
        warmup_steps=500,# number of warmup steps for learning rate scheduler
        prediction_loss_only=True,
        metric_for_best_model = &quot;eval_loss&quot;,
        load_best_model_at_end = True,
        evaluation_strategy=&quot;epoch&quot;,
    )
</code></pre>
<p><code>training_args</code> are the training arguments constructed to train the model.</p>
<p>Trainer:</p>
<pre><code>trainer = Trainer(
        model=gpt2_model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        callbacks = [early_stop_callback],
    )
early_stop_callback = EarlyStoppingCallback(early_stopping_patience  = 3)
</code></pre>
<p>Training:</p>
<pre><code>trainer.train()
trainer.save_model(model_path)
</code></pre>
<p>Here, the training is done for only 1 epoch in 4 GPUS using <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html"" rel=""nofollow noreferrer"">ml.p3.8xlarge</a> instance.</p>
<p>The training is done by torch-distribution like below,</p>
<pre><code>python -m torch.distributed.launch finetuning_gpt2_script.py
</code></pre>
<p>While training at the end of the epoch, observed the below error,</p>
<p><code>RuntimeError: Input tensor at index 3 has invalid shape [2, 2, 16, 128, 64] but expected [2, 4, 16, 128, 64]</code></p>
<ol>
<li>Is the <code>RuntimeError</code> because of the way the <code>train_dataset</code> and <code>test_dataset</code>constructed using <code>TextData</code> ?</li>
<li>Am I doing wrong in the <code>torch-distribution</code> ?</li>
</ol>
","2021-01-21 06:11:18","","","2022-02-28 05:45:52","<python><pytorch><amazon-sagemaker><huggingface-transformers><gpt-2>","1","0","1","1142","","","","","","",""
"66647600","1","15404079","","Speeding up Inference time on GPT2 - optimizing tf.sess.run()","<p>I am trying to optimize the inference time on GPT2. The current time to generate a sample after calling the script is 55 secs on Google Colab. I put in timestamps to try to isolate where the bottleneck is.
This is the code:</p>
<pre><code> for _ in range(nsamples // batch_size):
            out = sess.run(output, feed_dict={
                context: [context_tokens for _ in range(batch_size)]
            })[:, len(context_tokens):]
            for i in range(batch_size):
                generated += 1
                text = enc.decode(out[i])
                print(&quot;=&quot; * 40 + &quot; SAMPLE &quot; + str(generated) + &quot; &quot; + &quot;=&quot; * 40)
                print(text)
        print(&quot;=&quot; * 80)
</code></pre>
<p>The line</p>
<pre><code>out = sess.run(output, feed_dict={
                context: [context_tokens for _ in range(batch_size)]
            })[:, len(context_tokens):] 
</code></pre>
<p>is where the complexity lies. Does anyone have any way I can improve this piece of code ? Thank you so much!</p>
","2021-03-16 00:39:22","","","2021-07-27 01:54:51","<tensorflow><gpt-2><sess.run>","1","0","0","281","","","","","","",""
"72922146","1","19453849","","Why do 'callback' and 'tweet' fail in my Twitter GPT-3 OpenAI bot?","<p>I am trying to develop a Twitter bot using OpenAI gpt-3 library, following <a href=""https://www.youtube.com/watch?v=V7LEihbOv3Y&amp;t=1s"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=V7LEihbOv3Y&amp;t=1s</a> this tutorial.</p>
<p>I am getting this same error again and again.
<a href=""https://i.stack.imgur.com/cT9lZ.png"" rel=""nofollow noreferrer"">I clicked on AUTH link and authorized the bot. When I clicked on the other two links, I got this error.</a>
I have recently got Elevated access for my Twitter bot and hence, tried using this bot again.</p>
<p>What should I do?
Please help, thank you.</p>
<p>Code :-</p>
<pre><code>=== Serving from '/Users/tanmayjuneja/Documents/demos/twitterbot'...

⚠  Your requested &quot;node&quot; version &quot;16&quot; doesn't match your global version &quot;12&quot;. Using node@12 from host.
i  functions: Watching &quot;/Users/tanmayjuneja/Documents/demos/twitterbot/functions&quot; for Cloud Functions...
✔  functions[us-central1-auth]: http function initialized (http://localhost:5000/twitterbot-e8123/us-central1/auth).
✔  functions[us-central1-callback]: http function initialized (http://localhost:5000/twitterbot-e8123/us-central1/callback).
✔  functions[us-central1-tweet]: http function initialized (http://localhost:5000/twitterbot-e8123/us-central1/tweet).
⚠  functions: The Cloud Firestore emulator is not running, so calls to Firestore will affect production.
i  functions: Beginning execution of &quot;auth&quot;
⚠  Google API requested!
   - URL: &quot;https://oauth2.googleapis.com/token&quot;
   - Be careful, this may be a production service.
i  functions: Finished &quot;auth&quot; in ~1s
i  functions: Beginning execution of &quot;callback&quot;
⚠  Google API requested!
   - URL: &quot;https://oauth2.googleapis.com/token&quot;
   - Be careful, this may be a production service.
i  functions: Finished &quot;callback&quot; in ~3s
i  functions: Beginning execution of &quot;callback&quot;
⚠  functions: Error: Request failed with code 400
    at RequestHandlerHelper.createResponseError (/Users/tanmayjuneja/Documents/demos/twitterbot/functions/node_modules/twitter-api-v2/dist/client-mixins/request-handler.helper.js:103:16)
    at RequestHandlerHelper.onResponseEndHandler (/Users/tanmayjuneja/Documents/demos/twitterbot/functions/node_modules/twitter-api-v2/dist/client-mixins/request-handler.helper.js:252:25)
    at Gunzip.emit (events.js:327:22)
    at endReadableNT (_stream_readable.js:1221:12)
    at processTicksAndRejections (internal/process/task_queues.js:84:21)
⚠  Your function was killed because it raised an unhandled error.
i  functions: Beginning execution of &quot;tweet&quot;
⚠  Google API requested!
   - URL: &quot;https://oauth2.googleapis.com/token&quot;
   - Be careful, this may be a production service.
⚠  functions: Error: Request failed with status code 400
    at createError (/Users/tanmayjuneja/Documents/demos/twitterbot/functions/node_modules/axios/lib/core/createError.js:16:15)
    at settle (/Users/tanmayjuneja/Documents/demos/twitterbot/functions/node_modules/axios/lib/core/settle.js:17:12)
    at IncomingMessage.handleStreamEnd (/Users/tanmayjuneja/Documents/demos/twitterbot/functions/node_modules/axios/lib/adapters/http.js:322:11)
    at IncomingMessage.emit (events.js:327:22)
    at endReadableNT (_stream_readable.js:1221:12)
    at processTicksAndRejections (internal/process/task_queues.js:84:21)
⚠  Your function was killed because it raised an unhandled error.
</code></pre>
","2022-07-09 14:24:19","","2022-07-09 14:26:36","2022-07-09 14:26:36","<javascript><twitter><twitter-oauth><openai-api><gpt-3>","0","0","1","93","","","","","","",""
"73797902","1","11108470","73853263","GPT-3 API invalid_request_error: you must provide a model parameter","<p>I'm new to APIs and I'm trying to understand how to get a response from a prompt using OpenAI's GPT-3 API (using api.openai.com/v1/completions). I'm using Postman to do so.
The documentation says that there is only one required parameter, which is the &quot;model.&quot; However, I get an error saying that &quot;you must provide a model parameter,&quot; even though I already provided it.</p>
<p>What am I doing wrong?</p>
<p><a href=""https://i.stack.imgur.com/QtLP9.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/QtLP9.png"" alt=""API error screenshot"" /></a></p>
","2022-09-21 08:49:56","","2023-01-11 20:14:51","2023-05-11 08:16:18","<rest><postman><http-post><openai-api><gpt-3>","4","2","13","19728","","2","34170","<p>You can get this to work the following way in Postman with the POST setting:</p>
<ol>
<li><p>Leave all items in the Params tab empty</p>
</li>
<li><p>In the Authorization tab, paste your OpenAI API token as the Type Bearer Token (as you likely already did)</p>
</li>
<li><p>In the Headers tab, add key &quot;Content-Type&quot; with value &quot;application/json&quot;</p>
</li>
<li><p>In the Body tab, switch to Raw, and add e.g.</p>
<pre><code> {  
     &quot;model&quot;:&quot;text-davinci-002&quot;,
     &quot;prompt&quot;:&quot;Albert Einstein was&quot;
 }
</code></pre>
</li>
<li><p>Hit Send. You'll get back the completions for your prompt.</p>
</li>
</ol>
<p>Note alternatively, you can add the model into the Post URL, like <code>https://api.openai.com/v1/engines/text-davinci-002/completions</code></p>
<p>While above works, it might not be using the Postman UI to its full potential -- after all, we're raw-editing JSON instead of utilizing nice key-value input boxes. If you find out how to do the latter, let us know.</p>
<p><a href=""https://i.stack.imgur.com/3MKez.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3MKez.png"" alt=""enter image description here"" /></a></p>
","2022-09-26 11:06:35","2","16"
"75091786","1","20972936","75092193","OpenAI Unity - POST Request not working properly (400 status)","<p>I'm connecting <a href=""https://beta.openai.com/docs/api-reference/completions/create"" rel=""nofollow noreferrer"">GPT3 OpenAI</a> but I just cant manage to make a proper POST request to it (I'm following some guides but for them it works...).</p>
<pre class=""lang-cs prettyprint-override""><code>private IEnumerator Upload ( )
{
    WWWForm form = new WWWForm();
    form.AddField ( &quot;prompt&quot;, prompt );
    form.AddField ( &quot;max_tokens&quot;, maxTokens );
    form.AddField ( &quot;model&quot;, model );
    form.AddField ( &quot;temperature&quot;, temperature );

    using ( UnityWebRequest wR = UnityWebRequest.Post ( &quot;https://api.openai.com/v1/completions&quot;, form ) )
    {
        wR.SetRequestHeader ( &quot;Authorization&quot;, &quot;Bearer &quot; + apiKey );
        wR.SetRequestHeader ( &quot;Content-Type&quot;, &quot;json&quot; );
        yield return wR.SendWebRequest ( );
        if ( wR.result != UnityWebRequest.Result.Success )
        {
            Debug.Log ( &quot;ERROR:\n&quot; + wR.error );
        }
        else
        {
            Debug.Log ( &quot;Success:\n&quot; + wR.result + &quot;\nUpload Completed!);
        }
    }
}
</code></pre>
<p>My code is always returning me a bad request (a.k.a <code>400 Bad Request</code>).</p>
","2023-01-12 04:58:07","","2023-01-12 06:45:51","2023-01-30 07:17:48","<unity-game-engine><httprequest><openai-api><gpt-3>","1","0","4","282","","2","20971780","<p>Remove the &quot;Content-Type&quot; from the headers. The content is not JSON, it's form data.</p>
<p>i.e.</p>
<pre class=""lang-cs prettyprint-override""><code>using ( UnityWebRequest wR = UnityWebRequest.Post ( &quot;https://api.openai.com/v1/completions&quot;, form ) )
{
    wR.SetRequestHeader ( &quot;Authorization&quot;, &quot;Bearer &quot; + apiKey );
    //wR.SetRequestHeader ( &quot;Content-Type&quot;, &quot;json&quot; );
    yield return wR.SendWebRequest ( );
    if ( wR.result != UnityWebRequest.Result.Success )
    {
        Debug.Log ( &quot;ERROR:\n&quot; + wR.error );
    }
    else
    {
        Debug.Log ( &quot;Success:\n&quot; + wR.result + &quot;\nUpload Completed!);
    }
}
</code></pre>
","2023-01-12 06:05:33","0","3"
"72992885","1","19554677","","Is there a way I can build a language model as described below?","<p><strong>Here's what I wanna do:</strong>
I have collected data in a txt file (ie lyrics of some 100 odd songs from an artist) and I want to use it to train an AI language model, then want it to give me some kind of output when I input a small phrase.
<strong>Here's what I tried:</strong>
I used the gpt-2-simple (by Max Woolf on github) and trained it with a .txt file that i created but it gave me a completely unrelated random output.
How can I do what I want to do?
<strong>Here's the code that I used:</strong></p>
<pre><code>import gpt_2_simple as gpt2
import os
import requests

model_name = &quot;124M&quot;
if not os.path.isdir(os.path.join(&quot;models&quot;, model_name)):
    print(f&quot;Downloading {model_name} model...&quot;)
    gpt2.download_gpt2(model_name=model_name)

file_name = &quot;myway.txt&quot;

sess = gpt2.start_tf_sess()
gpt2.finetune(sess,
              file_name,
              model_name=model_name,
              steps=10)  

gpt2.generate(sess)
</code></pre>
<p><a href=""https://i.stack.imgur.com/UpSPV.png"" rel=""nofollow noreferrer"">The output that I got</a></p>
<p>The file has lyrics from Led Zeppelin so I doubt that the output is relevant to the training that I wanted to give.</p>
","2022-07-15 10:53:20","","","2022-07-15 10:53:20","<tensorflow><machine-learning><gpt-2>","0","0","0","39","","","","","","",""
"73094271","1","2195440","","What is suffix and prefix prompt in openai Codex?","<p>I have been trying to understand what is the suffix prompt in addition to the prefix prompt in Codex.</p>
<p>They have provided an <a href=""https://beta.openai.com/docs/guides/code/inserting-code"" rel=""nofollow noreferrer"">example</a></p>
<pre><code>def get_largest_prime_factor(n):
    if n &lt; 2:
        return False
    def is_prime(n): &gt;  for i in range(2, n): &gt;  if n % i == 0: &gt;  return False &gt;  return True &gt;     largest = 1
    for j in range(2, n + 1):
        if n % j == 0 and is_prime(j):
    return largest
</code></pre>
<p>From this example it is not clear to me how to create a suffix prompt?</p>
<p>What I understand is suffix prompt is for code insert model. My use case is also <code>insert</code> mode i.e., code needs to be updated in the middle of a code snippet.</p>
<p>Can anyone please provide a snippet showing how I can use the suffix prompt so that Codex works in the insert mode?</p>
","2022-07-23 21:21:41","","2023-01-18 21:33:15","2023-01-18 21:33:15","<python><deep-learning><openai-api><gpt-2><gpt-3>","1","0","1","1177","","","","","","",""
"73136017","1","4544236","","GPT-2 fails when passing in multiple context tokens","<p>I've set up an anaconda environment to run the GPT-2 models found at <a href=""https://github.com/openai/gpt-2"" rel=""nofollow noreferrer"">https://github.com/openai/gpt-2</a>.</p>
<p>I can run the generate_unconditional_samples.py script on my GPU without issue, however, when I run the interactive_conditional_samples.py script, it crashes if there is more than one context token.</p>
<p>The interactive_conditional_samples.py script works fine as long as the model prompt only produces one context token, for instance using the prompt &quot;please&quot; produces the list of tokens [29688] and correctly generates text. However, it crashes if the model prompt produces two or more context tokens, for instance using the prompt &quot;pig&quot; produces the list of tokens [79, 328] and crashes immediately. I have tried a number of different prompts and I'm pretty confident that the key difference between when it works and when it doesn't is the number of context tokens.</p>
<p>When it crashes I'm getting the error:</p>
<pre><code>failed to run cuBLAS routine: CUBLAS_STATUS_EXECUTION_FAILED
</code></pre>
<p>And a little further down I see:</p>
<pre><code>Blas xGEMMBatched launch failed : a.shape=[25,2,64], b.shape=[25,2,64], m=2, n=2, k=64, batch_size=25
         [[{{node sample_sequence/model/h0/attn/MatMul}}]]
         [[sample_sequence/while/Exit_3/_1375]]
</code></pre>
<p>If anyone has any insight on what might be going wrong, and how I can fix it, I'd really appreciate the help.</p>
","2022-07-27 10:06:56","","","2022-07-27 10:06:56","<python><tensorflow><gpt-2>","0","0","0","85","","","","","","",""
"75155794","1","239427","75209141","Why does GPT-2 vocab contain weird words?","<p>I was looking at the vocabulary of GPT-2.</p>
<p><a href=""https://huggingface.co/gpt2/blob/main/vocab.json"" rel=""nofollow noreferrer"">https://huggingface.co/gpt2/blob/main/vocab.json</a></p>
<p>I found to my surprise very weird tokens that I did not expect.
For example, it contains the token (index 35496):
ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ</p>
<p>How did this happen is this token common in the GPT-2 training data??
In general, how was the vocabulary for GPT-2 built, and is there a problem here?</p>
","2023-01-18 07:21:49","","2023-01-18 07:27:51","2023-01-23 11:55:36","<machine-learning><gpt-2>","1","0","0","262","","2","12750353","<p>Information about the model available here <a href=""https://huggingface.co/gpt2"" rel=""nofollow noreferrer"">https://huggingface.co/gpt2</a></p>
<blockquote>
<p>The OpenAI team wanted to train this model on a corpus as large as possible. To build it, they scraped all the web pages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from this dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights 40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText <a href=""https://github.com/openai/gpt-2/blob/master/domains.txt"" rel=""nofollow noreferrer"">here</a>.</p>
</blockquote>
<p>Accordingly to hugging face <a href=""https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2Tokenizer"" rel=""nofollow noreferrer"">GPT2Tokenizer</a>, the tokenizer is based on <a href=""https://en.wikipedia.org/wiki/Byte_pair_encoding"" rel=""nofollow noreferrer"">BPE</a>, such token could have ended up there due to an encoding issue.</p>
<p>You can see that this is the the char codes for <code>ÃÂ</code> are <code>195</code>, and <code>194</code>, <code>C3 C2</code> that could be a two-byte encoded character in a different encoding? Or part of binary data that leaked into the corpus?</p>
<p>If that token was not frequent it is likely that it will never be relevant at the output. But it is an issue in the sense that the model wastes resources describing the behavior for that token.</p>
","2023-01-23 11:55:36","4","1"
"75390542","1","21159312","75390600","Discordbot.py:NotFound: 404 Not Found (error code: 10062): Unknown interaction","<p>Here is the code</p>
<pre><code>@bot.tree.command(description=&quot;create a txt with chat-gpt&quot;)
async def gpt(interaction: discord.Interaction, *, prompt:str):
    async with aiohttp.ClientSession() as session:
        payload = {
            &quot;model&quot;:&quot;text-davinci-003&quot;,
            &quot;prompt&quot;:prompt,
            &quot;temperature&quot;: 0.5,
            &quot;max_tokens&quot;: 50,
            &quot;presence_penalty&quot;: 0,
            &quot;frequency_penalty&quot;: 0,
            &quot;best_of&quot;: 1,
        }
        headers = {&quot;Authorization&quot;: f&quot;Bearer {API_KEY}&quot;}
        async with session.post(&quot;https://api.openai.com/v1/completions&quot;, json=payload, headers=headers) as resp:
            response = await resp.json()
            em = discord.Embed(title=&quot;Chat gpt’s responce:&quot;, description=response)
            await interaction.response.defer()
            await asyncio.sleep(delay=0)
            await interaction.followup.send(embed=em)
</code></pre>
<p>when I try the slash command, it tells me this as an error code</p>
<pre><code>The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;/home/haoroux/.local/lib/python3.10/site-packages/discord/app_commands/tree.py&quot;, line 1242, in _call
    await command._invoke_with_namespace(interaction, namespace)
  File &quot;/home/haoroux/.local/lib/python3.10/site-packages/discord/app_commands/commands.py&quot;, line 887, in _invoke_with_namespace
    return await self._do_call(interaction, transformed_values)
  File &quot;/home/haoroux/.local/lib/python3.10/site-packages/discord/app_commands/commands.py&quot;, line 880, in _do_call
    raise CommandInvokeError(self, e) from e
discord.app_commands.errors.CommandInvokeError: Command 'gpt' raised an exception: NotFound: 404 Not Found (error code: 10062): Unknown interaction
</code></pre>
<p>And I really can't find the solution even after everything I tried so if you have some time please help me</p>
<p>I changed the delay, I also tried to put the embed=em in response.defer()
I tried to override the embed but nothing to do it does not work</p>
","2023-02-08 19:01:25","","2023-02-08 19:03:46","2023-02-08 19:07:31","<python><discord.py><gpt-3>","1","0","0","268","","2","4680300","<p>I think the issue is that your interaction is timing out before you respond - hence the <code>404</code> error. You should put the <code>defer()</code> further up - ideally as soon as possible and <em>before</em> you make any API calls.</p>
<pre class=""lang-py prettyprint-override""><code>@bot.tree.command(description=&quot;create a txt with chat-gpt&quot;)
async def gpt(interaction: discord.Interaction, *, prompt:str):
    await interaction.response.defer()
    async with aiohttp.ClientSession() as session:
        payload = {
            &quot;model&quot;:&quot;text-davinci-003&quot;,
            &quot;prompt&quot;:prompt,
            &quot;temperature&quot;: 0.5,
            &quot;max_tokens&quot;: 50,
            &quot;presence_penalty&quot;: 0,
            &quot;frequency_penalty&quot;: 0,
            &quot;best_of&quot;: 1,
        }
        headers = {&quot;Authorization&quot;: f&quot;Bearer {API_KEY}&quot;}
        async with session.post(&quot;https://api.openai.com/v1/completions&quot;, json=payload, headers=headers) as resp:
            response = await resp.json()
            em = discord.Embed(title=&quot;Chat gpt’s responce:&quot;, description=response)
            await asyncio.sleep(delay=0)
            await interaction.followup.send(embed=em)
</code></pre>
","2023-02-08 19:07:31","0","2"
"73158634","1","19564052","","Can someone help me understand -""InvalidRequestError"" in Gpt3 open ai using python as I did not find any solution?","<p>I am experimenting with open ai and ran into an error while running a python script using GPT-3 open ai library and despite everything looking good, the error is persisting again.</p>
<p>This is the code:</p>
<pre><code>import openai
api_key='key_api '
openai.api_key=api_key
openai.File.create(file=open(&quot;x.jsonl&quot;), purpose='answers')
openai.Answer.create(
    search_model=&quot;ada&quot;, 
    model=&quot;curie&quot;, 
    question=&quot; which puppy is happy? &quot;, 
    file=&quot;file-gmoSN2DZmc3g2H2XuVZKNRdK&quot;, 
    examples_context=&quot;In 2017, U.S. life expectancy was 78.6 years.&quot;, 
    examples=[[&quot;What is human life expectancy in the United States?&quot;, &quot;78 years.&quot;]], 
    max_rerank=10,
    max_tokens=5,
    stop=[&quot;\n&quot;, &quot;&lt;|endoftext|&gt;&quot;]
)
</code></pre>
<p>The error I am getting is:</p>
<blockquote>
<p>InvalidRequestError: Org org-eS7ut1FIwDuompY1esiMLuFR does not have access to the answers endpoint, likely because it is deprecated. Please see <a href=""https://community.openai.com/t/answers-classification-search-endpoint-deprecation/18532"" rel=""nofollow noreferrer"">https://community.openai.com/t/answers-classification-search-endpoint-deprecation/18532</a> for more information and reach out to deprecation@openai.com if you have any questions.</p>
</blockquote>
<p>Can someone explain this error or suggest a way to get around with this?</p>
","2022-07-28 20:07:44","","2022-07-29 04:08:50","2022-07-29 04:08:50","<python-3.x><openai-api><gpt-3>","0","1","0","288","","","","","","",""
"73207664","1","13297517","","Issues running GPT-J-6B demo inference on colab","<p>I'm trying to run the GPT-J 6B demo available here : <a href=""https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb</a></p>
<p>Unfortunatelay I have some issues that are blocking me so far :</p>
<p>Firstly, when I'm running this part (the first cell of the colab notebook)</p>
<pre><code>!apt install zstd

# the &quot;slim&quot; version contain only bf16 weights and no optimizer parameters, which minimizes bandwidth and memory
!time wget -c https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd

!time tar -I zstd -xf step_383500_slim.tar.zstd

!git clone https://github.com/kingoflolz/mesh-transformer-jax.git
!pip install -r mesh-transformer-jax/requirements.txt

# jax 0.2.12 is required due to a regression with xmap in 0.2.13
!pip install mesh-transformer-jax/ jax==0.2.12 tensorflow==2.5.0
</code></pre>
<p>I don't understand why it try to download several versions of tensorflow while it's specified &quot;tensorflow==2.5.0&quot; in the code.
Installing all these versions take a very long time.
Here is a screenshot of a part of the output:
<a href=""https://i.stack.imgur.com/NZPSF.png"" rel=""nofollow noreferrer"">output (image)</a></p>
<p>Moreover, at the end of the execution, I have this :
<a href=""https://i.stack.imgur.com/9oHnt.png"" rel=""nofollow noreferrer"">excecution's end message (image)</a></p>
<p>Then, when trying to import the libraries in the following code cells, I receive missing modules errors. The missing modules seems to vary depending on the result of the first execution cell.
<a href=""https://i.stack.imgur.com/iVmOl.png"" rel=""nofollow noreferrer"">missing module (image)</a></p>
<p>I believe that colab run out of disk memory trying to download model and dependencies but why this demo exists on colab if it can't be run on it ?</p>
","2022-08-02 12:35:40","","2022-08-02 13:08:49","2022-08-02 13:08:49","<python><tensorflow><deep-learning><google-colaboratory><gpt-3>","0","0","1","616","","","","","","",""
"74712335","1","6288172","","how to fine tune a GPT-2 model?","<p>i'm using huggingface transformers package to load a pretrained GPT-2 model. I want to use GPT-2 for text generation, but the pretrained version isn't enough so I want to fine tune it with a bunch of personal text data.</p>
<p>i'm not sure how I should prepare my data and train the model. I have tokenized the text data I have to train GPT-2 on, but i'm not sure what the &quot;labels&quot; will be for text generation since this isn't a classification problem.</p>
<p>How do I train GPT-2 on this data using Keras API?</p>
<p>my model:</p>
<pre><code>modelName = &quot;gpt2&quot;
generator = pipeline('text-generation', model=modelName)
</code></pre>
<p>my tokenizer:</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(modelName)
</code></pre>
<p>my tokenized dataset:</p>
<pre><code>from datasets import Dataset
def tokenize_function(examples):
    return tokenizer(examples['dataset']) # 'dataset' column contains a string of text. Each row is a string of text (in sequence)
dataset = Dataset.from_pandas(conversation)
tokenized_dataset = dataset.map(tokenize_function, batched=False)
print(tokenized_dataset)
</code></pre>
<p>How should I use this tokenized dataset to fine tune my GPT-2 model?</p>
","2022-12-07 05:55:51","","","2023-06-12 19:04:34","<python><tensorflow><dataset><huggingface-transformers><gpt-2>","2","2","1","1530","","","","","","",""
"75396481","1","8949058","75397187","OpenAI GPT-3 API error: ""This model's maximum context length is 4097 tokens""","<p>I am making a request to the completions endpoint. My prompt is 1360 tokens, as verified by the Playground and the Tokenizer. I won't show the prompt as it's a little too long for this question.</p>
<p>Here is my request to openai in Nodejs using the openai npm package.</p>
<pre><code>const response = await openai.createCompletion({
  model: 'text-davinci-003',
  prompt,
  max_tokens: 4000,
  temperature: 0.2
})
</code></pre>
<p>When testing in the playground my total tokens after response are 1374.</p>
<p>When submitting my prompt via the completions API I am getting the following error:</p>
<pre><code>error: {
  message: &quot;This model's maximum context length is 4097 tokens, however you requested 5360 tokens (1360 in your prompt; 4000 for the completion). Please reduce your prompt; or completion length.&quot;,
  type: 'invalid_request_error',
  param: null,
  code: null
}
</code></pre>
<p>If you have been able to solve this one, I'd love to hear how you did it.</p>
","2023-02-09 09:18:40","","2023-03-13 14:20:48","2023-05-15 06:09:51","<openai-api><gpt-3>","3","1","15","33120","","2","10347145","<p>The <code>max_tokens</code> parameter is <strong>shared</strong> between the prompt and the completion. Tokens from the prompt and the completion all together should not exceed the token limit of a particular GPT-3 model.</p>
<p>As stated in the official <a href=""https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them"" rel=""noreferrer"">OpenAI article</a>:</p>
<blockquote>
<p>Depending on the model used, requests can use up to <code>4097</code> tokens shared
between prompt and completion. If your prompt is <code>4000</code> tokens, your
completion can be <code>97</code> tokens at most.</p>
<p>The limit is currently a technical limitation, but there are often
creative ways to solve problems within the limit, e.g. condensing your
prompt, breaking the text into smaller pieces, etc.</p>
</blockquote>
<p><em>Note: For counting tokens <strong>before(!)</strong> sending an API request, see <a href=""https://stackoverflow.com/questions/75804599/openai-api-how-do-i-count-tokens-before-i-send-an-api-request/75804651#75804651"">this answer</a>.</em></p>
<p><a href=""https://platform.openai.com/docs/models/gpt-3"" rel=""noreferrer"">GPT-3 models</a>:</p>
<p><a href=""https://i.stack.imgur.com/RExzL.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/RExzL.png"" alt=""Table"" /></a></p>
","2023-02-09 10:21:27","3","15"
"75349226","1","9610309","75418423","How to avoid word limit in OpenAI API in R?","<p>I registered at this <a href=""https://platform.openai.com/"" rel=""nofollow noreferrer"">link</a> to get a key for the OpenAI API.</p>
<p>And I used the &quot;<code>chatgpt</code>&quot; package in R.</p>
<pre><code>library(chatgpt)
</code></pre>
<p>And set environment:</p>
<pre><code>Sys.setenv(OPENAI_API_KEY = &quot;sk-YOURAPI&quot;)
</code></pre>
<p>I used this function:</p>
<pre><code>chatgpt::ask_chatgpt(&quot;How to make a strawberry pie to donate to my wife? Ingredients, please.&quot;)
</code></pre>
<p>Sometimes the output is missing words. For example:</p>
<pre><code>*** ChatGPT input:

How to make a strawberry pie to donate to my wife? Ingredients, please.
[1] ... and your wife is&quot;
</code></pre>
<p>It does not complete the text after <code>is</code>. Is there any solution for this on <code>R</code>?</p>
<p>Similar question on <a href=""https://www.reddit.com/r/ChatGPT/comments/zec65u/is_there_a_limit_to_the_code_output_that_gpt/"" rel=""nofollow noreferrer"">reddit</a>.</p>
<p><strong>EDIT</strong></p>
<p>I tried increasing the processing time of the function (so that the chat would finish typing all the content, without stopping midway). Like this:</p>
<pre><code>for (i in 35) {

  print(chatgpt::ask_chatgpt(&quot;How to make a strawberry pie to donate to my wife? Ingredients, please.&quot;))

  Sys.sleep(i)

}
</code></pre>
<p>Editors note: Prior March 1st, 2023 there was no official ChatGPT API.</p>
","2023-02-05 00:01:42","","2023-03-01 22:29:22","2023-03-01 22:29:22","<r><chatbot><openai-api><gpt-3>","2","3","0","1661","","2","8949058","<p>What you are running into is prompt engineering. GPT is a sophisticated autocomplete engine.</p>
<p>If you are not getting the right response you need to recraft your prompt.</p>
<p>You can always test your prompts in the OpenAI playground:
<a href=""https://platform.openai.com/playground"" rel=""nofollow noreferrer"">https://platform.openai.com/playground</a></p>
<p>I was able to get a list of ingredients and cooking steps by using the following prompt:</p>
<blockquote>
<p>How can I make a strawberry pie to donate to my wife? Please provide first a numbered list of ingredients, and secondly a numbered lists of steps.</p>
</blockquote>
<p>Here is the output I got in the playground:</p>
<pre><code>Ingredients:
1. 2 ½ cups of fresh or frozen strawberries
2. 1 9-inch pre-made pie crust
3. ¾ cup of granulated sugar
4. 2 tablespoons of cornstarch
5. ¼ teaspoon of salt
6. 1 tablespoon of fresh lemon juice

Steps:
 1. Preheat oven to 425 degrees F.
 2. Place the pre-made pie crust in a 9-inch pie dish and set aside.
 3. In a medium bowl, combine the strawberries, sugar, cornstarch, salt, and lemon juice. Stir until the mixture is combined.
 4. Pour the strawberry mixture into the pre-made pie crust.
 5. Place the pie dish on a baking sheet and bake for 15 minutes.
 6. Reduce the oven temperature to 375 degrees F and bake for an additional 25 minutes.
 7. Allow the pie to cool completely before serving.
</code></pre>
<p>Another thing to note, per the Github repo for the chatgpt R library it says &quot;The {chatgpt} R package provides a set of features to assist in R coding.&quot;</p>
<p>Ref: <a href=""https://github.com/jcrodriguez1989/chatgpt"" rel=""nofollow noreferrer"">https://github.com/jcrodriguez1989/chatgpt</a></p>
<p>I would use the OpenAI APIs directly, this way you will have a lot more control over your response. I am not an R specialist, but this is how the OpenAI Playground showed me how to do it.</p>
<pre><code>library(httr)

response &lt;- GET(&quot;https://api.openai.com/v1/completions&quot;, 
   query = list(
      prompt = &quot;How can I make a strawberry pie to donate to my wife? Please provide first a numbered list of ingredients, and secondly a numbered lists of steps.&quot;,
      max_tokens = 200,
      model = 'text-davinci-003'
   ),
   add_headers(Authorization = &quot;bearer YOUR_OPENAI_API_KEY&quot;)
)

content(response)
</code></pre>
<p>Ref: OpenAI playground</p>
","2023-02-11 06:13:05","4","4"
"74717631","1","9224324","","how to finetune gpt-2 in hugging-face's pytorch transformer library","<p>I want to finetune gpt-2 by this link <a href=""https://www.modeldifferently.com/en/2021/12/generaci%C3%B3n-de-fake-news-con-gpt-2/"" rel=""nofollow noreferrer"">https://www.modeldifferently.com/en/2021/12/generaci%C3%B3n-de-fake-news-con-gpt-2/</a>
it works correctly in google colab. but when i run it on my lab gpu, i encounter the following error:</p>
<p>x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)
i am so confused. please help.
thanks.</p>
","2022-12-07 13:43:18","","","2022-12-07 13:43:18","<gpt-2>","0","0","0","104","","","","","","",""
"74721925","1","20717038","","Problems getting GPT-3 to conduct statistical analysis of a dataframe or json file","<p>I'm having problems getting gpt-3 to do simple statistical summaries of a dataframe/json file - using python pandas as the suggested prompt, oddly it does categorical analysis of  the df, ie value_counts etc but seems to hallucinate when faced with numbers but i'm sure its just my error. I'm not sure if its the way i'm feeding the df in or something else? cheers.</p>
<pre><code>import pandas as pd 
import openai 
from sqlalchemy import create_engine

openai.api_key =  &quot;API_KEY&quot;

query = &quot;Select * from table&quot;
engine = create_engine()

df = pd.read_sql_query(query, engine)

completion = openai.Completion.create(
    engine=&quot;text-davinci-003&quot;,
    temperature=0.5,
    max_tokens=100,
    n = 1,
    stop=None,
   prompt = (f&quot;print sum of values in df called {df}&quot; ) ) `

a = completion.choices[0].text  
print(a)
</code></pre>
<p>i tried to use different versions of the file and different ways of calling but to no effect, changed engines also</p>
","2022-12-07 19:19:50","","2023-01-06 10:12:35","2023-01-06 10:12:35","<javascript><python><pandas><openai-api><gpt-3>","0","3","3","562","","","","","","",""
"74860930","1","20821790","","Is it possible to use continue in the Open AI API?","<p>I am testing using the Open AI API with the end-point:</p>
<pre><code>https://api.openai.com/v1/completions
</code></pre>
<p>But, I find that on the website <a href=""https://chat.openai.com/chat"" rel=""nofollow noreferrer"">https://chat.openai.com/chat</a> , we can simplely use the continue to ask the AI to give the answer with the context. And we can use continue mulittime to expand more the the max token limit.</p>
<p>Is it possible to use &quot;continue&quot; in the API or have the same effect?</p>
<p>I have try to use the  <code>user</code> field in the API, but still not work.</p>
","2022-12-20 09:28:32","","2022-12-26 02:25:15","2023-02-11 02:19:48","<openai-api><gpt-3>","1","1","0","1799","","","","","","",""
"74907860","1","1101221","","torch parameter is going to nan when after first optim.step() while doing GPT's downstream task","<ol>
<li><p>Foundation<br />
I'm doing finetuning with GPTJForCausalLM. (pretrained is 6B and fp16).</p>
</li>
<li><p>Environment<br />
ubuntu 20.04 (nvidia-docker)<br />
cuda(in docker) version is 11.4<br />
RTX 3090 (24G VRAM)<br />
torch.__version__  1.12.1<br />
transformers.__version__  4.12.5<br />
jupyter lab version : 3.5.0
python3 version : Python 3.7.13<br />
pretrained model : (kakao-kogpt 6B 16fp) <a href=""https://github.com/kakaobrain/kogpt"" rel=""nofollow noreferrer"">https://github.com/kakaobrain/kogpt</a></p>
</li>
<li><p>Problem<br />
If I freeze some layer's parameters and doing <code>model.forward()-loss.backward()-optim.step()</code>, then not freezed parameters are gone to <code>nan</code>. Only first step enough to make <code>nan</code>.</p>
</li>
<li><p>Question<br />
Why I can get this error?. Many of internet search result cannot help me.</p>
</li>
<li><p>Bug reproduction Code</p>
</li>
</ol>
<pre><code>!git clone https://github.com/kakaobrain/kogpt.git
!pip install -r ./kogpt/requirements.txt

from transformers import GPTJForCausalLM
from transformers import AutoTokenizer, AutoModelForCausalLM 
import transformers
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import copy
import inspect
import torch.optim as optim
import types
import re
import numpy as np

#model github's official code, and it works well.
tokenizer = AutoTokenizer.from_pretrained(
      'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b
      bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]', mask_token='[MASK]'
    )

model = AutoModelForCausalLM.from_pretrained(
      'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b
      pad_token_id=tokenizer.eos_token_id,
      torch_dtype='auto', low_cpu_mem_usage=True
    )

#for working test, I just freeze all layers but not freeze last layer.
ls = list(model.modules())
for i, m in enumerate(ls):
    if i == len(ls) - 1:
        #unfreeze for last layer
        m.requires_grad_(True)
    else:
        #freeze other layers
        m.requires_grad_(False)

optim = optim.AdamW(model.parameters(), lr=1e-5)
_ = model.cuda()
_ = model.train()

sample = ['Some text data for finetuning-1', 'Some text data for finetuning-2']
with torch.cuda.amp.autocast(): #this line is not affect change result.
    #make batch
    batch = tokenizer(sample, padding=True, truncation=True, max_length=64, return_tensors='pt')
    batch = {k:v.cuda() for k, v in batch.items()}

    #forward
    out = model(**batch)

    #loss
    loss = F.cross_entropy(out.logits[:, :-1, :].flatten(0,-2), 
                       batch['input_ids'][:,1:].flatten(),
                       reduction='mean')


print(loss.grad)
#None &lt;- result

print(loss.is_leaf)
#False &lt;- result

print(loss)
#tensor(6.6850, device='cuda:0', grad_fn=&lt;NllLossBackward0&gt;) &lt;- result

print(list(model.parameters())[-1])
#Parameter containing: &lt;- result
#tensor([-0.0454,  0.0815, -0.0442,  ..., -0.0566, -0.0557, -0.0552],
#       device='cuda:0', dtype=torch.float16, requires_grad=True)


loss.backward()

print(loss.grad)
#None &lt;- print result

print(loss.is_leaf)
#False &lt;- print result

print(list(model.parameters())[-1])
#Parameter containing: &lt;- result
#tensor([-0.0454,  0.0815, -0.0442,  ..., -0.0566, -0.0557, -0.0552],
#       device='cuda:0', dtype=torch.float16, requires_grad=True)

optim.step()

print(list(model.parameters())[-1])
#Parameter containing: &lt;- result, this is the problem point.
#tensor([   nan, 0.0815,    nan,  ...,    nan,    nan,    nan], device='cuda:0',
#       dtype=torch.float16, requires_grad=True)
</code></pre>
<ol start=""6"">
<li>More Information<br />
The work I really wanted to do is using <code>LoRA</code> downstream.<br />
I did it first time with this code.</li>
</ol>
<pre><code>#(... same as section 5.)

#load model
model = AutoModelForCausalLM.from_pretrained(
      'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b
      pad_token_id=tokenizer.eos_token_id,
      torch_dtype='auto', low_cpu_mem_usage=True
    )

#my lora adapter adder code, (refer to https://github.com/huggingface/transformers/issues/14839)
def forward_linear_with_adapter(self, input: torch.Tensor) -&gt; torch.Tensor:
    #replace NN.Linear's forward()
    out = F.linear(input, self.weight, self.bias)
    if self.lora_adapter:
        out += self.lora_adapter(input)
    return out

def AddLoRAtoLinear(layer, adapter_dim=16, _dtype=None):
    #add adapter
    dt = _dtype if _dtype else layer._parameters['weight'].dtype
    layer.lora_adapter = nn.Sequential(
        nn.Linear(layer.in_features, adapter_dim, bias=False, dtype=dt),
        nn.Linear(adapter_dim, layer.out_features, bias=False, dtype=dt)
    )
    #make trainable
    layer.lora_adapter.requires_grad_(True)
    nn.init.zeros_(layer.lora_adapter[1].weight)
    
    #bind forward with adapter
    layer.forward = types.MethodType(forward_linear_with_adapter, layer)
    
def forward_embedding_with_adapter(self, input: torch.Tensor) -&gt; torch.Tensor:
    #replace NN.Embedding's forward()
    out = F.embedding(input, self.weight, self.padding_idx, self.max_norm, self.norm_type, self.scale_grad_by_freq, self.sparse)
    if self.lora_adapter:
        out += self.lora_adapter(input)
    return out

def AddLoRAtoEmbedding(layer, adapter_dim=16, _dtype=None):
    dt = _dtype if _dtype else layer._parameters['weight'].dtype
    #add adapter
    layer.lora_adapter = nn.Sequential(
        nn.Embedding(layer.num_embeddings, adapter_dim, dtype=dt),
        nn.Linear(adapter_dim, layer.embedding_dim, bias=False, dtype=dt)
    )
    #make trainable
    layer.lora_adapter.requires_grad_(True)
    nn.init.zeros_(layer.lora_adapter[1].weight) #follow LoRA paper's
    
    #bind forward with adapter
    layer.forward = types.MethodType(forward_embedding_with_adapter, layer)

def MakeLoRA(model):
    #freeze all other parameters.
    for _ in model.parameters():
        _.requires_grad_(False)
    
    #apply LoRA only embedding &amp; linear layers.
    needchange = []
    for module in model.modules():
        if type(module) == nn.Linear or type(module) == nn.Embedding:
            needchange.append(module)

    for module in needchange:
        if type(module) == nn.Linear:
            #run custom LoRA attach function to this layer
            AddLoRAtoLinear(module)
        elif type(module) == nn.Embedding:
            #run custom LoRA attach function to this layer
            AddLoRAtoEmbedding(module)
    return model



if False:
  #instead of doing this(from code in section 5), do MakeLoRA()
  ls = list(model.modules())
  for i, m in enumerate(ls):
      if i == len(ls) - 1:
          #unfreeze for last layer
          m.requires_grad_(True)
      else:
          #freeze other layers
          m.requires_grad_(False)
else:
  #change model to has LoRA
  model = MakeLoRA(model)


#(and do it same as section 5... and make same nan error )
</code></pre>
","2022-12-24 13:11:27","","2022-12-24 16:15:36","2022-12-24 16:15:36","<python><pytorch><huggingface-transformers><gpt-3>","0","0","0","152","","","","","","",""
"74915677","1","20853939","","Encountering an error while making an essay app with gpt 3","<p>so i was making a essay app i am getting this error index.html:24<br />
POST <a href=""https://api.openai.com/v1/completions"" rel=""nofollow noreferrer"">https://api.openai.com/v1/completions</a> 429
This is my code
`



Essay Generator


Essay Generator

Topic:<br>
<br>
Word Count:<br>
<br><br>
Generate Essay
</p>
   
<pre><code>&lt;script&gt;
 function sendTopicAndWordCount() {
  // Get the topic and word count from the form
  var topic = document.getElementById(&quot;topic&quot;).value;
  var wordCount = document.getElementById(&quot;wordCount&quot;).value;

  // Make the API request using a CORS proxy
  fetch(&quot;https://api.openai.com/v1/completions&quot;, {
    method: &quot;POST&quot;,
    headers: {
      &quot;Content-Type&quot;: &quot;application/json&quot;,
      &quot;Authorization&quot;: &quot;Bearer key&quot;
    },
    body: JSON.stringify({
      &quot;model&quot;: &quot;text-davinci-003&quot;,
      &quot;prompt&quot;: &quot;essay on&quot; + topic,
     &quot;max_tokens&quot;: wordCount,
      &quot;temperature&quot;: 0.1
    })
  })
  .then(response =&gt; response.json())
  .then(response =&gt; {
    // Display the API's response in the div
    document.getElementById(&quot;response&quot;).innerHTML = response.text;
  });
  // https://cors-anywhere.herokuapp.com/https://api.openai.com/v1/completions
  //https://api.openai.com/v1/text-davinci/questions
}
     &lt;/script&gt;
       &lt;/body&gt;
    &lt;/html&gt;
    `
</code></pre>
<p>I am trying to build an essay generator</p>
","2022-12-25 20:15:54","","","2023-06-23 12:51:09","<gpt-3>","0","2","0","50","","","","","","",""
"75567331","1","21095790","75571367","OpenAI GPT-3 API error: ""You must provide a model parameter""","<p>I am trying to POST a question to openAI API via SWIFT. It works fine, if I use the same payload via Postman, but in the Xcode-Condole I got the following response from openAI:</p>
<pre><code>Response data string:
{
     &quot;error&quot;: {
         &quot;message&quot;: &quot;you must provide a model parameter&quot;,
         &quot;type&quot;: &quot;invalid_request_error&quot;,
         &quot;param&quot;: null,
         &quot;code&quot;: null
    }
 }
</code></pre>
<p>This is my code:</p>
<pre><code> func getActivityAnalysis(){
    
    let url = URL(string: &quot;https://api.openai.com/v1/completions&quot;)
    guard let requestUrl = url else { fatalError() }
    
    // Prepare URL Request Object
    var request = URLRequest(url: requestUrl)
    request.setValue(&quot;Bearer blaaaablaa&quot;, forHTTPHeaderField: &quot;Authorization&quot;)
    request.httpMethod = &quot;POST&quot;
    
    
    let prompt = &quot;just a test&quot;
    let requestBody = OpenAIRequest(model: &quot;text-davinci-003&quot;, prompt: prompt, max_tokens: 300, temperature: 0.5)
    
    let encoder = JSONEncoder()
    encoder.outputFormatting = .prettyPrinted
    let data = try! encoder.encode(requestBody)
    print(String(data: data, encoding: .utf8)!)
    
     
    // Set HTTP Request Body
    request.httpBody = data
    
    print(&quot;\(request.httpMethod!) \(request.url!)&quot;)
    print(request.allHTTPHeaderFields!)
    print(String(data: request.httpBody ?? Data(), encoding: .utf8)!)
    
    
    
    // Perform HTTP Request
    let task = URLSession.shared.dataTask(with: request) { (data, response, error) in
            
            // Check for Error
            if let error = error {
                print(&quot;Error took place \(error)&quot;)
                return
            }
     
            // Convert HTTP Response Data to a String
            if let data = data, let dataString = String(data: data, encoding: .utf8) {
                print(&quot;Response data string:\n \(dataString)&quot;)
                self.openAIResponse = dataString
            }
    }
    task.resume()
    
}`
</code></pre>
<p>If I print the http request, it seems fine for me as well:</p>
<pre><code> POST https://api.openai.com/v1/completions
 [&quot;Authorization&quot;: &quot;Bearer blaaaaa&quot;]
 {
    &quot;temperature&quot; : 0.5,
    &quot;model&quot; : &quot;text-davinci-003&quot;,
    &quot;prompt&quot; : &quot;just a test&quot;,
    &quot;max_tokens&quot; : 300
 }
</code></pre>
<p>I tried to use the same payload in my Postman request. It worked fine here. I also tried to use different encodings, but it always throws the same error.</p>
<p>Not sure, what I am doing wrong. Maybe someone can help?</p>
<p>Thank you in advance.</p>
<p>Bets,
Tobi</p>
","2023-02-25 17:52:50","","2023-03-13 14:47:19","2023-03-13 14:47:19","<swift><openai-api><gpt-3>","1","1","1","1806","","2","10347145","<p>Your HTTP request reveals the problem. You need to add <code>'Content-Type: application/json'</code>.</p>
<p>According to <a href=""https://www.geeksforgeeks.org/what-is-the-correct-json-content-type/"" rel=""nofollow noreferrer"">GeeksforGeeks</a>:</p>
<blockquote>
<p><code>Content-Type</code> is an HTTP header that is used to indicate the media type
of the resource and in the case of responses, it tells the browser
about what actually content type of the returned content is.</p>
</blockquote>
","2023-02-26 10:31:57","0","1"
"75710776","1","429476","75712209","Huggingface GPT2 loss understanding","<p>(Also posted here <a href=""https://discuss.huggingface.co/t/newbie-understanding-gpt2-loss/33590"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/newbie-understanding-gpt2-loss/33590</a>)</p>
<p>I am getting stuck with understanding the GPT2 loss.  I want to give the model the label having the target it will generate so that I can see that loss is zero.</p>
<p>I have a input text
<code>input_text  = &quot;Welcome to New York&quot;</code>
The current model predicts the next word as <code>City</code>
The loss will never be zero if I give the label as input_text. How do I simulate giving the label &quot;Welcome to New York City&quot; so that the internal neural net (irrespective of the model) will give a loss of zero or near that?</p>
<p>To explain more what I mean, here is the snippet.</p>
<p>Note - I have read the forum and documents that the labels can be the same as the input text, that the model will shift left the labels, and that the loss is not calculated for the last token. But then still loss should become zero, which it is not.</p>
<blockquote>
<p>Labels for language modeling. Note that the labels are shifted inside the model,
i.e. you can set labels = input_ids....</p>
</blockquote>
<pre class=""lang-py prettyprint-override""><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer

model_name = 'gpt2'
tokenizer = GPT2Tokenizer.from_pretrained(model_name,model_max_length=1024,padding_side='left')
tokenizer.pad_token = tokenizer.eos_token # == &lt;|endoftext|&gt; = 50256
model = GPT2LMHeadModel.from_pretrained(model_name)

batch_size=5
input_text  = &quot;&lt;|endoftext|&gt; Welcome to New York&quot;
target_text = &quot;Welcome to New York City&quot;

# encode the inputs
encoding = tokenizer(input_text,padding=True,max_length=batch_size,truncation=True,return_tensors=&quot;pt&quot;,)
input_ids, attention_mask = encoding.input_ids, encoding.attention_mask
# encode the targets
target_encoding = tokenizer(target_text,padding=True, max_length=batch_size, truncation=True,return_tensors=&quot;pt&quot;,)
labels = target_encoding.input_ids
# replace padding token id's of the labels by -100 so it's ignored by the loss
labels[labels == tokenizer.pad_token_id] = -100  # in our case there is no padding
print(f&quot;input_ids={input_ids}&quot;)
print(f&quot;attention_mask={attention_mask}&quot;) # all ones
print(f&quot;labels ={labels}&quot;)
# forward pass
outputs = model(input_ids=input_ids,labels=labels) 
print(f&quot;Model Loss {outputs.loss}&quot;)
# Test the model to check what it predicts next
outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask,max_new_tokens=1)
answer = tokenizer.decode(outputs[0], skip_special_tokens=False)
print(f&quot;Result '{answer}'&quot;)
</code></pre>
<p>Output</p>
<pre><code>input_ids=tensor([[50256, 19134,   284,   968,  1971]]) # not sure what eostoken (50256) in input does to model
attention_mask=tensor([[1, 1, 1, 1, 1]])
labels =tensor([[14618,   284,   968,  1971,  2254]]) # 2254 = City;  which is that the model should predict
Model Loss 8.248174667358398
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Result '&lt;|endoftext|&gt; Welcome to New York City'
</code></pre>
<p>When I try something proper as is done everywhere</p>
<pre class=""lang-py prettyprint-override""><code>input_text  = &quot;Welcome to New York&quot;
target_text = input_text
</code></pre>
<p>I get a loss of about  3.26</p>
<pre class=""lang-py prettyprint-override""><code>input_ids=tensor([[14618,   284,   968,  1971]]) # 1971 = York
attention_mask=tensor([[1, 1, 1, 1]])
labels =tensor([[14618,   284,   968,  1971]])
</code></pre>
<pre><code>Model Loss 3.2614505290985107
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Result 'Welcome to New York City'
</code></pre>
<p>Is it that</p>
<pre class=""lang-py prettyprint-override""><code>outputs = model(input_ids=input_ids, labels=labels) 
</code></pre>
<p>is generating more than 1 token.</p>
<h2>Updated-</h2>
<p>Based on answer by Jindfitch - Putting it here as the SO moderators have delted when I try to add that as answer.</p>
<blockquote>
<p>You try to fine-tune the model to be absolutely sure that City will follow with 100% probability</p>
</blockquote>
<p>I trained  the GPT2 with this particular text (trained only the last 2 layers and froze the others) and took the model whose loss was the lowest and used that tested again, and sure enough, the loss was much lower - <code>Model Loss 0.01076329406350851</code></p>
<p>For anyone else who would like to follow. The training code is below.</p>
<p>Note training with this small text and the way I have done I am not really fully sure if it is proper, as the training loss seemed to jump around a bit (that is increased after some epochs, i this case Epoch 8)</p>
<pre><code>2023-03-12 16:03:20,579 [INFO] Epoch 7 complete. Loss: 0.18975284695625305 saving ./test/gpt2-epoch-8-2023-03-12 16:02:19.289492
2023-03-12 16:03:20,985 [INFO] Epoch 9 of 10
2023-03-12 16:03:27,655 [INFO] Epoch 8 complete. Loss: 0.3775772750377655 saving ./test/gpt2-epoch-9-2023-03-12 16:02:19.289492
2023-03-12 16:03:27,655 [INFO] Epoch 10 of 10
2023-03-12 16:03:34,140 [INFO] Epoch 9 complete. Loss: 6.827305332990363e-05 saving ./test/gpt2-epoch-10-2023-03-12 16:02:19.289492
</code></pre>
<p>Training script - <a href=""https://github.com/alexcpn/tranformer_learn/blob/gpt-loss-learn/gpt2_train_model.py"" rel=""nofollow noreferrer"">https://github.com/alexcpn/tranformer_learn/blob/gpt-loss-learn/gpt2_train_model.py</a></p>
<p>Training Ouput log <a href=""https://github.com/alexcpn/tranformer_learn/blob/gpt-loss-learn/training/training_2023-03-12%2016%3A02%3A19.289492.log"" rel=""nofollow noreferrer"">https://github.com/alexcpn/tranformer_learn/blob/gpt-loss-learn/training/training_2023-03-12%2016%3A02%3A19.289492.log</a></p>
<p>Training data
<code>Welcome to New York City </code> (space in the end)
<a href=""https://github.com/alexcpn/tranformer_learn/blob/gpt-loss-learn/data/small.txt"" rel=""nofollow noreferrer"">https://github.com/alexcpn/tranformer_learn/blob/gpt-loss-learn/data/small.txt</a></p>
<p>Eval script - <a href=""https://github.com/alexcpn/tranformer_learn/blob/gpt-loss-learn/older/gpt2_loss_learn.py"" rel=""nofollow noreferrer"">https://github.com/alexcpn/tranformer_learn/blob/gpt-loss-learn/older/gpt2_loss_learn.py</a></p>
<p>I removed the token corresponding to 'City' from Input-ids when giving the model to generate</p>
<pre><code># remove the last token off for input-id's as well as attention Mask
input_ids = input_ids[:,:-1] # input_text  = &quot;Welcome to New York&quot;
attention_mask = attention_mask[:,:-1]
print(f&quot;input_ids={input_ids}&quot;)
outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask,max_new_tokens=1)
</code></pre>
<p>Eval Script Output</p>
<pre><code>python3 ./older/gpt2_loss_learn.py 
input_ids=tensor([[14618,   284,   968,  1971,  2254]])
attention_mask=tensor([[1, 1, 1, 1, 1]])
labels =tensor([[14618,   284,   968,  1971,  2254]])
Model Loss 0.01076329406350851
input_ids=tensor([[14618,   284,   968,  1971]])
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
Result 'Welcome to New York City'
</code></pre>
","2023-03-12 02:34:07","","2023-03-31 14:26:44","2023-03-31 14:26:44","<pytorch><huggingface-transformers><gpt-2>","1","0","1","1148","","2","5652313","<p>The default loss function is negative log-likelihood. The actual model output is not the token <code>City</code> but a categorical distribution over the entire 50k vocabulary. Depending on the generation strategy, you either sample from these distributions or take the most probable token.</p>
<p>The token <code>City</code>, apparently the most probable one, gets some probability, and the loss is then minus the logarithm of this probability. Loss close to zero would mean the token would get a probability close to one. However, the token distribution also considers many plausible but less likely follow-ups. Loss 3.26 corresponds to the probability of <code>exp(-3.26)</code>, approximately 3.8%. It seems small, but in a 50k vocabulary, it is approximately 2000 times more probable than a random guess.</p>
<p>You can try to fine-tune the model to be absolutely sure that <code>City</code> will follow with 100% probability, but it would probably break other language modeling capabilities.</p>
","2023-03-12 09:32:59","1","4"
"70069026","1","3408256","70157536","How to use files in the Answer api of OpenAI","<p>As finally OpenAI opened the GPT-3 related API publicly,
I am playing with it to explore and discover his potential.</p>
<p>I am trying the Answer API, the simple example that is in the documentation:
<a href=""https://beta.openai.com/docs/guides/answers"" rel=""nofollow noreferrer"">https://beta.openai.com/docs/guides/answers</a></p>
<p>I upload the <code>.jsonl</code> file as indicated, and I can see it succesfully uploaded with the <code>openai.File.list()</code> api.</p>
<p>When I try to use it, unfortunately, I always get the same error:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; openai.File.create(purpose='answers', file=open('example.jsonl') )
&lt;File file id=file-xxx at 0x7fbc9eca5e00&gt; JSON: {
  &quot;bytes&quot;: 140,
  &quot;created_at&quot;: 1637597242,
  &quot;filename&quot;: &quot;example.jsonl&quot;,
  &quot;id&quot;: &quot;file-xxx&quot;,
  &quot;object&quot;: &quot;file&quot;,
  &quot;purpose&quot;: &quot;answers&quot;,
  &quot;status&quot;: &quot;uploaded&quot;,
  &quot;status_details&quot;: null
}

#Use the file in the API:
openai.Answer.create(
    search_model=&quot;ada&quot;, 
    model=&quot;curie&quot;, 
    question=&quot;which puppy is happy?&quot;, 
    file=&quot;file-xxx&quot;, 
    examples_context=&quot;In 2017, U.S. life expectancy was 78.6 years.&quot;, 
    examples=[[&quot;What is human life expectancy in the United States?&quot;, &quot;78 years.&quot;]], 
    max_rerank=10,
    max_tokens=5,
    stop=[&quot;\n&quot;, &quot;&lt;|endoftext|&gt;&quot;]
)
&lt;some exception, then&gt;
openai.error.InvalidRequestError: File is still processing.  Check back later.

</code></pre>
<p>I have waited several hours, and I do not think this content deserve such a long wait...
Do you know if it is a normal behaviour, or if I miss something?</p>
<p>Thanks</p>
","2021-11-22 16:19:49","","2023-01-15 17:50:20","2023-01-15 17:50:20","<python><openai-api><gpt-3>","1","0","2","4247","0","2","3408256","<p>After a few hours (the day after) the file metadata status changed from <code>uploaded</code> to <code>processed</code> and the file could be used in the Answer API as stated in the documentation.</p>
<p>I think this need to be better documented in the original OpenAI API reference.</p>
","2021-11-29 15:55:58","0","2"
"70408322","1","13222954","70422274","OpenAI GPT3 Search API not working locally","<p>I am using the python client for GPT 3 search model on my own Jsonlines files. When I run the code on Google Colab Notebook for test purposes, it works fine and returns the search responses. But when I run the code on my local machine (Mac M1) as a web application (running on localhost) using flask for web service functionalities, it gives the following error:</p>
<pre><code>openai.error.InvalidRequestError: File is still processing.  Check back later.
</code></pre>
<p>This error occurs even if I implement the exact same example as given in OpenAI documentation. The link to the <a href=""https://beta.openai.com/docs/guides/search#:%7E:text=openai.Engine(%22ada%22).search(%0A%20%20%20%20search_model%3D%22ada%22%2C%20%0A%20%20%20%20query%3D%22happy%22%2C%20%0A%20%20%20%20max_rerank%3D5%2C%0A%20%20%20%20file%3D%22file%2DLwjuy0q2ezi00jdpfCbl28CO%22%0A)"" rel=""nofollow noreferrer"">search example is given here</a>.</p>
<p>It runs perfectly fine on local machine and on colab notebook if I use the completion API that is used by the GPT3 playground. (<a href=""https://beta.openai.com/docs/developer-quickstart/python-bindings#:%7E:text=import%20os%0Aimport%20openai%0A%0A%23%20Load%20your%20API%20key%20from%20an%20environment%20variable%20or%20secret%20management%20service%0Aopenai.api_key%20%3D%20os.getenv(%22OPENAI_API_KEY%22)%0A%0Aresponse%20%3D%20openai.Completion.create(engine%3D%22davinci%22%2C%20prompt%3D%22This%20is%20a%20test%22%2C%20max_tokens%3D5)"" rel=""nofollow noreferrer"">code link here</a>)</p>
<p>The code that I have is as follows:</p>
<pre><code>import openai

openai.api_key = API_KEY

file = openai.File.create(file=open(jsonFileName), purpose=&quot;search&quot;)

response = openai.Engine(&quot;davinci&quot;).search(
          search_model = &quot;davinci&quot;, 
          query = query, 
          max_rerank = 5,
          file = file.id
        )
for res in response.data: 
   print(res.text)

</code></pre>
<p>Any idea why this strange behaviour is occurring and how can I solve it? Thanks.</p>
","2021-12-19 00:56:26","","","2021-12-20 13:05:32","<python><openai-api><gpt-3>","1","0","2","1902","","2","13222954","<p>The problem was on this line:</p>
<p><code>file = openai.File.create(file=open(jsonFileName), purpose=&quot;search&quot;)</code></p>
<p>It returns the call with a file ID and status uploaded which makes it seem like the upload and file processing is complete. I then passed that fileID to the search API, but in reality it had not completed processing and so the search API threw the error <code>openai.error.InvalidRequestError: File is still processing.  Check back later.</code></p>
<p>The returned file object looks like this (misleading):</p>
<p><a href=""https://i.stack.imgur.com/6xDNh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6xDNh.png"" alt=""enter image description here"" /></a></p>
<p>It worked in google colab because the openai.File.create call and the search call were in 2 different cells, which gave it the time to finish processing as I executed the cells one by one. If I write all of the same code in one cell, it gave me the same error there.</p>
<p>So, I had to introduce a wait time for 4-7 seconds depending on the size of your data, <code>time.sleep(5)</code> after openai.File.create call before calling the openai.Engine(&quot;davinci&quot;).search call and that solved the issue. :)</p>
","2021-12-20 13:05:32","0","1"
"74924661","1","6843825","","OpenAI GPT-3 API error: ""TypeError: openai.completions is not a function""","<p>I am trying to run the test code in the tutorial <a href=""https://harishgarg.com/writing/building-a-chat-app-with-gpt-3-reactjs-and-nextjs-a-step-by-step-guide/"" rel=""nofollow noreferrer"">https://harishgarg.com/writing/building-a-chat-app-with-gpt-3-reactjs-and-nextjs-a-step-by-step-guide/</a> and I get</p>
<blockquote>
<p>TypeError: openai.completions is not a function</p>
</blockquote>
<p>from the following code I put in my.js and run with &quot;node my.js&quot; in git bash window on Windows 10</p>
<pre><code>
    const openai = require('openai');
    openai.apiKey = &quot;api-key&quot;;
    openai.completions({
         engine: &quot;text-davinci-003&quot;,
                   prompt: &quot;Hello, how are you?&quot;,
                   max_tokens: 32,
                   n: 1,
                   stop: &quot;.&quot;,
                   temperature: 0.5,
                  }).then((response) =&gt; {
                      console.log(response.data.choices[0].text);
    });



</code></pre>
<p>I have tried various alternate code snippets from OpenAI docs and some suggested in other questions but have not been able to get it to work.</p>
","2022-12-26 23:24:29","","2023-03-13 13:18:13","2023-03-13 13:18:13","<typescript><next.js><openai-api><gpt-3>","1","3","3","2434","","","","","","",""
"74981741","1","657477","","GPT-3 codex doc sample for explanations doesn't provide explanation","<p>In the docs is this example:</p>
<pre><code>SELECT DISTINCT department.name
FROM department
JOIN employee ON department.id = employee.department_id
JOIN salary_payments ON employee.id = salary_payments.employee_id
WHERE salary_payments.date BETWEEN '2020-06-01' AND '2020-06-30'
GROUP BY department.name
HAVING COUNT(employee.id) &gt; 10;
-- Explanation of the above query in human readable format
-- 
</code></pre>
<p>I have tried copying this into the playground with code-davinci-002 but it just spews out the same query over and over again.</p>
<p>ChatGPT/davinci-003 works fine but the actual codex cant handle it's own sample from it's docs.</p>
<p>Is code-davinci-002 capable of writing explanations of code?</p>
","2023-01-02 11:16:13","","2023-01-18 21:36:16","2023-01-18 21:36:16","<sql><nlp><openai-api><gpt-3>","0","1","0","62","","","","","","",""
"74988265","1","294260","","Multiple Requests until it's complete?","<p>My first GPT-3 project is to pull some basic JSON from a body of text. Using NodeJS.</p>
<p>In the Playground:</p>
<p>The completion works great. I have to keep pressing the &quot;Submit button&quot; though, to get it to finish.</p>
<p>Take at least 5 times. I guess this is because of the token limitations on each request.</p>
<p>In NodeJS:</p>
<p>Using the same prompt that worked in Playground (which required multiple Submits), would the equivalent way in a programmatic NodeJS environment be to just keep submitting requests? Or like maybe partial requests? Maybe keep stringify-ing the concat'd completion until it is a valid JSON object?</p>
<p>What is a recommended way to make multiple requests and keep track of when a completion is done using OpenAI GPT-3?</p>
","2023-01-03 01:03:14","","2023-01-03 02:11:44","2023-01-03 02:11:44","<node.js><gpt-3>","0","0","0","48","","","","","","",""
"74988365","1","20915380","","OpenAI GPT-3 API error: ""AttributeError: module 'openai' has no attribute 'GPT'""","<p>I have the latest version of OpenAi, but some of the attributes are missing. I have tried to reinstall it, didn't solve it. GPT and Chat are the ones i've discovered not working yet.</p>
<p>Bare in mind, i'm new to python and have basic knowledge of the language. The code is taken from GitHub</p>
<p>My code if it tells you something:</p>
<pre><code>import openai
import pyttsx3
import speech_recognition as sr
from api_key import API_KEY


openai.api_key = API_KEY

engine = pyttsx3.init()

r = sr.Recognizer()
mic = sr.Microphone(device_index=1)


conversation = &quot;&quot;
user_name = &quot;You&quot;
bot_name = &quot;DAI&quot;

while True:
    with mic as source:
        print(&quot;\nlistening...&quot;)
        r.adjust_for_ambient_noise(source, duration=0.2)
        audio = r.listen(source)
    print(&quot;no longer listening.\n&quot;)

    try:
        user_input = r.recognize_google(audio)
    except:
        continue

    prompt = user_name + &quot;: &quot; + user_input + &quot;\n&quot; + bot_name+ &quot;: &quot;

    conversation += prompt  # allows for context

    # fetch response from open AI api
    response = openai.Completion.create(engine='text-davinci-003', prompt=conversation, max_tokens=100)
    response_str = response[&quot;choices&quot;][0][&quot;text&quot;].replace(&quot;\n&quot;, &quot;&quot;)
    response_str = response_str.split(user_name + &quot;: &quot;, 1)[0].split(bot_name + &quot;: &quot;, 1)[0]

    conversation += response_str + &quot;\n&quot;
    print(response_str)

    engine.say(response_str)
    engine.runAndWait()

</code></pre>
<p>All help will be appreciated</p>
<p><strong>Edit:</strong></p>
<p>The last answer made the error go away, but it put out a new one:</p>
<p>Thank you, it worked. But it still didn't work, I still get an error. I will try to resolve it if I can. Would appreciate any help and tips I get. This is probably not the last error I will encounter.</p>
<pre><code>This is the error: Traceback (most recent call last):
  File &quot;/Users/danieforsell22b/Desktop/GPT3VoiceBot/gpt3Bot.py&quot;, line 22, in &lt;module&gt;
    r.adjust_for_ambient_noise(source, duration=0.2)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/speech_recognition/__init__.py&quot;, line 569, in adjust_for_ambient_noise
    assert source.stream is not None, &quot;Audio source must be entered before adjusting, see documentation for ``AudioSource``; are you using ``source`` outside of a ``with`` statement?&quot;
           ^^^^^^^^^^^^^^^^^^^^^^^^^
AssertionError: Audio source must be entered before adjusting, see documentation for ``AudioSource``; are you using ``source`` outside of a ``with`` statement?

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/Users/danieforsell22b/Desktop/GPT3VoiceBot/gpt3Bot.py&quot;, line 20, in &lt;module&gt;
    with mic as source:
  File &quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/speech_recognition/__init__.py&quot;, line 201, in __exit__
    self.stream.close()
    ^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'close'
</code></pre>
","2023-01-03 01:29:18","","2023-03-13 13:20:06","2023-03-13 13:20:06","<python><module><attributeerror><openai-api><gpt-3>","1","0","0","1708","","","","","","",""
"74996136","1","20920901","","extend dialogflow webhook deadline time for gpt api call","<p>I am trying to use a script I found on the internet to extend the maximum time for a webhook request through Google Dialogflow (max 5 seconds to timeout). I need to extend the time because I make an API call to openai and it sometimes takes longer than 5 seconds. My idea was to start the 2 functions in parallel. The broadbridge_webhook_results() function is there to extend the time by triggering a followupEventInput at Dialogflow after 3,5 seconds, so a new call comes through Dialogflow and the 5 seconds start from new. This goes apparently up to 2 times. In the meantime the API call should be made towards openai. As soon as the API call was successful, the answer should be sent back to Dialogflow. Unfortunately, I am currently not getting anywhere and I think that the thread functionality was set up or understood incorrectly by me.</p>
<p>The following code I have so far:</p>
<pre><code>import os
import openai
import time
import backoff
from datetime import datetime, timedelta
from flask import Flask, request, render_template
from threading import Thread
import asyncio

app = Flask(__name__)

conversation_History = &quot;&quot;
user_Input = &quot;&quot;
reply=''
answer = &quot;&quot;

@app.route('/') 
def Default(): 
    return render_template('index.html')

@backoff.on_exception(backoff.expo, openai.error.RateLimitError)
def ask(question):
  global conversation_History
  global answer
  global reply
  openai.api_key = os.getenv(&quot;gtp_Secret_Key&quot;)  
  #start_sequence = &quot;\nAI:&quot;
  #restart_sequence = &quot;\nHuman: &quot;
  response = openai.Completion.create(
    model=&quot;text-davinci-003&quot;,
    prompt=&quot;I am a chatbot from OpenAI. I'm happy to answer your questions.\nHuman:&quot; + conversation_History + &quot; &quot;+ question +&quot;\nAI: &quot;,    
    temperature=0.9,
    max_tokens=500,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0.6,
    stop=[&quot; Human:&quot;, &quot; AI:&quot;]
    )
  conversation_History = conversation_History + question + &quot;\nAI&quot; + answer + &quot;\nHuman:&quot;
  answer = response.choices[0].text  

def broadbridge_webhook_results():
  global answer
  
  now = datetime.now()
  current_time = now.strftime(&quot;%H:%M:%S&quot;)
  print(&quot;Current Time =&quot;, current_time)

  extended_time = now + timedelta(seconds=3)
  print(&quot;extended Time =&quot;, extended_time.time())

  req = request.get_json(force=True)

  action = req.get('queryResult').get('action')
  reply=''

  if action=='input.unknown' or action=='input.welcome':
    time.sleep(3.5)

    if now&lt;=extended_time and not len(answer) == 0:
      reply={
              &quot;fulfillmentText&quot;: answer,
              &quot;source&quot;: &quot;webhookdata&quot;
            }

    reply={      
      &quot;followupEventInput&quot;: {        
        &quot;name&quot;: &quot;extent_webhook_deadline&quot;,        
        &quot;languageCode&quot;: &quot;en-US&quot;
        }
      }

  if action=='followupevent':
    print(&quot;enter into first followup event&quot;)
    time.sleep(3.5)

    if now&lt;=extended_time and not len(answer) == 0:
      reply={
        &quot;fulfillmentText&quot;: answer,
        &quot;source&quot;: &quot;webhookdata&quot;
      }

    reply={      
      &quot;followupEventInput&quot;: {        
        &quot;name&quot;: &quot;extent_webhook_deadline_2&quot;,        
        &quot;languageCode&quot;: &quot;en-US&quot;
        }
      }
    
  if action=='followupevent_2':
    print(&quot;enter into second followup event&quot;)
    time.sleep(3.5)

    reply={
      &quot;fulfillmentText&quot;: answer,
      &quot;source&quot;: &quot;webhookdata&quot;
    }
        
    print(&quot;Final time of execution:=&gt;&quot;, now.strftime(&quot;%H:%M:%S&quot;))

@app.route('/webhook', methods=['GET', 'POST'])
def webhook():
  global answer
  global reply

  answer=&quot;&quot;
  req = request.get_json(silent=True, force=True)
  user_Input = req.get('queryResult').get('queryText')
  Thread(target=broadbridge_webhook_results()).start()
  Thread(target=ask(user_Input)).start()
  
  return reply

#conversation_History = conversation_History + user_Input + &quot;\nAI&quot; + answer + &quot;\nHuman:&quot;
#if now&lt;=extended_time and not len(answer) == 0:
  
if __name__ == '__main__':
  app.run(host='0.0.0.0', port=8080)
</code></pre>
","2023-01-03 16:36:55","","","2023-01-25 13:56:24","<python><dialogflow-es><gpt-3>","1","0","0","148","","","","","","",""
"75022045","1","1303952","","ML.Net : How to define Shape Vectors for GPT2 ONNX model?","<p>I'm trying to use ML.Net to consume an ONNX GPT-2 Model.
<a href=""https://github.com/onnx/models/blob/main/text/machine_comprehension/gpt-2/README.md"" rel=""nofollow noreferrer"">https://github.com/onnx/models/blob/main/text/machine_comprehension/gpt-2/README.md</a></p>
<p>I'm stuck on defining the shape dicionary.</p>
<p>The following are the Input and Output model properties, extracted with Netron:</p>
<pre><code>name: input1
type: int64[input1_dynamic_axes_1,input1_dynamic_axes_2,input1_dynamic_axes_3]

name: output1
type: float32[input1_dynamic_axes_1,input1_dynamic_axes_2,input1_dynamic_axes_3,50257]

name: output2
type: float32[2,input1_dynamic_axes_2,12,input1_dynamic_axes_3,64]

name: output3
type: float32[2,input1_dynamic_axes_2,12,input1_dynamic_axes_3,64]

name: output4
type: float32[2,input1_dynamic_axes_2,12,input1_dynamic_axes_3,64]

name: output5
type: float32[2,input1_dynamic_axes_2,12,input1_dynamic_axes_3,64]

name: output6
type: float32[2,input1_dynamic_axes_2,12,input1_dynamic_axes_3,64]

name: output7
type: float32[2,input1_dynamic_axes_2,12,input1_dynamic_axes_3,64]

name: output8
type: float32[2,input1_dynamic_axes_2,12,input1_dynamic_axes_3,64]

name: output9
type: float32[2,input1_dynamic_axes_2,12,input1_dynamic_axes_3,64]

name: output10
type: float32[2,input1_dynamic_axes_2,12,input1_dynamic_axes_3,64]

name: output11
type: float32[2,input1_dynamic_axes_2,12,input1_dynamic_axes_3,64]

name: output12
type: float32[2,input1_dynamic_axes_2,12,input1_dynamic_axes_3,64]

name: output13
type: float32[2,input1_dynamic_axes_2,12,input1_dynamic_axes_3,64]
</code></pre>
<p>What are the correct values for the shape dictionary?
How to represente the input?</p>
<pre><code>int64[ input1_dynamic_axes_1, input1_dynamic_axes_2, input1_dynamic_axes_3 ]
</code></pre>
<p>I've tryied:</p>
<pre><code>{  &quot;input1&quot;, new[] { 1, 32, 32 } }
</code></pre>
<p>Totally guessing...</p>
<p>And the next?</p>
<pre><code>float32[ input1_dynamic_axes_1, input1_dynamic_axes_2, input1_dynamic_axes_3, 50257 ]

float32[ 2, input1_dynamic_axes_2, 12, input1_dynamic_axes_3, 64]
</code></pre>
<p>I'm passing Shape Dictionary to ApplyOnnxModel as in this article <a href=""https://rubikscode.net/2021/10/25/using-huggingface-transformers-with-ml-net/"" rel=""nofollow noreferrer"">https://rubikscode.net/2021/10/25/using-huggingface-transformers-with-ml-net/</a></p>
<pre><code>    var pipeline = _mlContext.Transforms
                    .ApplyOnnxModel(modelFile: bertModelPath,
                    shapeDictionary: shapeDictionary,
                                    outputColumnNames: outputColumnNames,
                                    inputColumnNames: inputColumnNames,
                                    gpuDeviceId: useGpu ? 0 : (int?)null,
                                    fallbackToCpu: true);
</code></pre>
<p>Thank you!</p>
","2023-01-05 17:11:15","","","2023-01-05 17:11:15","<onnx><ml.net><gpt-2>","0","0","0","172","","","","","","",""
"75050453","1","19119619","","TFGPT2LMHeadModel to TFLite changes the input and output shape","<p>The TFGPT2LMHeadModel convertion to TFlite renders unexpected input and output shape
as oppoed to the pre trained model gpt2-64.tflite , how can we fix the same ?</p>
<pre><code>!wget https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-64.tflite

import numpy as np
import tensorflow as tf

tflite_model_path = 'gpt2-64.tflite'
# Load the TFLite model and allocate tensors
interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
interpreter.allocate_tensors()

# Get input and output tensors
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input_shape = input_details[0]['shape']

#print the output
input_data = np.array(np.random.random_sample((input_shape)), dtype=np.int32)
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data.shape)
print(input_shape)
</code></pre>
<p>Gives output as</p>
<pre><code>&gt;(1, 64, 50257)

&gt; [ 1 64]
</code></pre>
<p>which is as expected</p>
<p>but when we try to convert TFGPT2LMHeadModel to TFLITE , we get different output as below</p>
<pre><code>import tensorflow as tf
from transformers import TFGPT2LMHeadModel
import numpy as np

model = TFGPT2LMHeadModel.from_pretrained('gpt2') # or 'distilgpt2'

input_spec = tf.TensorSpec([1, 64], tf.int32)
model._set_inputs(input_spec, training=False)

converter = tf.lite.TFLiteConverter.from_keras_model(model)

# For FP16 quantization:
# converter.optimizations = [tf.lite.Optimize.DEFAULT]
# converter.target_spec.supported_types = [tf.float16]

tflite_model = converter.convert()

open(&quot;gpt2-64-2.tflite&quot;, &quot;wb&quot;).write(tflite_model)

tflite_model_path = 'gpt2-64-2.tflite'
# Load the TFLite model and allocate tensors
interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
interpreter.allocate_tensors()

# Get input and output tensors
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input_shape = input_details[0]['shape']

#print the output
input_data = np.array(np.random.random_sample((input_shape)), dtype=np.int32)
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data.shape)
print(input_shape)
</code></pre>
<p>Output:</p>
<pre><code>&gt;(2, 1, 12, 1, 64)
&gt;[1 1]
</code></pre>
<p>How can we fix the same ?</p>
","2023-01-08 18:50:28","","2023-01-08 18:52:34","2023-01-08 18:52:34","<tensorflow><huggingface-transformers><huggingface><tflite><gpt-2>","0","0","0","151","","","","","","",""
"75127878","1","15811628","","Python string column iteration","<p>I am working on openAI, and stuck I have tried to sort this issue on my own but didn't get any resolution. I want my code to run the sentence generation operation on every row of the Input_Description_OAI column and give me the output in another column (OpenAI_Description). Can someone please help me with the completion of this task. I am new to python.</p>
<p>The dataset looks like:</p>
<p><a href=""https://i.stack.imgur.com/7B5Yw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7B5Yw.png"" alt=""enter image description here"" /></a></p>
<pre><code>    import os
    import openai
    import wandb
    import pandas as pd
    openai.api_key = &quot;MY-API-Key&quot;
    data=pd.read_excel(&quot;/content/OpenAI description.xlsx&quot;)
    data
        
    data[&quot;OpenAI_Description&quot;] = data.apply(lambda _: ' ', axis=1)
    data
        
    gpt_prompt = (&quot;Write product description for: Brand: COILCRAFT ; MPN: DO5010H-103MLD..&quot;)
        
        
    response = openai.Completion.create(engine=&quot;text-curie-001&quot;, prompt=gpt_prompt, 
    temperature=0.7, max_tokens=1000, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0)
        
        
    print(response['choices'][0]['text'])
    data['OpenAI_Description'] = data.apply(gpt_prompt,response['choices'][0]['text'], axis=1)
</code></pre>
<p>I got the error after execution on first row as:</p>
<pre><code>---------------------------------------------------------------------------
TypeError 
Traceback (most recent call last)
&lt;ipython-input-32-c798fbf9bc16&gt; in &lt;module&gt;
     15 print(response['choices'][0]['text'])
     16 #data.add_data(gpt_prompt,response['choices'][0]['text'])
---&gt; 17 data['OpenAI_Description'] = data.apply(gpt_prompt,response['choices'][0]['text'], axis=1)
     18 

TypeError: apply() got multiple values for argument 'axis'
</code></pre>
","2023-01-15 19:33:53","","2023-01-16 12:24:23","2023-01-16 12:24:23","<python><pandas><deep-learning><data-science><gpt-3>","0","0","0","75","","","","","","",""
"75198769","1","12906445","","fine tuning GPT2 on Colab gives error: Your session crashed after using all available RAM","<p>I'm new to ml, and trying to create a ml model fine tuning GPT2.</p>
<p>I get the dataset and preprocessed it (<code>file_name</code>). But when I actually try to run below code, fine tuning GPT2, Colab always say 'Your session crashed after using all available RAM.'</p>
<pre><code>gpt2.finetune(sess,
              dataset=file_name,
              model_name='124M',
              steps=50,
              restore_from='fresh',
              run_name='run1',
              print_every=10,
              sample_every=10,
              save_every=10,
              batch_size=16
              )
</code></pre>
<p>I'm already on the Colab pro version and RAM size is 25GB, and file size is only about 500MB. I tried lowering training steps and batch size but this error is keep happening.</p>
<p>Any idea how can I stop this behavior?</p>
","2023-01-22 07:12:35","","","2023-01-22 07:12:35","<python><gpt-2>","0","0","0","84","","","","","","",""
"75210265","1","11932718","","OpenAI Prompt in Python","<p>I just want to be sure if I write the prompt correctly. In a text which includes a journal's title, abstract and keyword information, I want to extract only the names of the ML/AI methods used/proposed for the problem. You can see the code snippet below where <em>test_text</em> is the text which I read from a txt file. I used ### as mentioned in best practices (<a href=""https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api"" rel=""nofollow noreferrer"">https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api</a>).</p>
<pre><code>response2 = openai.Completion.create(
model=“text-davinci-003”,
prompt=“Extract the specific names of used artificial intelligence methods or machine learning methods by using commas from the following text. Text:###{” + str(test_text) + “}### \nA:”,
temperature=0,
max_tokens=100,
top_p=1,
frequency_penalty=0.0,
presence_penalty=0.0,
stop=[“\n”]
)
</code></pre>
<p>The sample result I got is as follows: Bounding box algorithm, Faster R-CNN, Artificial intelligence, Machine learning, Python.</p>
<p>I tried changing the structure of the prompt (for example instead of &quot;...names of used...&quot;, I tried &quot;...names of proposed...&quot;) so that the meaning of the sentence remains the same, but the results are almost the same. As you can see, irrelevant results also return.</p>
<ul>
<li>Do you think is the above prompt correct to extract AI/ML methods used in the journal?</li>
<li>How can I update the prompt to get more accurate and robust results?</li>
<li>In addition, do you think that are “###” and “+” usages, etc. correct?</li>
</ul>
<p>Thank you so much.</p>
","2023-01-23 13:35:15","","","2023-01-23 13:35:15","<python><machine-learning><artificial-intelligence><openai-api><gpt-3>","0","0","0","418","","","","","","",""
"76001873","1","20920040","76001924","Invalid URL (POST /v1/chat/completions) error in Python","<p>I have a tts Python program that interprets speech-to-text data, and after that it asks this prompt to the GPT davinci-003 API and answers back, but I just switched to GPT 3.5 turbo, and it doesn't work because of the <em>Invalid URL (POST /v1/chat/completions)</em> error.</p>
<p>I tried checking the model endpoint compatibility web page and also tried asking <a href=""https://en.wikipedia.org/wiki/GPT-3"" rel=""nofollow noreferrer"">GPT-3</a> and <a href=""https://en.wikipedia.org/wiki/GPT-4"" rel=""nofollow noreferrer"">GPT-4</a>. I didn’t get any answer.</p>
<p>I checked <a href=""https://en.wikipedia.org/wiki/Reddit"" rel=""nofollow noreferrer"">Reddit</a> as well, but I didn’t find anything. Also, I checked Stack Overflow, but I didn’t find anything either.</p>
<p>This is the API endpoint URL or at least the one I tried.</p>
<p><a href=""https://i.stack.imgur.com/8FOrS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/8FOrS.png"" alt=""Enter image description here"" /></a></p>
<p>My current GPT-3.5 turbo engine code:</p>
<p><a href=""https://i.stack.imgur.com/9zxYw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9zxYw.png"" alt=""Enter image description here"" /></a></p>
","2023-04-13 05:18:25","","2023-04-16 11:26:53","2023-04-16 11:28:19","<python><gpt-3><chatgpt-api>","1","2","-1","165","","2","15446995","<p><a href=""https://platform.openai.com/docs/api-reference/engines"" rel=""nofollow noreferrer"">Using <em>engines</em> is deprecated</a>. Use <em>model</em> instead.</p>
<pre><code>response = openai.ChatCompletion.create(
    model=&quot;gpt-3.5-turbo&quot;,
    messages=messages,
    max_tokens=1000,
    n=1,
    temperature=0.3,
)
</code></pre>
","2023-04-13 05:26:33","0","0"
"76160057","1","21801792","76161653","OpenAI GPT-3.5 and GPT-4 API: How do I customize answers from a GPT-3.5 or GPT-4 model if I can't fine-tune them?","<p>We have seen some companies use GPT-3.5 or GPT-4 models to train their own data and provide customized answers. But GPT-3.5 and GPT-4 models are not available for fine-tuning.</p>
<p>I've seen the document from OpenAI about this issue, but I had seen OpenAI only allow fine-tuning <code>davinci</code>, for example.</p>
<p>How do I customize answers from a GPT-3.5 or GPT-4 model if I can't fine-tune them?</p>
","2023-05-03 02:27:37","","2023-05-05 21:10:26","2023-05-05 21:10:26","<openai-api><chatgpt-api><fine-tune><gpt-4>","1","1","2","855","","2","10347145","<p><strong>They don't fine-tune GPT-3.5 or GPT-4 models.</strong></p>
<p>What they do is use <a href=""https://gpt-index.readthedocs.io/en/stable/"" rel=""nofollow noreferrer"">LlamaIndex</a> (formerly GPT-Index) or <a href=""https://python.langchain.com/en/latest/index.html"" rel=""nofollow noreferrer"">LangChain</a>. Both of them enable you to connect OpenAI models with your existing data sources.</p>
","2023-05-03 08:02:41","1","2"
"76138660","1","17945984","76187971","problem with running OpenAI Cookbook's chatbot","<p>I'm having trouble running the chatbot app in the OpenAI Cookbook repository.</p>
<h1>What I tried</h1>
<p>I installed the necessary packages with 'pip install -r requirements.txt'. I made .env file with my OpenAI API Key, and inserted the code below in chatbot.py line 9.</p>
<pre><code>import os
openai.api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)
</code></pre>
<p>The setup above is by my guess, because the doc is totally unclear about how to set up.</p>
<p>I run the app in local by the command &quot;streamlit run apps/chatbot-kickstarter/chat.py.&quot; It didn't work properly. The app run but when I entered text and pressed 'submit' button in the app, I got an error:</p>
<pre><code>Uncaught app exception
Traceback (most recent call last):
  File &quot;C:\Users\XXX\AppData\Local\Programs\Python\Python310\lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py&quot;, line 565, in _run_script
exec(code, module.__dict__)
  File &quot;F:\PythonProjects\openai-cookbook\apps\chatbot-kickstarter\chat.py&quot;, line 71, in &lt;module&gt;
response = query(messages)
  File &quot;F:\PythonProjects\openai-cookbook\apps\chatbot-kickstarter\chat.py&quot;, line 51, in query
response = st.session_state['chat'].ask_assistant(question)
  File &quot;F:\PythonProjects\openai-cookbook\apps/chatbot-kickstarter\chatbot.py&quot;, line 61, in ask_assistant
if 'searching for answers' in assistant_response['content'].lower():
TypeError: string indices must be integers
</code></pre>
<p>I use Python 3.10.6.</p>
<p>I would appreciate any help or guidance to resolve these issues.</p>
","2023-04-29 22:05:30","","2023-05-02 12:22:49","2023-05-06 08:47:43","<python><streamlit><openai-api><gpt-3><chatgpt-api>","2","5","-1","102","","2","17945984","<p>Putting the key directly in chatbot.py just worked. It shouldn't be taken from environment variables.</p>
","2023-05-06 08:47:43","0","0"
"75444023","1","16920990","","Max prompt token not working when using GPT-3 text-davinci-003","<p>i am integrate GPT-3 text-davinci-003 algorithm with node js all working good but when we have pass token every time different but every time Same gives reply GPT-3 text-davinci-003 so please give me solution.</p>
<pre><code> const response = await openai.createCompletion({
   model: text-davinci-003,
   prompt: 'what is javascript',
   temperature: 0.05,
   max_tokens: 1000,
   top_p: 0.5,
   frequency_penalty: 0,
   presence_penalty: 0,
   stop: [&quot;END OF POLICY&quot;]
});
           
</code></pre>
<p>i am passing 1000 token but this reply only give max token 160,180,210 like but i want to give large token reply</p>
","2023-02-14 06:05:04","","2023-02-14 09:27:49","2023-02-24 19:19:33","<javascript><node.js><typescript><openai-api><gpt-3>","1","0","2","1347","","","","","","",""
"75497540","1","2317643","","Does Visual Studio Code support Environment Variables for the ChatGPT extension?","<p>I would like to open the editor and be able to use the ChatGPT extension without needing to log in via browser or store the OpenAI api key in a workspace file. Ideally, I would be able to use the environment variable stored in <code>.zshenv</code>.</p>
<p>I know that Visual Studio Code allows for some environment variables to be set but not all via <code>${ENV_EXAMPLE}</code> but this has not worked for me in the <code>settings.json</code> file:</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;chatgpt.response.showNotification&quot;: true,
  &quot;chatgpt.gpt3.maxTokens&quot;: 2048,
  &quot;chatgpt.gpt3.apiKey&quot;: &quot;${OPEN_AI_API_KEY}&quot;,
}

</code></pre>
","2023-02-19 01:55:50","","","2023-02-19 08:59:43","<security><visual-studio-code><openai-api><gpt-3>","1","0","0","241","","","","","","",""
"75536615","1","5239482","","nanoGPT with custom dataset","<p>I am trying to use nanoGPT from <a href=""https://github.com/karpathy/nanoGPT"" rel=""nofollow noreferrer"">https://github.com/karpathy/nanoGPT</a> on my custom input file.</p>
<p>I have posted this issue on the repo itself  ( at <a href=""https://github.com/karpathy/nanoGPT/issues/172"" rel=""nofollow noreferrer"">issue 172</a> ) but not getting any response there, hence lookin for some advice here on stackoverflow.</p>
<p>Following the same steps as described here for the Shakespere input file, I tried this on a custom input file which contains multiple paragraphs with different headings. The file contents looks like this</p>
<pre><code>Heading 1
Some information related to heading 1 goes here

Heading 2
Some information related to heading 2 goes here
</code></pre>
<p>Containing 20 such paragraphs.</p>
<p>The &quot;prepare.py&quot; and &quot;train.py&quot; file executed successfully for this input file.
However, when I try to generate one sample, the output is some incorrect english like this</p>
<pre><code>paceliYai ominger fromally.Satexas Stence taly mand gollag adeppirirhon temas poymais,Mcenterted Should had &amp; days to suratication tEO - ande ymanaN tor reeson travel from the enterns tleat sompoyers asubve the candidate can travel cof grotef dosiction of inotis, an too coan cile verand ginald to Employees. All embent ire thang falor ind the to pacomvertaly of the is enotiry for 
</code></pre>
<p>Is this input dataset format correct? Or something specific is needed?</p>
","2023-02-22 18:12:11","","","2023-02-22 18:12:11","<gpt-2>","0","0","0","369","","","","","","",""
"75547772","1","9720696","","Recovering input IDs from input embeddings using GPT-2","<p>Suppose I have the following text</p>
<pre><code>aim = 'Hello world! you are a wonderful place to be in.'
</code></pre>
<p>I want to use GPT2 to produce the input_ids and then produce the embedding and from embeddings recover the input_ids, to do this I do:</p>
<pre><code>from transformers import GPT2Tokenizer, GPT2Model
tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
model = GPT2Model.from_pretrained(&quot;gpt2&quot;)
</code></pre>
<p>The input_ids can be defines as:</p>
<pre><code>input_ids = tokenizer(aim)['input_ids']
#output: [15496, 995, 0, 345, 389, 257, 7932, 1295, 284, 307, 287, 13]
</code></pre>
<p>I can decode this to make sure it reproduce the aim:</p>
<pre><code>tokenizer.decode(input_id)
#output: 'Hello world! you are a wonderful place to be in.'
</code></pre>
<p>as expected! To produce the embedding I convert the input_ids to tensor:</p>
<pre><code>input_ids_tensor = torch.tensor([input_ids])
</code></pre>
<p>I can then procude my embeddings as:</p>
<pre><code># Generate the embeddings for input IDs 
with torch.no_grad():
    model_output = model(input_ids_tensor)
    last_hidden_states = model_output.last_hidden_state
    
# Extract the embeddings for the input IDs from the last hidden layer
input_embeddings = last_hidden_states[0,1:-1,:]
</code></pre>
<p>Now as mentioned earlier, the aim is to use input_embeddings and recover the input_ids, so I do:</p>
<pre><code>x = torch.unsqueeze(input_embeddings, 1) # to make the dim acceptable
with torch.no_grad():
    text = model(x.long())
    decoded_text = tokenizer.decode(text[0].argmax(dim=-1).tolist())
</code></pre>
<p>But doing this I get:</p>
<pre><code>IndexError: index out of range in self
</code></pre>
<p>at the level of <code>text = model(x.long())</code> I wonder what am I doing wrong? How can I recover the input_ids using the embedding I produced?</p>
","2023-02-23 16:41:17","","","2023-02-23 22:54:04","<python><pytorch><huggingface-transformers><gpt-2>","1","0","1","348","","","","","","",""
"67221901","1","3837352","","Mismatched tensor size error when generating text with beam_search (huggingface library)","<p>I'm using the huggingface library to generate text using the pre-trained distilgpt2 model. In particular, I am making use of the <a href=""https://huggingface.co/transformers/main_classes/model.html?highlight=sample#transformers.generation_utils.GenerationMixin.beam_search"" rel=""nofollow noreferrer"">beam_search</a> function, as I would like to include a LogitsProcessorList (which you can't use with the <a href=""https://huggingface.co/transformers/main_classes/model.html?highlight=sample#transformers.generation_utils.GenerationMixin.generate"" rel=""nofollow noreferrer"">generate</a> function).</p>
<p>The relevant portion of my code looks like this:</p>
<pre><code>beam_scorer = BeamSearchScorer(
            batch_size=btchsze,
            max_length=15,  # not sure why lengths under 20 fail
            num_beams=num_seq,
            device=model.device,
        )
j = input_ids.tile((num_seq*btchsze,1))
next_output = model.beam_search(
            j, 
            beam_scorer,
            eos_token_id=tokenizer.encode('.')[0],
            logits_processor=logits_processor
        )
</code></pre>
<p>However, the beam_search function throws this error when I try to generate using a max_length of less than 20:</p>
<pre><code>~/anaconda3/envs/techtweets37/lib/python3.7/site-packages/transformers-4.4.2-py3.8.egg/transformers/generation_beam_search.py in finalize(self, input_ids, final_beam_scores, final_beam_tokens, final_beam_indices, pad_token_id, eos_token_id)
    326         # fill with hypotheses and eos_token_id if the latter fits in
    327         for i, hypo in enumerate(best):
--&gt; 328             decoded[i, : sent_lengths[i]] = hypo
    329             if sent_lengths[i] &lt; self.max_length:
    330                 decoded[i, sent_lengths[i]] = eos_token_id

RuntimeError: The expanded size of the tensor (15) must match the existing size (20) at non-singleton dimension 0.  Target sizes: [15].  Tensor sizes: [20]
</code></pre>
<p>I can't seem to figure out where 20 is coming from: it's the same even if the input length is longer or shorter, even if I use a different batch size or number of beams. There's nothing I've defined as length 20, nor can I find any default. The max length of the sequence does effect the results of the beam search, so I'd like to figure this out and be able to set a shorter max length.</p>
","2021-04-22 23:09:25","","","2021-04-23 10:29:16","<python><nlp><pytorch><huggingface-transformers><gpt-2>","1","0","2","744","","","","","","",""
"67930127","1","15847023","","No module named 'tensorflow.contrib' even on tensorflow 1.13.2","<p>I cant import a gpt_2_simple package due to an error</p>
<pre><code>ModuleNotFoundError: No module named 'tensorflow.contrib'
</code></pre>
<p>I have installed python 3.7 and tried to install tensorflow 1.15.5, 1.15.2 and 1.13.2 and all of them were gaining this mistake. Im using windows.</p>
","2021-06-11 01:06:55","","","2021-06-11 18:48:37","<tensorflow><gpt-2>","1","0","1","214","","","","","","",""
"67959077","1","16215008","","How to get onnx format from pretrained GPT2 models?","<p>I'm trying to transform KoGPT2 model, which is pretrained by GPT2, to onnx format in order to change the model to tensorflow format.</p>
<p>I used <code>convert_graph_to_onnx</code> in <code>transformers</code> but it didn't work because of some reasons.</p>
<p>I don't know what this error implied. Is it possible to make onnx format with this model? Here is the code I implemented and last one is the error.</p>
<p>Thanks.</p>
<pre><code>import sys
!{sys.executable} -m pip install --upgrade git+https://github.com/huggingface/transformers
!{sys.executable} -m pip install --upgrade torch==1.6.0+cpu torchvision==0.7.0+cpu -f https://download.pytorch.org/whl/torch_stable.html
!{sys.executable} -m pip install --upgrade onnxruntime==1.4.0
!{sys.executable} -m pip install -i https://test.pypi.org/simple/ ort-nightly
!{sys.executable} -m pip install --upgrade onnxruntime-tools
</code></pre>
<pre><code>!rm -rf onnx/
from pathlib import Path
from transformers.convert_graph_to_onnx import convert

# Handles all the above steps for you
convert(framework=&quot;pt&quot;, model=&quot;skt/kogpt2-base-v2&quot;, output=Path('/content/drive/MyDrive/kogptonnx/kogpt.onnx'), opset=12)

# Tensorflow 
# convert(framework=&quot;tf&quot;, model=&quot;bert-base-cased&quot;, output=&quot;onnx/bert-base-cased.onnx&quot;, opset=11)
</code></pre>
<pre><code>ONNX opset version set to: 11
Loading pipeline (model: skt/kogpt2-base-v2, tokenizer: skt/kogpt2-base-v2)
Some weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2Model: ['lm_head.weight']
- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Using framework PyTorch: 1.6.0+cpu
Found input input_ids with shape: {0: 'batch', 1: 'sequence'}
Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}
Found output output_0 with shape: {0: 'batch', 1: 'sequence'}
Found output output_1 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_1 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_2 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_2 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_3 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_3 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_4 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_4 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_5 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_5 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_6 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_6 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_7 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_7 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_8 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_8 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_9 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_9 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_10 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_10 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_11 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_11 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_12 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Found output output_12 with shape: {0: 'batch', 1: 'sequence', 2: 'sequence'}
Ensuring inputs are in correct order
past_key_values is not present in the generated input list.
Generated inputs order: ['input_ids']
/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py:181: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  attn_weights = attn_weights / (float(value.size(-1)) ** 0.5)
/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py:186: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].bool()
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-6-78cc7242cbdd&gt; in &lt;module&gt;()
      4 
      5 # Handles all the above steps for you
----&gt; 6 convert(framework=&quot;pt&quot;, model=&quot;skt/kogpt2-base-v2&quot;, output=Path('/content/drive/MyDrive/kogptonnx/kogpt.onnx'), opset=11)
      7 
      8 # Tensorflow

6 frames
/usr/local/lib/python3.7/dist-packages/torch/onnx/utils.py in _optimize_graph(graph, operator_export_type, _disable_torch_constant_prop, fixed_batch_size, params_dict)
    187 
    188         graph = torch._C._jit_pass_onnx(graph, operator_export_type)
--&gt; 189         torch._C._jit_pass_lint(graph)
    190 
    191         torch._C._jit_pass_onnx_scalar_type_analysis(graph)

RuntimeError: Unable to cast from non-held to held instance (T&amp; to Holder&lt;T&gt;) (compile in debug mode for type information)
</code></pre>
","2021-06-13 14:01:15","","","2021-11-14 09:40:24","<python><tensorflow><pytorch><onnx><gpt-2>","1","0","2","781","","","","","","",""
"68233646","1","6272434","","ValueError when trying to fine-tune GPT-2 model in TensorFlow","<p>I am encountering a <code>ValueError</code> in my Python code when trying to fine-tune <a href=""https://huggingface.co/gpt2"" rel=""nofollow noreferrer"">Hugging Face's distribution of the GPT-2 model</a>. Specifically:</p>
<pre><code>ValueError: Dimensions must be equal, but are 64 and 0 for
'{{node Equal_1}} = Equal[T=DT_FLOAT, incompatible_shape_error=true](Cast_18, Cast_19)'
with input shapes: [64,0,1024], [2,0,12,1024].
</code></pre>
<p>I have around 100 text files that I concatenate into a string variable called <code>raw_text</code> and then pass into the following function to create training and testing TensorFlow datasets:</p>
<pre class=""lang-py prettyprint-override""><code>def to_datasets(raw_text):
    # split the raw text in smaller sequences
    seqs = [
        raw_text[SEQ_LEN * i:SEQ_LEN * (i + 1)]
        for i in range(len(raw_text) // SEQ_LEN)
    ]

    # set up Hugging Face GPT-2 tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    tokenizer.pad_token = tokenizer.eos_token

    # tokenize the character sequences
    tokenized_seqs = [
        tokenizer(seq, padding=&quot;max_length&quot;, return_tensors=&quot;tf&quot;)[&quot;input_ids&quot;]
        for seq in seqs
    ]

    # convert tokenized sequences into TensorFlow datasets
    trn_seqs = tf.data.Dataset \
        .from_tensor_slices(tokenized_seqs[:int(len(tokenized_seqs) * TRAIN_PERCENT)])
    tst_seqs = tf.data.Dataset \
        .from_tensor_slices(tokenized_seqs[int(len(tokenized_seqs) * TRAIN_PERCENT):])

    def input_and_target(x):
        return x[:-1], x[1:]

    # map into (input, target) tuples, shuffle order of elements, and batch
    trn_dataset = trn_seqs.map(input_and_target) \
        .shuffle(SHUFFLE_BUFFER_SIZE) \
        .batch(BATCH_SIZE, drop_remainder=True)
    tst_dataset = tst_seqs.map(input_and_target) \
        .shuffle(SHUFFLE_BUFFER_SIZE) \
        .batch(BATCH_SIZE, drop_remainder=True)

    return trn_dataset, tst_dataset
</code></pre>
<p>I then try to train my model, calling <code>train_model(*to_datasets(raw_text))</code>:</p>
<pre class=""lang-py prettyprint-override""><code>def train_model(trn_dataset, tst_dataset):
    # import Hugging Face GPT-2 model
    model = TFGPT2Model.from_pretrained(&quot;gpt2&quot;)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=tf.metrics.SparseCategoricalAccuracy()
    )

    model.fit(
        trn_dataset,
        epochs=EPOCHS,
        initial_epoch=0,
        validation_data=tst_dataset
    )
</code></pre>
<p>The <code>ValueError</code> is triggered on the <code>model.fit()</code> call. The variables in all-caps are settings pulled in from a JSON file. Currently, they are set to:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;BATCH_SIZE&quot;:64,
    &quot;SHUFFLE_BUFFER_SIZE&quot;:10000,
    &quot;EPOCHS&quot;:500,
    &quot;SEQ_LEN&quot;:2048,
    &quot;TRAIN_PERCENT&quot;:0.9
}
</code></pre>
<p>Any information regarding what this error means or ideas on how to resolve it would be greatly appreciated. Thank you!</p>
","2021-07-03 06:02:01","","","2021-07-29 19:55:56","<tensorflow><huggingface-transformers><transformer-model><pre-trained-model><gpt-2>","1","0","0","791","0","","","","","",""
"75084272","1","16461166","76444852","How to keep the format of the OpenAI API response when using the OpenAI GPT-3 API?","<p>When I use GPT3's playground, I often get results that are formatted with numbered lists and paragraphs like below:</p>
<pre><code>Here's what the above class is doing:

1. It creates a directory for the log file if it doesn't exist.
2. It checks that the log file is newline-terminated.
3. It writes a newline-terminated JSON object to the log file.
4. It reads the log file and returns a dictionary with the following

- list 1
- list 2
- list 3
- list 4
</code></pre>
<p>However, when I directly use their API and extract the response from json result, I get the crammed text version that is very hard to read, something like this:</p>
<pre><code>Here's what the above class is doing:1. It creates a directory for the log file if it doesn't exist.2. It checks that the log file is newline-terminated.3. It writes a newline-terminated JSON object to the log file.4. It reads the log file and returns a dictionary with the following-list 1-list 2-list 3- list4
</code></pre>
<p>My question is, how do people keep the formats from GPT results so they are displayed in a neater, more readable way?</p>
","2023-01-11 14:02:46","","2023-04-11 21:55:42","2023-06-10 05:31:51","<openai-api><gpt-3>","3","2","2","3189","","2","16461166","<p>The problem was on my side's frontend. The openAI API was returning the correct response, and I was rending the result with the wrong whitespace CSS settings</p>
","2023-06-10 05:31:51","0","1"
"71215965","1","10853823","71227037","How to save checkpoints for thie transformer gpt2 to continue training?","<p>I am retraining the GPT2 language model, and am following this blog :</p>
<p><a href=""https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171"" rel=""nofollow noreferrer"">https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171</a></p>
<p>Here, they have trained a network on GPT2, and I am trying to recreate a same. However, my dataset is too large(250Mb), so I want to continue training in intervals. In other words, I want to checkpoint the model training. If there is any help, or a piece of code that I can implement to checkpoint and continue training, it would help a great deal for me. Thank you.</p>
","2022-02-22 04:36:01","","","2022-02-22 19:10:58","<tensorflow><nlp><gpt-2>","1","0","0","570","","2","4762466","<pre><code>training_args = TrainingArguments(
    output_dir=model_checkpoint,
    # other hyper-params
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_set,
    eval_dataset=dev_set,
    tokenizer=tokenizer
)

trainer.train()
# Save the model to model_dir
trainer.save_model()

def prepare_model(tokenizer, model_name_path):
    model = AutoModelForCausalLM.from_pretrained(model_name_path)
    model.resize_token_embeddings(len(tokenizer))
    return model

# Assume tokenizer is defined, You can simply pass the saved model directory path.
model = prepare_model(tokenizer, model_checkpoint)
</code></pre>
","2022-02-22 19:10:58","0","1"
"70834489","1","17067836","70881747","Choosing a good prompt for GPT-3","<p>I am trying to generate a quiz from a text that look like this:</p>
<pre><code>Text: &quot;Mary has little lamb and John has a cow. The lamb is one month old. It eats grass and milk, which Mary brings from the the farm.&quot;

Keywords: &quot;lamb&quot;, &quot;cow&quot;, &quot;one month old&quot;, &quot;farm.&quot;

1. What does Mary have?
A. Lamb
B. Cow
C. Dog
D. Cat

A. Lamb

2. What does John have?
A. Cow
B. Lamb
C. Dog
D. Cat

A. Cow

3. How old is Mary's lamb?
A. One month old
B. One year old
C. Two months old
D. Two years old

etc.
</code></pre>
<p>It works perfectly when I don't give keywords, with multiple prompts, with all these answer options, correct answer, everything. <strong>The problem is when I want to generate these questions such that the correct answer is the keyword.</strong> I tried all kinds of prompts, even giving examples like what I did above, but it doesn't work.</p>
","2022-01-24 13:16:53","2022-12-13 15:42:12","2022-12-13 15:42:17","2022-12-13 15:42:17","<nlp><openai-api><gpt-3>","1","0","0","570","","2","17132650","<p>Using GPT-J I tested the following:</p>
<p>Input:</p>
<pre><code>Create a quiz about the following text according to the keywords: Mary has little lamb and John has a cow. The lamb is one month old. It eats grass and milk, which Mary brings from the the farm.

Keywords: &quot;lamb&quot;, &quot;cow&quot;, &quot;one month old&quot;, &quot;farm.&quot;

Question 1: What does Mary have?
A. Lamb
B. Cow
C. Dog
D. Cat

Answer: A. Lamb

Question 2:
</code></pre>
<p>Output:</p>
<pre><code>Create a quiz about the following text according to the keywords: Mary has little lamb and John has a cow. The lamb is one month old. It eats grass and milk, which Mary brings from the the farm.

Keywords: &quot;lamb&quot;, &quot;cow&quot;, &quot;one month old&quot;, &quot;farm.&quot;

Question 1: What does Mary have?
A. Lamb
B. Cow
C. Dog
D. Cat

Answer: A. Lamb

Question 2: What does John have?
A. Lamb
B. Cow
C. Dog
D. Cat

Answer: B. Cow

Question 3: How old is the lamb?
A. 1 month
B. 1 year
C. 2 months
D. 1 year and a half

Answer: C. 2 months
</code></pre>
<p>Giving an example of what you want in the input (1 question + answer) can help GPT to understand the structure of the desired output. Explicitly explaining the GPT what the desired task is in the input (see line 1) can help it to understand the task in my experience.</p>
<p>It does not execute the task perfectly, but using GPT-3 might help and I hope this is a step in the right direction for you.</p>
","2022-01-27 16:00:13","0","1"
"68312300","1","16412367","","How to push trained NLP model to huggingface.co via git-lfs?","<p>I used &quot;<code>!sudo apt-get install git-lfs</code>&quot;, and I'm on windows, am I wrong somewhere: how can I get pas this error message?</p>
<p>Text:</p>
<pre class=""lang-sh prettyprint-override""><code>[ ] model.push_to_hub (MY_MODEL_NAME, use_auth_token=HUGGINGFACE_API_KEY)
    tokenizer.push_to_hub(MY MODEL_NAME, use_auth_token-HUGGINGFACE_API_KEY)

07/09/2021 01:13:08 - INFO huggingface_hub.repository - git version 2.17.1
Sorry, no usage text found for &quot;git-lfs&quot;
--------------------------------------------------------------------
Called ProcessError                Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/huggingface hub/repository.py in git commit(self, commit_message)
    396                 encoding=&quot;utf-8&quot;,
--&gt; 397                 cwd-self.local dir,
    398              )
------------------------------------ 5 frames ----------------------
Called ProcessError: Command '['git', 'commit', '-m', ' add model'] returned non-zero exit status 1.

During handling of the above exception, another exception occurred:

OSError                             Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/huggingface_hub/repository.py in git commit(self, commit_message)

    401                 raise EnvironmentError(exc.stderr)
    402              else:
--&gt; 403                 raise EnvironmentError (exc.stdout)
    404
    405     def git push(self) -&gt; str:

OSError: On branch main
Your branch is ahead of origin/main' by 1 commit. 
  (use &quot;git push&quot; to publish your local commits)

nothing to commit, working tree clean
</code></pre>
<p>Screenshot:</p>
<p><a href=""https://i.stack.imgur.com/qcXOQ.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qcXOQ.jpg"" alt=""Screenshot"" /></a></p>
","2021-07-09 06:39:58","","2021-07-09 07:17:53","2021-07-09 07:17:53","<python><git><nlp><huggingface-transformers><gpt-2>","0","3","0","378","0","","","","","",""
"68729645","1","10836319","","How to get the language modeling loss by passing 'labels' while using ONNX inference session?","<p>When using GPT2 we can simply pass on the 'labels' parameter to get the loss as follows:</p>
<pre><code>import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2', return_dict=True)

inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)
outputs = model(**inputs, labels=inputs[&quot;input_ids&quot;])
loss = outputs.loss
</code></pre>
<p>But, not able to find out how to get the same loss in an ONNX inference session. I am using the below code which only returns the 'last_hidden_state':</p>
<pre><code>import onnxruntime as ort

from transformers import GPT2TokenizerFast
#tokenizer = GPT2TokenizerFast.from_pretrained(&quot;gpt2&quot;)

ort_session = ort.InferenceSession(&quot;onnx/gpt2/model.onnx&quot;)

inputs = tokenizer(&quot;Using BERT in ONNX!&quot;, return_tensors=&quot;np&quot;)
outputs = ort_session.run([&quot;last_hidden_state&quot;], dict(inputs))
</code></pre>
","2021-08-10 15:30:00","","2021-08-13 18:48:01","2021-08-13 18:48:01","<pytorch><huggingface-transformers><language-model><onnxruntime><gpt-2>","1","0","0","446","","","","","","",""
"69928517","1","17200786","","Encoding issues on OpenAI predictions after fine-tuning","<p>I'm following <a href=""https://beta.openai.com/docs/guides/fine-tuning/create-a-fine-tuned-model"" rel=""nofollow noreferrer"">this OpenAI tutorial</a> about fine-tuning.</p>
<p>I already generated the dataset with the openai tool. The problem is that the outputs encoding (inference result) is mixing UTF-8 with non UTF-8 characters.</p>
<p>The generated model looks like this:</p>
<pre><code>{&quot;prompt&quot;:&quot;Usuario: Quién eres\\nAsistente:&quot;,&quot;completion&quot;:&quot; Soy un Asistente\n&quot;}
{&quot;prompt&quot;:&quot;Usuario: Qué puedes hacer\\nAsistente:&quot;,&quot;completion&quot;:&quot; Ayudarte con cualquier gestión o ofrecerte información sobre tu cuenta\n&quot;}
</code></pre>
<p>For instance, if I ask &quot;¿Cómo estás?&quot; and there's a trained completion for that sentence: &quot;Estoy bien, ¿y tú?&quot;, the inference often returns exactly the same (which is good), but sometimes it adds non-encoded words: &quot;Estoy bien, ¿y tú? CuÃ©ntame algo de ti&quot;, adding &quot;Ã©&quot; instead of &quot;é&quot;.</p>
<p>Sometimes, it returns exactly the same sentence that was trained for, with no encoding issues.
I don't know if the inference is taking the non-encoded characters from my model or from somewhere else.</p>
<p>What should I do?
Should I encode the dataset in UTF-8?
Should I leave the dataset with UTF-8 and decode the bad encoded chars in the response?</p>
<p>The OpenAI docs for fine-tuning don't include anything about encoding.</p>
","2021-11-11 12:44:06","","","2021-12-09 00:36:53","<utf-8><character-encoding><openai-api><gpt-3><fine-tune>","1","0","3","2259","","","","","","",""
"70784728","1","17067836","70802282","GPT-3 question answering based on keywords","<p>I am currently getting accustomed to GPT3, and I am trying to generate questions from a text by also inputting some keywords from that text. Ideally, they would be the answers to that question.</p>
<p>What I tried was to input the text, and simply write <code>Keywords: dog, cat, mouse</code> etc., so just enumerating the words, and then input some question examples. But obviously, it is not used to this structure and I was wondering if it was even possible to do it like that.</p>
","2022-01-20 10:40:50","","","2022-01-21 13:43:27","<nlp><openai-api><gpt-3>","1","0","1","594","","2","17132650","<p>I am currently working on something similar (but I'm using GPT-J). One thing I find helpful is describing in the input what you want GPT to do.</p>
<p>e.g.: I want to generate a sentence containing all given key-words:</p>
<p>input:</p>
<pre><code>Generate a sentence which contains all of the keywords.
Keywords: dog, cat, mouse
Sentence: 
</code></pre>
<p>output:</p>
<pre><code>Generate a sentence which contains all of the keywords.
Keywords: dog, cat, mouse
Sentence: I have a dog, a cat and a mouse.
</code></pre>
<p>Let me know if this helps you, or if you find any other solutions which might help me as well!</p>
","2022-01-21 13:43:27","0","2"
"63626014","1","13624094","63636417","How to Get Rid of GPT-2 Warning Message?","<p>Every time I run GPT-2, I am receiving this message. Is there a way I can get this to go away?</p>
<pre><code>Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</code></pre>
","2020-08-28 00:57:18","","2020-11-29 11:52:40","2020-11-29 11:52:40","<python><huggingface-transformers><gpt-2>","1","1","2","895","","2","6664872","<p>Yes you need to change the <a href=""https://docs.python.org/2/library/logging.html#levels"" rel=""nofollow noreferrer"">loglevel</a> <strong>before you import anything</strong> from the transformers library:</p>
<pre class=""lang-py prettyprint-override""><code>import logging
logging.basicConfig(level='ERROR')

from transformers import GPT2LMHeadModel

model = GPT2LMHeadModel.from_pretrained('gpt2')
</code></pre>
","2020-08-28 15:06:19","0","2"
"76500504","1","7037351","76502140","What is the cause of HFValidationError in this code and how do I resolve this error?","<p>My python code in Chaquopy android studio Project:</p>
<pre><code>import torch as tc
from transformers import GPT2Tokenizer, GPT2Model



def generate_text(txt):
    &quot;&quot;&quot;
    Generate chat
    https://huggingface.co/gpt2
    &quot;&quot;&quot;

    #Load Model files
    tokenizer = GPT2Tokenizer.from_pretrained('assets/') #This line causing error
    model = GPT2Model.from_pretrained('assets/')
    #Move moel to GPU if avilable
    device = tc.device(&quot;cuda&quot; if tc.cuda.is_available() else &quot;cpu&quot;)
    model.to(device)

    encoded_input = tokenizer(txt, return_tensors='pt')
    output = model(**encoded_input)

    return str(output)

</code></pre>
<p>Now it is showing following error :</p>
<pre><code>E/AndroidRuntime: FATAL EXCEPTION: main
    Process: com.example.chaquopy_130application, PID: 4867
    com.chaquo.python.PyException: HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'assets/'.
        at &lt;python&gt;.huggingface_hub.utils._validators.validate_repo_id(_validators.py:164)
        at &lt;python&gt;.huggingface_hub.utils._validators._inner_fn(_validators.py:110)
        at &lt;python&gt;.huggingface_hub.utils._deprecation.inner_f(_deprecation.py:103)
        at &lt;python&gt;.transformers.file_utils.get_list_of_files(file_utils.py:2103)
        at &lt;python&gt;.transformers.tokenization_utils_base.get_fast_tokenizer_file(tokenization_utils_base.py:3486)
        at &lt;python&gt;.transformers.tokenization_utils_base.from_pretrained(tokenization_utils_base.py:1654)
        at &lt;python&gt;.pythonScript.generate_text(pythonScript.py:30)

</code></pre>
<p>I have put all files of 124M GPT-2 model  <em>checkpoint</em>, <em>encoder.json</em>, <em>hparams.json</em>, <em>model.ckpt.data-00000-of-00001</em>, <em>model.ckpt.index</em>, <em>model.ckpt.meta</em>, <em>vocab.bpe</em> files inside of 'assets' folder.</p>
<p><a href=""https://i.stack.imgur.com/qFwJt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/qFwJt.png"" alt=""file structure"" /></a></p>
","2023-06-18 12:20:29","","2023-06-19 07:05:19","2023-06-19 07:05:19","<android-studio><python-3.8><chaquopy><gpt-2>","1","0","1","42","","2","220765","<p>The <a href=""https://huggingface.co/docs/transformers/v4.30.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained"" rel=""nofollow noreferrer""><code>from_pretrained</code></a> documentation is not entirely clear about how it distinguishes huggingface repository names from local paths, although all the local path examples end with a slash. In any case, when loading data files with Chaquopy, you must always use absolute paths, as it says in <a href=""https://chaquo.com/chaquopy/doc/current/android.html#android-data"" rel=""nofollow noreferrer"">the FAQ</a>.</p>
<p>So assuming your &quot;assets&quot; directory is at the same level as the Python code, you can do this:</p>
<pre><code>from os.path import dirname
tokenizer = GPT2Tokenizer.from_pretrained(f'{dirname(__file__)}/assets/')
</code></pre>
","2023-06-18 19:08:47","1","1"
"70672460","1","8613875","70728130","Hugging face - Efficient tokenization of unknown token in GPT2","<p>I am trying to train a dialog system using GPT2. For tokenization, I am using the following configuration for adding the special tokens.</p>
<pre><code>from transformers import (
     AdamW,
     AutoConfig,
     AutoTokenizer,
     PreTrainedModel,
     PreTrainedTokenizer,
     get_linear_schedule_with_warmup,
)

SPECIAL_TOKENS = {
    &quot;bos_token&quot;: &quot;&lt;|endoftext|&gt;&quot;,
    &quot;eos_token&quot;: &quot;&lt;|endoftext|&gt;&quot;,
    &quot;pad_token&quot;: &quot;[PAD]&quot;,
    &quot;additional_special_tokens&quot;: [&quot;[SYS]&quot;, &quot;[USR]&quot;, &quot;[KG]&quot;, &quot;[SUB]&quot;, &quot;[PRED]&quot;, &quot;[OBJ]&quot;, &quot;[TRIPLE]&quot;, &quot;[SEP]&quot;, &quot;[Q]&quot;,&quot;[DOM]&quot;]
}
tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)
tokenizer.add_special_tokens(SPECIAL_TOKENS)
</code></pre>
<p>Next, when I am trying to tokenize a sequence(dialog's utterance) and later convert into ids, some of the most important tokens in my sequence are getting mapped as unknown tokens, since the ids of these important tokens becomes the same as bos and eos as they all map to &lt;|endoftext|&gt; as in the GPT2's <a href=""https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/gpt2/tokenization_gpt2.py#L104"" rel=""nofollow noreferrer"">source code</a>.</p>
<p>Here is a working example -</p>
<pre><code>tokenized_sequence = ['[PRED]', 'name', '[SUB]', 'frankie_and_bennys', '[PRED]', 'address', '[SUB]', 'cambridge_leisure_park_clifton_way_cherry_hinton', '[PRED]', 'area', '[SUB]', 'south', '[PRED]', 'food', '[SUB]', 'italian', '[PRED]', 'phone', '[SUB]', '01223_412430', '[PRED]', 'pricerange', '[SUB]', 'expensive', '[PRED]', 'postcode', '[SUB]', 'cb17dy']
important_tokens = ['frankie_and_bennys','cambridge_leisure_park_clifton_way_cherry_hinton','italian','postcode', 'cb17dy']
tokens_to_ids = [50262, 3672, 50261, 50256, 50262, 21975, 50261, 50256, 50262, 20337, 50261, 35782, 50262, 19425, 50261, 50256, 50262, 4862, 50261, 50256, 50262, 50256, 50261, 22031, 50262, 50256, 50261, 50256]
ids_to_tokens = [PRED]name[SUB]&lt;|endoftext|&gt;[PRED]address[SUB]&lt;|endoftext|&gt;[PRED]area[SUB]south[PRED]food[SUB]&lt;|endoftext|&gt;[PRED]phone[SUB]&lt;|endoftext|&gt;[PRED]&lt;|endoftext|&gt;[SUB]expensive[PRED]&lt;|endoftext|&gt;[SUB]&lt;|endoftext|&gt;
</code></pre>
<p>As you can see the important_tokens are being mapped to the id  50256 (that is to |endoftext|), the model fails to see and learn these important tokens and hence generate very poor and often hallucinated responses.</p>
<p>What could be a quick and efficient fix for this issue?</p>
","2022-01-11 19:35:14","","","2023-06-08 21:05:57","<python><nlp><huggingface-transformers><huggingface-tokenizers><gpt-2>","1","0","2","2802","","2","8704180","<p>For the important_tokens which contain several actual words (like <code>frankie_and_bennys</code>), you can replace <code>underscore</code> with the <code>space</code> and feed them normally, Or add them as a special token. I prefer the first option because this way you can use pre-trained embedding for their subtokens. For the ones which aren't actual words (like <code>cb17dy</code>), you must add them as special tokens.</p>
<pre><code>from transformers import GPT2TokenizerFast
tokenizer = GPT2TokenizerFast.from_pretrained(&quot;gpt2&quot;)
your_string = '[PRED] name [SUB] frankie and bennys frankie_and_bennys [PRED]  cb17dy'
SPECIAL_TOKENS = {
    &quot;bos_token&quot;: &quot;&lt;|endoftext|&gt;&quot;,
    &quot;eos_token&quot;: &quot;&lt;|endoftext|&gt;&quot;,
    &quot;pad_token&quot;: &quot;[PAD]&quot;,
    &quot;additional_special_tokens&quot;: [&quot;[SYS]&quot;, &quot;[USR]&quot;, &quot;[KG]&quot;, &quot;[SUB]&quot;, &quot;[PRED]&quot;, &quot;[OBJ]&quot;, &quot;[TRIPLE]&quot;, &quot;[SEP]&quot;, &quot;[Q]&quot;,&quot;[DOM]&quot;, 'frankie_and_bennys', 'cb17dy']
}
tokenizer.add_special_tokens(SPECIAL_TOKENS)
print(tokenizer(your_string)['input_ids'])
print(tokenizer.convert_ids_to_tokens(tokenizer(your_string)['input_ids']))
</code></pre>
<p>the output</p>
<pre><code>[50262, 1438, 220, 50261, 14346, 494, 290, 275, 1697, 893, 220, 50268, 220, 50262, 220, 220, 50269]
['[PRED]', 'Ġname', 'Ġ', '[SUB]', 'Ġfrank', 'ie', 'Ġand', 'Ġb', 'enn', 'ys', 'Ġ', 'frankie_and_bennys', 'Ġ', '[PRED]', 'Ġ', 'Ġ', 'cb17dy']
</code></pre>
","2022-01-16 07:28:04","0","3"
"75256485","1","3111375","","Training / using OpenAI GPT-3 for translations","<p>I'm trying to use OpenAI for translation of my products descriptions from one language to some other languages (EN, DE, CZ, SK, HU, PL, SI...). The translations, especially to SK/CZ/HU/PL languages are (mainly gramatically) quite bad (using <code>text-davinci-003</code> model). I've got an idea - I already have a few thousands of similar products fully translated into all of these languages by professional translators. Is it possible to use those existing correct translations to train GPT-3 and then use this model to translate new texts? Has anybody already tried something similar?</p>
","2023-01-27 09:56:11","","2023-01-27 18:35:54","2023-02-15 12:19:52","<nlp><translate><openai-api><machine-translation><gpt-3>","0","4","1","347","","","","","","",""
"75266549","1","257493","","Fine-tune a davinci model to be similar to InstructGPT","<p>I have a few-shot GPT-3 text-davinci-003 prompt that produces &quot;pretty good&quot; results, but I quickly run out of tokens per request for interesting use cases. I have a data set (n~20) which I'd like to train the model with more but there is no way to fine-tune these InstructGPT models, only base GPT models.</p>
<p>As I understand it I can either:</p>
<ul>
<li>A: Find a way to harvest 10x more data (I don't see an easy option here)</li>
<li>or B: Find a way to fine-tune Davinci into something capable of simpler InstructGPT behaviours</li>
</ul>
<p>(Please let me know if there's a third option. I've attempted to increase epochs from 4 to 10 but the quality is really nowhere near as good).</p>
<p>Is there any way to fine-tune Davinci up to the point where it can model some of the things Instruct does? I don't need full capabilities, but if I can make it narrowed down to my use case it would be ideal.</p>
<p>--</p>
<p>By the way there is a common misconception that fine-tuning a GPT-3 model on a base (davinci, ada, babbage, etc...) will train it on the latest, eg: text-davinci-003. This is not how GPT works and is explained by GPT blog posts and support posts:
<a href=""https://help.openai.com/en/articles/6819989-can-i-fine-tune-on-text-davinci-003"" rel=""nofollow noreferrer"">https://help.openai.com/en/articles/6819989-can-i-fine-tune-on-text-davinci-003</a></p>
<p>Please don't claim <code>openai api fine_tunes.create -t &quot;model_prepared.jsonl&quot; -m &quot;davinci&quot;</code> will create a model based on text-davinci-003, it is not true, it uses base davinci.</p>
","2023-01-28 09:09:22","","","2023-04-09 01:15:38","<gpt-3><fine-tune>","2","1","2","1491","","","","","","",""
"75348532","1","4352114","","How to use the OpenAI stream=true property with a Django Rest Framework response, and still save the content returned?","<p>I'm trying to use the stream=true property as follows.</p>
<pre><code>completion = openai.Completion.create(
            model=&quot;text-davinci-003&quot;,
            prompt=&quot;Write me a story about dogs.&quot;,
            temperature=0.7,
            max_tokens=MAX_TOKENS,
            frequency_penalty=1.0,
            presence_penalty=1.0,
            stream=True,
        )
</code></pre>
<p>Unfortunately, I don't know what to do from here to return it to my React frontend. Typically, I've used standard response objects, setting a status and the serializer.data as the data. From my readings online, it seems I have to use the <code>StreamingHttpResponse</code>, but I'm not sure how to integrate that with the iterator object of <code>completion</code>, and actually save the outputted data once it is done streaming, as the view will end after returning the iterator to the endpoint. Any help?</p>
","2023-02-04 21:34:09","","","2023-04-02 11:24:59","<reactjs><django><gpt-3>","2","0","4","981","","","","","","",""
"75559316","1","16961408","","Using sliding windows to evaluate sentiment for text groupings","<p>I am trying to create a sliding window to evaluate sentiment on groupings of utterances within a conversation, with the goal of:</p>
<ol>
<li>Evaluating the sentiment of a single utterance in a conversational grouping</li>
<li>Evaluating a grouping of sentiment based on the statement from item 1 above and then adding a new utterance string to the predictor (the next utterance in the conversation) so that the predictor evaluates the previous string in context with the new string.  Note that the individual additive statement in this step would also receive a sentiment score</li>
<li>Repeating item 1 and 2 by adding a new utterance string to data to be evaluated (wherein the new 3rd string utterance gets evaluated but also is evaluated in the context of the previous 2 utterances - such that now there are three utterances to be evaluated in addition to the individual newly added string.<br />
For example:</li>
</ol>
<pre><code>Statement 1: Neutral
Statement 2:  Positive
Statement 1+2:  Neutral
Statement 3:  Negative
Statement 1+2+3:  Neutral
etc... 
</code></pre>
<p>I could then transform the classifications to integer values rather simply, and then come up with the mean of entire statement grouping for a final &quot;conversation&quot; sentiment classification.</p>
<p>Here is my list of utterances:</p>
<pre><code>conversation = [
&quot;Hi, how are you?&quot;,
&quot;I'm not doing very well, thanks for asking. How about you?&quot;,
&quot;It is the best of times and the worst of times.&quot;,
&quot;I'm not sure what to make of that.&quot;,
&quot;Do you have any plans for the weekend?&quot;,
&quot;Not yet, I'm still deciding.&quot;,
&quot;How about you?&quot;,
&quot;I'm planning to go hiking on Saturday.&quot;]
</code></pre>
<p>Here is my routine -</p>
<pre><code>#Define the size of the sliding window
window_size = 5
sentiment_scores = []
for i in range(len(conversation) - window_size + 1):
    # Get the window of utterances
    window = conversation[i:i+window_size]
    print(&quot;this is the conversation with window&quot;,conversation[i:i+window_size])
    
    # Add one or two utterances from the previous window to the beginning of the current window
    if i &gt; 0:
        window = conversation[i-1:i+window_size]
        
    
    # Add one or two utterances from the next window to the end of the current window
    if i &lt; len(conversation) - window_size:
        window = conversation[i:i+window_size+1]
        print(&quot;This is the  window when a conversation has been added&quot;, window)

    # Join the utterances in the window into a single string
    text = &quot; &quot;.join(window)
    # print(text)

    # Use the OpenAI completion class to evaluate sentiment for the window
    response = openai.Completion.create(
        engine=&quot;text-davinci-003&quot;,
        prompt = f&quot;classify the sentiment of this text as Positive, Negative, or Neutral: {text}\nResult:&quot;,
        temperature=0,
        max_tokens=1,
        n=1,
        stop=None,
        frequency_penalty=0,
        presence_penalty=0
    )

    # Extract the sentiment score from the response
    sentiment = response.choices[0].text.strip()
    print(sentiment)

    # Add the sentiment score to the list
    sentiment_scores.append(sentiment)
    print(sentiment_scores)

</code></pre>
<p>Unfortunately, what is being returned is not correct because I am apparently not layering in the utterances the way I am describing above.  Using one of my debug prints, this is what I am seeing:</p>
<pre><code>This is the  window when a conversation has been added ['Hi, how are you?', &quot;I'm not doing very well, thanks for asking. How about you?&quot;, 'It is the best of times and the worst of times.', &quot;I'm not sure what to make of that.&quot;, 'Do you have any plans for the weekend?', &quot;Not yet, I'm still deciding.&quot;]
Neutral
['Neutral']
This is the  window when a conversation has been added [&quot;I'm not doing very well, thanks for asking. How about you?&quot;, 'It is the best of times and the worst of times.', &quot;I'm not sure what to make of that.&quot;, 'Do you have any plans for the weekend?', &quot;Not yet, I'm still deciding.&quot;, 'How about you?']
Neutral
['Neutral', 'Neutral']
This is the  window when a conversation has been added ['It is the best of times and the worst of times.', &quot;I'm not sure what to make of that.&quot;, 'Do you have any plans for the weekend?', &quot;Not yet, I'm still deciding.&quot;, 'How about you?', &quot;I'm planning to go hiking on Saturday.&quot;]
Neutral
['Neutral', 'Neutral', 'Neutral']
This is the  window when a conversation has been added ['It is the best of times and the worst of times.', &quot;I'm not sure what to make of that.&quot;, 'Do you have any plans for the weekend?', &quot;Not yet, I'm still deciding.&quot;, 'How about you?', &quot;I'm planning to go hiking on Saturday.&quot;]
Neutral
['Neutral', 'Neutral', 'Neutral', 'Neutral']
</code></pre>
<p>As you can see, it appears my logic is evaluating all statements.</p>
<p>I could then transform the classifications to integer values rather simply, and then come up with the mean of entire statement grouping for a final &quot;conversation&quot; sentiment classification.</p>
<p>Any thoughts or assistance would be greatly appreciated.</p>
","2023-02-24 16:54:29","","","2023-02-27 14:45:59","<python><data-science><openai-api><gpt-3>","1","0","0","39","","","","","","",""
"73411215","1","19739568","","Getting logits from T5 Hugging Face model using forward() method without labels","<p>For my use case, I need to obtain the logits from T5's forward() method without inputting labels. I know that forward() and .generate() are different (<a href=""https://stackoverflow.com/questions/67328345/how-to-use-forward-method-instead-of-model-generate-for-t5-model"">see here</a>). I have also seen <a href=""https://stackoverflow.com/questions/72177055/forward-outputs-on-multiple-sequences-is-wrong?noredirect=1#comment127536983_72177055"">this</a> post in which the logits were obtained but labels had to be generated first. Is it possible to obtain the logits from the forward() method without inputting the labels?</p>
","2022-08-19 02:15:34","","","2022-08-20 09:37:09","<nlp><huggingface-transformers><bert-language-model><gpt-2>","1","0","2","540","","","","","","",""
"75621041","1","20784837","","How can i migrate from text-davinci-003 model to gpt-3.5-turbo model using OpenAI API?","<p>I tried to change my code to be able to use the new OpenAI model but my application stops working,</p>
<p>BEFORE: In Bold are parts of the code that I changed and where working using text-davinci-003 model</p>
<pre><code>**var url = &quot;https://api.openai.com/v1/completions&quot;**

private fun getResponse(query: String) {

        val queue: RequestQueue = Volley.newRequestQueue(applicationContext)

        val jsonObject: JSONObject? = JSONObject()
**
        jsonObject?.put(&quot;model&quot;, &quot;text-davinci-003&quot;)**
        jsonObject?.put(&quot;messages&quot;, messagesArray)
        jsonObject?.put(&quot;temperature&quot;, 0)
        jsonObject?.put(&quot;max_tokens&quot;, 100)
        jsonObject?.put(&quot;top_p&quot;, 1)
        jsonObject?.put(&quot;frequency_penalty&quot;, 0.0)
        jsonObject?.put(&quot;presence_penalty&quot;, 0.0)

        val postRequest: JsonObjectRequest =

            object : JsonObjectRequest(Method.POST, url, jsonObject,
                Response.Listener { response -&gt;

                    val responseMsg: String =
                        response.getJSONArray(&quot;choices&quot;).getJSONObject(0).getString(&quot;text&quot;)
                },

                Response.ErrorListener { error -&gt;
                    Log.e(&quot;TAGAPI&quot;, &quot;Error is : &quot; + error.message + &quot;\n&quot; + error)
                }) {
                override fun getHeaders(): kotlin.collections.MutableMap&lt;kotlin.String, kotlin.String&gt; {
                    val params: MutableMap&lt;String, String&gt; = HashMap()
                    // adding headers on below line.
                    params[&quot;Content-Type&quot;] = &quot;application/json&quot;
                    params[&quot;Authorization&quot;] =
                        &quot;Bearer MYTOKEN&quot;
                    return params;
                }
            }
</code></pre>
<p>AFTER: In Bold are parts of the code that I changed and arent working with gpt-3.5-turbo model</p>
<pre><code>var url = &quot;https://api.openai.com/v1/chat/completions&quot;

private fun getResponse(query: String) {

        val queue: RequestQueue = Volley.newRequestQueue(applicationContext)

        val jsonObject: JSONObject? = JSONObject()

        // start changes
        jsonObject?.put(&quot;model&quot;, &quot;gpt-3.5-turbo&quot;); 
        val messagesArray = JSONArray()
        val messageObject1 = JSONObject()
        messageObject1.put(&quot;role&quot;, &quot;user&quot;)
        messageObject1.put(&quot;content&quot;, query)
        messagesArray.put(messageObject1)
        jsonObject?.put(&quot;messages&quot;, messagesArray)
        // end changes

        jsonObject?.put(&quot;temperature&quot;, 0)
        jsonObject?.put(&quot;max_tokens&quot;, 100)
        jsonObject?.put(&quot;top_p&quot;, 1)
        jsonObject?.put(&quot;frequency_penalty&quot;, 0.0)
        jsonObject?.put(&quot;presence_penalty&quot;, 0.0)

        val postRequest: JsonObjectRequest 

            object : JsonObjectRequest(Method.POST, url, jsonObject,
                Response.Listener { response -&gt;

                    val responseMsg: String =
                        response.getJSONArray(&quot;choices&quot;).getJSONObject(0).getString(&quot;text&quot;)

                Response.ErrorListener { error -&gt;
                    Log.e(&quot;TAGAPI&quot;, &quot;Error is : &quot; + error.message + &quot;\n&quot; + error)
                }) {
                override fun getHeaders(): kotlin.collections.MutableMap&lt;kotlin.String, kotlin.String&gt; {
                    val params: MutableMap&lt;String, String&gt; = HashMap()

                    params[&quot;Content-Type&quot;] = &quot;application/json&quot;
                    params[&quot;Authorization&quot;] =
                        &quot;Bearer MYTOKEN&quot;
                    return params;
                }
            }
</code></pre>
<p>I only changed the parts that are in bold and now it does nto work, the application stops.</p>
","2023-03-02 21:35:36","","2023-03-04 20:16:40","2023-03-04 20:16:40","<android><kotlin><openai-api><gpt-3><chatgpt-api>","0","2","2","975","","","","","","",""
"75650841","1","15800270","","How to train or fine-tune GPT-2 / GPT-J model for generative question answering?","<p>I am new at using Huggingface models. Though I have some basic understanding of its Model, Tokenizers and Training.</p>
<p>I am looking for a way to leverage the generative models like GPT-2, and GPT-J from the Huggingface community and tune them for the question <strong>Closed Generative question answering</strong> - where we train the model first with the &quot;specific domain data&quot; <em>such as medical</em> and then asking questions related to that.</p>
<p>If possible, will you please walk me through the process?
Thank you so much 🤗</p>
","2023-03-06 12:27:08","","2023-03-06 14:23:03","2023-04-23 16:02:17","<python><machine-learning><nlp><huggingface-transformers><gpt-2>","1","1","0","573","","","","","","",""
"75680684","1","3563517","","how to read tokens from huggingface GPT2 tflite model using Interpreter","<p>I have recently converted a pre trained gpt2 model to tflite and trying to use an interpreter for generating text from the prompt.</p>
<p>Please find my code below which does the following:</p>
<ol>
<li>Converting the pre-trained model to tflite, works fine.</li>
<li>Creating an interpreter from the saved tflite model, works fine.</li>
<li>Using tokenizer, generate tokens and set tensors for input.</li>
<li>Invoke through an interpreter.</li>
<li>Extract output details</li>
</ol>
<p>Now, how could we accurately convert the output to tokens which is input to tokenizer's decode method?</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig, TFAutoModelForTokenClassification
from transformers import GPT2TokenizerFast

hub_gpt_model = TFGPT2LMHeadModel.from_pretrained(&quot;gpt-2&quot;)
hub_gpt_tokenizer = AutoTokenizer.from_pretrained(&quot;gpt-2&quot;)

input_ids = tf.keras.layers.Input((1024, ), batch_size=1, dtype=tf.int32, name='input_ids')
attention_mask = tf.keras.layers.Input((1024, ), batch_size=1, dtype=tf.int32, name='attention_mask')
inputs = {&quot;input_ids&quot;: input_ids, &quot;attention_mask&quot;: attention_mask}
output = hub_gpt_model(inputs)
hub_gpt_model_x = tf.keras.models.Model(inputs=inputs, outputs=output)
converter = tf.lite.TFLiteConverter.from_keras_model(hub_gpt_model_x)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.inference_input_type = tf.float32
tflite_quant_model = converter.convert()
with open('models/botGPT_lite/botGPT2_cosine_lite.tflite', 'wb') as f:
    f.write(tflite_quant_model)
interpreter = tf.lite.Interpreter(model_path='models/botGPT_lite/botGPT2_cosine_lite.tflite')
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

def pad_up_to(t, max_in_dims, constant_values):
    s = tf.shape(t)
    paddings = [ [ 0, m - s[i] ] for (i, m) in enumerate(max_in_dims) ]
    return tf.pad(t, paddings, 'CONSTANT', constant_values=constant_values)

import numpy as np

sentance = 'try(errorhandler) opencsvtxt(csvtxt)'
review_token = hub_gpt_tokenizer.encode(sentance, return_tensors = 'tf')
padded = pad_up_to(review_token, [1, 1024], 0)
input_mask = tf.ones_like(padded)
input_type_ids = tf.zeros_like(padded)

print(padded)
interpreter.set_tensor(
    input_details[0]['index'],
    padded,
)
# input_mask
interpreter.set_tensor(input_details[1]['index'], input_mask)

# input ids
#interpreter.set_tensor(input_details[2]['index'],input_ids)
interpreter.invoke()
output_details = interpreter.get_output_details()[0]
tflite_model_predictions = interpreter.get_tensor(output_details['index'])
print(&quot;Prediction results shape:&quot;, tflite_model_predictions.shape)
# HOW TO GET TOKENS THAT CAN BE INPUT TO TOKENIZER.DECODE METHOD?
predictions = np.argmax(tflite_model_predictions[0], axis=2)

</code></pre>
","2023-03-09 05:17:22","","2023-03-23 11:36:33","2023-03-23 11:36:33","<huggingface-transformers><tensorflow-lite><transformer-model><tflite><gpt-2>","0","0","0","141","","","","","","",""
"73467393","1","","","gpt3 fine tuning with openai not learning","<p>For my fine tuning jsonl files, I wanted a model that could predict the gender of the speaker given a statement. For instance, the prompt: &quot;i went to buy a skirt today&quot; has completion as &quot;female&quot;.</p>
<p>I created several examples and gave it to gpt3 to finetune. I then fed the sentence &quot;i went to pick my wife up from the shops&quot; to the resulting model. I expected to get a gender as response but I got a whole story about picking up my wife from the shops.</p>
<p>It's as if gpt-3 didn't learn anything from my fine tuning at all.</p>
<p>I have a few questions:</p>
<ol>
<li><p>Is fine tuning equivalent to writing a few examples in openai playground and getting gpt-3 to guess what comes next?</p>
</li>
<li><p>After fine tuning, do you only pay for the tokens in the prompt/completion of subsequent runs? So If I spend $100 training a model on a million examples, I will then only have to pay for the individual prompt/completion of subsequent calls?</p>
</li>
<li><p>The chat bot for instance, come with a context sentence before the back and forth exchange of 2 chat participants. Something like &quot;this is a conversation between a rude man named John and a young girl named Sarah&quot;. How can i incorporate such context into fine tuning structure of {&quot;prompt&quot;:&quot;...&quot;,&quot;completion&quot;:...&quot;}?</p>
</li>
</ol>
","2022-08-24 04:20:47","","2022-12-13 15:35:02","2022-12-13 15:35:02","<python-3.x><nlp><openai-api><gpt-3>","1","0","2","805","","","","","","",""
"72199570","1","5851623","","Fine tuning GPT2 for generative question anwering","<p>I am trying to finetune gpt2 for a generative question answering task.</p>
<p>Basically I have my data in a format similar to:</p>
<p>Context : Matt wrecked his car today.
Question: How was Matt's day?
Answer: Bad</p>
<p>I was looking on the huggingface documentation to find out how I can finetune GPT2 on a custom dataset and I did find the instructions on finetuning at this address:
<a href=""https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling</a></p>
<p>The issue is that they do not provide any guidance on how your data should be prepared so that the model can learn from it. They give different datasets that they have available, but none is in a format that fits my task well.</p>
<p>I would really appreciate if someone with more experience could help me.</p>
<p>Have a nice day!</p>
","2022-05-11 10:38:19","","","2023-06-20 10:10:16","<machine-learning><gpt-2>","2","0","3","3785","","","","","","",""
"72479175","1","14735451","","How to force GPT2 to generate specific tokens in each sentence?","<p>My input is a string and the outputs are vector representations (corresponding to the generated tokens). I'm trying to force the outputs to have specific tokens (e.g., 4 commas/2 of the word &quot;to&quot;, etc). That is, <strong>each generated sentence</strong> must have those.</p>
<p>Is there a potential loss component that can force GPT2 to generate specific tokens? Another approach that will be easier and more robust (but I'm not sure is possible), is similar to the masking of tokens in BERT. That is, instead of forcing GPT2 to generate sentences with unique tokens, to have the predefined tokens in the sentence beforehand:</p>
<pre><code>[MASK][MASK][specific_token][MASK][MASK][specific_token]
</code></pre>
<p>However, an issue with this approach is that there isn't a predefined number of tokens that should be generated/masked before or after the <code>[specific_token]</code>, nor there is a predefined number of sentences to generate for each given input (else I would have used BERT).</p>
<p><strong>Code:</strong></p>
<pre><code>from transformers import logging
from transformers import GPT2Tokenizer, GPT2Model
import torch 

checkpoint = 'gpt2'
tokenizer = GPT2Tokenizer.from_pretrained(checkpoint)
model = GPT2Model.from_pretrained(checkpoint)

num_added_tokens = tokenizer.add_special_tokens({'pad_token': '[CLS]'})
embedding_layer = model.resize_token_embeddings(len(tokenizer))  # Update the model embeddings with the new vocabulary size

input_string = 'Architecturally, the school has a Catholic character.'
token_ids = tokenizer(input_string, truncation = True, padding=True)
output = model(torch.tensor(token_ids['input_ids']))
</code></pre>
","2022-06-02 16:07:42","","","2022-06-05 22:07:46","<machine-learning><pytorch><huggingface-transformers><language-model><gpt-2>","0","0","1","411","","","","","","",""
"73779456","1","9951","","Why do generating text with gpt2 keep increasing memory consumption?","<p>I have a python script running an infinite loop, calling <code>gpt2.generate</code>, running on CPU (not GPU).</p>
<p>After the model is loaded and the first spike of memory usage is over, the RAM consumption keep increasing by about 100Mo every minute.</p>
<p>There is nothing in the loop storing anything, no typical python memory leak trap, python memory profiler is not showing a specific culprit.</p>
<p>So I was wondering if it was not in the C code.</p>
<p>Searching the web, I notice you can't limit memory consumption on CPU, only on GPU, with tensorflow.</p>
<p>Any other clue of what could cause this?</p>
","2022-09-19 21:26:28","","","2022-09-19 21:26:28","<python><tensorflow><gpt-2>","0","0","0","91","","","","","","",""
"73938457","1","18992575","","Reproducibility when using best_of in GPT-3 settings","<p>I want to do some tests using GPT-3. Instead of setting temperature = 0, I would like to use the best_of function. However, this gives me non-reproducible results since they differ in each iteration of code execution. Does anyone have an idea how I could achieve reproducible/deterministic results and still use best_of? Is there a way to use a random seed in the API for GPT-3?</p>
<p>Thanks!</p>
","2022-10-03 16:46:24","","","2022-12-05 18:24:37","<random-seed><gpt-3>","1","1","1","893","","","","","","",""
"73999135","1","5157277","","Open AI. Response from openai.Completion.create incomplete","<p>I have this code in Python. I would like to use the &quot;openai.Completion.create&quot; function:</p>
<pre><code>import os

import openai
from flask import Flask, redirect, render_template, request, url_for

app = Flask(__name__)
openai.api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)


@app.route(&quot;/&quot;, methods=(&quot;GET&quot;, &quot;POST&quot;))
def index():
    if request.method == &quot;POST&quot;:
        animal = request.form[&quot;animal&quot;]
        response = openai.Completion.create(
            model=&quot;text-davinci-002&quot;,
            prompt=generate_prompt(animal),
            temperature=1,
        )
        return redirect(url_for(&quot;index&quot;, result=response.choices[0].text))

    result = request.args.get(&quot;result&quot;)
    return render_template(&quot;index.html&quot;, result=result)


def generate_prompt(animal):
    return format(animal.capitalize()
    )
</code></pre>
<p>Why the completion is not complete?</p>
<p><a href=""https://i.stack.imgur.com/9F5Yj.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9F5Yj.png"" alt=""enter image description here"" /></a></p>
","2022-10-08 17:28:49","","","2022-10-08 17:28:49","<python><openai-api><gpt-3>","0","2","1","2322","","","","","","",""
"75389044","1","16415800","","How can I correctly implement the OpenAI API in swiftUI using a REST API? closed","<p>I currently have made a REST API using Alamofire in swiftUI. The request is a post method to this URL: <a href=""https://api.openai.com/v1/engines/text-davinci-002/completions"" rel=""nofollow noreferrer"">https://api.openai.com/v1/engines/text-davinci-002/completions</a>, which is basically sending a question to the OpenAI API and <em>supposed</em> to get the answer in JSON format. I have made an API key in OpenAI and have included it in my code.</p>
<p>The problem I am facing is that when getting the input, I recieve the following error:</p>
<p><code>Error: Response status code was unacceptable: 400.</code></p>
<p>I first tried to make sure if my API key was correct and it was. I also tried to see if a cURL statement would work to check if it was an error with the URL.<br />
I used this cURL statement:</p>
<pre><code>curl -X POST \
  https://api.openai.com/v1/engines/text-davinci-002/completions \
  -H 'Authorization: Bearer my-api-key' \
  -H 'Content-Type: application/json' \
  -d '{
        &quot;prompt&quot;: &quot;What is quantum mechanics?&quot;,
        &quot;temperature&quot;: 0.7,
        &quot;max_tokens&quot;: 20,
        &quot;top_p&quot;: 1,
        &quot;frequency_penalty&quot;: 0,
        &quot;presence_penalty&quot;: 0
}'
</code></pre>
<p>Which worked fine and gave me a correct output:</p>
<pre><code>{&quot;id&quot;:&quot;cmpl-6hhWBlzZHrLtV4RKCtjL63CLoTvPi&quot;,&quot;object&quot;:&quot;text_completion&quot;,&quot;created&quot;:1675873407,&quot;model&quot;:&quot;text-davinci-002&quot;,&quot;choices&quot;:[{&quot;text&quot;:&quot;\n\nQuantum mechanics is a branch of physics that studies the behavior of matter and energy in the&quot;,&quot;index&quot;:0,&quot;logprobs&quot;:null,&quot;finish_reason&quot;:&quot;length&quot;}],&quot;usage&quot;:{&quot;prompt_tokens&quot;:5,&quot;completion_tokens&quot;:20,&quot;total_tokens&quot;:25}}
</code></pre>
<p>If it helps, here's my code:</p>
<pre><code>//
//  chat.swift
//  titan
//
//  Created by Refluent on 08/02/2023.
//

import SwiftUI
import Alamofire

struct Message: Hashable {
    let sender: String
    let content: String
}

struct OpenAIResponse: Decodable {
    let completions: [Completion]
    
    struct Completion: Decodable {
        let text: String
    }
}

struct chatView: View {
    @State private var response: String = &quot;&quot;
    @State private var messages: [Message] = []
    @State private var userInput: String = &quot;&quot;
    
    let apiKey = &quot;my-api-key&quot;
    let model = &quot;text-davinci-002&quot;
    
    var body: some View {
        VStack {
            Text(&quot;Response from OpenAI API:&quot;)
            List(messages, id: \.self) { message in
                Text(&quot;\(message.sender): \(message.content)&quot;)
            }
            
            TextField(&quot;Enter your question&quot;, text: $userInput)
                .padding()
            
            Button(action: {
                self.sendRequest()
            }) {
                Text(&quot;Send&quot;)
            }
        }
    }
    
    func sendRequest() {
        let headers: HTTPHeaders = [&quot;Authorization&quot;: &quot;Bearer \(apiKey)&quot;, &quot;Content-Type&quot;: &quot;application/json&quot;]
        
        let parameters: Parameters = [&quot;prompt&quot;: userInput, &quot;model&quot;: model]
        
        messages.append(Message(sender: &quot;Me&quot;, content: userInput))
        userInput = &quot;&quot;
        
        AF.request(&quot;https://api.openai.com/v1/engines/text-davinci-002/completions&quot;, method: .post, parameters: parameters, headers: headers)
            .validate()
            .responseDecodable(of: OpenAIResponse.self) { response in
                switch response.result {
                case .success(let value):
                    let text = value.completions[0].text
                    self.messages.append(Message(sender: &quot;ChatGPT&quot;, content: text))
                case .failure(let error):
                    self.messages.append(Message(sender: &quot;ChatGPT&quot;, content: &quot;Error: \(error.localizedDescription)&quot;))
                }
            }
    }
}
</code></pre>
<p>For more context about my code, here it is:</p>
<p>I'm sending a post request to <a href=""https://api.openai.com/v1/engines/text-davinci-002/completions"" rel=""nofollow noreferrer"">this URL</a>, then getting the output and saving it in an array alongside the original input. The input and the output is then displayed as a chat.</p>
<p>Does anyone know why I am recieving this error? Did I miss a crucial part in my code?</p>
<p>Thanks in advance btw.</p>
<p>After reading through the comments, I realised that this isn't exactly something to do in <code>Swift</code>, so I will be implementing this in the server side. I apoligize for wasting your time.</p>
","2023-02-08 16:47:12","","2023-03-24 12:50:52","2023-03-24 12:50:52","<curl><swiftui><alamofire><openai-api><gpt-3>","1","5","-2","616","","","","","","",""
"75404485","1","20994974","","GPT3 fine tuned model returns additional questions and answers","<p>I have fine tuned a custom dataset using GPT3. I created a simple program to take user input (a question) and return the correct response. The program works, however it returns additional question and answers from the dataset I uploaded to the model.</p>
<p>I tried to reduce the max tokens cap and have set the temperature to 0, but I cannot seem to figure out how to stop the program from returning the additional questions and answers. Has anyone encountered this problem and if so how can I fix it?</p>
<p>Here is my code:</p>
<pre class=""lang-py prettyprint-override""><code>import openai

openai.api_key = &quot;MY_API_KEY&quot;

def respond(prompt):
    completions = openai.Completion.create(
        engine=&quot;MY_FINED_TUNED_MODEL&quot;,
        prompt=prompt,
        max_tokens=50,
        n=1,
        stop=None,
        temperature=0,
    )

    message = completions.choices[0].text
    return message

while True:
    prompt = input(&quot;Enter your question: &quot;)
    if prompt.lower() == &quot;end&quot;:
        break
    response = respond(prompt)
    print(response)
</code></pre>
","2023-02-09 21:17:01","","2023-02-14 11:53:14","2023-02-14 11:53:14","<openai-api><gpt-3>","1","0","1","615","","","","","","",""
"71335585","1","16852041","71336842","HuggingFace | ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet con","<p>Not always, but occasionally when running my code this error appears.</p>
<p>At first, I doubted it was a connectivity issue but to do with cashing issue, as discussed on an older <a href=""https://github.com/huggingface/transformers/issues/8690"" rel=""noreferrer"">Git Issue</a>.</p>
<p>Clearing cache didn't help runtime:</p>
<pre class=""lang-sh prettyprint-override""><code>$ rm ~/.cache/huggingface/transformers/ *
</code></pre>
<p>Traceback references:</p>
<ul>
<li>NLTK also gets <code>Error loading stopwords: &lt;urlopen error [Errno -2] Name or service not known</code>.</li>
<li>Last 2 lines re <code>cached_path</code> and <code>get_from_cache</code>.</li>
</ul>
<hr />
<p>Cache (before cleared):</p>
<pre class=""lang-sh prettyprint-override""><code>$ cd ~/.cache/huggingface/transformers/
(sdg) me@PF2DCSXD:~/.cache/huggingface/transformers$ ls
16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0
16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.json
16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.lock
4029f7287fbd5fa400024f6bbfcfeae9c5f7906ea97afcaaa6348ab7c6a9f351.723d8eaff3b27ece543e768287eefb59290362b8ca3b1c18a759ad391dca295a.h5
4029f7287fbd5fa400024f6bbfcfeae9c5f7906ea97afcaaa6348ab7c6a9f351.723d8eaff3b27ece543e768287eefb59290362b8ca3b1c18a759ad391dca295a.h5.json
4029f7287fbd5fa400024f6bbfcfeae9c5f7906ea97afcaaa6348ab7c6a9f351.723d8eaff3b27ece543e768287eefb59290362b8ca3b1c18a759ad391dca295a.h5.lock
684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f
684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.json
684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.lock
c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.json
c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock
fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51
fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51.json
fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51.lock
</code></pre>
<p>Code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline, set_seed

generator = pipeline('text-generation', model='gpt2')  # Error
set_seed(42)
</code></pre>
<p>Traceback:</p>
<pre><code>2022-03-03 10:18:06.803989: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-03-03 10:18:06.804057: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
[nltk_data] Error loading stopwords: &lt;urlopen error [Errno -2] Name or
[nltk_data]     service not known&gt;
2022-03-03 10:18:09.216627: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2022-03-03 10:18:09.216700: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2022-03-03 10:18:09.216751: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (PF2DCSXD): /proc/driver/nvidia/version does not exist
2022-03-03 10:18:09.217158: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-03-03 10:18:09.235409: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
All model checkpoint layers were used when initializing TFGPT2LMHeadModel.

All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.
Traceback (most recent call last):
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/runpy.py&quot;, line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/runpy.py&quot;, line 87, in _run_code
    exec(code, run_globals)
  File &quot;/mnt/c/Users/me/Documents/GitHub/project/foo/bar/__main__.py&quot;, line 26, in &lt;module&gt;
    nlp_setup()
  File &quot;/mnt/c/Users/me/Documents/GitHub/project/foo/bar/utils/Modeling.py&quot;, line 37, in nlp_setup
    generator = pipeline('text-generation', model='gpt2')
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/pipelines/__init__.py&quot;, line 590, in pipeline
    tokenizer = AutoTokenizer.from_pretrained(
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py&quot;, line 463, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py&quot;, line 324, in get_tokenizer_config
    resolved_config_file = get_file_from_repo(
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/file_utils.py&quot;, line 2235, in get_file_from_repo
    resolved_file = cached_path(
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/file_utils.py&quot;, line 1846, in cached_path
    output_path = get_from_cache(
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/file_utils.py&quot;, line 2102, in get_from_cache
    raise ValueError(
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.
</code></pre>
<hr />
<p><strong>Failed Attempts</strong></p>
<ol>
<li>I closed my IDE and bash terminal. Ran <code>wsl.exe --shutdown</code> in PowerShell. Relaunched IDE and bash terminal with same error.</li>
<li>Disconnecting/ different VPN.</li>
<li>Clear cache <code>$ rm ~/.cache/huggingface/transformers/ *</code>.</li>
</ol>
","2022-03-03 10:28:24","","2022-03-03 13:51:06","2023-05-15 19:12:21","<python-3.x><tensorflow><huggingface-transformers><valueerror><gpt-2>","4","3","5","9848","","2","16852041","<p>Since I am working in a <strong>conda venv</strong> and using <strong>Poetry</strong> for handling dependencies, I needed to re-<strong>install torch</strong> - a dependency for Hugging Face 🤗 Transformers.</p>
<hr />
<p>First, install torch:
<a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">PyTorch's website</a> lets you chose your exact setup/ specification for install. In my case, the command was</p>
<pre class=""lang-bash prettyprint-override""><code>conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch
</code></pre>
<p>Then add to Poetry:</p>
<pre><code>poetry add torch
</code></pre>
<p>Both take ages to process. Runtime was back to normal :)</p>
","2022-03-03 11:59:39","0","0"
"71618602","1","12872310","71824786","Finetuning GPT-3 on Windows?","<p>While I have read the documentation on fine-tuning GPT-3, I do not understand how to do so. It seems that the proposed CLI commands do not work in the Windows CMD interface and I can not find any documentation on how to finetune GPT3 using a &quot;regular&quot; python script. I have tried to understand the functions defined in the package. However I can not make sense of them. Is there any information that I am missing or is it just not possible to fine-tune GPT-3 on a Windows machine?</p>
<p><a href=""https://beta.openai.com/docs/guides/fine-tuning"" rel=""nofollow noreferrer"">https://beta.openai.com/docs/guides/fine-tuning</a></p>
","2022-03-25 14:28:52","","","2022-04-11 08:29:25","<python><machine-learning><nlp><openai-api><gpt-3>","2","0","1","617","","2","12872310","<p>For anybody having a similar problem: We solved the problem by using the anaconda prompt cmd. There everything worked flawlessly.</p>
","2022-04-11 08:29:25","0","1"
"73642618","1","589921","73853380","GPT-3 cannot mix two actions into one prompt (summarisation and tense changing)","<p>(Just a heads up, this feels like a weird question to ask since there's not really any code involved, I'm not sure if this is the right place to ask)</p>
<p>I am trying to summarise a journal entry <em>and</em> convert it into second person past tense (i.e. &quot;I went to the shop&quot; -&gt; &quot;You went to the shop&quot;).</p>
<p>When I give the following prompt to GPT-3 (Da Vinci, all other params normal), it gives me a summary as expected:</p>
<pre><code>Summarise this text:

We took to the streets of London on the London hire bikes aka Boris Bikes / BoJo Bikes; previously Barclays Bikes and now Santander Bikes – bloomin heck this is complicated.  I knew the direction where I wanted to get to and knew how to get there except I didn’t really.

We started our journey at one of bike hire station in St John’s Wood and continued around Regents Park (the wrong way) Simon got us to one of the gateways to the path along the Regents Canal.  Sometimes they can be quite difficult to find; this was one of those times. This particular one was located at the back of a housing estate; only that Simon knew where it was there was no way I would have found it.

Off down the canal we went. Sunday afternoons are a busy time along the canal with local people mixed in with tourists from all over the world; so cycling along a narrow path is not easy as everyone walks on different sides of the path (according to where they come from)! We got towards Camden Market and the path got very busy, to the point that I almost went into the canal but with a wibble and a wobble I managed to stay in.  At that point the decision was easily made to get off that bike and walk it. The Santander App showed us where the nearest parking station was and that there was space available to park up.

Coffee time! Forget the major chains, we found a small local place called T &amp; G for some cups of coffee and a sarnie before we went out to find out next bike to get us to Granary Square in Kings Cross for our next stop. From the canal path there is a grassed set of steps going up to the Square but first we parked up the bikes on the other side of the canal. So many places to choose from to hang out, for drinks and for food or trains to Paris, Lille, Edinburgh or Manchester to start off with.

All in all, we went out and achieved what we intended to – a cycle along the canal with a couple of stops along the way for some food and drinks.  What better way to spend a Sunday afternoon.

// GPT-3's answer:

The author takes a Boris Bike (a bike available for rent in London) and cycles along the Regents Canal. They note that the path is busy on a Sunday afternoon, but manage to find a parking spot for their bike before exploring the area around Granary Square in Kings Cross.
</code></pre>
<p>That is a very good summary.</p>
<p>Now, I can convert the summary to second person POV like so:</p>
<pre><code>Convert the following to past tense second person:

The author takes a Boris Bike (a bike available for rent in London) and cycles along the Regents Canal. They note that the path is busy on a Sunday afternoon, but manage to find a parking spot for their bike before exploring the area around Granary Square in Kings Cross.

// GPT-3's answer:

You took a Boris Bike and cycled along the Regents Canal. You noted that the path was busy on a Sunday afternoon, but managed to find a parking spot for your bike before exploring the area around Granary Square in Kings Cross.
</code></pre>
<p>Again, excellent! But if I combine the prompts like so:</p>
<p><code>Summarise the following and convert the result to past tense second person:</code></p>
<p>It doesn't work well at all - in fact it just seems to ignore the summarisation part of the prompt, i.e. it just converts the whole passage to second person past tense. How can I fix this?</p>
","2022-09-08 00:26:15","","","2022-09-26 11:17:53","<gpt-3>","1","0","0","431","","2","34170","<p>Try the following format and see if it works good enough for you; I'm using this approach for a whole lot of scenarios to solve the issue you described. Use zero-temperature (unless you want to risk variations).</p>
<p>Our prompt:</p>
<pre><code>Story Text: &quot;We took to the streets of London on the London hire bikes aka Boris Bikes / BoJo Bikes; previously Barclays Bikes and now Santander Bikes – bloomin heck this is complicated.  I knew the direction where I wanted to get to and knew how to get there except I didn’t really.

We started our journey at one of bike hire station in St John’s Wood and continued around Regents Park (the wrong way) Simon got us to one of the gateways to the path along the Regents Canal.  Sometimes they can be quite difficult to find; this was one of those times. This particular one was located at the back of a housing estate; only that Simon knew where it was there was no way I would have found it.

Story Text: &quot;We took to the streets of London on the London hire bikes aka Boris Bikes / BoJo Bikes; previously Barclays Bikes and now Santander Bikes – bloomin heck this is complicated.  I knew the direction where I wanted to get to and knew how to get there except I didn’t really.

We started our journey at one of bike hire station in St John’s Wood and continued around Regents Park (the wrong way) Simon got us to one of the gateways to the path along the Regents Canal.  Sometimes they can be quite difficult to find; this was one of those times. This particular one was located at the back of a housing estate; only that Simon knew where it was there was no way I would have found it.

Off down the canal we went. Sunday afternoons are a busy time along the canal with local people mixed in with tourists from all over the world; so cycling along a narrow path is not easy as everyone walks on different sides of the path (according to where they come from)! We got towards Camden Market and the path got very busy, to the point that I almost went into the canal but with a wibble and a wobble I managed to stay in.  At that point the decision was easily made to get off that bike and walk it. The Santander App showed us where the nearest parking station was and that there was space available to park up.

Coffee time! Forget the major chains, we found a small local place called T &amp; G for some cups of coffee and a sarnie before we went out to find out next bike to get us to Granary Square in Kings Cross for our next stop. From the canal path there is a grassed set of steps going up to the Square but first we parked up the bikes on the other side of the canal. So many places to choose from to hang out, for drinks and for food or trains to Paris, Lille, Edinburgh or Manchester to start off with.&quot;

Following is the Summary of the Story Text (1) and Second Person Past Tense of that Summary (2): 
1) 
</code></pre>
<p>So, by using &quot;Following is the Summary of the Story Text (1) and Second Person Past Tense of that Summary (2): 1) &quot; we're biasing GPT-3 in a simple and syntactically strongly outlined way, and this bias is the very last thing in the prompt; we also help it by already providing the &quot;1) &quot; (but leaving its content empty).</p>
<p>GPT-3's zero-temperature result (model text-davinci-002):</p>
<pre><code> We took the London hire bikes for a ride and ended up at a coffee shop near Camden Market.
2) You took the London hire bikes for a ride and ended up at a coffee shop near Camden Market.
</code></pre>
<p>I suggest you also add &quot;3)&quot; as stop sequence in case GPT-3 adds too much. The result is now easily parsable by splitting alongside newlines, removing any &quot;2) &quot;, trimming, and then grabbing lines[0] and [1].</p>
","2022-09-26 11:17:53","0","1"
"55531061","1","11315879","56754191","How can I create and fit vocab.bpe file (GPT and GPT2 OpenAI models) with my own corpus text?","<p>This question is for those who are familiar with GPT or <a href=""https://github.com/openai/gpt-2"" rel=""noreferrer"">GPT2</a> OpenAI models. In particular, with the encoding task (Byte-Pair Encoding). This is my problem:</p>

<p>I would like to know how I could create my own vocab.bpe file.</p>

<p>I have a spanish corpus text that I would like to use to fit my own bpe encoder. I have succeedeed in creating the encoder.json with the <a href=""https://github.com/soaxelbrooke/python-bpe"" rel=""noreferrer"">python-bpe</a> library, but I have no idea on how to obtain the vocab.bpe file.
I have reviewed the code in <a href=""https://github.com/openai/gpt-2/blob/master/src/encoder.py"" rel=""noreferrer"">gpt-2/src/encoder.py</a> but, I have not been able to find any hint. Any help or idea?</p>

<p>Thank you so much in advance.</p>
","2019-04-05 08:15:51","","2020-11-29 12:07:28","2020-11-29 12:07:28","<python><encoding><nlp><gpt-2>","2","2","6","2490","0","2","11697642","<p>check out <a href=""https://github.com/rsennrich/subword-nmt/"" rel=""nofollow noreferrer"">here</a>, you can easily create the same vocab.bpe using the following command:</p>

<pre><code>python learn_bpe -o ./vocab.bpe -i dataset.txt --symbols 50000
</code></pre>
","2019-06-25 12:34:15","0","4"
"60833301","1","7896124","60833512","Train huggingface's GPT2 from scratch : assert n_state % config.n_head == 0 error","<p>I am trying to use a GPT2 architecture for musical applications and consequently need to train it from scratch. After a bit of googling I found that the issue #1714 from huggingface's github already had ""solved"" the question. When I try the to run the propose solution :</p>

<pre><code>from transformers import GPT2Config, GPT2Model

NUMLAYER = 4
NUMHEAD = 4
SIZEREDUCTION = 10 #the factor by which we reduce the size of the velocity argument.
VELSIZE = int(np.floor(127/SIZEREDUCTION)) + 1 
SEQLEN=40 #size of data sequences.
EMBEDSIZE = 5 

config = GPT2Config(vocab_size = VELSIZE, n_positions = SEQLEN, n_embd = EMBEDSIZE, n_layer = NUMLAYER, n_ctx = SEQLEN, n_head = NUMHEAD)  
model = GPT2Model(config)
</code></pre>

<p>I get the following error : </p>

<pre><code>Traceback (most recent call last):

  File ""&lt;ipython-input-7-b043a7a2425f&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/cnelias/Desktop/PHD/Swing project/code/script/GPT2.py', wdir='C:/Users/cnelias/Desktop/PHD/Swing project/code/script')

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 786, in runfile
    execfile(filename, namespace)

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/cnelias/Desktop/PHD/Swing project/code/script/GPT2.py"", line 191, in &lt;module&gt;
    model = GPT2Model(config)

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 355, in __init__
    self.h = nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in range(config.n_layer)])

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 355, in &lt;listcomp&gt;
    self.h = nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in range(config.n_layer)])

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 223, in __init__
    self.attn = Attention(nx, n_ctx, config, scale)

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 109, in __init__
    assert n_state % config.n_head == 0
</code></pre>

<p>What does it mean and how can I solve it ?</p>

<p>Also more generally, is there a documentation on how to do a forward call with the GPT2 ? Can I define my own <code>train()</code> function or do I have to use the model's build-in function ? Am I forced to use a <code>Dataset</code> to do the training or can I feed it individual tensors ? 
I looked for it but couldn't find answer to these on the doc, but maybe I missed something.</p>

<p>PS : I already read the blogpost fron huggingface.co, but it omits too much informations and details to be usefull for my application.</p>
","2020-03-24 14:42:13","","2020-11-29 12:10:24","2020-11-29 12:10:24","<python><nlp><huggingface-transformers><transformer-model><gpt-2>","1","0","0","690","","2","3607203","<p>I think the error message is pretty clear:</p>

<blockquote>
  <p><code>assert n_state % config.n_head == 0</code></p>
</blockquote>

<p>Tracing it back through <a href=""https://github.com/huggingface/transformers/blob/v2.5.1/src/transformers/modeling_gpt2.py#L99"" rel=""nofollow noreferrer"">the code</a>, we can see</p>

<blockquote>
  <p><code>n_state = nx  # in Attention: n_state=768</code></p>
</blockquote>

<p>which indicates that <code>n_state</code> represents the embedding dimension (which is generally 768 by default in BERT-like models). When we then look at the <a href=""https://huggingface.co/transformers/model_doc/gpt2.html#gpt2config"" rel=""nofollow noreferrer"">GPT-2 documentation</a>, it seems the parameter specifying this is <code>n_embd</code>, which you are setting to <code>5</code>. As the error indicates, the embedding dimension <strong>has to be evenly divisible through the number of attention heads</strong>, which were specified as <code>4</code>. So, choosing a different embedding dimension as a multiple of <code>4</code> should solve the problem. Of course, you can also change the number of heads to begin with, but it seems that odd embedding dimensions are not supported.</p>
","2020-03-24 14:53:57","3","2"
"62830783","1","7710572","62830856","Scripts missing for GPT-2 fine tune, and inference in Hugging-face GitHub?","<p>I am following the <a href=""https://huggingface.co/transformers/v2.0.0/examples.html"" rel=""nofollow noreferrer"">documentation</a> on the hugging face website, in there they say that to fine-tune GPT-2 I should use the script
<a href=""https://github.com/huggingface/transformers/blob/master/examples/run_lm_finetuning.py"" rel=""nofollow noreferrer"">run_lm_finetuning.py</a> for fine-tuning, and the script <a href=""https://github.com/huggingface/transformers/blob/master/examples/run_generation.py"" rel=""nofollow noreferrer"">run_generation.py</a>
for inference.
However, both scripts don't actually exist on GitHub anymore.</p>
<p>Does anybody know whether the documentation is outdated? or where to find those two scripts?</p>
<p>Thanks</p>
","2020-07-10 08:59:58","","2020-11-29 12:03:01","2022-05-25 07:40:03","<python><huggingface-transformers><language-model><gpt-2>","2","0","1","359","","2","5266133","<p>It looks like they've been moved around a couple times and the docs are indeed out of date, the current version can be found in <code>run_language_modeling.py</code> here <a href=""https://github.com/huggingface/transformers/tree/master/examples/language-modeling"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/tree/master/examples/language-modeling</a></p>
","2020-07-10 09:03:54","1","1"
"63321892","1","3593041","63442865","How can I use GPT 3 for my text classification?","<p>I am wondering if I can be able to use OpenAI GPT-3 for transfer learning in a text classification problem?
If so, how can I get start on it using Tensorflow, Keras.</p>
","2020-08-09 02:17:17","","2020-11-29 11:46:41","2020-11-29 11:46:41","<keras><text-classification><transfer-learning><openai-api><gpt-3>","1","7","6","9646","0","2","3488735","<p>(i substituted hateful language with ******** in the following samples)</p>
<p>Given samples like:</p>
<pre><code>(&quot;You look like ****** *** to me *******&quot;, true)
(&quot;**** you *********&quot;, true)
(&quot;**** my ****&quot;, true)
(&quot;hey my name is John can you help me?&quot;, false)
(&quot;hey my name is John, i think you ****** ***!&quot;, true)
(&quot;i have a problem with my network driver hpz-3332d&quot;, false)
</code></pre>
<p>GPT-3 can indeed then decide if a given input is hateful or not. GPT-3 actually is implementing filters that will very effectively tell if an arbitrary comment is hatefull or not. You would just enter the msg and let GPT3 autcomplete the  <code>, true|false)</code> part at the end, setting tokens to about ~6 and temperature setting 90%.</p>
<p>Boolean-ish classification that also relies on more complex context (you can insult someone without using foul-language) id doeable with GPT3 and can also be done with GPT2.</p>
","2020-08-16 23:31:25","5","6"
"65529156","1","1793799","65563077","Huggingface Transformer - GPT2 resume training from saved checkpoint","<p>Resuming the <code>GPT2</code> finetuning, implemented from <code>run_clm.py</code></p>
<p>Does GPT2 <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">huggingface</a>  has a parameter to resume the training from the saved checkpoint, instead training again from the beginning? Suppose the python notebook crashes while training, the checkpoints will be saved, but when I train the model again still it starts the training from the beginning.</p>
<p>Source: <a href=""https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_clm.py"" rel=""nofollow noreferrer"">here</a></p>
<p>finetuning code:</p>
<pre><code>!python3 run_clm.py \
    --train_file source.txt \
    --do_train \
    --output_dir gpt-finetuned \
    --overwrite_output_dir \
    --per_device_train_batch_size 2 \
    --model_name_or_path=gpt2 \
    --save_steps 100 \
    --num_train_epochs=1 \
    --block_size=200 \
    --tokenizer_name=gpt2
</code></pre>
<p>From the above code, <code>run_clm.py</code> is a script provided by <a href=""https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_clm.py"" rel=""nofollow noreferrer"">huggingface</a> to finetune gpt2 to train with the customized dataset</p>
","2021-01-01 11:07:28","","","2022-08-24 08:32:02","<python><pytorch><huggingface-transformers><language-model><gpt-2>","2","0","3","2826","","2","843036","<p>To resume training from checkpoint you use the <code>--model_name_or_path</code> parameter. So instead of giving the default <code>gpt2</code> you direct this to your latest checkpoint folder.</p>
<p>So your command becomes:</p>
<pre><code>!python3 run_clm.py \
    --train_file source.txt \
    --do_train \
    --output_dir gpt-finetuned \
    --overwrite_output_dir \
    --per_device_train_batch_size 2 \
    --model_name_or_path=/content/models/checkpoint-5000 \
    --save_steps 100 \
    --num_train_epochs=1 \
    --block_size=200 \
    --tokenizer_name=gpt2
</code></pre>
","2021-01-04 12:55:30","0","4"
"74656790","1","18002337","74657190","Prepare json file for GPT","<p>I would like to create a dataset to use it for fine-tuning GPT3. As I read from the following site <a href=""https://beta.openai.com/docs/guides/fine-tuning"" rel=""nofollow noreferrer"">https://beta.openai.com/docs/guides/fine-tuning</a>, the dataset should look like this</p>
<pre><code>{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}
{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}
{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}
...
</code></pre>
<p>For this reason I am creating the dataset with the following way</p>
<pre><code>import json

# Data to be written
dictionary = {
    &quot;prompt&quot;: &quot;&lt;text1&gt;&quot;, &quot;completion&quot;: &quot;&lt;text to be generated1&gt;&quot;}, {
    &quot;prompt&quot;: &quot;&lt;text2&gt;&quot;, &quot;completion&quot;: &quot;&lt;text to be generated2&gt;&quot;}

with open(&quot;sample2.json&quot;, &quot;w&quot;) as outfile:
    json.dump(dictionary, outfile)
</code></pre>
<p>However, when I am trying to load it, it looks like this which is not as we want</p>
<pre><code>import json
 
# Opening JSON file
with open('sample2.json', 'r') as openfile:
 
    # Reading from json file
    json_object = json.load(openfile)
 
print(json_object)
print(type(json_object))

&gt;&gt; [{'prompt': '&lt;text1&gt;', 'completion': '&lt;text to be generated1&gt;'}, {'prompt': '&lt;text2&gt;', 'completion': '&lt;text to be generated2&gt;'}]
&lt;class 'list'&gt;
</code></pre>
<p><strong>Could you please let me know how can I face this problem?</strong></p>
","2022-12-02 13:48:53","","","2022-12-02 14:59:31","<python><nlp><gpt-3>","1","0","1","1952","","2","15568504","<p>it's more like, writing <code>\n</code> a new line character after each json. so each line is JSON. somehow the link <a href=""https://jsonlines.org"" rel=""nofollow noreferrer"">jsonlines</a> throw server not found error on me.</p>
<p>you can have these options:</p>
<ol>
<li>write <code>\n</code> after each line:</li>
</ol>
<pre><code>import json
with open(&quot;sample2_op1.json&quot;, &quot;w&quot;) as outfile:
    for e_json in dictionary:
        json.dump(e_json, outfile)
        outfile.write('\n')
#read file, as it has \n, read line by line and load as json
with open(&quot;sample2_op1.json&quot;,&quot;r&quot;) as file:
    for line in file:
        print(json.loads(line),type(json.loads(line)))
</code></pre>
<ol start=""2"">
<li>which have way to read file too, its <a href=""https://jsonlines.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">jsonlines</a>
install the module <code>!pip install jsonlines</code></li>
</ol>
<pre><code>import jsonlines
#write to file
with jsonlines.open('sample2_op2.jsonl', 'w') as outfile:
    outfile.write_all(dictionary)
#read the file
with jsonlines.open('sample2_op2.jsonl') as reader:
    for obj in reader:
        print(obj)
</code></pre>
","2022-12-02 14:17:37","2","1"
"75313457","1","4831435","75313682","OpenAI GPT-3 API: openai.api_key = os.getenv() not working","<p>I am just trying some simple functions in Python with OpenAI APIs but running into an error:</p>
<p>I have a valid API secret key which I am using.</p>
<p>Code:</p>
<pre><code>&gt;&gt;&gt; import os
&gt;&gt;&gt; import openai
&gt;&gt;&gt; openai.api_key = os.getenv(&quot;I have placed the key here&quot;)
&gt;&gt;&gt; response = openai.Completion.create(model=&quot;text-davinci-003&quot;, prompt=&quot;Say this is a test&quot;, temperature=0, max_tokens=7)
</code></pre>
<p><a href=""https://i.stack.imgur.com/zCgm4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/zCgm4.png"" alt=""Simple test"" /></a></p>
","2023-02-01 16:44:32","","2023-03-13 14:08:23","2023-05-16 15:17:34","<python><openai-api><gpt-3>","1","3","3","5404","","2","10347145","<h3>Option 1: OpenAI API key NOT as an environmental variable</h3>
<p>Change this...</p>
<p><code>openai.api_key = os.getenv('sk-xxxxxxxxxxxxxxxxxxxx')</code></p>
<p>...to this.</p>
<p><code>openai.api_key = 'sk-xxxxxxxxxxxxxxxxxxxx'</code></p>
<br>
<h3>Option 2: OpenAI API key as an environmental variable (recommended)</h3>
<p>Change this...</p>
<p><code>openai.api_key = os.getenv('sk-xxxxxxxxxxxxxxxxxxxx'</code>)</p>
<p>...to this...</p>
<p><code>openai.api_key = os.getenv('OPENAI_API_KEY')</code></p>
<br>
<h4><a href=""https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety"" rel=""nofollow noreferrer"">How do I set the OpenAI API key as an environmental variable?</a></h4>
<p>STEP 1: Open <em>System</em> properties and select <em>Advanced system settings</em></p>
<p>STEP 2: Select <em>Environment Variables</em></p>
<p>STEP 3: Select <em>New</em></p>
<p>STEP 4: Add your name/key value pair</p>
<pre><code>Variable name: OPENAI_API_KEY

Variable value: sk-xxxxxxxxxxxxxxxxxxxx
</code></pre>
<p>STEP 5: Restart your computer</p>
","2023-02-01 17:01:26","2","8"
"75335523","1","21140352","75335600","Error 400 when using GPT API (in JavaScript)","<p>I keep getting a 400 Error when I try to run my very basic chatbot using the GPT API:
<a href=""https://i.stack.imgur.com/VzyEy.png"" rel=""nofollow noreferrer"">error</a></p>
<p>Attached is my code; am I doing something wrong with the API key?</p>
<pre><code>const chatHistoryContent = document.querySelector(&quot;#chat-history-content&quot;);
const chatMessageInput = document.querySelector(&quot;#chat-message-input&quot;);
const chatMessageSubmit = document.querySelector(&quot;#chat-message-submit&quot;);



chatMessageSubmit.addEventListener(&quot;click&quot;, async function () {
    const message = chatMessageInput.value;
    chatMessageInput.value = &quot;&quot;;

    // Add the user's message to the chat history
    const userMessageDiv = document.createElement(&quot;div&quot;);
    userMessageDiv.innerHTML = `You: ${message}`;
    chatHistoryContent.appendChild(userMessageDiv);

    // Use the OpenAI GPT-3 API to get a response from the chatbot
    const response = await getResponseFromAPI(message);

    // Add the chatbot's response to the chat history
    const chatbotMessageDiv = document.createElement(&quot;div&quot;);
    chatbotMessageDiv.innerHTML = `Max: ${response}`;
    chatHistoryContent.appendChild(chatbotMessageDiv);
});

async function getResponseFromAPI(message) {

    const apiKey = &quot;sk-myapikey&quot;;
    const endpoint = `https://api.openai.com/v1/engines/davinci/jobs`;

    const response = await fetch(endpoint, {
        method: &quot;POST&quot;,
        headers: {
            &quot;Content-Type&quot;: `application/json`,
            &quot;Authorization&quot;: `Bearer ${apiKey}`,
        },
        body: JSON.stringify({
            model: &quot;text-davinci-003&quot;,
            prompt: &quot;test prompt&quot;, 
            temperature: 0.5,
            max_tokens: 512,
            top_p: 1,
            frequency_penalty: 0,
            presence_penalty: 0,
        })
    });

    const data = await response.json();
    return data.choices[0].text;
}
</code></pre>
<p>Thanks</p>
<p>I have tried consulting many websites to see solutions to this but have had no luck.</p>
","2023-02-03 12:08:08","","2023-02-04 06:24:14","2023-02-09 09:22:10","<javascript><error-handling><openai-api><gpt-3>","2","1","1","1149","","2","1584167","<p>400 (Bad Request) error code typically means that client request's data is incorrect. So yes, must be something with your auth headers/body of request. Quite often response contains a reason, please try to print the text of response (before trying to get json output), e.g.</p>
<pre><code>console.log(response.text());
</code></pre>
<p>or just check Network Tab in Dev Console</p>
","2023-02-03 12:14:02","0","0"
"75762087","1","13908629","75771480","Trying to finetune GPT-2 in Vertex AI but it just freezes","<p>I've been following some tutorials on training GPT-2, and I've scraped together some code that works in Google Colab, but when I move it over to Vertex AI workbench, it just seems to sit there and do nothing when I run the training code. I have the GPU quotas all set up and I have a billing account, and I've enabled all relevant APIs. Here is the code I'm using for the tokenizer:</p>
<pre><code>def tokenize_function(examples):
        return base_tokenizer(examples['Prompt_Final'], padding=True)
    
# Split in train and test
df_train, df_val = train_test_split(df, train_size = 0.8)

# load the dataset from the dataframes
train_dataset = Dataset.from_pandas(df_train[['Prompt_Final']])
val_dataset = Dataset.from_pandas(df_val[['Prompt_Final']])

# tokenize the training and validation
tokenized_train_dataset = train_dataset.map(
    tokenize_function,
    batched=True,
    num_proc=1
)

tokenized_val_dataset = val_dataset.map(
    tokenize_function,
    batched=True,
    num_proc=1
)
</code></pre>
<p>And here is the code I'm using for the model:</p>
<pre><code>bos = '&lt;|startoftext|&gt;'
eos = '&lt;|endoftext|&gt;'
body = '&lt;|body|&gt;'

special_tokens_dict = {'eos_token': eos, 'bos_token': bos, 'pad_token': '&lt;pad&gt;',
                       'sep_token': body} 

# the new tokens are added to the tokenizer
num_added_toks = base_tokenizer.add_special_tokens(special_tokens_dict)

# model configuration
config = AutoConfig.from_pretrained('gpt2', 
                                    bos_token_id=base_tokenizer.bos_token_id,
                                    eos_token_id=base_tokenizer.eos_token_id,
                                    pad_token_id=base_tokenizer.pad_token_id,
                                    sep_token_id=base_tokenizer.sep_token_id,
                                    output_hidden_states=False)

# we load the pre-trained model with custom settings
base_model = GPT2LMHeadModel.from_pretrained('gpt2', config=config)

# model embeding resizing
base_model.resize_token_embeddings(len(base_tokenizer))

# make sure its using the gpu
base_model = base_model.to(device)
</code></pre>
<p>And here is the code I'm using for the model path, the training args, the data collator, and the Trainer.</p>
<pre><code>model_articles_path = r'Model/Model_Path'

training_args = TrainingArguments(
    output_dir=model_articles_path,  # output directory
    num_train_epochs=1,              # total num of training epochs
    per_device_train_batch_size=10,   # batch size per device during training
    per_device_eval_batch_size=10,    # batch size for evaluation
    warmup_steps=200,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir=model_articles_path, # directory for storing logs
    prediction_loss_only=True,
    evaluation_strategy= &quot;steps&quot;,
    save_steps=10,
    gradient_accumulation_steps=1,
    # gradient_checkpointing=True,
    eval_accumulation_steps=1,
    fp16=True
)

data_collator = DataCollatorForLanguageModeling(
        tokenizer=base_tokenizer,
        mlm=False
    )

trainer = Trainer(
    model=base_model,                      # the instantiated Transformers model to be trained
    args=training_args,                    # training arguments, defined above
    data_collator=data_collator,
    train_dataset=tokenized_train_dataset, # training dataset
    eval_dataset=tokenized_val_dataset     # validation dataset
)
</code></pre>
<p>When I run <code>trainer.train()</code> though, it will start with this warning, which I'm very used to</p>
<pre><code>/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  FutureWarning,
</code></pre>
<p>And then the code will just sit there. I can tell its running, and when I check nvidia-smi in the terminal I can tell its using the GPU, but it just sits there. I am using a Tesla P100-PCIE-16GB GPU, and I am using the GPT-2 small model, so it should be making quick work of it with only 1000 rows of data. I'm hopeful that I've just made a dumb mistake somewhere, but if someone has some experience in this department it'd be greatly appreciated.</p>
","2023-03-16 22:13:47","","","2023-03-17 19:27:02","<python><pytorch><huggingface-transformers><google-cloud-vertex-ai><gpt-2>","1","1","0","164","","2","13908629","<p>I got around this by using a workbook with these settings:</p>
<ul>
<li>Zone: US-Central1-b</li>
<li>Environment: NumPy/SciPy/scikit-learn (when making the workbook I chose the Python Cuda 11.0 option)</li>
<li>Machine Type: 8 vCPUS, 30GB RAM</li>
<li>GPUs: Nvidia V100 x1</li>
</ul>
<p>And in the workbook itself, I used this command to install PyTorch:</p>
<pre><code>!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117
</code></pre>
<p>After that, everything worked just fine, just like in Google Colab!</p>
","2023-03-17 19:27:02","0","0"
"75529578","1","21263614","75797335","Stream interrupted (client disconnected). To resume the stream, run: openai api fine_tunes.follow -i ft-***, while fine tuning openai","<p>(Ctrl-C will interrupt the stream, but not cancel the fine-tune)
[2023-02-22 07:37:41] Created fine-tune: ft-LaoFNTDDUfQbmxTHkViCcuc2</p>
<p>Stream interrupted (client disconnected).
To resume the stream, run:</p>
<p>openai api fine_tunes.follow -i ft-LaoFNTDDUfQbmxTHkViCcuc2</p>
<p>What's the issue I used two version of OpenAi 0.25.0 and 0.26.5
For both the version I am getting the same error</p>
<p>Can anyone help solve the above problem</p>
","2023-02-22 07:48:04","","","2023-03-21 04:31:51","<python><openai-api><gpt-3>","1","1","1","861","","2","1151189","<p><a href=""https://i.stack.imgur.com/kI0JP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kI0JP.png"" alt=""enter image description here"" /></a></p>
<p><strong>It was a temporary issue of OpenAI</strong></p>
","2023-03-21 04:31:51","0","0"
"75804599","1","4505301","75804651","OpenAI API: How do I count tokens before(!) I send an API request?","<p>OpenAI's text models have a context length, e.g.: Curie has a context length of 2049 tokens.
They provide max_tokens and stop parameters to control the length of the generated sequence. Therefore the generation stops either when stop token is obtained, or max_tokens is reached.</p>
<p>The issue is: when generating a text, I don't know how many tokens my prompt contains. Since I do not know that, I cannot set max_tokens = 2049 - number_tokens_in_prompt.</p>
<p>This prevents me from generating text dynamically for a wide range of text in terms of their length. What I need is to continue generating until the stop token.</p>
<p>My questions are:</p>
<ul>
<li>How can I count the number of tokens in Python API? So that I will set max_tokens parameter accordingly.</li>
<li>Is there a way to set max_tokens to the max cap so that I won't need to count the number of prompt tokens?</li>
</ul>
","2023-03-21 17:35:10","","2023-03-21 17:50:19","2023-05-22 10:54:08","<openai-api><gpt-3><chatgpt-api>","2","0","17","11694","","2","10347145","<p>As stated in the official <a href=""https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them"" rel=""noreferrer"">OpenAI article</a>:</p>
<blockquote>
<p>To further explore tokenization, you can use our interactive <a href=""https://platform.openai.com/tokenizer"" rel=""noreferrer"">Tokenizer</a>
tool, which allows you to calculate the number of tokens and see how
text is broken into tokens. <strong>Alternatively, if you'd like to tokenize
text programmatically, use <a href=""https://github.com/openai/tiktoken"" rel=""noreferrer"">Tiktoken</a> as a fast BPE tokenizer
specifically used for OpenAI models.</strong> Other such libraries you can
explore as well include <a href=""https://huggingface.co/docs/transformers/model_doc/gpt2#gpt2tokenizerfast"" rel=""noreferrer"">transformers package</a> for Python or the
<a href=""https://www.npmjs.com/package/gpt-3-encoder"" rel=""noreferrer"">gpt-3-encoder package</a> for NodeJS.</p>
</blockquote>
<p>A tokenizer can split the text string into a list of tokens, as stated in the official <a href=""https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb"" rel=""noreferrer"">OpenAI example</a> on counting tokens with Tiktoken:</p>
<blockquote>
<p>Tiktoken is a fast open-source tokenizer by OpenAI.</p>
<p>Given a text string (e.g., <code>&quot;tiktoken is great!&quot;</code>) and an encoding
(e.g., <code>&quot;cl100k_base&quot;</code>), a tokenizer can split the text string into a
list of tokens (e.g., <code>[&quot;t&quot;, &quot;ik&quot;, &quot;token&quot;, &quot; is&quot;, &quot; great&quot;, &quot;!&quot;]</code>).</p>
<p>Splitting text strings into tokens is useful because GPT models see
text in the form of tokens. Knowing how many tokens are in a text
string can tell you:</p>
<ul>
<li>whether the string is too long for a text model to process and</li>
<li>how much an OpenAI API call costs (as usage is priced by token).</li>
</ul>
</blockquote>
<p>Tiktoken supports 3 encodings used by OpenAI models (<a href=""https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb"" rel=""noreferrer"">source</a>):</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Encoding name</th>
<th>OpenAI models</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>cl100k_base</code></td>
<td><code>gpt-4</code>, <code>gpt-3.5-turbo</code>, <code>text-embedding-ada-002</code></td>
</tr>
<tr>
<td><code>p50k_base</code></td>
<td>Codex models, <code>text-davinci-002</code>, <code>text-davinci-003</code></td>
</tr>
<tr>
<td><code>r50k_base</code> (<code>gpt2</code>)</td>
<td>GPT-3 models like <code>davinci</code></td>
</tr>
</tbody>
</table>
</div>
<p>For <code>cl100k_base</code> and <code>p50k_base</code> encodings:</p>
<ul>
<li>Python: <a href=""https://github.com/openai/tiktoken/blob/main/README.md"" rel=""noreferrer"">tiktoken</a></li>
<li>.NET / C#: <a href=""https://github.com/dmitry-brazhenko/SharpToken"" rel=""noreferrer"">SharpToken</a></li>
<li>Java: <a href=""https://github.com/knuddelsgmbh/jtokkit"" rel=""noreferrer"">jtokkit</a></li>
</ul>
<p>For <code>r50k_base</code> (<code>gpt2</code>) encodings, tokenizers are available in many languages:</p>
<ul>
<li>Python: <a href=""https://github.com/openai/tiktoken/blob/main/README.md"" rel=""noreferrer"">tiktoken</a> (or alternatively <a href=""https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2TokenizerFast"" rel=""noreferrer"">GPT2TokenizerFast</a>)</li>
<li>JavaScript: <a href=""https://www.npmjs.com/package/gpt-3-encoder"" rel=""noreferrer"">gpt-3-encoder</a></li>
<li>.NET / C#: <a href=""https://github.com/dluc/openai-tools"" rel=""noreferrer"">GPT Tokenizer</a></li>
<li>Java: <a href=""https://github.com/hyunwoongko/gpt2-tokenizer-java"" rel=""noreferrer"">gpt2-tokenizer-java</a></li>
<li>PHP: <a href=""https://github.com/CodeRevolutionPlugins/GPT-3-Encoder-PHP"" rel=""noreferrer"">GPT-3-Encoder-PHP</a></li>
</ul>
<p>Note that <code>gpt-3.5-turbo</code> and <code>gpt-4</code> use tokens in the same way as other models as stated in the official <a href=""https://platform.openai.com/docs/guides/chat/managing-tokens"" rel=""noreferrer"">OpenAI documentation</a>:</p>
<blockquote>
<p>Chat models like <code>gpt-3.5-turbo</code> and <code>gpt-4</code> use tokens in the same way as
other models, but because of their message-based formatting, it's more
difficult to count how many tokens will be used by a conversation.</p>
<p>If a conversation has too many tokens to fit within a model’s maximum
limit (e.g., more than 4096 tokens for <code>gpt-3.5-turbo</code>), you will have
to truncate, omit, or otherwise shrink your text until it fits. Beware
that if a message is removed from the messages input, the model will
lose all knowledge of it.</p>
<p>Note too that very long conversations are more likely to receive
incomplete replies. For example, a <code>gpt-3.5-turbo</code> conversation that is
4090 tokens long will have its reply cut off after just 6 tokens.</p>
</blockquote>
","2023-03-21 17:39:41","2","19"
"75882988","1","21522645","75886728","OpenAI GPT-3 API error: ""Invalid URL (POST /v1/chat/completions)""","<p>Here is my code snippet:</p>
<pre><code>const { Configuration, OpenAI, OpenAIApi } = require (&quot;openai&quot;);
const configuration = new Configuration({
    apiKey: 'MY KEY'
})

const openai = new OpenAIApi(configuration)

async function start() {
    const response = await openai.createChatCompletion({
        model:&quot;text-davinci-003&quot;,
        prompt: &quot;Write a 90 word essay about Family Guy&quot;,
        temperature: 0,
        max_tokens: 1000
    })

    console.log(response.data.choices[0].text)
}

start()
</code></pre>
<p>when I run: <code>node index</code></p>
<p>I run into this issue:</p>
<pre><code>data: {
      error: {
        message: 'Invalid URL (POST /v1/chat/completions)',
        type: 'invalid_request_error',
        param: null,
        code: null
      }
    }
  },
  isAxiosError: true,
  toJSON: [Function: toJSON]
}
</code></pre>
<p>Node.js v18.15.0</p>
<p>I've looked all over the internet and tried some solutions but nothing seems to work. Please help!</p>
<p>Usually others have some link attached to their code when I look up this problem online. Very much a beginner at this stuff so any help would be much appreciated</p>
","2023-03-29 23:32:12","","2023-03-31 07:31:22","2023-05-16 08:40:40","<node.js><openai-api><gpt-3>","2","3","3","3339","","2","10347145","<p><strong>TL;DR: Treat the <code>text-davinci-003</code> as a GPT-3 model. See the code under OPTION 1.</strong></p>
<h2>Introduction</h2>
<p>At first glance, as someone who's been using the OpenAI API for the past few months, I thought the answer was straight and simple if you read the official OpenAI documentation. Well, I read the documentation once again, and now I understand why you're confused.</p>
<h3>Confusion number 1</h3>
<p>You want to use the <code>text-davinci-003</code> model. This model is originally from the GPT-3 model family. But if you take a look at the <a href=""https://platform.openai.com/docs/models/overview"" rel=""nofollow noreferrer"">OpenAI models overview</a> and click <a href=""https://platform.openai.com/docs/models/gpt-3"" rel=""nofollow noreferrer"">GPT-3</a>, you won't find <code>text-davinci-003</code> listed as a GPT-3 model. This is unexpected.</p>
<p><a href=""https://i.stack.imgur.com/MFI3u.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MFI3u.png"" alt=""Screenshot 1"" /></a></p>
<h3>Confusion number 2</h3>
<p>Moreover, the <code>text-davinci-003</code> is listed as a <a href=""https://platform.openai.com/docs/models/gpt-3-5"" rel=""nofollow noreferrer"">GPT-3.5</a> model.</p>
<p><a href=""https://i.stack.imgur.com/CXQJd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CXQJd.png"" alt=""Screenshot 2"" /></a></p>
<h3>Confusion number 3</h3>
<p>As if this isn't confusing enough, if you take a look at the <a href=""https://platform.openai.com/docs/models/model-endpoint-compatibility"" rel=""nofollow noreferrer"">OpenAI model endpoint compatibility</a>, you'll find the <code>text-davinci-003</code> listed under the <code>/v1/completions</code> endpoint. This API endpoint is used for the GPT-3 model family.</p>
<p><a href=""https://i.stack.imgur.com/XBt3B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XBt3B.png"" alt=""Screenshot 3"" /></a></p>
<br>
<h2>Wait, what?</h2>
<p><strong>The <code>text-davinci-003</code> isn't listed as a GPT-3 model. It's listed as a GPT-3.5 model but is compatible with the GPT-3 API endpoint. This doesn't make any sense.</strong></p>
<br>
<h2>Test</h2>
<p>Either the <code>text-davinci-003</code> could be treated as a GPT-3 model or a GPT-3.5 model, or perhaps both? Let's make a test.</p>
<h5>OPTION 1: Treat the <code>text-davinci-003</code> as a GPT-3 model --&gt; IT WORKS ✓</h5>
<p>If you treat the <code>text-davinci-003</code> as a GPT-3 model, then run <code>test-1.js</code>, and the OpenAI will return the following completion:</p>
<blockquote>
<p>This is indeed a test</p>
</blockquote>
<p><strong>test-1.js</strong></p>
<pre><code>const { Configuration, OpenAIApi } = require('openai');

const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});

const openai = new OpenAIApi(configuration);

async function getCompletionFromOpenAI() {
  const completion = await openai.createCompletion({
    model: 'text-davinci-003',
    prompt: 'Say this is a test',
    max_tokens: 7,
    temperature: 0,
  });

  console.log(completion.data.choices[0].text);
}

getCompletionFromOpenAI();
</code></pre>
<h5>OPTION 2: Treat the <code>text-davinci-003</code> as a GPT-3.5 model --&gt; IT DOESN'T WORK ✗</h5>
<p>If you treat the <code>text-davinci-003</code> as a GPT-3.5 model, then run <code>test-2.js</code>, and the OpenAI will return the following error:</p>
<pre><code>data: {
  error: {
    message: 'Invalid URL (POST /v1/chat/completions)',
    type: 'invalid_request_error',
    param: null,
    code: null
  }
}
</code></pre>
<p><strong>test-2.js</strong></p>
<pre><code>const { Configuration, OpenAIApi } = require('openai');

const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});

const openai = new OpenAIApi(configuration);

async function getCompletionFromOpenAI() {
  const completion = await openai.createChatCompletion({
    model: 'text-davinci-003',
    messages: [{ role: 'user', content: 'Hello!' }],
    temperature: 0,
  });

  console.log(completion.data.choices[0].message.content);
}

getCompletionFromOpenAI();
</code></pre>
<h2>Conclusion</h2>
<p>Treat the <code>text-davinci-003</code> as a GPT-3 model. See the code under OPTION 1.</p>
","2023-03-30 09:58:07","1","4"
"75904923","1","10560942","75904953","I am getting error here torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) when I call trainer.train() function of GPT2 model","<p>I am new to NLP and I was trying gpt2 to train on my own data.</p>
<pre><code>from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments

config = GPT2Config(vocab_size=10000, n_positions=256, n_ctx=256, n_embd=512, n_layer=12, n_head=8)

model = GPT2LMHeadModel(config=config)

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
train_data = TextDataset(tokenizer=tokenizer, file_path='train.txt', block_size=256)

training_args = TrainingArguments(
    output_dir='./models',
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=4,
    save_steps=1000,
    save_total_limit=2,
    prediction_loss_only=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
    prediction_loss_only=True,
)
trainer.train()
</code></pre>
<p>this is my code, and I have checked the train data is being loaded correclty and getting converted to embeddings.</p>
<p>My train data looks like:</p>
<p>&quot;&quot;&quot;
Hello How are you doing today?
whats up MD im doing good how are you doing?
Im alright, I just took a nap. But it was one of those naps that doesnt help anything. It just makes everything worse and you question all your life choices
oh wow haha so you still feel tired huh?
Yeah
did you go to bed late?
&quot;&quot;&quot;</p>
<p>When I am calling trainer.train() function, I am getting this error IndexError: index out of range in self, in this line torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse). The problem is I can't check the values of weight, input etc as it is an internal function.</p>
<p>Please help.</p>
<p>I tried changing the parameters like batch size and per_device_train_batch_size but I am still stuck.</p>
","2023-04-01 08:01:40","","","2023-04-01 08:10:03","<python><nlp><huggingface-transformers><torch><gpt-2>","1","1","0","88","","2","21524483","<p>The error you are experiencing is most likely due to the size of the vocabulary you have set in your GPT2Config.</p>
<p>You have set the vocab_size to 10000, but the actual size of the vocabulary in the GPT-2 model is 50257. Therefore, the model is expecting input token IDs to be between 0 and 50256, but some of the token IDs in your training data are outside this range.</p>
<p>To fix this, you should set the vocab_size in your GPT2Config to 50257. Also, make sure that the tokenizer you are using is the same as the one used to tokenize your training data. If the tokenizer is different, the token IDs in your training data may not match the expected token IDs of the model.</p>
<pre><code>from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments

config = GPT2Config(vocab_size=50257, n_positions=256, n_ctx=256, n_embd=512, n_layer=12, n_head=8)

model = GPT2LMHeadModel(config=config)

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
train_text = &quot;&quot;&quot;Hello How are you doing today? whats up MD im doing good     how are you doing? Im alright, I just took a nap. But it was one of those naps that doesnt help anything. It just makes everything worse and you question all your life choices oh wow haha so you still feel tired huh? Yeah did you go to bed late?&quot;&quot;&quot;
train_data = TextDataset(tokenizer=tokenizer, file_path=None,             split_text=train_text, block_size=256)

training_args = TrainingArguments(
    output_dir='./models',
    overwrite_output_dir=True,
num_train_epochs=1,
per_device_train_batch_size=4,
save_steps=1000,
save_total_limit=2,
prediction_loss_only=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
   prediction_loss_only=True,
)
trainer.train()
</code></pre>
","2023-04-01 08:10:03","1","2"
"67372903","1","468305","75949356","Access OpenAI (json) API from R","<p>I want to access the OpenAI API with the following curl command from R:</p>
<pre><code>curl https://api.openai.com/v1/engines/davinci/completions \
-H &quot;Content-Type: application/json&quot; \
-H &quot;Authorization: Bearer YOUR_API_KEY&quot; \
-d '{&quot;prompt&quot;: &quot;This is a test&quot;, &quot;max_tokens&quot;: 5}'
</code></pre>
<p>I think the curl package (on CRAN) will be the best option(?). I have never used this package so can anyone help me getting started with this simple call?</p>
","2021-05-03 17:01:19","","2023-01-24 18:23:08","2023-04-06 12:08:29","<r><json><curl><openai-api><gpt-3>","2","8","1","1465","0","2","8163634","<p>I created an R package named &quot;openapi&quot; (<a href=""https://github.com/zhanghao-njmu/openapi"" rel=""nofollow noreferrer"">https://github.com/zhanghao-njmu/openapi</a>), which supports all OpenAI APIs and can generate streaming returns (currently, other packages do not have good solutions), chatGPT app, and various RStudio add-ins. Welcome to use it.</p>
","2023-04-06 12:08:29","0","1"
"76153016","1","21794193","76166261","gpt chatbot not working after using open ai imports and langchain","<p>hi i am trying to build a chatbot using openai's api. it's basic function is to read a pdf, txtfile , etc. and answer based on it. i was following a tutorial on <a href=""https://beebom.com/how-train-ai-chatbot-custom-knowledge-base-chatgpt-api/"" rel=""nofollow noreferrer"">https://beebom.com/how-train-ai-chatbot-custom-knowledge-base-chatgpt-api/</a></p>
<p>i used the following code and installed neccessary dependencies:</p>
<pre><code>from gpt_index import SimpleDirectoryReader, GPTListIndex, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext
from langchain.chat_models import ChatOpenAI
import gradio as gr
import sys
import os

os.environ[&quot;OPENAI_API_KEY&quot;] = 'yourapikey'

def construct_index(directory_path):
    max_input_size = 4096
    num_outputs = 512
    max_chunk_overlap = 20
    chunk_size_limit = 600

    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)

    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=&quot;gpt-3.5-turbo&quot;, max_tokens=num_outputs))

    documents = SimpleDirectoryReader(directory_path).load_data()

    index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)

    index.save_to_disk('index.json')

    return index

def chatbot(input_text):
    index = GPTSimpleVectorIndex.load_from_disk('index.json')
    response = index.query(input_text, response_mode=&quot;compact&quot;)
    return response.response

iface = gr.Interface(fn=chatbot,
                     inputs=gr.components.Textbox(lines=7, label=&quot;Enter your text&quot;),
                     outputs=&quot;text&quot;,
                     title=&quot;Custom-trained AI Chatbot&quot;)

index = construct_index(&quot;docs&quot;)
iface.launch(share=True)

</code></pre>
<p>after running the code i get the following error:</p>
<pre><code>File &quot;C:\Users\USER\desktop\cht\app.py&quot;, line 1, in &lt;module&gt;
    from gpt_index import SimpleDirectoryReader, GPTListIndex, GPTSimpleVectorIndex, LLMPredictor, PromptHelper, ServiceContext
  File &quot;C:\Users\USER\AppData\Local\Programs\Python\Python311\Lib\site-packages\gpt_index\__init__.py&quot;, line 18, in &lt;module&gt;
    from gpt_index.indices.common.struct_store.base import SQLDocumentContextBuilder
  File &quot;C:\Users\USER\AppData\Local\Programs\Python\Python311\Lib\site-packages\gpt_index\indices\__init__.py&quot;, line 4, in &lt;module&gt;
    from gpt_index.indices.keyword_table.base import GPTKeywordTableIndex
  File &quot;C:\Users\USER\AppData\Local\Programs\Python\Python311\Lib\site-packages\gpt_index\indices\keyword_table\__init__.py&quot;, line 4, in &lt;module&gt;
    from gpt_index.indices.keyword_table.base import GPTKeywordTableIndex
  File &quot;C:\Users\USER\AppData\Local\Programs\Python\Python311\Lib\site-packages\gpt_index\indices\keyword_table\base.py&quot;, line 16, in &lt;module&gt;
    from gpt_index.indices.base import DOCUMENTS_INPUT, BaseGPTIndex
  File &quot;C:\Users\USER\AppData\Local\Programs\Python\Python311\Lib\site-packages\gpt_index\indices\base.py&quot;, line 23, in &lt;module&gt;
    from gpt_index.indices.prompt_helper import PromptHelper
  File &quot;C:\Users\USER\AppData\Local\Programs\Python\Python311\Lib\site-packages\gpt_index\indices\prompt_helper.py&quot;, line 12, in &lt;module&gt;
    from gpt_index.langchain_helpers.chain_wrapper import LLMPredictor
  File &quot;C:\Users\USER\AppData\Local\Programs\Python\Python311\Lib\site-packages\gpt_index\langchain_helpers\chain_wrapper.py&quot;, line 13, in &lt;module&gt;
    from gpt_index.prompts.base import Prompt
  File &quot;C:\Users\USER\AppData\Local\Programs\Python\Python311\Lib\site-packages\gpt_index\prompts\__init__.py&quot;, line 3, in &lt;module&gt;
    from gpt_index.prompts.base import Prompt
  File &quot;C:\Users\USER\AppData\Local\Programs\Python\Python311\Lib\site-packages\gpt_index\prompts\base.py&quot;, line 9, in &lt;module&gt;
    from langchain.schema import BaseLanguageModel
ImportError: cannot import name 'BaseLanguageModel' from 'langchain.schema' (C:\Users\USER\AppData\Local\Programs\Python\Python311\Lib\site-packages\langchain\schema.py)
</code></pre>
<p>how can i fix this?</p>
","2023-05-02 08:24:15","","","2023-05-03 16:40:36","<python><window><openai-api><gpt-3><langchain>","2","4","0","3150","","2","21807871","<p>fixed this with</p>
<pre><code>pip install langchain==0.0.118
</code></pre>
<p>and</p>
<pre><code>pip install gpt_index==0.4.24
</code></pre>
<p>not ideal but got this code to work after these changes</p>
","2023-05-03 16:40:36","0","7"
"76067091","1","5832020","76320287","GPU out of memory fine tune flan-ul2","<blockquote>
<p>OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB
(GPU 0; 15.78 GiB total capacity; 14.99 GiB already allocated; 3.50
MiB free; 14.99 GiB reserved in total by PyTorch) If reserved memory
is &gt;&gt; allocated memory try setting max_split_size_mb to avoid
fragmentation.  See documentation for Memory Management and
PYTORCH_CUDA_ALLOC_CONF</p>
</blockquote>
<p>I have Standard_NC24s_v3 single node GPU with 448GB memory and 4 GPUs. However the error message says the total capacity is 15.78GiB. Is the fine tune not using 4 GPUs? How to get all the 4 GPUs used in the fine tune of Flan-UL2 using huggingface transformers?</p>
","2023-04-20 18:13:02","","2023-04-20 18:33:12","2023-05-24 05:23:00","<gpu><huggingface-transformers><huggingface-tokenizers><gpt-3><fine-tune>","1","2","1","224","","2","5832020","<p>I solve the issue by using the following package versions.</p>
<pre><code>!pip install transformers==4.28.1
!pip install sentencepiece==0.1.97
!pip install accelerate==0.18.0
!pip install bitsandbytes==0.37.2
!pip install torch==1.13.1
</code></pre>
","2023-05-24 05:23:00","0","0"
"75882872","1","12226377","76323834","How to overcome Rate limit error while working with GPT3 Models using Tenacity","<p>In my situation I am trying to pass a prompt using a helper function to the actual GPT3 models, in my case text-ada-001 and then eventually applying it on a pandas column using the following code. but I am recovering the following error:</p>
<pre class=""lang-py prettyprint-override""><code>    def sentiment_prompt(text):
    return &quot;&quot;&quot;Is the sentiment Positive, Negative or Neutral for the following text:
    
    &quot;{}&quot;
    &quot;&quot;&quot;.format(text)
    def sentiment_text(text):
        response = openai.Completion.create(
           engine=&quot;text-ada-001&quot;,
           prompt=sentiment_prompt(text),
           max_tokens=1000,
           temperature=0,
           top_p=1,
           frequency_penalty=0,
           presence_penalty=0
    )
    sentiment = response.choices[0].text
    return sentiment
</code></pre>
<p>and then eventually applying to my pandas column:</p>
<pre><code>    df['sentiment'] = df['text'].apply(lambda x :sentiment_text(x))
</code></pre>
<p><strong>And the error;</strong></p>
<pre><code>    RateLimitError: Rate limit reached for default-global-with-image-limits in organization org-XXXX on requests per min. Limit: 60 / min. Please try again in 1s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method.
</code></pre>
<p>To overcome this error I was looking into this <a href=""https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb"" rel=""nofollow noreferrer"">link</a> and found that tenacity could help resolve my issue. But I am not sure how to structure my code. I am doing the following at the moment</p>
<p>How do I use the code suggested in the link to overcome the Rate Limit error?</p>
","2023-03-29 23:04:29","","2023-04-01 07:42:04","2023-05-28 11:16:01","<pandas><openai-api><gpt-3><tenacity>","1","1","0","229","","2","8789910","<p>Import tenacity at the beginning of your code and then add its decoration where you are calling the OpenAI library with create. So your code would look like this:</p>
<pre><code>from tenacity import (
    retry,
    stop_after_attempt,
    wait_random_exponential,
) 

@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))
def sentiment_text(text):
        your_prompt = &quot;&quot;&quot;Is the sentiment Positive, Negative or Neutral for the 
                         following text:
    
                         &quot;{}&quot;
                      &quot;&quot;&quot;.format(text)
        response = openai.Completion.create(
           engine=&quot;text-ada-001&quot;,
           prompt=your_prompt ,
           max_tokens=1000,
           temperature=0,
           top_p=1,
           frequency_penalty=0,
           presence_penalty=0
        )
        sentiment = response.choices[0].text
        return sentiment
</code></pre>
","2023-05-24 13:00:31","0","1"
"76363168","1","19107509","76371360","OpenAI API: How do I handle errors in Python?","<p>I tried using the below code, but the OpenAI API doesn't have the <code>AuthenticationError</code> method in the library. How can I effectively handle such error.</p>
<pre><code>import openai

# Set up your OpenAI credentials
openai.api_key = 'YOUR_API_KEY'

try:
    # Perform OpenAI API request
    response = openai.some_function()  # Replace with the appropriate OpenAI API function

    # Process the response
    # ...
except openai.AuthenticationError:
    # Handle the AuthenticationError
    print(&quot;Authentication error: Invalid API key or insufficient permissions.&quot;)
    # Perform any necessary actions, such as displaying an error message or exiting the program

</code></pre>
","2023-05-30 08:54:55","","2023-06-02 13:55:45","2023-06-02 14:02:24","<python><openai-api><gpt-3><chatgpt-api><gpt-4>","1","1","1","195","","2","10347145","<p><strong>Your code isn't correct.</strong></p>
<p>Change this...</p>
<pre><code>except openai.AuthenticationError
</code></pre>
<p>...to this.</p>
<pre><code>except openai.error.AuthenticationError
</code></pre>
<p>Try the following, as shown in the official <a href=""https://help.openai.com/en/articles/6897213-openai-library-error-types-guidance"" rel=""nofollow noreferrer"">OpenAI article</a>:</p>
<pre><code>try:
  #Make your OpenAI API request here
  response = openai.Completion.create(model = &quot;text-davinci-003&quot;, prompt = &quot;Hello world&quot;)
except openai.error.Timeout as e:
  #Handle timeout error, e.g. retry or log
  print(f&quot;OpenAI API request timed out: {e}&quot;)
  pass
except openai.error.APIError as e:
  #Handle API error, e.g. retry or log
  print(f&quot;OpenAI API returned an API Error: {e}&quot;)
  pass
except openai.error.APIConnectionError as e:
  #Handle connection error, e.g. check network or log
  print(f&quot;OpenAI API request failed to connect: {e}&quot;)
  pass
except openai.error.InvalidRequestError as e:
  #Handle invalid request error, e.g. validate parameters or log
  print(f&quot;OpenAI API request was invalid: {e}&quot;)
  pass
except openai.error.AuthenticationError as e:
  #Handle authentication error, e.g. check credentials or log
  print(f&quot;OpenAI API request was not authorized: {e}&quot;)
  pass
except openai.error.PermissionError as e:
  #Handle permission error, e.g. check scope or log
  print(f&quot;OpenAI API request was not permitted: {e}&quot;)
  pass
except openai.error.RateLimitError as e:
  #Handle rate limit error, e.g. wait or log
  print(f&quot;OpenAI API request exceeded rate limit: {e}&quot;)
  pass
</code></pre>
","2023-05-31 08:00:20","0","0"
"76398258","1","18217301","76398870","OpenAI GPT-3.5 ""prompt"" argument not working","<p>I am trying to make a flutter app with the openAI api that works like a chatbot, and I want to add a prompt so that the responses are more specialized, like in the openAI playground on their website.</p>
<p>I am testing the API post function with postman and it worked perfectly fine for me before I tried adding a prompt. I assumed that to add a prompt you just have to add a &quot;prompt&quot;: line in the body like when you work with the text-davinci model, but when I do that I get this message returned:</p>
<pre><code>{
    &quot;error&quot;: {
        &quot;message&quot;: &quot;Unrecognized request argument supplied: prompt&quot;,
        &quot;type&quot;: &quot;invalid_request_error&quot;,
        &quot;param&quot;: null,
        &quot;code&quot;: null
    }
}
</code></pre>
<p>Is there a different way you need to do this with the gpt models, or does the prompt argument just not exist for them?</p>
","2023-06-03 22:26:20","","2023-06-04 22:14:05","2023-06-04 22:14:05","<http><openai-api><gpt-3>","1","0","0","72","","2","23235","<p>The OPEN AI 3.5-turbo model only supports the newer <a href=""https://platform.openai.com/docs/api-reference/chat/create"" rel=""nofollow noreferrer"">chat completion API</a> which does not have a 'prompt' json body field.</p>
<p>I would assume you are using the older <a href=""https://platform.openai.com/docs/api-reference/completions/create"" rel=""nofollow noreferrer"">competion API</a> json body format aginst the newer chat completion API endpoint and that is the reason for your error.</p>
<p>--- update ---</p>
<p>I can reproduce your exact error response with:</p>
<blockquote>
<p>curl <a href=""https://api.openai.com/v1/chat/completions"" rel=""nofollow noreferrer"">https://api.openai.com/v1/chat/completions</a> -H &quot;Content-Type:
application/json&quot;  -H &quot;Authorization: Bearer $OPENAI_API_KEY&quot;  -d '{
&quot;model&quot;: &quot;gpt-3.5-turbo&quot;, &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;:
&quot;Hello!&quot;}], &quot;prompt&quot;: &quot;testing&quot; }'</p>
</blockquote>
<p>This is just a modified example from the api link above. Removing the added &quot;prompt&quot; field from the json makes it work fine.</p>
<p>So my advice in the comment stands.  Figure out what you are sending in the body of your http GET request.  It will have a &quot;prompt&quot; field, remove it and any other field that should not be there and it should work.</p>
","2023-06-04 03:55:30","4","0"
"76411359","1","22027390","76412710","OpenAI API: How do I migrate from text-davinci-003 to gpt-3.5-turbo in NodeJS?","<p>How do I migrate from <code>text-davinci-003</code> to <code>gpt-3.5-turbo</code>?</p>
<p>What I tried to do is the following:</p>
<p>Changing this...</p>
<pre><code>model: &quot;text-davinci-003&quot;
</code></pre>
<p>...to this.</p>
<pre><code>model: &quot;gpt-3.5-turbo&quot;
</code></pre>
<p>Also, changing this...</p>
<pre><code>const API_URL = &quot;https://api.openai.com/v1/completions&quot;;
</code></pre>
<p>...to this.</p>
<pre><code>const API_URL = &quot;https://api.openai.com/v1/chat/completions&quot;;
</code></pre>
<p>The Problem is that it does not work. The code I will be giving is the unmodified code, so that anyone can help me what to change.</p>
<p>Why I wanted this upgrade?
I was irritated by <code>text-davinci-003</code>'s completion. Like sending &quot;Hello&quot; gives me an entire letter not a greeting.</p>
<p>Live Sample (Via Github Pages):
<a href=""https://thedoggybrad.github.io/chat/chatsystem"" rel=""nofollow noreferrer"">https://thedoggybrad.github.io/chat/chatsystem</a></p>
<p>Github Repository:
<a href=""https://github.com/thedoggybrad/chat/tree/main/chatsystem"" rel=""nofollow noreferrer"">https://github.com/thedoggybrad/chat/tree/main/chatsystem</a></p>
","2023-06-06 03:56:25","","2023-06-07 10:50:21","2023-06-07 10:50:21","<openai-api><gpt-3><chatgpt-api><text-davinci-003>","1","3","0","149","","2","10347145","<p>You want to use the <code>gpt-3.5-turbo</code> model.</p>
<p>There are three main differences between the ChatGPT API (i.e., the GPT-3.5 API) and the GPT-3 API:</p>
<ol>
<li>API endpoint
<ul>
<li>GPT-3 API: <code>https://api.openai.com/v1/completions</code></li>
<li>ChatGPT API: <code>https://api.openai.com/v1/chat/completions</code></li>
</ul>
</li>
<li>The <code>prompt</code> parameter (GPT-3 API) is replaced by the <code>messages</code> parameter (ChatGPT API)</li>
<li>Response access
<ul>
<li>GPT-3 API: <code>response.choices[0].text.trim()</code></li>
<li>ChatGPT API: <code>response.choices[0].message.content.trim()</code></li>
</ul>
</li>
</ol>
<p>Try this:</p>
<pre><code>const getChatResponse = async (incomingChatDiv) =&gt; {
    const API_URL = &quot;https://api.openai.com/v1/chat/completions&quot;; /* Changed */
    const pElement = document.createElement(&quot;p&quot;);

    // Define the properties and data for the API request
    const requestOptions = {
        method: &quot;POST&quot;,
        headers: {
            &quot;Content-Type&quot;: &quot;application/json&quot;,
            &quot;Authorization&quot;: `Bearer ${API_KEY}`
        },
        body: JSON.stringify({
            model: &quot;gpt-3.5-turbo&quot;,
            messages: [{role: &quot;user&quot;, content: `${userText}`}], /* Changed */
            max_tokens: 2048,
            temperature: 0.2,
            n: 1,
            stop: null
        })
    }

    // Send POST request to API, get response and set the reponse as paragraph element text
    try {
        const response = await (await fetch(API_URL, requestOptions)).json();
        pElement.textContent = response.choices[0].message.content.trim(); /* Changed */
    } catch (error) { // Add error class to the paragraph element and set error text
        pElement.classList.add(&quot;error&quot;);
        pElement.textContent = &quot;Oops! Something went wrong while retrieving the response. Please try again.&quot;;
    }

    // Remove the typing animation, append the paragraph element and save the chats to local storage
    incomingChatDiv.querySelector(&quot;.typing-animation&quot;).remove();
    incomingChatDiv.querySelector(&quot;.chat-details&quot;).appendChild(pElement);
    localStorage.setItem(&quot;all-chats-thedoggybrad&quot;, chatContainer.innerHTML);
    chatContainer.scrollTo(0, chatContainer.scrollHeight);
}
</code></pre>
","2023-06-06 08:22:50","0","0"
"66276186","1","1793799","66278433","HuggingFace - GPT2 Tokenizer configuration in config.json","<p>The GPT2 finetuned model is uploaded in <a href=""https://huggingface.co/models"" rel=""nofollow noreferrer"">huggingface-models</a> for the inferencing</p>
<p>Below error is observed during the inference,</p>
<p><strong>Can't load tokenizer using from_pretrained, please update its configuration: Can't load tokenizer for 'bala1802/model_1_test'. Make sure that: - 'bala1802/model_1_test' is a correct model identifier listed on 'https://huggingface.co/models' - or 'bala1802/model_1_test' is the correct path to a directory containing relevant tokenizer files</strong></p>
<p>Below is the configuration - config.json file for the Finetuned huggingface model,</p>
<pre><code>{
  &quot;_name_or_path&quot;: &quot;gpt2&quot;,
  &quot;activation_function&quot;: &quot;gelu_new&quot;,
  &quot;architectures&quot;: [
    &quot;GPT2LMHeadModel&quot;
  ],
  &quot;attn_pdrop&quot;: 0.1,
  &quot;bos_token_id&quot;: 50256,
  &quot;embd_pdrop&quot;: 0.1,
  &quot;eos_token_id&quot;: 50256,
  &quot;gradient_checkpointing&quot;: false,
  &quot;initializer_range&quot;: 0.02,
  &quot;layer_norm_epsilon&quot;: 1e-05,
  &quot;model_type&quot;: &quot;gpt2&quot;,
  &quot;n_ctx&quot;: 1024,
  &quot;n_embd&quot;: 768,
  &quot;n_head&quot;: 12,
  &quot;n_inner&quot;: null,
  &quot;n_layer&quot;: 12,
  &quot;n_positions&quot;: 1024,
  &quot;resid_pdrop&quot;: 0.1,
  &quot;summary_activation&quot;: null,
  &quot;summary_first_dropout&quot;: 0.1,
  &quot;summary_proj_to_labels&quot;: true,
  &quot;summary_type&quot;: &quot;cls_index&quot;,
  &quot;summary_use_proj&quot;: true,
  &quot;task_specific_params&quot;: {
    &quot;text-generation&quot;: {
      &quot;do_sample&quot;: true,
      &quot;max_length&quot;: 50
    }
  },
  &quot;transformers_version&quot;: &quot;4.3.2&quot;,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 50257
}
</code></pre>
<p>Should I configure the GPT2 Tokenizer just like the <code>&quot;model_type&quot;: &quot;gpt2&quot;</code> in the config.json file</p>
","2021-02-19 10:53:15","","","2021-02-19 13:25:37","<pytorch><huggingface-transformers><language-model><huggingface-tokenizers><gpt-2>","1","0","1","2211","","2","6664872","<p>Your repository does not contain the required files to create a tokenizer. It seems like you have only uploaded the files for your model. Create an object of your tokenizer that you have used for training the model and save the required files with <a href=""https://huggingface.co/transformers/internal/tokenization_utils.html?highlight=save_pretrained#transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained"" rel=""nofollow noreferrer"">save_pretrained()</a>:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import GPT2Tokenizer

t = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
t.save_pretrained('/SOMEFOLDER/')
</code></pre>
<p>Output:</p>
<pre><code>('/SOMEFOLDER/tokenizer_config.json',
 '/SOMEFOLDER/special_tokens_map.json',
 '/SOMEFOLDER/vocab.json',
 '/SOMEFOLDER/merges.txt',
 '/SOMEFOLDER/added_tokens.json')
</code></pre>
","2021-02-19 13:25:37","0","1"
"66901602","1","6010395","66905568","What is tokenizer.max len doing in this class definition?","<p>I am following Rostylav's tutorial found <a href=""https://colab.research.google.com/drive/15wa925dj7jvdvrz8_z3vU7btqAFQLVlG#scrollTo=7KrNfVNueNhR"" rel=""nofollow noreferrer"">here</a> and am runnning into an error I dont quite understand:</p>
<pre><code>AttributeError                            
Traceback (most recent call last)
&lt;ipython-input-22-523c0d2a27d3&gt; in &lt;module&gt;()
----&gt; 1 main(trn_df, val_df)

&lt;ipython-input-20-1f17c050b9e5&gt; in main(df_trn, df_val)
     59     # Training
     60     if args.do_train:
---&gt; 61         train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)
     62 
     63         global_step, tr_loss = train(args, train_dataset, model, tokenizer)

&lt;ipython-input-18-3c4f1599e14e&gt; in load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate)
     40 
     41 def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):
---&gt; 42     return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)
     43 
     44 def set_seed(args):

&lt;ipython-input-18-3c4f1599e14e&gt; in __init__(self, tokenizer, args, df, block_size)
      8     def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):
      9 
---&gt; 10         block_size = block_size - (tokenizer.max_len - tokenizer.max_len_single_sentence)
     11 
     12         directory = args.cache_dir

AttributeError: 'GPT2TokenizerFast' object has no attribute 'max_len'
</code></pre>
<p>This is the class I believe is causing the error, however I am not able to understand what Tokenize.max_len is supposed to do so I can try to fix it:</p>
<pre><code>   class ConversationDataset(Dataset):

    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):

        block_size = block_size - (tokenizer.max_len - tokenizer.max_len_single_sentence)

        directory = args.cache_dir
        cached_features_file = os.path.join(
            directory, args.model_type + &quot;_cached_lm_&quot; + str(block_size)
        )

        if os.path.exists(cached_features_file) and not args.overwrite_cache:
            logger.info(&quot;Loading features from cached file %s&quot;, cached_features_file)
            with open(cached_features_file, &quot;rb&quot;) as handle:
                self.examples = pickle.load(handle)
        else:
            logger.info(&quot;Creating features from dataset file at %s&quot;, directory)

            self.examples = []
            for _, row in df.iterrows():
                conv = construct_conv(row, tokenizer)
                self.examples.append(conv)

            logger.info(&quot;Saving features into cached file %s&quot;, cached_features_file)
            with open(cached_features_file, &quot;wb&quot;) as handle:
                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, item):
        return torch.tensor(self.examples[item], dtype=torch.long)
 
# Cacheing and storing of data/checkpoints

def load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):
    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)
</code></pre>
<p>Thank you for reading!</p>
","2021-04-01 09:07:34","","","2021-04-01 13:38:17","<python><google-colaboratory><huggingface-transformers><huggingface-tokenizers><gpt-2>","1","0","1","888","","2","6664872","<p>The attribute <code>max_len</code> was <a href=""https://huggingface.co/transformers/migration.html?highlight=max_len#removed-some-deprecated-attributes"" rel=""nofollow noreferrer"">migrated</a> to <code>model_max_length</code>. It represents the maximum number of tokens a model can handle (i.e. including special tokens) (<a href=""https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=model_max_length#transformers.PreTrainedTokenizerFast"" rel=""nofollow noreferrer"">documentation</a>).</p>
<p><code>max_len_single_sentence</code> on the other side represents the maximum number of tokens a single sentence can have (i.e. without special tokens) (<a href=""https://huggingface.co/transformers/internal/tokenization_utils.html?highlight=max_len_single_sentence#transformers.tokenization_utils_base.PreTrainedTokenizerBase.max_len_single_sentence"" rel=""nofollow noreferrer"">documentation</a>).</p>
","2021-04-01 13:38:17","1","1"
"67325687","1","9693537","67653823","How to save pre-trained API on GPT-3?","<p>I have a question about GPT-3. As we know we can give some examples to the network and &quot;adjust&quot; the model.</p>
<ol>
<li>Show examples to the model.</li>
<li>Save these examples.</li>
<li>Reuse the APIs.</li>
</ol>
<hr/>
<pre><code>import openai

class Example():
    &quot;&quot;&quot;Stores an input, output pair and formats it to prime the model.&quot;&quot;&quot;
def __init__(self, inp, out):
    self.input = inp
    self.output = out

def get_input(self):
    &quot;&quot;&quot;Returns the input of the example.&quot;&quot;&quot;
    return self.input

def get_output(self):
    &quot;&quot;&quot;Returns the intended output of the example.&quot;&quot;&quot;
    return self.output

def format(self):
    &quot;&quot;&quot;Formats the input, output pair.&quot;&quot;&quot;
    return f&quot;input: {self.input}\noutput: {self.output}\n&quot;


class GPT:
    &quot;&quot;&quot;The main class for a user to interface with the OpenAI API.
    A user can add examples and set parameters of the API request.&quot;&quot;&quot;
def __init__(self, engine='davinci',
             temperature=0.5,
             max_tokens=100):
    self.examples = []
    self.engine = engine
    self.temperature = temperature
    self.max_tokens = max_tokens

def add_example(self, ex):
    &quot;&quot;&quot;Adds an example to the object. Example must be an instance
    of the Example class.&quot;&quot;&quot;
    assert isinstance(ex, Example), &quot;Please create an Example object.&quot;
    self.examples.append(ex.format())
</code></pre>
<p>Now when I use &quot;give&quot; examples to the model I have the following code:</p>
<pre><code>gpt2 = GPT(engine=&quot;davinci&quot;, temperature=0.5, max_tokens=100)
gpt2.add_example(Example('Two plus two equals four', '2 + 2 = 4'))
gpt2.add_example(Example('The integral from zero to infinity', '\\int_0^{\\infty}'))

prompt1 = &quot;x squared plus y squared plus equals z squared&quot;
output1 = gpt2.submit_request(prompt1)
</code></pre>
<p>However, I am not able to save this &quot;pre-trained&quot; API. Every time I have to retrain it - is there any way to reuse it?</p>
","2021-04-29 22:07:10","","2021-05-22 19:39:56","2023-01-12 01:55:51","<python><gpt-3>","2","0","5","637","","2","14852784","<blockquote>
<p>Every time I have to retrain it - is there any way to reuse it?</p>
</blockquote>
<p>No, there isn't any way to reuse it. You are mixing up the terms: You don't need to train GPT-3, you need to pass in examples to the prompt. As you don't have any kind of container in which you could store previous results (and thus &quot;train&quot; your model), it's required to pass examples including your task each and every time.</p>
<p>To perfect the engineering process (and therefore reduce the cost per request) is a difficult process and will take a long time with trial and error.</p>
<p>Though let's be honest: Even with passing the examples every time, GPT-3 is extremely cost efficient. Depending on your specific situation, you (on average) only spend a few hundred tokens for a complex completion with Davinci.</p>
","2021-05-22 20:19:07","3","2"
"67707374","1","15365513","67845671","How can text completion using the GPT-2 language model generate a full URL?","<p>I found <a href=""https://bellard.org/textsynth"" rel=""nofollow noreferrer"">this</a> auto text completion on Mr Fabrice Bellard's website. Then I ask like in the picture:
<a href=""https://i.stack.imgur.com/Pdxwf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Pdxwf.png"" alt=""What I asked the machine"" /></a></p>
<p>So my question is: Is the respond text is generated randomly or somehow controlled by the text I typed (and if it's controlled by the text I typed, why it isn't related to the question I typed?)? And how's possible for it to generate a link to a web page (which cannot be accessed by me)?</p>
<p>I'm new to AI and neutral networks (and that sort of thing), so forgive me if this is a stupid question (because I'm so curious about it).</p>
","2021-05-26 14:49:19","","","2021-06-05 02:05:49","<artificial-intelligence><gpt-2>","1","1","0","477","0","2","2444877","<p>Gtp-2 was trained on massive amounts of text all around the internet and is able to generate text by predicting the next word in a sequence of tokens. In theory, the content generated should be driven by the input you provide. Beware that the URLs generated are not real, the model is inventing them.</p>
<p>You might also to check GPT-3 as it does a much better job at generating text following the context of the input.</p>
","2021-06-05 02:05:49","0","1"
"68946827","1","364966","68961549","Spacy-Transformers: Access GPT-2?","<p>I'm using Spacy-Transformers to build some NLP models.</p>
<p>The <a href=""https://spacy.io/universe/project/spacy-transformers#gatsby-noscript"" rel=""nofollow noreferrer"">Spacy-Transformers docs</a> say:</p>
<blockquote>
<p><strong>spacy-transformers</strong></p>
<p><em>spaCy pipelines for pretrained BERT, XLNet and GPT-2</em></p>
</blockquote>
<p>The sample code on that page shows:</p>
<pre><code>import spacy

nlp = spacy.load(&quot;en_core_web_trf&quot;)
doc = nlp(&quot;Apple shares rose on the news. Apple pie is delicious.&quot;)
</code></pre>
<p>Based on what I've learned from <a href=""https://www.youtube.com/watch?v=vyOgWhwUmec"" rel=""nofollow noreferrer"">this video</a>,&quot;en_core_web_trf&quot; appears to be the <code>spacy.load()</code> package to use a BERT model. I've searched the <a href=""https://spacy.io/universe/project/spacy-transformers#gatsby-noscript"" rel=""nofollow noreferrer"">Spacy-Transformers docs</a> and haven't yet seen an equivalent package, to access GPT-2. Is there a specific <code>spacy.load()</code> package, to load in order to use a GPT-2 model?</p>
","2021-08-27 00:48:37","","","2021-08-28 05:16:12","<machine-learning><nlp><spacy><gpt-2>","1","0","1","429","","2","355715","<p>The <code>en_core_web_trf</code> uses a specific Transformers model, but you can specify arbitrary ones using the <code>TransformerModel</code> wrapper class from <code>spacy-transformers</code>. See <a href=""https://spacy.io/api/architectures#TransformerModel"" rel=""nofollow noreferrer"">the docs</a> for that. An example config:</p>
<pre><code>[model]
@architectures = &quot;spacy-transformers.TransformerModel.v1&quot;
name = &quot;roberta-base&quot; # this can be the name of any hub model
tokenizer_config = {&quot;use_fast&quot;: true}
</code></pre>
","2021-08-28 05:16:12","2","1"
"69182644","1","16814329","69183100","GPT 2 - TypeError: Cannot cast array data from dtype('O') to dtype('int64') according to the rule 'safe'","<p>I am working with gpt2, python 3.9 and tensorflow 2.5 and when connecting to flask (flask run in terminal) I get a following message:</p>
<blockquote>
<p>TypeError: Cannot cast array data from dtype('O') to dtype('int64') according to the rule 'safe'</p>
</blockquote>
<p>Here is the code in generator.py</p>
<pre><code>#!/usr/bin/env python3

import fire
import json
import os
import numpy as np
import tensorflow.compat.v1 as tf

# import model, sample, encoder
  from text_generator import model
  from text_generator import sample
  from text_generator import encoder


class AI:
 def generate_text(self, input_text):
    model_name = '117M_Trained'
    seed = None,
    nsamples = 1
    batch_size = 1
    length = 150
    temperature = 1
    top_k = 40
    top_p = 1
    models_dir = 'models'
    self.response = ''

    models_dir = os.path.expanduser(os.path.expandvars(models_dir))
    if batch_size is None:
        batch_size = 1
    assert nsamples % batch_size == 0

    enc = encoder.get_encoder(model_name, models_dir)
    hparams = model.default_hparams()
    cur_path = os.path.dirname(__file__) + '/models' + '/' + model_name
    with open(cur_path + '/hparams.json') as f:
        hparams.override_from_dict(json.load(f))

    if length is None:
        length = hparams.n_ctx // 2
    elif length &gt; hparams.n_ctx:
        raise ValueError(&quot;Can't get samples longer than window size: %s&quot; % hparams.n_ctx)

    with tf.Session(graph=tf.Graph()) as sess:
        context = tf.placeholder(tf.int32, [batch_size, None])
        np.random.seed(seed)
        tf.set_random_seed(seed)
        output = sample.sample_sequence(
            hparams=hparams, length=length,
            context=context,
            batch_size=batch_size,
            temperature=temperature, top_k=top_k, top_p=top_p
        )

        saver = tf.train.Saver()
        ckpt = tf.train.latest_checkpoint(cur_path)
        saver.restore(sess, ckpt)

        context_tokens = enc.encode(input_text)
        generated = 0
        for _ in range(nsamples // batch_size):
            out = sess.run(output, feed_dict={
                context: [context_tokens for _ in range(batch_size)]
            })[:, len(context_tokens):]
            for i in range(batch_size):
                generated += 1
                text = enc.decode(out[i])
                self.response = text

    return self.response


  ai = AI()
  text = ai.generate_text('How are you?')
  print(text)
</code></pre>
<p>Any help is appreciated 🙏 ps I have also added below the entire traceback</p>
<pre><code>     * Serving Flask app 'text_generator' (lazy loading)
 * Environment: development
 * Debug mode: on
2021-09-14 19:58:08.687907: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Traceback (most recent call last):
  File &quot;_mt19937.pyx&quot;, line 178, in numpy.random._mt19937.MT19937._legacy_seeding
TypeError: 'tuple' object cannot be interpreted as an integer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/Users/dusandev/miniconda3/bin/flask&quot;, line 8, in &lt;module&gt;
    sys.exit(main())
  File &quot;/Users/dusandev/miniconda3/lib/python3.9/site-packages/flask/cli.py&quot;, line 990, in main
    cli.main(args=sys.argv[1:])
  File &quot;/Users/dusandev/miniconda3/lib/python3.9/site-packages/flask/cli.py&quot;, line 596, in main
    return super().main(*args, **kwargs)
  File &quot;/Users/dusandev/miniconda3/lib/python3.9/site-packages/click/core.py&quot;, line 1062, in main
    rv = self.invoke(ctx)
  File &quot;/Users/dusandev/miniconda3/lib/python3.9/site-packages/click/core.py&quot;, line 1668, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File &quot;/Users/dusandev/miniconda3/lib/python3.9/site-packages/click/core.py&quot;, line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File &quot;/Users/dusandev/miniconda3/lib/python3.9/site-packages/click/core.py&quot;, line 763, in invoke
    return __callback(*args, **kwargs)
  File &quot;/Users/dusandev/miniconda3/lib/python3.9/site-packages/click/decorators.py&quot;, line 84, in new_func
    return ctx.invoke(f, obj, *args, **kwargs)
  File &quot;/Users/dusandev/miniconda3/lib/python3.9/site-packages/click/core.py&quot;, line 763, in invoke
    return __callback(*args, **kwargs)
  File &quot;/Users/dusandev/miniconda3/lib/python3.9/site-packages/flask/cli.py&quot;, line 845, in run_command
    app = DispatchingApp(info.load_app, use_eager_loading=eager_loading)
  File &quot;/Users/dusandev/miniconda3/lib/python3.9/site-packages/flask/cli.py&quot;, line 321, in __init__
    self._load_unlocked()
  File &quot;/Users/dusandev/miniconda3/lib/python3.9/site-packages/flask/cli.py&quot;, line 346, in _load_unlocked
    self._app = rv = self.loader()
  File &quot;/Users/dusandev/miniconda3/lib/python3.9/site-packages/flask/cli.py&quot;, line 402, in load_app
    app = locate_app(self, import_name, name)
  File &quot;/Users/dusandev/miniconda3/lib/python3.9/site-packages/flask/cli.py&quot;, line 256, in locate_app
    __import__(module_name)
  File &quot;/Users/dusandev/Desktop/AI/text_generator/__init__.py&quot;, line 2, in &lt;module&gt;
    from .routes import generator
  File &quot;/Users/dusandev/Desktop/AI/text_generator/routes.py&quot;, line 2, in &lt;module&gt;
    from .generator import ai
  File &quot;/Users/dusandev/Desktop/AI/text_generator/generator.py&quot;, line 74, in &lt;module&gt;
    text = ai.generate_text('How are you?')
  File &quot;/Users/dusandev/Desktop/AI/text_generator/generator.py&quot;, line 46, in generate_text
    np.random.seed(seed)
  File &quot;mtrand.pyx&quot;, line 244, in numpy.random.mtrand.RandomState.seed
  File &quot;_mt19937.pyx&quot;, line 166, in numpy.random._mt19937.MT19937._legacy_seeding
  File &quot;_mt19937.pyx&quot;, line 186, in numpy.random._mt19937.MT19937._legacy_seeding
TypeError: Cannot cast array data from dtype('O') to dtype('int64') according to the rule 'safe'
</code></pre>
","2021-09-14 18:08:06","","2021-09-14 18:32:38","2021-09-14 18:44:59","<python><gpt-2>","1","1","0","398","","2","10199456","<p>The problem is the line <code>None,</code> in your code. This is causing the tuple <code>(None,)</code> as the input to the <code>np.random.seed(seed)</code>. It accepts integer, but you are sending the tuple.</p>
","2021-09-14 18:44:59","1","0"
"70531364","1","14879655","70710672","Structuring dataset for OpenAI's GPT-3 fine tuning","<p>The <a href=""https://beta.openai.com/docs/guides/fine-tuning"" rel=""nofollow noreferrer"">fine tuning</a> endpoint for OpenAI's API seems to be fairly new, and I can't find many examples of fine tuning datasets online.</p>
<p>I'm in charge of a voicebot, and I'm testing out the performance of GPT-3 for general open-conversation questions. I'd like to <a href=""https://beta.openai.com/docs/api-reference/fine-tunes"" rel=""nofollow noreferrer"">train</a> the model on the &quot;fixed&quot; intent-response pairs we're currently using: this would probably end up performing better in terms of company voice and style.</p>
<p>I have ready a long JSON file of data extracted from our current conversational engine, which matches user input to <strong>intents</strong> and returns the specified response. I'd like to train a GPT-3 model on this data.</p>
<p>As of now, for some quick testing, I've set up my calls to the API just like they <a href=""https://beta.openai.com/examples/default-chat"" rel=""nofollow noreferrer"">suggest</a>. I have a &quot;fixed&quot; intro text in the form</p>
<pre><code>&lt;name&gt; is &lt;company&gt;'s voicebot. he is kind and professional...

This is a conversation between &lt;name&gt; and a customer:
</code></pre>
<p>which is pre-pended to each query, and then a small python class which keeps track of the context which starts with</p>
<pre><code>User: &lt;request the user provides&gt;
Bot:
</code></pre>
<p>then with each turn the api's response is appended, this way I'm keeping track of what is said. After a few questions, the query or prompt string i'm sending looks like this:</p>
<pre><code>&lt;name&gt; is &lt;company&gt;'s voicebot. he is kind and professional...

This is a conversation between &lt;name&gt; and a user:

User: &lt;request&gt;
Bot: &lt;response&gt;
User: &lt;request&gt;
Bot: &lt;response&gt;
... and so on
Bot:
</code></pre>
<p>My question is, do i have to provide the same &quot;format&quot; for my training data? Is it advisable?
The <a href=""https://beta.openai.com/docs/guides/fine-tuning/prepare-training-data"" rel=""nofollow noreferrer"">docs</a> indicate that the training set should be in this format:</p>
<pre><code>{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}
{&quot;prompt&quot;: &quot;&lt;prompt text&gt;&quot;, &quot;completion&quot;: &quot;&lt;ideal generated text&gt;&quot;}
...
</code></pre>
<p>But does the prompt need to include my intro text (the description) each time or do i simply provide a series of user/bot exchanges with a <code>Bot:</code> in the end and in the completion the answer i'd expect?
What would be a <em>best practice</em> in this case?
My fear is that if i wanted to slightly change the intro prompt a month from now I'd have to retrain the whole thing again because each response was trained with that specific block of text prepended.</p>
","2021-12-30 12:00:37","","2022-01-10 11:45:53","2022-01-14 12:37:01","<python><machine-learning><training-data><openai-api><gpt-3>","1","0","3","2278","0","2","14879655","<p>I contacted OpenAI's support and they were extremely helpful: I'll leave their answer here.</p>
<blockquote>
<p>the prompt does not need the fixed intro every time. Instead, you'll just want to provide at least a few hundred prompt-completion pairs of user/bot exchanges.
We have a sample of a chatbot fine-tuning dataset <a href=""https://beta.openai.com/docs/guides/fine-tuning/case-study-customer-support-chatbot"" rel=""nofollow noreferrer"">here</a>.</p>
</blockquote>
","2022-01-14 12:37:01","0","2"
"71203411","1","2293224","71368459","Openai: `prompt` column/key is missing. Please make sure you name your columns/keys appropriately, then retry","<p>I want to run GPT-3 for text classification. As the first step, I prepare data using openai CLI. I got a csv file which looks like as follow:</p>
<p><a href=""https://i.stack.imgur.com/az0u5.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/az0u5.png"" alt=""enter image description here"" /></a></p>
<p>I wrote following command for preparing the data:</p>
<pre><code>openai tools fine_tunes.prepare_data -f &quot;Path\\training_dataset.csv&quot;
</code></pre>
<p>However, I got following error:</p>
<p><a href=""https://i.stack.imgur.com/Bwmeb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Bwmeb.png"" alt=""enter image description here"" /></a></p>
<p>I am not sure about &quot;name columns/keys appropriately&quot;. Is there any convention that I should follow? Any help would be really appreciated to fix the error</p>
","2022-02-21 08:46:04","","2022-02-21 09:17:09","2023-01-18 12:07:02","<python><openai-api><gpt-3>","2","0","1","2265","0","2","18387639","<p>You may convert your csv/tsv file to json, rename the header as prompt and completion.</p>
<p>Like this:
| prompt | completion |
| -------- | -------------- |
| text1    | result1        |
| text2    | result2        |</p>
","2022-03-06 07:32:52","1","2"
"73370817","1","15152059","73371474","How to use GPT-3 for fill-mask tasks?","<p>I use the following code to get the most likely replacements for a masked word:</p>
<pre><code>!pip install git+https://github.com/huggingface/transformers.git
import torch
import pandas as pd
from transformers import AutoModelForMaskedLM, AutoTokenizer, pipeline

unmasker = pipeline('fill-mask', model='bert-base-uncased', top_k=100)
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = AutoModelForMaskedLM.from_pretrained('bert-base-uncased')

results = unmasker(f&quot;The sun is [MASK].&quot;)
for i in results:
  print(i[&quot;token_str&quot;], i[&quot;score&quot;]*100)
</code></pre>
<p>For example, the most likely replacement for &quot;[MASK]&quot; in the sequence &quot;The sun is [MASK].&quot; is &quot;rising&quot; (33.61%), &quot;shining&quot; (9.33%), and &quot;up&quot; (7.38%).</p>
<p><strong>My question: is there a way to achieve the same with GPT-3?</strong> There is a &quot;complete&quot; and &quot;insert&quot; preset in the OpenAI playground, however, it gives me full sentences (instead of single words) and no probabilities. Can someone help?</p>
","2022-08-16 08:15:15","","","2022-08-16 09:08:10","<python><nlp><gpt-3>","1","3","1","854","","2","14719844","<p>First of all, I don't think you can access properties like token or scores in GPT-3, all you have is the generated text.</p>
<p>Second of all, in my experience GPT-3 is ALL about the correct prompt. You just have to give it instructions like you were talking to a human being.</p>
<p>In you specific case, I would use a prompt like this:</p>
<p>Prompt:</p>
<blockquote>
<p>The sun is [MASK].</p>
<p>Replace [MASK] with the most probable 5 words to replace, and give me
their probabilities.</p>
</blockquote>
<p>Result:</p>
<blockquote>
<p>The sun is shining.</p>
<ol>
<li>shining - 0.47</li>
<li>bright - 0.18</li>
<li>sunny - 0.13</li>
<li>hot - 0.10</li>
<li>beautiful - 0.09</li>
</ol>
</blockquote>
<p>If you want to do that programmatically, here's the code:</p>
<pre><code>import openai
openai.organization = &quot;your org key, if you have one&quot;
openai.api_key = &quot;you api key&quot;
openai.Engine.list()

my_prompt = '''The sun is [MASK].
    
    Replace [MASK] with the most probable 5 words to replace, and give me their probabilities.'''

# Here set parameters as you like
response = openai.Completion.create(
  engine=&quot;text-davinci-002&quot;,
  prompt=my_prompt,
  temperature=0,
  max_tokens=500,
  # top_p=1,
  # frequency_penalty=0.0,
  # presence_penalty=0.0,
  # stop=[&quot;\n&quot;]
)

print(response['choices'][0]['text'])
</code></pre>
","2022-08-16 09:08:10","1","1"
"74666268","1","702977","74744388","OpenAI: Stream interrupted (client disconnected)","<p>I'm trying OpenAI.</p>
<p>I have prepared the training data, and used <code>fine_tunes.create</code>. Several minutes later, it showed <code>Stream interrupted (client disconnected)</code>.</p>
<pre><code>$ openai api fine_tunes.create -t data_prepared.jsonl
Upload progress: 100%|██████████████████████████████████████████████| 47.2k/47.2k [00:00&lt;00:00, 44.3Mit/s]
Uploaded file from data_prepared.jsonl: file-r6dbTH7rVsp6jJMgbX0L0bZx
Created fine-tune: ft-JRGzkYfXm7wnScUxRSBA2M2h
Streaming events until fine-tuning is complete...

(Ctrl-C will interrupt the stream, but not cancel the fine-tune)
[2022-12-02 11:10:08] Created fine-tune: ft-JRGzkYfXm7wnScUxRSBA2M2h
[2022-12-02 11:10:23] Fine-tune costs $0.06
[2022-12-02 11:10:24] Fine-tune enqueued. Queue number: 11

Stream interrupted (client disconnected).
To resume the stream, run:

  openai api fine_tunes.follow -i ft-JRGzkYfXm7wnScUxRSBA2M2h
</code></pre>
<p>I tried <code>fine_tunes.follow</code>, several minutes later, it still failed:</p>
<pre><code>$ openai api fine_tunes.follow -i ft-JRGzkYfXm7wnScUxRSBA2M2h
[2022-12-02 11:10:08] Created fine-tune: ft-JRGzkYfXm7wnScUxRSBA2M2h
[2022-12-02 11:10:23] Fine-tune costs $0.06
[2022-12-02 11:10:24] Fine-tune enqueued. Queue number: 11

Stream interrupted (client disconnected).
To resume the stream, run:

  openai api fine_tunes.follow -i ft-JRGzkYfXm7wnScUxRSBA2M2h
</code></pre>
<p><code>openai api fine_tunes.list</code> showed:</p>
<pre><code>$ openai api fine_tunes.list
{
  &quot;data&quot;: [
    {
      &quot;created_at&quot;: 1669975808,
      &quot;fine_tuned_model&quot;: null,
      &quot;hyperparams&quot;: {
        &quot;batch_size&quot;: 2,
        &quot;learning_rate_multiplier&quot;: 0.1,
        &quot;n_epochs&quot;: 4,
        &quot;prompt_loss_weight&quot;: 0.01
      },
      &quot;id&quot;: &quot;ft-JRGzkYfXm7wnScUxRSBA2M2h&quot;,
      &quot;model&quot;: &quot;curie&quot;,
      &quot;object&quot;: &quot;fine-tune&quot;,
      &quot;organization_id&quot;: &quot;org-YyoQqNIrjGHYDnKt9t3T6x2J&quot;,
      &quot;result_files&quot;: [],
      &quot;status&quot;: &quot;pending&quot;,
      &quot;training_files&quot;: [
        {
          &quot;bytes&quot;: 47174,
          &quot;created_at&quot;: 1669975808,
          &quot;filename&quot;: &quot;data_prepared.jsonl&quot;,
          &quot;id&quot;: &quot;file-r6dbTH7rVsp6jJMgbX0L0bZx&quot;,
          &quot;object&quot;: &quot;file&quot;,
          &quot;purpose&quot;: &quot;fine-tune&quot;,
          &quot;status&quot;: &quot;processed&quot;,
          &quot;status_details&quot;: null
        }
      ],
      &quot;updated_at&quot;: 1669975824,
      &quot;validation_files&quot;: []
    }
  ],
  &quot;object&quot;: &quot;list&quot;
}
</code></pre>
<p>And <code>$ openai api completions.create -m ft-JRGzkYfXm7wnScUxRSBA2M2h -p aprompt</code> returned <code>Error: That model does not exist (HTTP status code: 404)</code>.</p>
<p>Could anyone help?</p>
","2022-12-03 11:24:58","","","2023-05-30 19:06:58","<openai-api><gpt-3>","2","1","9","4882","","2","702977","<p>It was a temporary issue of OpenAI, the team fixed that.</p>
","2022-12-09 14:20:22","1","1"
"74823070","1","4211617","74823327","Can you create a custom model using GPT-3 to answer questions only about a specific topic?","<p>I'm using GPT-3 to create a chatbot that can answer questions related to a specific topic. Can GPT-3 be trained to detect questions that are irrelevant to the topic and refuse to answer them?</p>
<p>Example: Let's say I want to create a chatbot that can only answer questions about
Javascript. If it is asked to list the seven wonders of the world, it should refuse to answer.</p>
","2022-12-16 10:21:20","","","2023-06-20 18:45:02","<nlp><chatbot><gpt-3>","1","0","1","560","","2","15592565","<p>This has been successful for me, you may wish to try it as well.</p>
<p>I want you to act as a javascript guide. You are here to help answer any questions I may have about the language. If I have any questions,  you will do your best to provide a helpful response. Please note that if my question is not related to Javascript, you have to write only &quot;Error&quot;. Let's get started.</p>
","2022-12-16 10:45:12","1","3"
"75041247","1","287316","75043933","What's the correct URL to test OpenAI API?","<p>I'm trying to test the GPT-3 API with a request using curl in Windows CMD:</p>
<pre><code>curl -X POST -H &quot;Content-Type: application/json&quot; -H &quot;Authorization: Bearer MY_KEY&quot; -d &quot;{\&quot;text\&quot;: \&quot;is this working\&quot;}&quot; https://api.openai.com/v1/conversations/text-davinci-003/messages
</code></pre>
<p>Given that I did change &quot;MY_KEY&quot; for my key.</p>
<p>But I got:</p>
<pre><code>{
  &quot;error&quot;: {
    &quot;message&quot;: &quot;Invalid URL (POST /v1/conversations/text-davinci-003/messages)&quot;,
    &quot;type&quot;: &quot;invalid_request_error&quot;,
    &quot;param&quot;: null,
    &quot;code&quot;: null
  }
}
</code></pre>
<p>I also tried the model name as <code>text-davinci-002</code> and <code>text-davinci-001</code>, but get the same invalid URL error. What's the correct URL here? I can't find it on the docs (or in chatGPT itself).</p>
","2023-01-07 14:50:32","","2023-01-24 18:16:59","2023-05-04 17:46:28","<curl><openai-api><gpt-3>","2","1","0","3920","","2","20819591","<p>Sending a POST request to <code>/v1/conversations/text-davinci-003/messages</code> will not return the result you want, because this URL is not used by the OpenAI API.</p>
<p>Here's an example of a cURL request which completes the message <code>Say this is a test</code></p>
<pre class=""lang-bash prettyprint-override""><code>curl https://api.openai.com/v1/completions \
-H &quot;Content-Type: application/json&quot; \
-H &quot;Authorization: Bearer YOUR_API_KEY&quot; \
-d '{&quot;model&quot;: &quot;text-davinci-003&quot;, &quot;prompt&quot;: &quot;Say this is a test&quot;, &quot;temperature&quot;: 0, &quot;max_tokens&quot;: 7}'
</code></pre>
<p>And this is an example of what the API will respond with:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;id&quot;: &quot;cmpl-GERzeJQ4lvqPk8SkZu4XMIuR&quot;,
    &quot;object&quot;: &quot;text_completion&quot;,
    &quot;created&quot;: 1586839808,
    &quot;model&quot;: &quot;text-davinci:003&quot;,
    &quot;choices&quot;: [
        {
            &quot;text&quot;: &quot;This is indeed a test&quot;,
            &quot;index&quot;: 0,
            &quot;logprobs&quot;: null,
            &quot;finish_reason&quot;: &quot;length&quot;
        }
    ],
    &quot;usage&quot;: {
        &quot;prompt_tokens&quot;: 5,
        &quot;completion_tokens&quot;: 7,
        &quot;total_tokens&quot;: 12
    }
}
</code></pre>
<p>This is the full list of API paths:</p>
<p>Instead, you can use the URLs listed in the <a href=""https://beta.openai.com/docs/api-reference/"" rel=""noreferrer"">OpenAI documentation</a>:</p>
<ul>
<li><h4>List models</h4>
GET <code>https://api.openai.com/v1/models</code></li>
<li><h4>Retrieve model</h4>
GET <code>https://api.openai.com/v1/models/{model}</code></li>
<li><h4>Create completion</h4>
POST <code>https://api.openai.com/v1/completions</code></li>
<li><h4>Create edit</h4>
POST <code>https://api.openai.com/v1/edits</code></li>
<li><h4>Create image</h4>
POST <code>https://api.openai.com/v1/images/generations</code></li>
<li><h4>Create image edit</h4>
POST <code>https://api.openai.com/v1/images/edits</code></li>
<li><h4>Create image variation</h4>
POST <code>https://api.openai.com/v1/images/variations</code></li>
<li><h4>Create embeddings</h4>
POST <code>https://api.openai.com/v1/embeddings</code></li>
</ul>
<p>More found in the <a href=""https://beta.openai.com/docs/api-reference/"" rel=""noreferrer"">OpenAI documentation</a>.</p>
","2023-01-07 21:34:25","2","8"
"75051126","1","9990572","75104866","Open AI's GPT Davinci - Asking it questions, but it's returning gibberish?","<p>I ask it &quot;What is COVID-19?&quot;, by using the following code:</p>
<pre><code>string key = &quot;XXXX&quot;;

OpenAIAPI api = new OpenAIAPI(key,Engine.Davinci);

var results = api.Completions.CreateCompletionsAsync(new CompletionRequest(&quot;What is COVID-19?&quot;, temperature: 0.1, max_tokens: 200), 1).Result;

var answer = results.Completions[0].Text;
</code></pre>
<p>Which returns:</p>
<blockquote>
<p>Covid-19 is a drug that is used to treat the symptoms of schizophrenia. It is a combination of two drugs, clozapine and olanzapine.</p>
<p>How does COVID-19 work?</p>
<p>Covid-19 is a combination of two drugs, clozapine and olanzapine. Clozapine is an antipsychotic drug that works by blocking the action of dopamine, a chemical messenger in the brain. Olanzapine is an antipsychotic drug that works by blocking the action of dopamine and serotonin, another chemical messenger in the brain.</p>
<p>How is COVID-19 used?</p>
<p>Covid-19 is used to treat schizophrenia.</p>
<p>How does COVID-19 work?</p>
<p>Covid-19 is a combination of two drugs, clozapine and olanzapine. Clozapine is an antipsychotic drug that works by blocking</p>
</blockquote>
<p>I'm baffled, what am I doing wrong here? Shouldn't this emulate similar results to ChatGPT? I am using the following NuGet for OpenAI access: <a href=""https://github.com/OkGoDoIt/OpenAI-API-dotnet"" rel=""nofollow noreferrer"">https://github.com/OkGoDoIt/OpenAI-API-dotnet</a></p>
","2023-01-08 20:41:38","","2023-01-09 09:06:01","2023-01-13 04:30:39","<c#><.net><openai-api><gpt-3>","1","1","0","358","","2","9990572","<p>I solved this by using <code>OpenAIAPI api = new OpenAIAPI(key, &quot;text-davinci-003&quot;);</code> rather than <code>Engine.Davinci</code>.</p>
","2023-01-13 04:30:39","0","1"
"75176667","1","20930898","75182746","OpenAI GPT-3 API error: ""Cannot specify both model and engine""","<p>So I'm working on some python code that works with chatgpt3. What it does is it sends a request with a prompt and then gets the reply, but I keep getting Errors. The error is</p>
<pre><code>Traceback (most recent call last):
  File &quot;main.py&quot;, line 16, in &lt;module&gt;
    print(response_json['choices'][0]['text'])
KeyError: 'choices'
</code></pre>
<p>Here is my code:</p>
<pre><code>import json
import requests
import os
data = {
    &quot;prompt&quot;: &quot;What is the meaning of life?&quot;,
    &quot;model&quot;: &quot;text-davinci-002&quot;
}

response = requests.post(&quot;https://api.openai.com/v1/engines/davinci/completions&quot;, json=data, headers={
    &quot;Content-Type&quot;: &quot;application/json&quot;,
    &quot;Authorization&quot;: f&quot;Bearer {apikey}&quot;,
})

response_json = json.loads(response.text)

print(response_json['choices'][0]['text'])

</code></pre>
<p>I do have an API key that is valid and the JSON code I don't get the JSON code.</p>
<pre><code>{'error': {'message': 'Cannot specify both model and engine', 'type': 'invalid_request_error', 'param': None, 'code': None}}
</code></pre>
<p>I have tried different API keys and that didn't work. i even looked up all the different models for chatgpt and it still doesn't work</p>
","2023-01-19 18:19:45","","2023-03-13 13:49:34","2023-06-23 15:22:41","<python><json><python-3.x><openai-api><gpt-3>","1","2","0","4009","","2","10347145","<p>All <a href=""https://platform.openai.com/docs/deprecations"" rel=""nofollow noreferrer"">Engines API endpoints</a> are deprecated.</p>
<p><a href=""https://i.stack.imgur.com/lQu9c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lQu9c.png"" alt=""Deprecated"" /></a></p>
<p>Change the URL from this...</p>
<pre><code>https://api.openai.com/v1/engines/davinci/completions
</code></pre>
<p>...to this.</p>
<pre><code>https://api.openai.com/v1/completions
</code></pre>
<p>If you run <code>test.py</code> the OpenAI API will return a completion. You'll get a different completion because the <code>temperature</code> parameter is not set to <code>0</code>. I got the following completion:</p>
<blockquote>
<p>The meaning of life is to find out and fulfil the purpose and meaning...</p>
</blockquote>
<p><strong>test.py</strong></p>
<pre><code>import json
import requests
import os

data = {
    &quot;prompt&quot;: &quot;What is the meaning of life?&quot;,
    &quot;model&quot;: &quot;text-davinci-003&quot;
}

response = requests.post(&quot;https://api.openai.com/v1/completions&quot;, json=data, headers={
    &quot;Content-Type&quot;: &quot;application/json&quot;,
    &quot;Authorization&quot;: f&quot;Bearer {apikey}&quot;
})

response_json = json.loads(response.text)

print(response_json[&quot;choices&quot;][0][&quot;text&quot;])
</code></pre>
","2023-01-20 10:28:06","1","2"
"75210324","1","7541847","75210429","OpenAI GPT-3 API error: ""You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth""","<p>I am getting an error for the following PHP code:</p>
<pre><code>$curl = curl_init(&quot;https://api.openai.com/v1/engines/davinci/completions&quot;);

$data = array(
  'prompt' =&gt; 'how many sundays in 2023',
  'max_tokens' =&gt; 256,
  'temperature' =&gt; 0.7,
  'model' =&gt; 'text-davinci-003'
);

curl_setopt($curl, CURLOPT_POST, 1);
curl_setopt($curl, CURLOPT_POSTFIELDS, http_build_query($data));
curl_setopt($curl, CURLOPT_RETURNTRANSFER, 1);
curl_setopt($curl, CURLOPT_HTTPHEADER, ['Authorization: Bearer sk-MY-API-KEY']);
curl_setopt($curl, CURLOPT_HTTPHEADER, ['Content-Type: application/json']);

$result = curl_exec($curl);
curl_close($curl);

$result = json_decode($result);
print $result-&gt;choices[0]-&gt;text;
</code></pre>
<p>I correctly provided the API Key, but getting this error:</p>
<blockquote>
<p>Error message: You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY)</p>
</blockquote>
","2023-01-23 13:39:13","","2023-03-13 13:55:37","2023-03-13 13:55:37","<php><openai-api><gpt-3>","1","0","1","1987","","2","10347145","<p>All <a href=""https://beta.openai.com/docs/api-reference/engines"" rel=""nofollow noreferrer"">Engines endpoints</a> are deprecated.</p>
<p><a href=""https://i.stack.imgur.com/lQu9c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lQu9c.png"" alt=""Deprecated"" /></a></p>
<p>This is the correct <a href=""https://platform.openai.com/docs/api-reference/completions"" rel=""nofollow noreferrer"">Completions endpoint</a>:</p>
<pre><code>https://api.openai.com/v1/completions
</code></pre>
<h3>Working example</h3>
<p>If you run <code>test.php</code> the OpenAI API will return the following completion:</p>
<blockquote>
<p>string(23) &quot;</p>
<p>This is indeed a test&quot;</p>
</blockquote>
<p><strong>test.php</strong></p>
<pre><code>&lt;?php
    $ch = curl_init();

    $url = 'https://api.openai.com/v1/completions';

    $api_key = 'sk-xxxxxxxxxxxxxxxxxxxx';

    $post_fields = '{
        &quot;model&quot;: &quot;text-davinci-003&quot;,
        &quot;prompt&quot;: &quot;Say this is a test&quot;,
        &quot;max_tokens&quot;: 7,
        &quot;temperature&quot;: 0
    }';

    $header  = [
        'Content-Type: application/json',
        'Authorization: Bearer ' . $api_key
    ];

    curl_setopt($ch, CURLOPT_URL, $url);
    curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);
    curl_setopt($ch, CURLOPT_POST, 1);
    curl_setopt($ch, CURLOPT_POSTFIELDS, $post_fields);
    curl_setopt($ch, CURLOPT_HTTPHEADER, $header);

    $result = curl_exec($ch);
    if (curl_errno($ch)) {
        echo 'Error: ' . curl_error($ch);
    }
    curl_close($ch);

    $response = json_decode($result);
    var_dump($response-&gt;choices[0]-&gt;text);
?&gt;
</code></pre>
","2023-01-23 13:48:07","5","0"
"75237628","1","1816135","75249088","tokenizer.save_pretrained TypeError: Object of type property is not JSON serializable","<p>I am trying to save the GPT2 tokenizer as follows:</p>
<pre><code>from transformers import GPT2Tokenizer, GPT2LMHeadModel
tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
tokenizer.pad_token = GPT2Tokenizer.eos_token
dataset_file = &quot;x.csv&quot;
df = pd.read_csv(dataset_file, sep=&quot;,&quot;)
input_ids = tokenizer.batch_encode_plus(list(df[&quot;x&quot;]), max_length=1024,padding='max_length',truncation=True)[&quot;input_ids&quot;]

# saving the tokenizer
tokenizer.save_pretrained(&quot;tokenfile&quot;)
</code></pre>
<p>I am getting the following error:
TypeError: Object of type property is not JSON serializable</p>
<p>More details:</p>
<pre><code>TypeError                                 Traceback (most recent call last)
Cell In[x], line 3
      1 # Save the fine-tuned model
----&gt; 3 tokenizer.save_pretrained(&quot;tokenfile&quot;)

File /3tb/share/anaconda3/envs/ak_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2130, in PreTrainedTokenizerBase.save_pretrained(self, save_directory, legacy_format, filename_prefix, push_to_hub, **kwargs)
   2128 write_dict = convert_added_tokens(self.special_tokens_map_extended, add_type_field=False)
   2129 with open(special_tokens_map_file, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
-&gt; 2130     out_str = json.dumps(write_dict, indent=2, sort_keys=True, ensure_ascii=False) + &quot;\n&quot;
   2131     f.write(out_str)
   2132 logger.info(f&quot;Special tokens file saved in {special_tokens_map_file}&quot;)

File /3tb/share/anaconda3/envs/ak_env/lib/python3.10/json/__init__.py:238, in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)
    232 if cls is None:
    233     cls = JSONEncoder
    234 return cls(
    235     skipkeys=skipkeys, ensure_ascii=ensure_ascii,
    236     check_circular=check_circular, allow_nan=allow_nan, indent=indent,
    237     separators=separators, default=default, sort_keys=sort_keys,
--&gt; 238     **kw).encode(obj)

File /3tb/share/anaconda3/envs/ak_env/lib/python3.10/json/encoder.py:201, in JSONEncoder.encode(self, o)
    199 chunks = self.iterencode(o, _one_shot=True)
...
    178     &quot;&quot;&quot;
--&gt; 179     raise TypeError(f'Object of type {o.__class__.__name__} '
    180                     f'is not JSON serializable')

TypeError: Object of type property is not JSON serializable
</code></pre>
<p>How can I solve this issue?</p>
","2023-01-25 17:21:03","","","2023-01-26 16:38:08","<python><huggingface-transformers><gpt-2>","1","3","0","124","","2","8395595","<p>The Problem is on the line:</p>
<p><code>tokenizer.pad_token = GPT2Tokenizer.eos_token</code></p>
<p>Here the initializer is wrong, that's why this error occurred.
A simple solution is to modify this line to:
<code>tokenizer.pad_token = tokenizer.eos_token</code></p>
<p>For the reference purpose, your final code will look like this:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import GPT2Tokenizer, GPT2LMHeadModel
tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
tokenizer.pad_token = tokenizer.eos_token
dataset_file = &quot;x.csv&quot;
df = pd.read_csv(dataset_file, sep=&quot;,&quot;)
input_ids = tokenizer.batch_encode_plus(list(df[&quot;x&quot;]), max_length=1024,padding='max_length',truncation=True)[&quot;input_ids&quot;]

# saving the tokenizer
tokenizer.save_pretrained(&quot;tokenfile&quot;)
</code></pre>
","2023-01-26 16:38:08","0","1"
"75251168","1","10897106","75258801","BCELoss between logits and labels not working","<p>I am using a GPT2 model that outputs <code>logits</code> (before softmax) in the shape <code>(batch_size, num_input_ids, vocab_size)</code> and I need to compare it with the labels that are of shape <code>(batch_size, num_input_ids)</code> to calculate BCELoss. How do I calculate it?</p>
<pre><code>logits = output.logits #--of shape (32, 56, 592)
logits = torch.nn.Softmax()(logits)
labels = labels #---------of shape (32, 56)

torch.nn.BCELoss()(logits, labels)
</code></pre>
<p>but the dimensions do not match, so how do I contract <code>logits</code> to <code>labels</code> shape or expand <code>labels</code> to <code>logits</code> shape?</p>
","2023-01-26 20:12:44","","","2023-01-27 13:32:34","<pytorch><nlp><loss-function><text-classification><gpt-2>","1","2","0","286","","2","5652313","<p><strong>Binary cross-entropy</strong> is used when the final classification layer is a <strong>sigmoid layer</strong>, i.e., for each output dimension, only a true/false output is possible. You can imagine it as assigning some tags to the input. This also means that the <code>labels</code> need to have the same dimension as the <code>logits</code>, having 0/1 for each logit. Statistically speaking, for 592 output dimensions, you predict 592 Bernoulli (= binary) distributions. The expected shape is 32 × 56 × 592.</p>
<p>When using the <strong>softmax layer</strong>, you assume only one target class is possible; you predict a single categorical distribution over 592 possible output classes. However, in this case, the correct loss function is not binary cross-entropy but <strong>categorical cross-entropy</strong>, implemented by the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"" rel=""nofollow noreferrer""><code>CrossEntropyLoss</code></a> class in PyTorch. Note that it takes the logits directly before the softmax normalization and does the normalization internally. The expected shape is 32 × 56, as in the code snippet.</p>
","2023-01-27 13:32:34","2","0"
"75361743","1","7343560","75362164","How do I make an API call to GPT-3 correctly?","<p>I am trying to make an API call to GPT-3 but I am getting an error (Bad request 400). Here is my code:</p>
<pre><code>url = &quot;https://api.openai.com/v1/engines/gpt-3/jobs&quot;

headers = {
    &quot;Content-Type&quot;: &quot;application/json&quot;,
    &quot;Authorization&quot;: &quot;sk-apikey&quot;
}

data = {
    &quot;model&quot;: &quot;text-davinci-002&quot;,
    &quot;prompt&quot;: &quot;Correct this to standard English : Who are you&quot;,
    &quot;max_tokens&quot;: 60        
}

response = requests.post(url, headers=headers, data=json.dumps(data))
</code></pre>
","2023-02-06 13:09:22","","2023-02-06 13:14:08","2023-02-06 14:05:29","<python><python-requests><gpt-3>","1","1","0","430","","2","17749677","<p>Try changing the url and fixing the Authorization header...</p>
<pre><code>url = &quot;https://api.openai.com/v1/completions&quot;

headers = {
    &quot;Content-Type&quot;: &quot;application/json&quot;,
    &quot;Authorization&quot;: f&quot;Bearer {api_key}&quot;
}

data = {
    &quot;model&quot;: &quot;text-davinci-002&quot;,
    &quot;prompt&quot;: &quot;Correct this to standard English : Who are you \n&quot;,
    &quot;max_tokens&quot;: 60        
}

response = requests.post(url, headers=headers, data=json.dumps(data))
response.json()
</code></pre>
","2023-02-06 13:50:01","1","1"
"75373129","1","14752540","75373214","OpenAI GPT-3 API error: ""This model's maximum context length is 2049 tokens""","<p>I have 2 issues relating to the response result from OpenAI completion.</p>
<p>The following result doesn't return back the full text when I give a content of 500 word and prompt &quot;Fix grammar mistakes&quot; <strong>(Is tokens issue?)</strong></p>
<p><a href=""https://i.stack.imgur.com/dLg4Y.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/dLg4Y.jpg"" alt=""enter image description here"" /></a></p>
<p>The second issue is when the text sometimes have some double quotes OR single quotes it messes with the JSON format So I delete any type of quotes from the content (not sure if it's the best solution but I may prefer done it on JS not PHP)</p>
<pre><code>curl_setopt($ch, CURLOPT_POSTFIELDS, &quot;{\n  \&quot;model\&quot;: \&quot;text-davinci-001\&quot;,\n  \&quot;prompt\&quot;: \&quot;&quot; . $open_ai_prompt  . &quot;:nn&quot; . $content_text  . &quot;\&quot;,\n  \&quot;temperature\&quot;: 0,\n  \&quot;top_p\&quot;: 1.0,\n  \&quot;frequency_penalty\&quot;: 0.0,\n  \&quot;presence_penalty\&quot;: 0.0\n}&quot;);
</code></pre>
<blockquote>
<p>&quot;message&quot;: &quot;We could not parse the JSON body of your request. (HINT:
This likely means you aren't using your HTTP library correctly. The
OpenAI API expects a JSON payload, but what was sent was not valid
JSON.</p>
</blockquote>
","2023-02-07 12:07:58","","2023-03-13 14:10:31","2023-04-26 07:42:06","<javascript><php><json><openai-api><gpt-3>","1","3","0","1659","","2","10347145","<h3>Regarding token limits</h3>
<p>First of all, I think you don't understand how tokens work: <strong>500 words is more than 500 tokens</strong>. Use the <a href=""https://platform.openai.com/tokenizer"" rel=""nofollow noreferrer"">Tokenizer</a> to calculate the number of tokens.</p>
<p>As stated in the official <a href=""https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them"" rel=""nofollow noreferrer"">OpenAI article</a>:</p>
<blockquote>
<p>Depending on the model used, requests can use up to <code>4097</code> tokens <strong>shared</strong>
between prompt and completion. If your prompt is <code>4000</code> tokens, your
completion can be <code>97</code> tokens at most.</p>
<p>The limit is currently a technical limitation, but there are often
creative ways to solve problems within the limit, e.g. condensing your
prompt, breaking the text into smaller pieces, etc.</p>
</blockquote>
<p>Switch <code>text-davinci-001</code> for a GPT-3 model because the token limits are higher.</p>
<p><a href=""https://platform.openai.com/docs/models/gpt-3"" rel=""nofollow noreferrer"">GPT-3 models</a>:</p>
<p><a href=""https://i.stack.imgur.com/RExzL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RExzL.png"" alt=""Table"" /></a></p>
<br>
<h3>Regarding double quotes in JSON</h3>
<p>You can escape double quotes in JSON by using <code>\</code> in front of double quotes like this:</p>
<pre><code>&quot;This is how you can escape \&quot;double quotes\&quot; in JSON.&quot;
</code></pre>
<p>But... This is more of a quick fix. For proper solution, see @ADyson's comment above:</p>
<blockquote>
<p>Don't build your JSON by hand like that. Make a PHP object / array
with the correct structure, and then use <code>json_encode()</code> to turn it into
valid JSON, it will automatically handle any escaping etc which is
needed, and you can also use the options to tweak certain things about
the output - check the PHP documentation.</p>
</blockquote>
<hr />
<p><strong>EDIT 1</strong></p>
<p>You need to set the <a href=""https://platform.openai.com/docs/api-reference/completions/create#completions/create-max_tokens"" rel=""nofollow noreferrer""><code>max_tokens</code></a> parameter higher. Otherwise, the output will be shorter than your input. You will not get the whole fixed text back, but just a part of it.</p>
<hr />
<p><strong>EDIT 2</strong></p>
<p>Now you set the <code>max_tokens</code> parameter too high! If you set <code>max_tokens = 5000</code>, this is too much even for the most capable GPT-3 model (i.e., <code>text-davinci-003</code>). The prompt and the completion <strong>together</strong> can be <code>4097</code> tokens.</p>
<p>You can figure this out if you take a look at the error you got:</p>
<pre><code>&quot;error&quot;: {&quot;message&quot;: &quot;This model's maximum context length is 4097 tokens, however you requested 6450 tokens (1450 in your prompt; 5000 for the completion). Please reduce your prompt; or completion length.&quot;}
</code></pre>
","2023-02-07 12:15:14","10","3"
"75405943","1","21015092","75406005","I do not know why I can't access open ai's api for use in a react app","<p>I am trying to access openai's api for a react application. I am getting an &quot;unsafe header&quot; error, an error 400, and at the same time &quot;https://api.openai.com/v1/completions&quot; is sending me a prompt about not providing my api key, even though I am providing the api key through a .env file. I do not know what to do, and I'm wondering what exactly I did wrong.</p>
<p>This is the react function I am using:</p>
<pre><code>const configuration = new Configuration({
    apiKey: process.env.REACT_APP_OPENAI_API_KEY,
    organization: &quot;org-xut9Kn1LqNLyDiHEMAQlnJ0k&quot;
});

const openai = new OpenAIApi(configuration);

const handleSuggestions = async (text) =&gt; {
  const response = await openai.createCompletion({
      model: &quot;text-davinci-001&quot;,
      prompt: &quot;autocomplete this word, letter or sentence: &quot; + text,
      max_tokens: 100,
      n: 1,
      stop: text.length - 1,
      temperature: 0.15,
  });
  console.log(response);
  const data = await response.json();
  setSuggestions(response.choices[0].text.split(' ').slice(text.split(' ').length - 1).join(' ').split(' '));
};
</code></pre>
<p>``</p>
<p>I am getting a &quot;unsafe header &quot;User-Agent&quot;&quot; error as well as an error 400 from &quot;https://api.openai.com/v1/completions&quot; in my browser console while running the react app. This is the full prompt I am getting back from &quot;https://api.openai.com/v1/completions&quot;:</p>
<pre><code>{
    &quot;error&quot;: {
        &quot;message&quot;: &quot;You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.&quot;,
        &quot;type&quot;: &quot;invalid_request_error&quot;,
        &quot;param&quot;: null,
        &quot;code&quot;: null
    }
}
</code></pre>
<p>Please what can I do, and what exactly is wrong with the code? Also, hoping this counts as a &quot;Minimal, Reproducible Example&quot;, as I am pretty new to stack overflow.</p>
","2023-02-10 00:52:22","","2023-02-10 05:24:30","2023-02-10 05:24:30","<javascript><reactjs><openai-api><gpt-3>","1","5","-2","667","","2","904956","<p>You should be making the request from a server not your client.</p>
<p><a href=""https://stackoverflow.com/questions/33143776/ajax-request-refused-to-set-unsafe-header"">Ajax request: Refused to set unsafe header</a></p>
<p>I highly recommend checking out Next.js 13 as it uses React under-the-hood and let's you create &quot;Server&quot; components that are essentially isomorphic.</p>
<p>Here's an example Next.js 13 <code>app/pages.tsx</code> file:</p>
<pre><code>const App = async () =&gt; {
  console.log(&quot;App.js&quot;);

  const results = await fetch(
    `http://api.weatherapi.com/v1/forecast.json?key=&lt;API_KEY&gt;&amp;q=Stockholm&amp;days=6&amp;aqi=no&amp;alerts=no`
  );

  const json = await results.json();
  console.log(&quot;json&quot;, json);

  return (
    &lt;&gt;
      &lt;h3&gt;{json.location.name}&lt;/h3&gt;
      &lt;p&gt;{json.location.temp_c}&lt;/p&gt;
      &lt;p&gt;{json.location.localtime}&lt;/p&gt;
    &lt;/&gt;
  );
};

export default App;
</code></pre>
<p><a href=""https://codesandbox.io/p/sandbox/next-js-v12-v13-weatherapi-example-4nsp1p?file=%252Fapp%252Fpage.tsx"" rel=""nofollow noreferrer"">Check out this working Next.js 13 / React18 sandbox</a> which hits the Weather API - If you'd like fork it and see if your API calls work on the server inside this <code>app.pages.tsx</code> file. Otherwise you will need to use a Firebase Function or some backend server.</p>
","2023-02-10 01:03:36","1","0"
"75648132","1","139150","75671537","OpenAI GPT-3 API: Why do I get only partial completion? Why is the completion cut off?","<p>I tried the following code but got only partial results like</p>
<pre><code>[{&quot;light_id&quot;: 0, &quot;color
</code></pre>
<p>I was expecting the full JSON as suggested on this page:</p>
<p><a href=""https://medium.com/@richardhayes777/using-chatgpt-to-control-hue-lights-37729959d94f"" rel=""nofollow noreferrer"">https://medium.com/@richardhayes777/using-chatgpt-to-control-hue-lights-37729959d94f</a></p>
<pre><code>import json
import os
import time
from json import JSONDecodeError
from typing import List

import openai
openai.api_key =  &quot;xxx&quot;

HEADER = &quot;&quot;&quot;
I have a hue scale from 0 to 65535. 
red is 0.0
orange is 7281
yellow is 14563
purple is 50971
pink is 54612
green is 23665
blue is 43690

Saturation is from 0 to 254
Brightness is from 0 to 254

Two JSONs should be returned in a list. Each JSON should contain a color and a light_id. 
The light ids are 0 and 1. 
The color relates a key &quot;color&quot; to a dictionary with the keys &quot;hue&quot;, &quot;saturation&quot; and &quot;brightness&quot;. 

Give me a list of JSONs to configure the lights in response to the instructions below. 
Give only the JSON and no additional characters. 
Do not attempt to complete the instruction that I give.
Only give one JSON for each light. 
&quot;&quot;&quot;

completion = openai.Completion.create(model=&quot;text-davinci-003&quot;, prompt=HEADER)
print(completion.choices[0].text)
</code></pre>
","2023-03-06 07:29:41","","2023-03-13 14:52:25","2023-03-29 08:26:43","<openai-api><gpt-3>","2","1","3","1710","","2","10347145","<h3>In general</h3>
<p>If you get partial completion (i.e., if the completion is cut off), it's because <strong>the <code>max_tokens</code> parameter is set too low or you didn't set it at all</strong> (in this case, it defaults to <code>16</code>). You need to set it higher, <strong>but the token count of your prompt and completion together cannot exceed the model's context length</strong>.</p>
<p>See the official <a href=""https://platform.openai.com/docs/api-reference/completions/create#completions/create-max_tokens"" rel=""nofollow noreferrer"">OpenAI documentation</a>:</p>
<p><a href=""https://i.stack.imgur.com/iXVdX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iXVdX.png"" alt=""Screenshot"" /></a></p>
<h3>Your case</h3>
<p>If you don't set <code>max_tokens = 1024</code> the completion you get will be cut off. Take a careful look at the <a href=""https://medium.com/@richardhayes777/using-chatgpt-to-control-hue-lights-37729959d94f"" rel=""nofollow noreferrer"">tutorial</a> you're referring to once again.</p>
<p>If you run <code>test.py</code> the OpenAI API will return a completion:</p>
<blockquote>
<p>Light 0 should be red:  [{&quot;light_id&quot;: 0, &quot;color&quot;: {&quot;hue&quot;: 0,
&quot;saturation&quot;: 254, &quot;brightness&quot;: 254}},{&quot;light_id&quot;: 1, &quot;color&quot;: }]</p>
<p>Light 1 should be orange: [{&quot;light_id&quot;: 0, &quot;color&quot;: {&quot;hue&quot;: 0,
&quot;saturation&quot;: 254, &quot;brightness&quot;: 254}},{&quot;light_id&quot;: 1, &quot;color&quot;:
{&quot;hue&quot;: 7281, &quot;saturation&quot;: 254, &quot;brightness&quot;: 254}}]</p>
</blockquote>
<p><strong>test.py</strong></p>
<pre><code>import openai
import os

openai.api_key = os.getenv('OPENAI_API_KEY')

HEADER = &quot;&quot;&quot;
I have a hue scale from 0 to 65535. 
red is 0.0
orange is 7281
yellow is 14563
purple is 50971
pink is 54612
green is 23665
blue is 43690

Saturation is from 0 to 254
Brightness is from 0 to 254

Two JSONs should be returned in a list. Each JSON should contain a color and a light_id. 
The light ids are 0 and 1. 
The color relates a key &quot;color&quot; to a dictionary with the keys &quot;hue&quot;, &quot;saturation&quot; and &quot;brightness&quot;. 

Give me a list of JSONs to configure the lights in response to the instructions below. 
Give only the JSON and no additional characters. 
Do not attempt to complete the instruction that I give.
Only give one JSON for each light. 
&quot;&quot;&quot;

completion = openai.Completion.create(model=&quot;text-davinci-003&quot;, prompt=HEADER, max_tokens=1024)
print(completion.choices[0].text)
</code></pre>
","2023-03-08 09:52:51","0","7"
"75640144","1","4883557","75739103","OpenAI converting API code from GPT-3 to chatGPT-3.5","<p>Below is my working code for the GPT-3 API. I am having trouble converting it to work with chatGPT-3.5.</p>
<pre><code>&lt;?php include('../config/config.php'); ?&gt;
&lt;!DOCTYPE html&gt;
&lt;html lang=&quot;en&quot;&gt;
&lt;head&gt;
&lt;meta charset=&quot;UTF-8&quot;&gt;
&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt;
&lt;title&gt;Chatbot&lt;/title&gt;
&lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.3/font/bootstrap-icons.css&quot;&gt;
&lt;link href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css&quot; rel=&quot;stylesheet&quot; integrity=&quot;sha384-GLhlTQ8iRABdZLl6O3oVMWSktQOp6b7In1Zl3/Jr59b6EGGoI1aFkw7cmDA6j6gD&quot; crossorigin=&quot;anonymous&quot;&gt;
&lt;link href=&quot;style.css&quot; rel=&quot;stylesheet&quot;&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;div class=&quot;container py-5&quot;&gt;
  &lt;h1 class=&quot;mb-5 text-center&quot;&gt;
    &lt;div class=&quot;logo&quot;&gt; &lt;img src=&quot;/images/Logo-PocketAI.svg&quot; height=&quot;80&quot; width=&quot;210&quot; aria-label=&quot;PocketAI.Online Logo&quot; title=&quot;PocketAI.Online Logo&quot; alt=&quot;SPocketAI.Online Logo&quot; class=&quot;img-fluid&quot;&gt; &lt;/div&gt;
  &lt;/h1&gt;
  &lt;div class=&quot;form-floating mb-3&quot;&gt;
    &lt;select class=&quot;form-select&quot; id=&quot;tab-select&quot; aria-label=&quot;Select your purpose&quot;&gt;
      &lt;option value=&quot;exam&quot; selected&gt;Exam&lt;/option&gt;
      &lt;option value=&quot;feedback&quot;&gt;Feedback&lt;/option&gt;
    &lt;/select&gt;
    &lt;label for=&quot;tab-select&quot;&gt;Select your purpose:&lt;/label&gt;
  &lt;/div&gt;
  &lt;div class=&quot;input-group mb-3&quot;&gt;
    &lt;div class=&quot;form-floating&quot;&gt;
      &lt;textarea class=&quot;form-control&quot; placeholder=&quot;Enter your question or comment here&quot; id=&quot;prompt&quot;&gt;&lt;/textarea&gt;
      &lt;label for=&quot;prompt&quot;&gt;Enter your question or comment here&lt;/label&gt;
    &lt;/div&gt;
    &lt;div class=&quot;input-group-append username w-100 mt-3 mb-4&quot;&gt;
      &lt;button class=&quot;btn btn-outline-primary w-100&quot; type=&quot;button&quot; id=&quot;send-button&quot;&gt;Send&lt;/button&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  &lt;div id=&quot;output&quot; class=&quot;mb-3&quot; style=&quot;height: 300px; overflow: auto; border: 1px solid lightgray; padding: 10px;&quot;&gt;&lt;/div&gt;
  &lt;div id=&quot;exam-instructions&quot; class=&quot;mb-3&quot; style=&quot;display: block;&quot;&gt;
    &lt;h3&gt;Exam&lt;/h3&gt;
    &lt;p&gt;PocketAI can create multiple choice and true false questions in a format that enables import into Brightspace D2L quizzes using Respondus. Place PocketAI output into a Word document before importing with Respondus. Ask PocketAI questions like the following: &lt;br&gt;
      &lt;br&gt;
      Create 3 multiple choice questions about carbohydrates for a freshman Nutrition online college course.&lt;br&gt;
      Create 2 true false questions about business for a sophomore Business face to face college course.&lt;/p&gt;
  &lt;/div&gt;
  &lt;div id=&quot;feedback-instructions&quot; class=&quot;mb-3&quot; style=&quot;display: none;&quot;&gt;
    &lt;h3&gt;Feedback&lt;/h3&gt;
    &lt;p&gt;Enter text to receive writing feedback.&lt;/p&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;script&gt;
const previousPrompts = [];
const userName = &quot;&lt;strong&gt;User&lt;/strong&gt;&quot;;
const chatbotName = &quot;&lt;strong&gt;PocketAI&lt;/strong&gt;&quot;;

const selectDropdown = document.getElementById(&quot;tab-select&quot;);

selectDropdown.addEventListener(&quot;change&quot;, function() {
  const activeTabId = this.value;
  
  // hide all instruction sections
  document.querySelectorAll(&quot;[id$='-instructions']&quot;).forEach(function(instructionSection) {
    instructionSection.style.display = &quot;none&quot;;
  });
  
  // show the instruction section for the active tab
  document.getElementById(`${activeTabId}-instructions`).style.display = &quot;block&quot;;
});

document.getElementById(&quot;send-button&quot;).addEventListener(&quot;click&quot;, function() {
  const prompt = document.getElementById(&quot;prompt&quot;).value;
  const activeTabId = selectDropdown.value;

  const endpoint = &quot;https://api.openai.com/v1/completions&quot;;
  const apiKey = &quot;&lt;?=$OPEN_AI_KEY;?&gt;&quot;;

  document.getElementById(&quot;send-button&quot;).innerHTML = '&lt;span class=&quot;spinner-border spinner-border-sm&quot; role=&quot;status&quot; aria-hidden=&quot;true&quot;&gt;&lt;/span&gt; Sending...';

  let promptText = &quot;&quot;;
  
  switch (activeTabId) {
    case &quot;exam&quot;:
        promptText = &quot;Create quiz questions in the following format: Begin each question with a number followed by a period, and then include the question wording. For each question, include four answer choices listed as letters (A, B, C, D) followed by a period and at least one space before the answer wording. Designate the correct answer by placing an asterisk (*) directly in front of the answer letter (do not put a space between the asterisk and the answer choice). Place the asterisk in front of the answer letter, only the front. It is important that correct answers are identified. Don't make up answers, only select factual answers. For example formatting (don't use this specific example), \&quot;1. What is the recommended daily intake of dietary fiber? A. 10 grams B. 25 grams *C. 50 grams D. 75 grams\&quot;. Format true false questions the same way. If you are unsure of the correct answer, don't create the question. Every quiz question and answer must be 100% correct and factual. Do not make up answers. All answers must be correct.&quot;;
      break;
     case &quot;feedback&quot;:
      promptText = &quot;Can you provide feedback on the writing, grammar, sentence structure, punctuation, and style of this student's paper? The paper should be analyzed for its strengths and weaknesses in terms of written communication. Please provide suggestions for improvement and examples to help the student understand how to make the writing better. The feedback should be specific and provide actionable steps that the student can take to improve their writing skills. Please include at least three examples of areas that could be improved and specific suggestions for how to improve them, such as correcting grammar errors, restructuring sentences, or improving the use of punctuation.&quot;;
      break;
  }
  
  const requestData = {
    prompt: previousPrompts.join(&quot;\n&quot;) + promptText + &quot;\n&quot; + prompt,
    max_tokens: 400,
      model: &quot;text-davinci-003&quot;,
    n: 1,
    stop: &quot;&quot;,
    temperature: 0.5,
      top_p: 0.0,
      frequency_penalty: 0.0,
      presence_penalty: 0
  };
        
  const requestOptions = {
    method: &quot;POST&quot;,
    headers: {
      &quot;Content-Type&quot;: &quot;application/json&quot;,
      &quot;Authorization&quot;: `Bearer ${apiKey}`,
    },
    body: JSON.stringify(requestData),
  };
  
  fetch(endpoint, requestOptions)
    .then(response =&gt; response.json())
    .then(data =&gt; {
      const reply = data.choices[0].text;
      
      // Add the user message to the chat history
      const userMessage = `&lt;div class=&quot;message-container&quot;&gt;
        &lt;div class=&quot;username&quot;&gt;${userName}:&amp;nbsp;&lt;/div&gt;
        &lt;div class=&quot;user-message&quot;&gt;${prompt}&lt;/div&gt;
      &lt;/div&gt;`;
      document.getElementById(&quot;output&quot;).innerHTML += userMessage;
      
      const chatbotMessage = `&lt;div class=&quot;message-container&quot;&gt;
  &lt;div class=&quot;username&quot;&gt;${chatbotName}:&amp;nbsp;&lt;/div&gt;
  &lt;div class=&quot;chatbot-message&quot; style=&quot;white-space: pre-wrap&quot;&gt;${reply}&lt;i class=&quot;bi bi-clipboard-check copy-button&quot; data-bs-toggle=&quot;tooltip&quot; data-bs-placement=&quot;bottom&quot; title=&quot;Copy to clipboard&quot; data-text=&quot;${reply}&quot; style=&quot;cursor: pointer;&quot;&gt;&lt;/i&gt;&lt;/div&gt;
&lt;/div&gt;`; 
document.getElementById(&quot;output&quot;).innerHTML += chatbotMessage;

// Add an event listener to each &quot;Copy to Clipboard&quot; button
document.addEventListener(&quot;click&quot;, function(event) {
  if (event.target.classList.contains(&quot;copy-button&quot;)) {
    const textToCopy = event.target.dataset.text;
    navigator.clipboard.writeText(textToCopy);
  }
});
     // Scroll to the bottom of the chat history
      document.getElementById(&quot;output&quot;).scrollTop = document.getElementById(&quot;output&quot;).scrollHeight;
    
      // Clear the user input field
      document.getElementById(&quot;prompt&quot;).value = &quot;&quot;;
    
      previousPrompts.push(prompt);
      // Clear the spinner and show the &quot;Send&quot; button again
      document.getElementById(&quot;send-button&quot;).innerHTML = 'Send';
    })
    .catch(error =&gt; {
      console.error(error);
    
      // Hide the spinner and show the &quot;Send&quot; button again
      document.getElementById(&quot;send-button&quot;).innerHTML = 'Send';
    });
});

document.getElementById(&quot;prompt&quot;).addEventListener(&quot;keydown&quot;, function(event) {
  if (event.keyCode === 13) {
    event.preventDefault();
    document.getElementById(&quot;send-button&quot;).
click();
  }
});
&lt;/script&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js&quot; integrity=&quot;sha384-w76AqPfDkMBDXo30jS1Sgez6pr3x5MlQ1ZAGC+nuZB+EYdgRZgiwxhTBTkF7CXvN&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>I have read <a href=""https://openai.com/blog/introducing-chatgpt-and-whisper-apis"" rel=""nofollow noreferrer"">https://openai.com/blog/introducing-chatgpt-and-whisper-apis</a> and referred to this - <a href=""https://stackoverflow.com/questions/75613656/openai-chatgpt-gpt-3-5-turbo-api-how-to-access-the-message-content"">OpenAI ChatGPT (gpt-3.5-turbo) API: How to access the message content?</a> but still can't make it work.</p>
<p>I've tried changing the requestData to this, but no luck:</p>
<pre><code>const requestData = {
    model: &quot;gpt-3.5-turbo&quot;,
    messages: [
      { role: &quot;user&quot;, content: prompt }
    ],
    max_tokens: 400,
    temperature: 0.5,
    top_p: 1,
    frequency_penalty: 0,
    presence_penalty: 0
  };
</code></pre>
<p>Any help will be greatly appreciated!</p>
","2023-03-05 04:10:35","","2023-03-06 01:56:04","2023-03-14 22:56:50","<php><openai-api><gpt-3><chatgpt-api>","1","1","-3","1308","","2","15493697","<p>better check your <code>requestData</code> object, the GPT 3.5 turbo doesn't need these props</p>
<blockquote>
<p>max_tokens,temperature,top_p: 1,frequency_penalty,presence_penalty</p>
</blockquote>
<p>I made the same mistake too, GPT 3.5 turbo is wayyyyy easier to use than I expected. Here's OpenAI sample:</p>
<pre><code>const { Configuration, OpenAIApi } = require(&quot;openai&quot;);

const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});
const openai = new OpenAIApi(configuration);

const completion = await openai.createChatCompletion({
  model: &quot;gpt-3.5-turbo&quot;,
  messages: [{role: &quot;user&quot;, content: &quot;Hello world&quot;}],
});
console.log(completion.data.choices[0].message);
</code></pre>
","2023-03-14 22:56:50","0","1"
"75799722","1","10003538","75801820","How to deal with stack expects each tensor to be equal size eror while fine tuning GPT-2 model?","<p>I tried to fine tune a model with my personal information. So I can create a chat box where people can learn about me via chat gpt.</p>
<p>However, I got the error of</p>
<blockquote>
<p>RuntimeError: stack expects each tensor to be equal size, but got [47] at entry 0 and [36] at entry 1</p>
</blockquote>
<p>Because I have different length of input</p>
<p>Here are 2 of my sample input</p>
<blockquote>
<p>What is the webisite of ABC company ? -&gt; <a href=""https://abcdef.org/"" rel=""nofollow noreferrer"">https://abcdef.org/</a></p>
</blockquote>
<blockquote>
<p>Do you know the website of ABC company ? -&gt; It is <a href=""https://abcdef.org/"" rel=""nofollow noreferrer"">https://abcdef.org/</a></p>
</blockquote>
<p>Here is what I have tried so far</p>
<pre><code>import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel
from torch.utils.data import Dataset, DataLoader

class QADataset(Dataset):
    def __init__(self, questions, answers, tokenizer, max_length):
        self.questions = questions
        self.answers = answers
        self.tokenizer = tokenizer
        self.max_length = max_length

        # Add a padding token to the tokenizer
        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})

    def __len__(self):
        return len(self.questions)

    def __getitem__(self, index):
        question = self.questions[index]
        answer = self.answers[index]

        input_text = f&quot;Q: {question} A: {answer}&quot;
        input_ids = self.tokenizer.encode(input_text, add_special_tokens=True, max_length=self.max_length, padding=True, truncation=True)

        if input_ids is None:
            return None

        input_ids = torch.tensor(input_ids, dtype=torch.long)
        print(f&quot;Input ids size: {input_ids.size()}&quot;)
        return input_ids

# Set up the tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Load the question and answer data
questions = [&quot;What is the webisite of ABC company ?&quot;, &quot;Do you know the website of ABC company ?&quot;]
answers = [&quot;https://abcdef.org/&quot;, &quot;It is https://abcdef.org/&quot;]

# Create the dataset and data loader
max_length = 64
dataset = QADataset(questions, answers, tokenizer, max_length=max_length)
data_loader = DataLoader(dataset, batch_size=8, shuffle=True)

# Fine-tune the model on the QA dataset
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
criterion = torch.nn.CrossEntropyLoss()

for epoch in range(3):
    running_loss = 0.0
    for batch in data_loader:
        batch = batch.to(device)

        outputs = model(batch, labels=batch)
        loss, _ = outputs[:2]

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f&quot;Epoch {epoch + 1} loss: {running_loss / len(data_loader)}&quot;)

# Save the fine-tuned model
model.save_pretrained(&quot;qa_finetuned_gpt2&quot;)
</code></pre>
<p>I dont have a solid background of AI, it is more like reading references and try to implement it.</p>
","2023-03-21 10:01:28","","2023-04-10 08:03:00","2023-04-10 08:05:32","<python><tensorflow><artificial-intelligence><huggingface-transformers><gpt-2>","2","0","2","275","","2","610569","<p>Yes seems like you didn't pad your inputs. The model expects the size to be the same for each text. So if it's too short, you pad it, and if it's too long, it should be truncated.</p>
<p>See also</p>
<ul>
<li><a href=""https://huggingface.co/docs/transformers/pad_truncation"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/pad_truncation</a></li>
<li><a href=""https://stackoverflow.com/questions/65246703/how-does-max-length-padding-and-truncation-arguments-work-in-huggingface-bertt"">How does max_length, padding and truncation arguments work in HuggingFace&#39; BertTokenizerFast.from_pretrained(&#39;bert-base-uncased&#39;)?</a></li>
<li><a href=""https://ai.stackexchange.com/questions/37624/why-do-transformers-have-a-fixed-input-length"">https://ai.stackexchange.com/questions/37624/why-do-transformers-have-a-fixed-input-length</a></li>
</ul>
<p>Try changing how the tokenizer process the inputs:</p>
<pre><code>
# Define the data loading class
class MyDataset(Dataset):
    def __init__(self, data_path, tokenizer):
        self.data_path = data_path
        self.tokenizer = tokenizer

        with open(self.data_path, 'r') as f:
            self.data = f.read().split('\n')

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        text = self.data[index]
        inputs = self.tokenizer.encode(text, add_special_tokens=True, 
            truncation=True, max_length=80, padding=&quot;max_length&quot;)
        return torch.tensor(inputs)

</code></pre>
","2023-03-21 13:24:11","3","2"
"75931067","1","11669260","75931129","Convert Python Script to Work with ""GPT-3.5-turbo"" model","<p>I have the following python code working for the text-davinci-003</p>
<pre><code>import openai
import time

openai.api_key = &quot;skXXXXXXX&quot;
model_engine = &quot;text-davinci-003&quot;

# Define the prompt for the conversation
prompt = &quot;Conversation with an AI:&quot;

while True:
    # Get the user's input
    user_input = input(prompt + &quot; &quot;)
    
    # Check if the user wants to exit
    if user_input.lower() in [&quot;quit&quot;, &quot;exit&quot;, &quot;bye&quot;]:
        print(&quot;Goodbye!&quot;)
        break
    
    # Generate a response from the OpenAI API
    response = openai.Completion.create(
        engine=model_engine,
        prompt=prompt + &quot; &quot; + user_input,
        max_tokens=1024,
        n=1,
        stop=None,
        temperature=0.7,
    )

    # Print the AI's response
    message = response.choices[0].text.strip()
    print(&quot;AI: &quot; + message)

    # Wait for a bit before continuing
    time.sleep(1)
</code></pre>
<p>For the life of me I can't get it to work with &quot;GPT-3.5-turbo&quot;. I have tried the following code from a github repo but I get errors:</p>
<pre><code>import openai

# load and set our key
openai.api_key = open(&quot;key.txt&quot;, &quot;r&quot;).read().strip(&quot;\n&quot;)

completion = openai.ChatCompletion.create(
  model=&quot;gpt-3.5-turbo&quot;, # this is &quot;ChatGPT&quot; $0.002 per 1k tokens
  messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the circumference in km of the planet Earth?&quot;}]
)

reply_content = completion.choices[0].message.content
print(reply_content)
</code></pre>
<p>But it fails with the error: <code>AttributeError: module 'openai' has no attribute 'ChatCompletion'</code></p>
<p>Can someone kindly help!!!</p>
<p>Thanks.</p>
","2023-04-04 15:15:40","","","2023-04-04 15:22:56","<python><chat><openai-api><gpt-3>","1","0","0","693","","2","2121074","<p>It sounds like you might have an old version of the OpenAI package. You could try this:</p>
<pre><code>pip install --upgrade openai
</code></pre>
","2023-04-04 15:22:56","1","3"
"75935538","1","4761307","75937188","OpenAI GPT-3 API error: ""AttributeError: 'builtin_function_or_method' object has no attribute 'text'""","<p>I'm looking for some help in extracting the &quot;text&quot; from ChatGPT's &quot;openai.Completion.create&quot; function.</p>
<p>This the function I'm using to generate the &quot;response&quot;:</p>
<pre><code>#Have ChatGPT generate keywords from article
def generate_keywords(article):
    response = openai.Completion.create(
        model=&quot;text-davinci-003&quot;,
        prompt=article,
        temperature=0.7,
        max_tokens=60,
        top_p=1.0,
        frequency_penalty=0.0,
        presence_penalty=1
    )
    return response
#---
</code></pre>
<p>&quot;article&quot; in this case is the text I am feeding ChatGPT.</p>
<p>&quot;response&quot; when printed, provides me with the following output:</p>
<pre><code>{
  &quot;choices&quot;: [
    {
      &quot;finish_reason&quot;: &quot;length&quot;,
      &quot;index&quot;: 0,
      &quot;logprobs&quot;: null,
      **&quot;text&quot;: &quot;, Iraq 2008. Image by Flickr User VABusDriverNow, I can\u2019t speak for all of us, but I know that after the war we were still running. Running from our pasts, guilt, shame, fear, and the unexplainable anger that comes with being a&quot;**
    }
  ],
  &quot;created&quot;: 1680666103,
  &quot;id&quot;: &quot;cmpl-71oJjQfWtHlTbcVsyfi7zzJRktzVT&quot;,
  &quot;model&quot;: &quot;text-davinci-003&quot;,
  &quot;object&quot;: &quot;text_completion&quot;,
  &quot;usage&quot;: {
    &quot;completion_tokens&quot;: 60,
    &quot;prompt_tokens&quot;: 1090,
    &quot;total_tokens&quot;: 1150
  }
}
</code></pre>
<p>I want to extract the &quot;text&quot; from this data structure.</p>
<p>When I run this:</p>
<pre><code>keywords = generate_keywords(article)
print(keywords.values.text)
</code></pre>
<p>But I get this:</p>
<pre><code>File &quot;/Users/wolf/Development/OpenAI/generate_medium_story_image/generate_AI_image.py&quot;, line 63, in &lt;module&gt;
    print(keywords.values.text)
          ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'builtin_function_or_method' object has no attribute 'text'
</code></pre>
","2023-04-05 04:01:44","","2023-04-06 16:10:54","2023-04-06 16:10:54","<python><openai-api><gpt-3>","1","1","0","194","","2","10347145","<p>Return just the <code>text</code> from the completion like this:</p>
<pre><code>def generate_keywords(article):
    response = openai.Completion.create(
        model = 'text-davinci-003',
        prompt = article,
        temperature = 0.7,
        max_tokens = 60,
        top_p = 1.0,
        frequency_penalty = 0.0,
        presence_penalty = 1
    )
    return response['choices'][0]['text'] # Change this
</code></pre>
<p>Then just print <code>keywords</code> like this:</p>
<pre><code>keywords = generate_keywords(article)
print(keywords)
</code></pre>
","2023-04-05 08:20:35","0","2"
"75969974","1","1335473","75977040","OpenAI GPT-3 API: Why do I get an unexpected response?","<p>I am connecting to the GPT-3 API through a Jupyter Notebook. This is the code:</p>
<pre><code>import openai
import os

# Set up your API key
openai.api_key = os.environ[&quot;OPENAI_API_KEY&quot;]

# Choose the API endpoint
model_engine = &quot;davinci&quot;

# Create a prompt
prompt = &quot;Hello, ChatGPT!&quot;

# a temperature of 0.5 returns gibberish
# Generate a response
response = openai.Completion.create(
    engine = model_engine,
    prompt = prompt,
    max_tokens = 1024,
    temperature = 0.5,
    frequency_penalty = 0.5,
    presence_penalty = 0.5
)

# Print the response
print(response.choices[0].text)
</code></pre>
<p>Attempting to debug the code led to me playing around with the <code>temperature</code>, <code>frequency_penalty</code> and <code>presence_penalty</code>. I figure I'm doing something wrong if I can't make it work with such a simple prompt.</p>
<p>If you want an example of the unexpected responses I am getting from the simple prompt above, here is the beginning of a few of them:</p>
<blockquote>
<p>I’m here to review a product that I was sent for free. This is not a
paid advertisement and all opinions are my own. I have been using the
new Bamboo Pen &amp; Touch tablet from Wacom for about a month now and I
have to say that I am very impressed with this product! The Bamboo is
a tablet designed for the everyday user, whether you are an artist or
just someone who likes to sketch on the computer. It’s also great for
people like me who use their tablets primarily for writing.</p>
</blockquote>
<blockquote>
<p>ChatGPT is a chat bot powered by Google Assistant. It can handle up to
10,000 messages per month for free and more if you pay for premium
services. This bot can be used for customer support, sales and
marketing, human resources, and more. In this tutorial, I will show
you how to create a chatbot using ChatGPT with PHP. We will use
Laravel as a backend framework. If you don’t have an account at
ChatGPT yet, sign up here first.</p>
</blockquote>
<p>I was expecting a simple &quot;Hi, how can I assist&quot;. What's incorrect here?</p>
","2023-04-09 10:26:58","","2023-04-15 10:10:21","2023-04-15 10:10:21","<python><jupyter-notebook><openai-api><gpt-3>","1","0","-4","117","","2","10347145","<p>You are using a very old GPT-3 <code>davinci</code> model. The performance of the OpenAI API is model-related. Newer models are more capable.</p>
<ul>
<li><code>text-davinci-003</code> &lt;-- use this one</li>
<li><code>text-davinci-002</code></li>
<li><code>davinci</code></li>
</ul>
","2023-04-10 12:16:40","0","1"
"76102276","1","10759664","76105153","Disable layers in GPT-2 model","<p>I'm currently using a GPT-2 model that was trained on German texts. I would like to generate the next word in a text given a context chunk, but instead of using the whole model to predict the next word, I want each of the 12 layers to predict the next word separately, so I get 12 predictions for the next word. Put differently, I want to &quot;lesion&quot; all layers except for one, so they are not involved in the prediction of the next word at all.</p>
<p>This is my model:</p>
<pre><code># import modules
from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForCausalLM
import torch

# download pre-trained German GPT-2 model &amp; tokenizer
tokenizer = AutoTokenizer.from_pretrained(&quot;dbmdz/german-gpt2&quot;)

# initialise the model
model = AutoModelForCausalLM.from_pretrained(&quot;dbmdz/german-gpt2&quot;, pad_token_id = tokenizer.eos_token_id)
</code></pre>
<p>And here's an example of a context chunk:</p>
<pre><code>input_text = &quot;Orlando liebte von Natur aus einsame Orte, weite Ausblicke und das Gefühl, für immer und ewig&quot; # correct next word: &quot;allein&quot;
</code></pre>
<p>I thought maybe I could set all attention weights to 0 in the layers I want to exclude, but I don't have a clue if that's correct and how to modify the weights in the model. Does anyone have an idea how to solve this &amp; could explain what I need to do? I've never used GPT2 before, so this is super confusing for me.</p>
<p>Thanks in advance for your help / any ideas!</p>
","2023-04-25 14:24:00","","","2023-04-25 20:23:46","<python><nlp><gpt-2>","2","0","1","78","","2","1949646","<p>This is technically possibly, but probably won't give you anything useful in understanding your network. You can think of a network like this as computing
y = layer(layer(... (layer(layer(x,theta[0]),theta[1]) ...),theta[n-2]),theta[n-1]), where theta[i] are the weights of the ith layer. Setting the weights for a particular layer to 0 would make the input to layer i+1 garbage. There are residual connections between layers, so maybe something non-garbage would happen, but I wouldn't trust it to be useful.</p>
<p>Nonetheless, if you want to see what happens when you zero out all the weights for a layer, you could set weights to 0 using the model's state_dict</p>
<pre><code>from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForCausalLM
import torch
import re

# download pre-trained German GPT-2 model &amp; tokenizer
tokenizer = AutoTokenizer.from_pretrained(&quot;dbmdz/german-gpt2&quot;)

# initialise the model
model = AutoModelForCausalLM.from_pretrained(&quot;dbmdz/german-gpt2&quot;, pad_token_id = tokenizer.eos_token_id)

input_text = [&quot;Orlando liebte von Natur aus einsame Orte, weite Ausblicke und das Gefühl, für immer und ewig&quot;, # correct next word: &quot;allein&quot;
              &quot;Wo sich Fuchs und Hase gute Nacht&quot;, # correct next word sagen.
              ]
prompt = [torch.tensor(tokenizer.encode(s)).unsqueeze(0) for s in input_text]

ngenerate = 20

sample_output0 = [tokenizer.decode(model.generate(s,max_length=s.shape[-1]+ngenerate)[0,:]) for s in prompt]
print('\n***Before zeroing***')
for i,s in enumerate(sample_output0):
  print(f'{i}: {s}\n')

# zero-out layer 5

layeri = 5

# find weight names for this layer, will include the string 'transformer.h5.'
paramnames = filter(lambda s: re.search(f'transformer.h\.{layeri}\.',s) is not None,model.state_dict().keys())

# set these weights to 0
for paramname in paramnames:
  w = model.state_dict()[paramname]
  if w.ndim &gt; 0:
    w[:] = 0

# generate some sample output
print('\n***After zeroing***')
sample_output1 = [tokenizer.decode(model.generate(s,max_length=s.shape[-1]+ngenerate)[0,:]) for s in prompt]
print('Before zeroing')
for i,s in enumerate(sample_output1):
  print(f'{i}: {s}\n')
</code></pre>
<p>The output of this is:</p>
<pre><code>***Before zeroing***
0: Orlando liebte von Natur aus einsame Orte, weite Ausblicke und das Gefühl, für immer und ewig in der Nähe zu sein.
Er war ein großer Künstler, ein Künstler, der sich in der

1: Wo sich Fuchs und Hase gute Nacht sagen.
Die beiden sind seit Jahren befreundet.
Sie sind ein Paar.
Sie sind ein


***After zeroing***
Before zeroing
0: Orlando liebte von Natur aus einsame Orte, weite Ausblicke und das Gefühl, für immer und ewig zu sein.
Die Natur ist ein Paradies für sich.
Die Natur ist ein Paradies für sich

1: Wo sich Fuchs und Hase gute Nacht, die Sonne, die Sonne, die Sonne, die Sonne, die Sonne, die Sonne, die
</code></pre>
","2023-04-25 20:23:46","0","2"
"76173414","1","10311672","76173794","OpenAI GPT-3 API: What is the difference between davinci and text-davinci-003?","<p>I'm testing the different models for OpenAI, and I noticed that not all of them are developed or trained enough to give a reliable response.</p>
<p>The models I tested are the following:</p>
<pre><code>model_engine = &quot;text-davinci-003&quot;
model_engine = &quot;davinci&quot; 
model_engine = &quot;curie&quot; 
model_engine = &quot;babbage&quot; 
model_engine = &quot;ada&quot; 
</code></pre>
<p>I need to understand what the difference is between <code>davinci</code> and <code>text-davinci-003</code>, and how to improve the responses to match that response when you use ChatGPT.</p>
","2023-05-04 12:51:25","","2023-05-05 21:16:38","2023-05-05 22:22:46","<openai-api><gpt-3>","2","0","2","1519","","2","10347145","<p>TL;DR</p>
<ul>
<li><code>text-davinci-003</code> is the newer and more capable model</li>
<li><code>text-davinci-003</code> supports a longer context window (i.e., 4097 tokens)</li>
<li><code>text-davinci-003</code> was trained on a more recent dataset</li>
<li><code>gpt-3.5-turbo</code> and <code>gpt-4</code> are even more capable than <code>text-davinci-003</code></li>
</ul>
<p>As stated in the official <a href=""https://help.openai.com/en/articles/6643408-how-do-davinci-and-text-davinci-003-differ"" rel=""nofollow noreferrer"">OpenAI article</a>:</p>
<blockquote>
<p>While both <code>davinci</code> and <code>text-davinci-003</code> are powerful <a href=""https://platform.openai.com/docs/models"" rel=""nofollow noreferrer"">models</a>, they
differ in a few key ways.</p>
<p><code>text-davinci-003</code> is the newer and more capable model, designed
specifically for <a href=""https://openai.com/research/instruction-following"" rel=""nofollow noreferrer"">instruction-following</a> tasks. This enables it to
respond concisely and more accurately - even in zero-shot scenarios,
i.e. without the need for any examples given in the prompt. <code>davinci</code>,
on the other hand, can be fine-tuned on a specific task, which can
make it very effective if you have access to at least a few hundred
training examples.</p>
<p>Additionally, <code>text-davinci-003</code> supports a longer context window (max
prompt+completion length) than davinci - 4097 tokens compared to
<code>davinci</code>'s 2049.</p>
<p>Finally, <code>text-davinci-003</code> was trained on a more recent dataset,
containing data up to June 2021. These updates, along with its support
for <a href=""https://platform.openai.com/docs/guides/completion/inserting-text"" rel=""nofollow noreferrer"">Inserting text</a>, make <code>text-davinci-003</code> a particularly versatile and
powerful model we recommend for most use-cases.</p>
</blockquote>
<p>Use <code>text-davinci-003</code> because other models you mentioned in your question are less capable.</p>
<p>ChatGPT uses <code>text-davinci-002</code> at the moment for non-subscribed users. If you buy a ChatGPT Plus subscription, you can also use <code>gpt-3.5-turbo</code> or <code>gpt-4</code>. So, to get similar responses as you get from ChatGPT, it depends on whether you are subscribed or not. For sure, <code>gpt-3.5-turbo</code> and <code>gpt-4</code> are even more capable than <code>text-davinci-003</code>.</p>
","2023-05-04 13:32:39","0","1"
"76197173","1","15246353","76197360","Chrome Extension with Chat GPT-3.5 - ""you must provide a model parameter""","<p>I am making a chrome extension that uses Chat GPT 3.5 and have coded a simple prompt to send to the API using openai api and returns a value in the console.</p>
<p>I have my code below and keep getting this error...</p>
<pre><code>error: 
    code: null
    message: &quot;you must provide a model parameter&quot;
    param: null
    type: &quot;invalid_request_error&quot;
</code></pre>
<p>event though i have a model parameter.</p>
<pre><code>// Define the API key
const API_KEY = &quot;API KEY&quot;;

// Define the endpoint URL
const endpointUrl = &quot;https://api.openai.com/v1/chat/completions&quot;;

// Define the headers
const headers = {
  &quot;Content-Type&quot;: &quot;application/json&quot;,
  Authorization: `Bearer ${API_KEY}`,
};

// Define the maximum number of completions to return
const maxCompletions = 1;

// Define the prompt to send to the API
const prompt = {
   model: &quot;gpt-3.5-turbo&quot;,
   prompt: &quot;Hello, world!&quot;,
   temperature: 0.5,
};

// Send a POST request to the endpoint with the prompt and headers
fetch(endpointUrl, {
  method: &quot;POST&quot;,
  headers,
  body: JSON.stringify({
    prompt,
    max_completions: maxCompletions,
}),
})
  .then((response) =&gt; response.json())
  .then((data) =&gt; {
    // Log the response data to the console
    console.log(data);
  })
  .catch((error) =&gt; {
    console.error(error);
  });
</code></pre>
","2023-05-08 02:29:40","","","2023-05-08 03:35:25","<javascript><google-chrome-extension><openai-api><gpt-3>","1","1","3","320","","2","21148174","<p>I used your code and experienced the same error. I investigated the network request and saw that the payload was malformed:</p>
<pre><code>{
   prompt: {
      model: &quot;gpt-3.5-turbo&quot;, 
      prompt: &quot;Hello, world!&quot;, 
      temperature: 0.5}, 
   max_completions: 1
}
</code></pre>
<p>So, as far as ChatGPT's api is concerned, you're only sending <code>prompt</code> and <code>max_completions</code>. The reason you request is formed this way is because you're passing an object filled with other objects into <code>JSON.stringify()</code>.</p>
<p>Also, I'm not sure where you're getting the <code>max_completions</code> property from, as it is not in the API doc, so I left that out. Here is the change you need to make:</p>
<pre><code>// in your fetch call:
fetch(endpointUrl, {
  method: &quot;POST&quot;,
  headers,
  body: JSON.stringify(prompt), // prompt is an object, so no need to wrap it in another object.
}).then...
</code></pre>
<p>Another issue is that you're calling the <a href=""https://platform.openai.com/docs/api-reference/chat/create"" rel=""nofollow noreferrer"">create chat completion</a> endpoint, but the properties you are sending are not correct. You are required to send:</p>
<ul>
<li><strong>model</strong>: string</li>
<li><strong>messages</strong>: [{role: string, content: string}]</li>
</ul>
<p>So, you would also need to make an edit here:</p>
<pre><code>// in your prompt variable:
const prompt = {
   model: &quot;gpt-3.5-turbo&quot;,
   messages: [{role: &quot;user&quot;, content: &quot;Hello, World!&quot;}], // change prompt to messages array
   temperature: 0.5,
};
</code></pre>
<p>Cheers!</p>
","2023-05-08 03:35:25","1","1"
"76482024","1","10760045","76483595","How to get more detailed results sources with Langchain","<p>I am trying to put together a simple &quot;Q&amp;A with sources&quot; using Langchain and a specific URL as the source data. The URL consists of a single page with quite a lot of information on it.</p>
<p>The problem is that <code>RetrievalQAWithSourcesChain</code> is only giving me the entire URL back as the source of the results, which is not very useful in this case.</p>
<p>Is there a way to get more detailed source info?
Perhaps the heading of the specific section on the page?
A clickable URL to the correct section of the page would be even more helpful!</p>
<p>I am slightly unsure whether the generating of the <code>result source</code> is a function of the language model, URL loader or simply <code>RetrievalQAWithSourcesChain</code> alone.</p>
<p>I have tried using <code>UnstructuredURLLoader</code> and <code>SeleniumURLLoader</code> with the hope that perhaps more detailed reading and input of the data would help - sadly not.</p>
<p>Relevant code excerpt:</p>
<pre><code>llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')
chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=VectorStore.as_retriever())

result = chain({&quot;question&quot;: question})

print(result['answer'])
print(&quot;\n Sources : &quot;,result['sources'] )
</code></pre>
","2023-06-15 11:31:53","","2023-06-15 13:52:57","2023-06-15 16:12:45","<python><openai-api><gpt-3><langchain><chatgpt-api>","1","2","1","329","","2","2392087","<p>ChatGPT is very flexible, and the more explicit you are better results you can get. <a href=""https://python.langchain.com/en/latest/reference/modules/chains.html#langchain.chains.ConstitutionalChain"" rel=""nofollow noreferrer"">This link show the docs for the function you are using</a>. there is a parameter for langchain.prompts.BasePromptTemplate that allows you to give ChatGPT more explicit instructions.</p>
<p>It looks like the base prompt template is this</p>
<blockquote>
<p>Use the following knowledge triplets to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n{context}\n\nQuestion: {question}\nHelpful Answer:</p>
</blockquote>
<p>You can add in another sentence giving ChatGPT more clear instructions</p>
<blockquote>
<p>Please format the answer with JSON of the form { &quot;answer&quot;: &quot;{your_answer}&quot;, &quot;relevant_quotes&quot;: [&quot;list of quotes&quot;] }. Substitutde your_answer as the answer to the question, but also include relevant quotes from the source material in the list.</p>
</blockquote>
<p>You may need to tweak it a little bit to get ChatGPT responding well. Then you should be able to parse it.</p>
<p>ChatGPT has 3 message types in the API</p>
<ul>
<li>User - a message from an end user to the model</li>
<li>model - a message from the model to the end user</li>
<li>system - a message from the prompt engineer to model to add instructions. Lang chain doesn't use this since it's a one-shot prompt</li>
</ul>
<p>I strongly recommend these <a href=""https://www.deeplearning.ai/short-courses/"" rel=""nofollow noreferrer"">courses</a> on ChatGPT since they are from Andrew Ng and very high quality.</p>
","2023-06-15 16:12:45","1","0"