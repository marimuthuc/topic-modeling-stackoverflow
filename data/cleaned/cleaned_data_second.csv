Post Link,PostTypeId,OwnerUserId,Answer Link,Title,Body,CreationDate,ClosedDate,LastEditDate,LastActivityDate,Tags,AnswerCount,CommentCount,Score,ViewCount,FavoriteCount,PostTypeId.1,OwnerUserId.1,Body.1,CreationDate.1,CommentCount.1,Score.1
75801940,1,19336351.0,,Unable to get word by word response from GPT API,try response gpt api word word like chatgpt generate not things work get response expect just not chunk able print partial response console unable ui help backend code fetch frontend,2023-03-21 13:36:31,,,2023-03-25 17:14:24,<node.js><reactjs><openai-api><gpt-3><chatgpt-api>,0,0,0,293,,,,,,,
75807326,1,9050016.0,,How to prepare dataset with multiple answers for single question to train the GPT3/davinci model,try finetune gpt model  columns context question answer multiple answer a question repeat question text multiple answer best way prepare optimize dataset let know put correct manner think have a separator differentiate multiple answer a single question,2023-03-21 23:50:31,,2023-03-21 23:55:13,2023-03-22 09:56:29,<python><nlp><data-mining><gpt-3><large-language-model>,0,0,0,33,,,,,,,
75807664,1,3944252.0,,"Issues Handling ChatGPT Streaming Response in Terminal using OpenAI API - Using Python, rich library",try integrate openai api model terminal enable chatgpt objective receive stream responses chatgpt print terminal successfully print entire response without stream face issue stream responses specifically function print word a new line not desire behavior use rich library handle markups code print like suggest a solution fix issue pretty new python,2023-03-22 01:09:17,,,2023-03-22 02:19:56,<python><openai-api><rich><gpt-4>,1,0,-2,1292,,,,,,,
75811293,1,21455152.0,,"OpenAI GPT-3 API error: ""InvalidRequestError: Resource not found""",try upload a json file use fine tune gpt model error try upload run command above follow error,2023-03-22 10:51:16,,2023-03-23 17:41:44,2023-03-23 17:50:39,<python><openai-api><gpt-3>,1,0,-1,2093,,,,,,,
75818642,1,19977480.0,,"Finetuning gpt2, validation loss increases with accuracy and f1 score",finetuning gpt text classification huggingface trainer observe  epochs validation loss start increase validation accuracy f score increase try  different seed observe effect know overfitting perform early stop graph validation loss accuracy expect validation loss increase a plateau a drop accuracy f score not a case,2023-03-23 01:48:21,,,2023-03-23 01:48:21,<text-classification><gpt-2><overfitting-underfitting>,0,0,0,83,,,,,,,
75840719,1,13103348.0,,Getting missing pandas error while trying to fine-tune GPT3,use follow command get follow error try reinstall use do not work try install pandas manually do not work resolve error,2023-03-25 09:19:15,,2023-03-25 09:43:49,2023-06-20 10:19:59,<openai-api><gpt-3><fine-tune>,2,1,2,511,,,,,,,
75856110,1,21500678.0,,How to fine-tune gpt2 with a custom set of unlabelled document,newbie gpt finetuning goal finetune gpt bert a set document order able query bot a topic contain document receive answer doubt develop saw fine tune a question answer chatbot require a label dataset contain question relatet a answer possible fine tune a language model unlabelled dataset train model data query a need finetune a specific task use annotate dataset a minumum number document order achieve good result possible a nonenglish language thank,2023-03-27 13:06:09,,2023-03-27 14:00:38,2023-03-27 14:29:19,<bert-language-model><transfer-learning><openai-api><gpt-2><fine-tune>,1,0,0,373,,,,,,,
75863060,1,11155486.0,,Time and cost to train Distill GPT-2 model on BookCorpus using AWS EC2,try calculate time train a distill gpt model bookcorpus dataset use multiple ec instance purpose language model method calculate train time language model,2023-03-28 06:40:06,,,2023-03-28 06:40:06,<amazon-ec2><nlp><gpt-2><distributed-training><nlg>,0,0,0,36,,,,,,,
75864319,1,21508401.0,,"import Image as PIL_Image ModuleNotFoundError: No module named 'Image' while running langchain with DirectoryLoader('source', glob='*.pdf')",just code error not error try pip install image not work help greatly appreciate work langchain documentloaders time directoryloader class suppose work case,2023-03-28 09:09:22,,2023-03-28 09:23:56,2023-03-28 09:23:56,<python-3.x><openai-api><gpt-3><gpt-4><langchain>,0,1,2,175,,,,,,,
75874606,1,649994.0,,"Error: PineconeClient: Project name not set, v0.0.10",download github mayooeargptpdfchatbotlangchain gpt amp langchain chatbot large pdf docs try run app follow error node version node v v pinecone version issue fetch experimental feature disable fetch use nodefetch,2023-03-29 08:02:07,,2023-03-29 15:55:21,2023-04-07 18:42:02,<openai-api><gpt-3><chatgpt-api><gpt-4><langchain>,1,0,2,208,,,,,,,
75878060,1,,,Langchain - Multiple input SequentialChain,experience langchain question not relevant trouble example documentation actually far understand sequentialchain receive input chain fee output n chain n chain let say work  chain input snippet a csv file description about csv come input snippet csv file output chain produce a python script output no sequential version work,2023-03-29 13:42:52,,2023-04-03 18:24:09,2023-04-03 18:24:09,<gpt-3><langchain>,0,0,1,1311,,,,,,,
75889488,1,21528260.0,,I never get embedded files loaded with from langchain.document_loaders import DirectoryLoader,code langchaindocument_loaders import textloader a single txt file work directoryloader nothing attach image code attach image contect texts enter image description begin give errors permissions not know happen change permissions store folder run code stay load time really appreciate help need code embed word text file store folder do not like capture,2023-03-30 14:26:29,,,2023-03-30 14:26:29,<python><openai-api><gpt-3><langchain>,0,1,0,555,,,,,,,
75889941,1,2292490.0,,Give GPT (with own knowledge base) an instruction on how to behave before user prompt,give gpt information csv format learn now like transmit instruction behave user prompt message_history look like get follow error remember convert tuples try cause chaos code,2023-03-30 15:09:19,,,2023-03-30 15:09:19,<python><prompt><openai-api><gpt-3><chatgpt-api>,0,3,1,420,,,,,,,
75901665,1,1935541.0,,TextCompleition Latency with Large Prompts - How to Avoid?,experiment forth text completion chat completion build interactive ai text completion ai follow instructions better a number message add prompt about  forth sentence  char latency start increase token usage less important notice able use text completion long conversations able without get a major latency hit do need intermediate step summarize previous conversation carry message request,2023-03-31 18:28:23,,,2023-03-31 18:28:23,<openai-api><gpt-3><fine-tune><gpt-4>,0,0,0,70,,,,,,,
75905776,1,21444092.0,,Questions about masks of padding in GPT,gpt series model use decoder transformer unidirectional attention source code gpt hug face implementation mask attention default attention_mask none gpt demo attention_mask derive valid lengths not assign pad tokens not mask attention just ignore loss computation correct mask pad attention do not matter final result wonder embed pad token change train,2023-04-01 11:01:05,,2023-04-02 04:00:34,2023-04-02 04:00:34,<huggingface-transformers><attention-model><gpt-2><zero-padding>,0,2,1,211,,,,,,,
75906140,1,20601880.0,,word to word gpt api responce stream in react native,a way implement gpt api word word api try implement javascript directly app not work want use chat gpt turbo api directly react native expo word word stream work example without stream,2023-04-01 12:17:21,,,2023-06-19 23:29:59,<react-native><openai-api><gpt-3><chatgpt-api><gpt-4>,0,3,0,272,,,,,,,
75906161,1,21346793.0,,In which form should be dataset in NLP model?,try make finetuning model tinkoffairudialogptmedium form dataset base generation form первый person второй second person dialogue try make finetuning json like generation answer bad,2023-04-01 12:22:47,,,2023-04-01 12:22:47,<machine-learning><gpt-2>,0,0,1,17,,,,,,,
75945693,1,2672447.0,,how to determine the expected prompt_tokens for gpt-4 chatCompletion,follow nodejs code get prompt_tokens  response want able determine expect prompt_tokens prior make request like model way encode best lib python tiktoken estimate prompt_tokens need pass text value script not sure use text python script message above nodejs print token_count  actual prompt_tokens response,2023-04-06 03:58:46,,2023-04-06 20:13:40,2023-04-06 20:47:39,<openai-api><chatgpt-api><gpt-4>,1,0,0,694,,,,,,,
75966973,1,21597554.0,,ChatBot - Trouble using custom gpt_index and langchain libraries for creating a GPT-3 based search index,fyi try build a chatbot base instructions give dan shipper try use custom libraries call gpt_index langchain create a gpt base search index use openai api successfully instal libraries follow code btw use google colab environment construct_index function path document follow error a mismatch expect arguments class provide arguments code unfortunately documentation examples custom libraries help understand correctly initialize gptsimplevectorindex class resolve error guidance use libraries greatly appreciate thank run google colab error,2023-04-08 18:20:39,,2023-04-09 17:02:12,2023-06-23 10:29:07,<python><openai-api><gpt-3>,1,0,1,1021,,,,,,,
75979815,1,21610884.0,,How to add 'message history' to llama-index based GPT-3 in Python,fairly new use llamaindex library train gpt use chatgpt standard api python notice standard chatgpt api simply follow code chatgpt message history context now use llamaindex library train gpt a specific context not know model consider message_history a code currently work dont know implement message history,2023-04-10 18:45:19,,,2023-04-24 14:10:47,<python><openai-api><gpt-3><llama-index><gpt-index>,0,0,6,1250,,,,,,,
75979901,1,20678352.0,,"how to fix ""KeyError: 0"" in the hugging face transformer train() function",hello guy die need help try finetune gptmeduim model hug face transformer run error just want start train keyerror  code beleive error originate section ide underline statement produce keyerror  deos not provide about error apart keyerror traceback recent  trainertrain  frame usrlocallibpythondistpackagestorchutilsdata_utilsfetchpy   data selfdatasetgetitems possibly_batched_index   data selfdataset idx idx possibly_batched_index   data selfdataset possibly_batched_index keyerror  try change train_arguements not work totally ideas error not explicit,2023-04-10 19:02:35,,,2023-04-10 19:02:35,<machine-learning><nlp><huggingface-transformers><gpt-2><text-generation>,0,2,0,197,,,,,,,
75999769,1,21490540.0,,Twitch Chat Bot program not responding,try make a python program take twitch chat input use gpt generate response say response use pyttsx library run program neither respond chat nor show error not able tell program actually connect twitch not code think asyncio cause problem try change  line do not work honestly no idea cause problem help,2023-04-12 20:56:50,,,2023-04-12 20:56:50,<python><chatbot><twitch><openai-api><gpt-3>,0,0,0,57,,,,,,,
76010864,1,15138014.0,,How can I train GPT-3 with my own company data using OpenAI's API?,want train gpt company data perform specific nlp task use openai api train gpt model data kind data preprocessing need perform train model python libraries frameworks help data preprocessing train process use openai api finetune model specific nlp task need train model separately best practice train gpt custom data use openai api research openai api read documentation train gpt custom data unsure about specific step require train gpt company data use openai api expect learn about data preprocessing step python libraries frameworks assist train process additionally like know use openai api finetune model specific nlp task need train model separately look best practice recommendations train gpt custom data use openai api,2023-04-14 01:02:24,,,2023-05-31 13:46:04,<openai-api><data-preprocessing><gpt-3>,1,2,2,937,,,,,,,
76014800,1,10562928.0,,"OpenAI GPT-3 API error: ""Cannot find module '@openai/api'""",have trouble use openai api nodejs specifically try use openaicompletion object get a error try instal openaiapi package use a  error indicate package not remove reinstall no luck try upgrade latest version nodejs currently  issue stuborn create a test script testjs follow code run script follow error test openai api use vsc thunder client work post request use receive follow response help understand cause error provide step try figure api not work solutions test try thank,2023-04-14 11:50:53,,2023-04-15 10:20:49,2023-04-15 10:20:49,<javascript><node.js><openai-api><gpt-3>,2,0,1,449,,,,,,,
76025799,1,15764986.0,,Create multi-message conversations with the GPT API,experiment gpt api openai learn use gptturbo model a quickstart example web necessary import api key endpoint define above code block add inputtext variable text input infinite loop inputresponse cycle go program terminate probably bad practice notice responses api not able reference previous part conversation like chatgpt web application rightfully not mention form conversation object look api documentation chat completion conversation request example follow mean send inputted message conversation a response a way least describe api send a message send message format a conversation reference previous message like a chatbot describe chatgpt app way implement above do not use openai python module use request json modules,2023-04-16 03:42:43,,2023-04-16 10:33:32,2023-05-15 18:40:14,<python><python-requests><openai-api><gpt-3><chatgpt-api>,1,0,1,961,,,,,,,
58195745,1,9389353.0,,Generate text from input on default model gpt-2-simple python,not figure life generate text default model feed a prefix download model code run follow error idea do wrong,2019-10-02 05:36:51,,2020-11-29 12:07:49,2020-11-29 12:07:49,<python><tensorflow><gpt-2>,1,0,1,1688,,,,,,,
64312421,1,1157814.0,,"OpenAI API and GPT-3, not clear how can I access or set up a learning/dev?",read tons gpt sample come cross code sample none mention run play code especially not mention not do research conclude not wrong no way run thing onpremises a dev machine a host service definition now oct th  openai api invite beta do miss,2020-10-12 05:59:30,,2023-01-19 04:31:06,2023-01-19 04:31:06,<nlp><openai-api><gpt-3>,2,0,4,1926,0.0,,,,,,
71376760,1,1165643.0,,How to output the list of probabilities on each token via model.generate?,right now print generate text want list n tokens step probability n a number specify similar openai beta playground select probabilities spectrum example prompt now a token say like vampire  corpse  easiest way huggingface transformers,2022-03-07 05:28:50,,2023-01-19 04:33:35,2023-01-19 04:33:35,<python><nlp><huggingface-transformers><gpt-3>,2,0,0,3655,,,,,,,
71303277,1,18339450.0,,How to remove input from from generated text in GPTNeo?,write a program generate text need remove input generate text code strs contain input give remove,2022-03-01 03:03:51,,2022-12-13 15:41:24,2022-12-13 15:41:24,<huggingface-transformers><gpt-2>,1,0,2,802,,,,,,,
75701297,1,8459132.0,,Not enough memory for fine tuning LLM with Hugging Face,run runtime errors not memory fine tune a pretrained llm a novelist curious happen fine tune a pretrained llm write chapters novel style successfully run a tutorial fine tune a bert model hug face a yelp dataset smaller yesterday cpu gb ram not nvidia gpu not sure error arise now things try give a runtime memory error change model neo gpt gpt smaller decrease batch size hyperparameter decrease max length tokens decrease dataset size code error readout novel save docx run code just save chapter try split chapter paragraph sentence make tokens smaller do not help do indicate really need nvidia gpu run machine learn task likely issue dataset setup code thank,2023-03-10 21:50:32,,,2023-03-10 21:50:32,<machine-learning><pytorch><huggingface><gpt-2>,0,0,0,362,,,,,,,
75753390,1,21409617.0,,how to create prompt and completion for numerical dataset in GPT3 model,try customize gpt  model sales domain possible fine tune gpt model use a dataset numerical categorical columns create prompt completion particular dataset create prompt completion dataset numerical categorical column,2023-03-16 07:53:02,,,2023-03-16 07:53:02,<prompt><completion><gpt-3>,0,0,0,94,,,,,,,
76091454,1,16861522.0,,How can I improve my ChatGPT API prompts?,have issue chatgpt api relate prompt engineer a dataset consist individual product title product descriptions awful design do not control need create aggregate title individual title finetuned curie model data a similar way prompt completion finetuned about  humanwritten title currently use settings vary no great effect wonder go wrong get responses like,2023-04-24 11:37:33,,2023-04-24 11:39:17,2023-04-24 11:39:17,<prompt><openai-api><gpt-3><chatgpt-api><gpt-4>,0,2,0,264,,,,,,,
76100128,1,17319114.0,,How to stop GPT-3.5-Turbo model from generating text (azure)?,use case use openai model host azure try generate a list senteces word a specific length let prompt example textdavinci model complete list expect stop gptturbo model generate tokens token limit reach tell model stop task use shoot prompt do not work hacky workarounds use a low value max_tokens hard estimate value part prompt change dynamically application need postprocessing remove waste tokens a counter examples use a specific number stop sequence use a general counter like above need ensure stop sequence wo not generate accidentally model stop use unusual counter like   a chance model malforms stop sequence generate limit reach a clean easy solution let model stop generate like textdavinci do,2023-04-25 10:24:01,,2023-04-26 10:38:20,2023-04-28 17:53:00,<azure><gpt-3><azure-openai><text-davinci-003>,1,0,2,377,,,,,,,
76100892,1,282855.0,,GPT4 - Unable to get response for a question?,title clearly describe issue experience not able a response a question dataset use use nomicaigptall execution simply stop no exception occur overcome situation ps use offline mode need process a bulk question ps issue occur a prevent generate a response give question,2023-04-25 11:57:01,,2023-04-29 21:06:24,2023-04-29 21:06:55,<gpt-3><chatgpt-api><gpt-4><gpt4all><pygpt4all>,1,0,1,515,,,,,,,
76169951,1,20205450.0,,model.bert() through the slicing error can anyone let me know why is it?,error want run code confuse give a index range error,2023-05-04 05:52:46,,,2023-05-04 05:52:46,<python><deep-learning><nlp><bert-language-model><gpt-2>,0,0,0,17,,,,,,,
76283060,1,1081396.0,,Fine-tune Openai GPT - Set general instructions?,want make gpt summarise paragraph far send a prompt like not use finetuned model far like mainly cost less run query suppose faster try train model provide a set prompt paragraph a set completions summaries finetuned model do not work assume do not train model correct way not a way provide finetuning process a set instructions set prompt completions gpt know want paragraph never exceed a certain number word example impression dataset not provide instructions gpt process input just send prompt use train,2023-05-18 17:15:25,,2023-05-18 17:22:04,2023-05-18 17:22:04,<openai-api><gpt-3>,0,0,0,43,,,,,,,
76293205,1,1354514.0,,My gpt2 code generates a few correct words and then goes into a loop of generating the same sequence again and again,follow gpt code sentence completion generate a good sentence end a loop repetitive sentence,2023-05-20 01:28:54,,,2023-05-22 17:55:58,<nlp><stanford-nlp><huggingface-transformers><gpt-2>,1,0,0,49,,,,,,,
76304353,1,15107876.0,,"IndexError: index out of range in self while using GPT2LMHeadModel.from_pretrained(""gpt2"")",work question answer code use pretrained gptlmheadmodel tokenization pass input attention mask model give index error code get error line error size input attention mask shape token less  max gpt problem help,2023-05-22 08:33:26,,,2023-05-22 08:33:26,<python><index-error><gpt-2>,0,0,0,54,,,,,,,
76380787,1,7006465.0,,Kotlin code to openapi call not working beyond building jsonObjectRequest,make a kotlin code openai api gptturbo use a valid my_token auth code debug code flow not get inside line val jsonobjectrequest object jsonobjectrequest trid use my_token prompt a simple python script use request lib work fine help kotlin code thank,2023-06-01 10:04:12,,,2023-06-01 10:04:12,<kotlin><openai-api><gpt-3><chatgpt-api>,0,0,0,7,,,,,,,
76381731,1,404348.0,,Use Open AI API in VBA code to answer user query,write a code vba take input user provide answer user query use open ai api json pasre code run not sure go wrong now give error object require json parse line get error object require line help,2023-06-01 12:12:17,,,2023-06-01 12:12:17,<json><vba><openai-api><gpt-3><chatgpt-api>,0,10,-1,62,,,,,,,
76413431,1,22028199.0,,Can't call transcribed text from whisper to openai chatbot,script take input microphone transcribe speech text pass text gpt texdavinci generate response script not generate gpt response use openai whisper asr web app speechtotext gpt textdavinci generate response run google colab thank,2023-06-06 09:55:35,,,2023-06-11 02:00:29,<python><google-colaboratory><openai-api><gpt-3><openai-whisper>,2,0,0,68,,,,,,,
76413465,1,22028890.0,,How to fune-tune and deploy ChatGPT on Cloud?,know finetune chatgpt not able deploy finetuned model servercloud help create finetune model chatgpt not get model systemservercloud,2023-06-06 09:59:06,,,2023-06-06 09:59:06,<openai-api><gpt-3><chatgpt-api><fine-tune><gpt-4>,0,0,0,19,,,,,,,
76526834,1,9904671.0,,Need ideas suggestions on Personality Mimic Chatbot,want make a chatbot clone behaviour habit a person use langchain retriever data a huge database multiple question far go now want add layer change retrived answer example person playful nature want convert answer a playful manner thing think send answer gpt ask increase cost look host huggingface model not specific usecase ideas a better efficient manner thank,2023-06-21 20:15:23,,,2023-06-21 20:15:23,<openai-api><huggingface><langchain><gpt-3>,0,0,0,10,,,,,,,
75787052,1,21436401.0,,GPT 4 API delays/data types,get api beta play app get far get api connection work do want pycharm a couple problems get pretty slow response time hit a usage cap frequently api account sufficiently fund assume improve new product stabilize not switch earlier model use case ask gpt a list items a python list format able typecast actual list set temperature low repetitive items set high not correct python format hit api   time probably consolidate a couple depend consistently get a properly format json response dubious ask a python list basically thing predictable ask a certain data format come format reliably build app suggestionsdiscussion appreciate try try various temperatures ask openai increase usage cap not try model,2023-03-20 06:12:24,,,2023-04-20 16:34:35,<gpt-4>,1,0,0,254,,,,,,,
62362406,1,13370109.0,,Is gpt-2 unusable with python?,follow tutorial run across issue use trainpy issue say search a lot internet turn tensorflowcontrib depreceated alternate way gpt not usable python try,2020-06-13 16:07:04,,2020-11-29 11:52:25,2020-11-29 11:52:25,<python><tensorflow><gpt-2>,1,3,2,1169,,,,,,,
59501673,1,2315835.0,,Tensor Flow issues with Python,struggle gpt tutorial work now have issue tensor flow note a completely clean install windows  x a lenovo thinkpad get follow error try train gpt handle above exception exception occur thoughts,2019-12-27 13:43:38,,2020-11-29 12:06:25,2020-11-29 12:06:25,<python-3.x><tensorflow><windows-10><gpt-2>,1,0,0,116,,,,,,,
67598327,1,11259950.0,,JSONDecodeError: Unexpected UTF-8 BOM (decode using utf-8-sig): line 1 column 1 (char 0) ---While Tuning gpt2.finetune,hope do good work fine tune gpt  model generate title base content work create a simple csv file contain title train model inputting model gpt  fine tune get follow error jsondecodeerror traceback recent  steps  save_every  sample_every step max number train step   gptgenerate sess,2021-05-19 06:57:50,,,2021-05-26 22:50:14,<utf-8><byte-order-mark><gpt-2>,1,0,0,789,,,,,,,
69098317,1,7211427.0,,Mycin like diagnosis system using gpt3 model,wonder build a mycin like expert use advance deep learn model like gpt fine tune medical domain knowledge build  years ago mycin use symbolic approach not sure possible now,2021-09-08 06:57:51,,,2021-09-08 06:57:51,<interactive><expert-system><gpt-3>,0,0,1,31,,,,,,,
65974247,1,9837081.0,,"Incrementally training || pause&resume training, GPT2 language model'ing",currently try learn python time learn machine learn gpt language model problems get finally get decent run probably know train model take alot cpugpu power amp time time spare problem run nonstop home computer yes know rent a gpu google want able train model follow question stop restart model train read about checkpoints outdated info topic havent able figure incrementally fee model fx  dataset let finish week fee  bonus question better aim epochs a lower data set a larger dataset epochs a good epochs package python  tensorflowgpu  tensorflowestimator  transformers  tokenizers  cudatoolkit  code tokenizer code model trainer,2021-01-30 23:33:47,,,2021-01-30 23:33:47,<python><tensorflow><tensorflow2.0><huggingface-transformers><gpt-2>,0,0,3,227,,,,,,,
66669890,1,12349188.0,,GPT2Simple having issues running,try run gptsimple sample get errors code take downgrade tensorflow  tensorflow  issue,2021-03-17 09:02:31,,2021-03-17 13:14:26,2021-03-18 15:25:40,<python><tensorflow><google-publisher-tag><gpt-2>,1,0,1,216,,,,,,,
67362300,1,6805178.0,,fill-mask usage from transformers pipeline,finetune a gpt language model generation text accord model use follow line code generator pipeline textgeneration tokenizer gpt model ataout print generator a time max_length  generated_text now want prediction word probabilities know use fillmask not know fillmask inplace textgeneration get error unrecognized configuration class kind automodel automodelformaskedlm model type bigbirdconfig wavvecconfig convbertconfig layoutlmconfig distilbertconfig albertconfig bartconfig mbartconfig camembertconfig xlmrobertaconfig longformerconfig robertaconfig squeezebertconfig bertconfig mobilebertconfig flaubertconfig xlmconfig electraconfig reformerconfig funnelconfig mpnetconfig tapasconfig debertaconfig debertavconfig ibertconfig generator pipeline fillmask tokenizer gpt model ataout line give above mention error let know fix issue kind help greatly appreciate thank advance code better understand transformers import gpttokenizer datacollatorforlanguagemodeling textdataset gptlmheadmodel trainingarguments trainer pipeline train_path parsed_datatxt test_path parsed_datatxt tokenizer gpttokenizerfrom_pretrained gpt data_collator datacollatorforlanguagemodeling tokenizertokenizer mlmfalse train_dataset textdataset tokenizertokenizer file_pathtrain_path block_size test_dataset textdataset tokenizertokenizer file_pathtest_path block_size model gptlmheadmodelfrom_pretrained gpt training_args trainingarguments output_dir ataout overwrite_output_dir true per_device_train_batch_size  per_device_eval_batch_size  learning_rate e num_train_epochs  trainer trainer model model args training_args data_collatordata_collator train_dataset train_dataset eval_dataset test_dataset trainertrain trainersave_model generator pipeline fillmask tokenizer gpt model ataout,2021-05-03 00:36:31,,2021-05-03 01:40:58,2021-05-03 01:40:58,<nlp><pytorch><artificial-intelligence><language-model><gpt-2>,0,0,1,628,,,,,,,
70373541,1,16638949.0,,Should I adjust the weights of embedding of newly added tokens?,a beginner neural language process recenttly try train a text generation model base gpt huggingface transformers add new tokens tokenizer resize embed model suppose add  new tokens add weight  tokens optimizer thank,2021-12-16 03:34:42,,,2022-07-14 10:56:27,<huggingface-transformers><pre-trained-model><gpt-2>,1,0,3,637,,,,,,,
72925542,1,19518604.0,,"When you prompt GPT3, what happens to the input data?",example let say open playground type quack do model  character figure letter word come happen gpt fill prompt quackery a tirade cell therapy weird,2022-07-10 01:00:42,,2023-01-21 05:22:24,2023-06-07 17:36:09,<nlp><artificial-intelligence><gpt-3>,2,0,-1,319,,,,,,,
73113552,1,12103619.0,,Open AI generate longer text with GPT-3,play gpt api openai struggle a way make long generate text piece code example output a things consider choose a student loan include rate repayment options loan federal private compare loan cost least money long run  word not close  tokens think parameter suppose run consecutive generate texts explain make generate text longer ideally  tokens huggingface model a parameter not thank a lot,2022-07-25 18:07:18,,2023-01-26 22:03:25,2023-01-26 22:03:25,<python><openai-api><gpt-3>,1,0,4,4698,,,,,,,
74869109,1,20490510.0,,"When using OPT-2.7B or any other natural language model, is there a way to trick it into having a conversation/ give it a pre prompt in the code",use code a variant add trick opt converse user a style similar a chatbot now start similar article a conversation a line see example current output user input hello model output hello egg match tsv mind hatch sure add now let know online sorry late reply online now hours just send a trade request ready no probs middle a battle moment soon thank hatch a nice day d attempt add a prompt start not a difference,2022-12-20 21:30:17,,,2023-03-20 06:31:21,<neural-network><huggingface-transformers><language-model><huggingface><gpt-2>,1,0,1,140,,,,,,,
74903974,1,1889865.0,,"OpenAI ""We could not parse the JSON body of your request.""",try write a chrome extension take webpage text send chatgpt js code get error not parse json body request output undefined do wrong ps html css extension work just fine,2022-12-23 21:02:51,,2023-01-11 18:12:22,2023-01-11 18:12:22,<javascript><google-chrome-extension><openai-api><gpt-3>,0,0,2,794,,,,,,,
74916280,1,7476541.0,,Error: That model does not exist (OpenAI),use a model finetuned gpt use cli stop work error message model do not exist a model use exist,2022-12-25 22:55:23,,,2022-12-25 22:55:23,<openai-api><gpt-3>,1,0,-1,2653,,,,,,,
74978917,1,20908437.0,,"""RuntimeError: Expected target size"" error for the nn.CrossEntropyLoss() function",try train a gpt model a tokenizedpadded input predict output batch size  max length  believe  come model loss function work properly train loop keep throw errors like size tensors loop loop try squeezingunsqueezing change shape logitsoutput_tensors shape think right step not figure change exactly,2023-01-02 04:03:01,,2023-01-03 00:55:58,2023-01-03 00:55:58,<machine-learning><pytorch><tensor><cross-entropy><gpt-2>,0,1,3,79,,,,,,,
74986827,1,1974376.0,,OpenAISwift package works for ios not for Mac,follow instructions build a simple swiftui gpt client use openaiswift client library app work expect ios try run a macos version get errors     gpt   networkd_settings_read_from_file sandbox prevent process read networkd settings file librarypreferencescomapplenetworkdplist add exception     gpt   networkd_settings_read_from_file sandbox prevent process read networkd settings file librarypreferencescomapplenetworkdplist add exception     gpt   nw_resolver_can_use_dns_xpc_block_invoke sandbox do not allow access comapplednssdservice macos openaikit project gitub state follow need add infoplist macos do not choices available xcode  project properties info section try paste dict object a text version infoplist not a way edit infoplist a text simple code use multiplatform app run macos ventura  thank help,2023-01-02 20:36:06,,2023-01-29 13:44:20,2023-01-29 13:44:20,<macos><swiftui><appstore-sandbox><entitlements><gpt-3>,2,0,0,260,,,,,,,
75046073,1,17275588.0,,Python + Open AI/GPT3 question: Why is part of my prompt spilling into the responses I receive?,happen probably  responses reason bits prompt spill start like a period a question mark a letter prompt remove prompt way response get print inside visual studio code terminal output version get write a correspond excel spreadsheet reason happen example responses example period question mark transpose end prompt tack response remove prompt originally excel spreadsheet begin a bug xlsxwriter open ai combo code,2023-01-08 07:16:08,,,2023-01-09 16:00:53,<python><machine-learning><artificial-intelligence><gpt-3>,1,0,0,396,,,,,,,
75136962,1,12342925.0,,"OpenAI GPT-3 API error: ""TypeError: Converting circular structure to JSON"" using ExpressJS",just experiment openai api a basic express app run try just send appropriate response a basic input currently keep fail use postman iterate code localhost package definitely instal api key correct specfied env file current work file sure kick spot dumb thing probably error generate terminal,2023-01-16 16:20:20,,2023-03-13 13:48:19,2023-03-20 07:44:11,<node.js><express><openai-api><gpt-3>,2,2,1,880,,,,,,,
75192212,1,18029046.0,,Template for RLHF with the TRL library,try implement a basic work template rlhf trl notebook target make gpt answer mailman maybe not get right mechanics trl look like train do not influence model correct template expect query model change,2023-01-21 09:06:58,,,2023-01-21 09:06:58,<pytorch><huggingface-transformers><kaggle><huggingface><gpt-2>,0,0,1,145,,,,,,,
75285557,1,4932296.0,,Removing tokens from the GPT tokenizer,remove unwanted subtokens gpt vocabulary tokenizer try exist approach use a roberta kind model show fail point initialize model component backend_tokenizer new vocabulary error methods use refer original script adapter use roberta use sentencepiece gpt use bpe,2023-01-30 14:05:41,,,2023-01-30 14:05:41,<python-3.x><nlp><huggingface-transformers><huggingface-tokenizers><gpt-2>,0,0,0,172,,,,,,,
75355374,1,21153622.0,,Python Telegram bot chat gpt,turn appear like,2023-02-05 20:43:21,,2023-02-06 00:25:04,2023-02-21 11:40:42,<python><telegram><openai-api><telebot><gpt-3>,0,1,0,830,,,,,,,
71399624,1,135043.0,,Memory usage in transforming fine tuning of GPTJ-6b to HuggingFace format,follow tutorial use tpus fine tune gptj work step transform huggingface format use to_hf_weightspy issue memory mb slimming apply issue file,2022-03-08 18:06:05,,2022-12-13 15:41:00,2022-12-13 15:41:00,<tpu><jax><gpt-3>,1,0,0,384,,,,,,,
60097717,1,9344014.0,,GPT-2 Continue training from checkpoint,try continue train a save checkpoint use colab setup gptsimple just work load save checkpoint googledrive work fine use generate text continue train checkpoint enter try use run_name different use not try restart runtime suggest doesn´t help get follow error asume need run continue train run throw error,2020-02-06 14:51:59,,2020-11-29 11:58:00,2021-04-13 11:19:13,<python><tensorflow><nlp><google-colaboratory><gpt-2>,2,0,2,4165,0.0,,,,,,
73335404,1,13297517.0,,NAN values appears when including a new padding token in my tokenizer,try finetune a dialogpt model a new dataset process data correctly add a new pad token tokenizer do not make issue ouputs far pretty clean try make a prediction model train nan value output nan value certainly cause new token pad add not know deal help,2022-08-12 14:05:34,,2022-08-12 14:10:29,2022-08-12 14:10:29,<python><deep-learning><huggingface-transformers><language-model><gpt-2>,0,0,1,120,,,,,,,
74503607,1,310370.0,,Text generation AI models generating repeated/duplicate text/sentences. What am I doing incorrectly? Hugging face models - Meta GALACTICA,day work available text generation model list want generate longer text output multiple different model repetition miss do incorrectly list freshly release meta galactica code example generate output facebook opt test code generate output,2022-11-19 20:48:50,,2023-01-29 06:21:42,2023-02-26 15:13:30,<python><nlp><huggingface-transformers><huggingface><gpt-2>,1,0,3,480,,,,,,,
75362603,1,15435637.0,,Is GPT-3 a model or a framework?,hear gpt call a large language model llm really a framework use gpt dataset train version a gpt model understand a model result train use frameworkslibraries train model ex tensor flow gpt just a model not able train data right make gpt a framework help better understand ai terminology,2023-02-06 14:26:30,,2023-03-02 05:33:20,2023-03-02 05:33:20,<machine-learning><nlp><artificial-intelligence><gpt-3>,1,1,-1,164,,,,,,,
75540828,1,21270404.0,,Chat bot not sending message to API,ask chat gpt write a chat bot code model gpt actually write site create message not send get a gpt api key use do not work problem code write gpt click send send a message do not work message not send api,2023-02-23 05:18:36,,2023-02-23 05:46:54,2023-05-08 13:23:33,<javascript><html><openai-api><gpt-3>,1,6,0,206,,,,,,,
75549286,1,8494468.0,,GPT Index: Issue using ComposableGraph with Vector Stores,have issue implement vector store composability error try composablegraph vector store do not work,2023-02-23 19:06:18,,,2023-02-23 19:06:18,<python><openai-api><gpt-3>,0,0,0,673,,,,,,,
75722268,1,1031215.0,,"Fine-tuning of OpeanAI model with unsupervised set, not supervised",want gpt model know about domain area example inbox want able ask question like a silicon valley bank account correct response familiarize finetuning mechanism official openai docs not exactly look want just dump email model ask learn finetuning require supervise style learn prompt reponses not example notebooks doc suggest use davinciinstruct ask a question base a wikipedia section answer question base section guess solve problem apply email not step screw options azure open ai integration allow finetuning problem call want finetuning fact pretraining process just decide finetuning documentation api hand finetuning guaranties wrong answer pretraining do not dont want wrong answer question a silicon valley bank account,2023-03-13 13:06:42,,,2023-03-27 18:17:41,<openai-api><pre-trained-model><gpt-3><fine-tune>,1,0,0,304,,,,,,,
75723546,1,8391698.0,,"How to resolve ""the size of tensor a (1024) must match the size of tensor b"" in happytransformer",follow code code use gpt language model transformers library generate text a give input text input text split smaller chunk  tokens gpt model use generate text chunk generate text concatenate produce final output text happytransformer library use simplify generation process provide a pretrained model interface generate text a give prefix settings gpt model tokenizer save a local directory output code generate text input text corrections grammar suggest prefix grammar give error resolve,2023-03-13 14:56:59,,,2023-03-14 00:54:19,<python><huggingface-transformers><huggingface><gpt-2>,1,3,2,241,,,,,,,
75760838,1,8682074.0,,Can i train chatgpt with custom data from a database?,let say a law firm table basic structure users email telephone employees kind email telephone case case casenumber party name attorney assign entries update status open_date close_date task case_id employee_assigned employee_assigner statusdue_at communication date user_id employee_id text kind duration cases_assignations employee_id case_id let say now want train a model chatgpt solution example employee type make sense like know strategy make possible,2023-03-16 19:26:17,2023-04-25 15:51:23,,2023-03-29 10:03:53,<openai-api><gpt-3>,0,3,3,3825,,,,,,,
75783316,1,13829794.0,,OpenAI API Finetune Model - no response,try learn finetune gpt model familiar work python use ides spyder know follow code ide spyder give no response no tracebacks no errors not sign model appear openai playground note test nickname model api key remove code know not suppose code api key string eventually cloak try use ada model cheapest just test promptsanswers excel code successfully save a csv promptresponse pair filenotfounderror winerror  file specify remove shelltrue,2023-03-19 16:09:47,,,2023-03-19 16:09:47,<python><pandas><subprocess><openai-api><gpt-3>,0,0,0,291,,,,,,,
75783524,1,21433400.0,,Train gpt-3 on email conversations,train gpt email data support team a quick answer a chatbot question ask customers email conversations customers support team customer ask question support answer customer ask question … filter important conversations fee gpt prepare convert right format train model anone ideas about realize step weather use fine tune embeddings gpt connect question answer give support team,2023-03-19 16:44:07,,,2023-03-28 07:17:15,<openai-api><gpt-3>,1,0,0,362,,,,,,,
67288454,1,2742509.0,,Train GPT2 with Trainer & TrainingArguments using/specifying attention_mask,use trainer amp trainingarguments train gpt model do not work datasets ids tokens corpus mask text indicate apply attention do train trainer amp trainingarguments pass model previous dataset follow nowhere specify about attention_mask tell trainer use feature attention_mask a look file transformerstrainerpy no reference attention mask thank advance,2021-04-27 18:07:17,,,2021-10-13 00:14:31,<huggingface-transformers><attention-model><gpt-2>,1,0,0,438,,,,,,,
75559672,1,15696244.0,,OpenAI GPT-3 API: Which file formats can be used for fine-tuning?,get turbulent time ai spill drop ocean pythonian attempt pythonanaconda do anybody experience data format passable gpt family ais documentation recommend use openai tool control follow documentation recommend format prompt completion string mark completion space start sting far able just simple examples way understand complex dataset effective dim dataframe example longer context text pass just tr partition need thank answer efforts advance check,2023-02-24 17:32:30,,2023-03-13 14:43:42,2023-03-13 14:43:42,<python><openai-api><gpt-3><fine-tune>,1,0,0,794,,,,,,,
75644866,1,19789151.0,,Incompatibilty between styled-components and react-simple-bot library,hi try create a chatbot use neoj a backend gpt a translation tool nl cypher follow tutorial webpage fanghua yu provide frontend nodejs use reactsimplebot problem latest version library not compatible library styledcomponents npm err code eresolve npm err eresolve not resolve npm err npm err resolve reactsimplechatbot npm err styledcomponents npm err node_modulesstyledcomponents npm err peer styledcomponents  babelpluginstyledcomponents npm err node_modulesbabelpluginstyledcomponents npm err babelpluginstyledcomponents  styledcomponents npm err styledcomponents  root project npm err npm err not resolve dependency npm err peer styledcomponents  reactsimplechatbot npm err node_modulesreactsimplechatbot npm err reactsimplechatbot  root project npm err npm err conflict peer dependency styledcomponents npm err node_modulesstyledcomponents npm err peer styledcomponents  reactsimplechatbot npm err node_modulesreactsimplechatbot npm err reactsimplechatbot  root project try a lot things obvious downgrade version styledcomponents library  compatible reactsimplebot a lot libraries leave deprecate a lot vulnerabilities app add  package audit  package s  package look fundingrun npm fund detail  vulnerabilities  low  moderate  high  critical try run app web browser do not display try run npm audit fix solve problem require npm audit fix force end break application run do not display try fix libraries a lot patience nothing really need help provide code need do not feel necessary,2023-03-05 19:38:26,,2023-03-06 03:51:27,2023-03-06 03:51:27,<node.js><neo4j><gpt-3>,0,0,0,126,,,,,,,
75784494,1,21434098.0,,How can I split my model among multiple GPUs?,try split multiple gpus return error tensors gpu not want dataparallel modelparallel  gpu minimum weight bias commute try split like help thank advance,2023-03-19 19:33:56,,2023-03-20 10:09:04,2023-03-20 10:09:04,<python><machine-learning><pytorch><nlp><gpt-2>,1,0,2,112,,,,,,,
75827960,1,20493358.0,,how do i stop this encoding error in the openai python module?,try make a chat completion bot use opeai gpt engine take voice input output a text speech file get encode error dont understand try format think output tts result instead say say about unknown character really dont understand partially im new cod,2023-03-23 21:04:00,,,2023-05-12 15:25:30,<python><encoding><character-encoding><openai-api><gpt-3>,1,0,3,262,,,,,,,
75837647,1,14888778.0,,How do I restart Hugging Face Transformer GPT2 finetuning?,try restart finetuning start begin normal want resume finetuning use a save checkpoint replace line train start begin normal,2023-03-24 19:50:39,,2023-03-24 19:51:07,2023-03-25 10:38:23,<python><pytorch><huggingface-transformers><gpt-2>,1,0,0,99,,,,,,,
73472819,1,15152059.0,,How to standardize GPT-3 word embeddings in Python?,a df columns a single word capability a correspond word embed like standardize mean sd word embeddings unfortunately not a function fit structure word embeddings python beginner sorry fail approach yield error help,2022-08-24 12:04:01,,2022-08-24 13:09:43,2022-08-24 13:09:43,<python><word-embedding><gpt-3>,0,0,0,159,,,,,,,
73899423,1,14022747.0,,NLP / ML Python: variation of topic modeling + summarization? Can someone point me in the right direction?,new nlp machine learn wonder point right direction look create a function take  input array string english sentence vary relation purpose let just assume totally unrelated sentence a topic string function return a coherent paragraph essay about indicate topic use available sentence like flavor topicmodeling summarization function write use predetermine array string thoughts libraries techniques investigate thank,2022-09-29 17:16:11,,2022-09-29 17:22:42,2022-09-29 17:22:42,<python><nlp><stanford-nlp><summarization><gpt-3>,0,2,1,81,,,,,,,
73945384,1,20156712.0,,Structure evaluation set GPT-2 text generation huggingface,i´m currently reproduce second task generate article headline tutorial understand ‘input_ids ’ train data prepare format ‘bos_token title sep_token content eos_token ’ now want add a compute_metrics function call trainer evaluate set model predict ‘content ’ give ‘title ’ prepare data evaluation set just ‘bos_token title sep_token ’ manipulate ‘attention_mask ’ indicate,2022-10-04 08:57:22,,,2022-10-04 08:57:22,<gpt-2><huggingface>,0,0,0,108,,,,,,,
75435496,1,21203402.0,,"OpenAI GPT-3 API error: ""Unknown endpoint for this model.""",new use apis interest inn new openai product gpt know not new just about try use api key python key invalid code not api key obvious reason receive error error message unknown endpoint model type invalid_request_error param none code none try use a new api key time do not work key invalid,2023-02-13 11:50:22,,2023-03-13 14:31:20,2023-05-15 06:26:31,<python><openai-api><gpt-3>,2,2,-2,2431,,,,,,,
57720955,1,4544413.0,,Can't import the encoder code for fine tuning GPT-2,try reproduce example article example code follow repo instal requirements download model follow step train model code execute issue require import follow modules encoder load_dataset a child directory generate follow error try create file import srcencoder srcload_dataset not work medium post author propose file encoderpy src execute code issue do break relative path model handle issue paths keep go file,2019-08-30 05:30:36,,2020-11-29 11:58:47,2023-04-02 17:27:44,<python><path><nlp><init><gpt-2>,5,1,3,2924,,,,,,,
69590991,1,17139319.0,,"How do I make ""msg.content"" constantly get new strings added to it instead of replaced?",try implement ai gpt a discord bot work gpt need know prompt basically context a conversation time way set constantly replace variable msgcontent new string grab messagecollector need make a message detect add string variable constantly do let say a timer go,2021-10-15 21:52:15,,2021-10-17 04:49:11,2021-10-17 04:49:11,<javascript><node.js><discord.js><openai-api><gpt-3>,0,3,1,48,,,,,,,
70118071,1,,,Trouble getting text from GPT2 returned?,basically try gpt respond a prompt variable text run error valueerror truth value array element ambiguous use aany aall code far help figure do wrong thank,2021-11-25 22:59:42,,,2022-02-24 19:27:43,<python><huggingface-transformers><gpt-2>,1,0,0,343,,,,,,,
75861442,1,21505798.0,,An error occurred: module 'openai' has no attribute 'ChatCompletion',try build a discord bot use gpt api function a chatbot discord recent version openai library run code tell error occur module openai no attribute chatcompletion try uninstalling reinstall openai library try use completions endpoint get error a chat model not support vcompletions endpoint do mean use vchatcompletions snippet code thats give issue,2023-03-28 00:33:57,,,2023-06-09 18:45:55,<python><openai-api><gpt-4>,6,2,2,2514,,,,,,,
75872837,1,21513148.0,,How can we set a scope to a fine tuned chat gpt3 chatbot,set a scope a fine tune chat gpt chatbot limit a particular context order avoid reply unnecessary question unrelated context,2023-03-29 03:18:42,,,2023-03-29 03:18:42,<scope><chatbot><gpt-3><fine-tune>,0,0,0,26,,,,,,,
76297340,1,10772341.0,,"TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]] when training GPT-2",learn train generative model use transformers library huggingface have error want tokenize text dataset code dataset id reponse context context context context context context context context context context context  es tan simple no se que te detiene ¡ muy persuasiva no es crimen librarse una alimaña ¿ por qué tener lastima hombre tan vil además también ha puesto sus ojos en okayo hace  años que soy victima mi marido solo estoy siendo franca contigo calmate ¡ eres peor que el diablo ¿ comprendes okayo recuerda constantemente mi fracaso  muy torpe joyce a la sala interrogación rápido ¡ muévanse pie muchachos a la sala interrogación rápido ¡ muévanse pie muchachos ¡ use su cuchillo hombre ¡ adelántese thomson ¡ bien hecho jenkins gracias muy bien el bungaló del mayor warden está al final del continúe conductor  pídemelo sólo lo que quieras tú ya no soy yo eres preciosa y maravillosa ¿ no así te gustaré haré y diré lo que quieras nunca así nunca querrás estar otras ¿ verdad siempre diré lo que tú desees y haré lo que tú pero yo sí pero  ¡ boris ¡ nicolás que alegría a mi corazón volviste ¡ regresan los vencedores ¡ miren ¡ ahí vienen está vivo boris está vivo dasha prometió avisarme cuando regrese pero en la fábrica dicen que él está en una u tampoco hay noticias stepan ¡ quién sabe ¿ por qué entonces no hay noticias él  entonces por qué no estamos en mejor situación dora hartley era una buena prueba mire lo que hace usted creer ¿ qué los indios aleja esa arma buenas noches es hora ir a la cama seguro sí recuerde que es secreto es bonita está bien ¿ ann martin hola bax,2023-05-20 21:02:24,,2023-05-20 21:15:34,2023-05-20 21:15:34,<python><huggingface-transformers><huggingface><gpt-2>,0,0,0,87,,,,,,,
76320182,1,6832612.0,,Gradio Interface does not output anything,button click gradio interface want output return value a method prompter class instantiate object class code gradio interface display appear colab cell click generate button machine busy output nothing open ai usage not change display output assume method work – outside gradio interface,2023-05-24 04:52:21,,,2023-05-24 04:52:21,<openai-api><gpt-3><gradio>,0,0,0,67,,,,,,,
76331086,1,21027624.0,,"Estimating OpenAI GPT-3.5-Turbo usage costs for french inputs, is this the right approach?",a corpus french document undergo process use openai extract information texts use french prompt prompt constitute text question specify task like accomplish use tiktoken estimate number tokens code follow right approach french language have estimate number tokens multiply  k token a rough estimate total price approach valid do number tokens include output generate tokens thank advance help,2023-05-25 10:04:23,,,2023-05-25 10:04:23,<token><openai-api><gpt-3>,0,0,0,49,,,,,,,
76405967,1,6623469.0,,How come azure openai models are faster than openai models?,recently try gpt model api call azure openai notice time take model azure least x faster reason like azure share detail change speed,2023-06-05 11:28:11,,,2023-06-06 18:07:36,<azure-cognitive-services><openai-api><gpt-4>,1,0,0,318,,,,,,,
76437658,1,11049287.0,,How to handle token limit in ChatGPT3.5 Turbo when creating tables?,end user copy table a pdf like paste text openai playground prompt gpt create table give text gpt generate a table like work expect input text sizeable say  tokens face follow error use python text preprocessing data ui input textual data like passages use approach suggest langchain not able use summarization iteratively tabular text loose rowscolumns input handle,2023-06-09 06:13:47,,,2023-06-22 17:14:03,<python-3.x><openai-api><gpt-3><azure-openai><llm>,1,0,0,46,,,,,,,
76479712,1,1305322.0,,tf.compat.v1.estimator.Estimator(): NameError: name 'model_fn' is not defined,try create a pet llm use gpt follow instructions code give syntax error call tfcompatvestimatorestimator model_fn argument nameerror model_fn not define try define model_fn model_fn model_fn hparams tfestimatormodekeystrain do not help not sure model_fn define code help appreciate,2023-06-15 06:31:44,,2023-06-16 06:08:25,2023-06-16 06:08:25,<python><tensorflow><gpt-2><llm>,0,0,0,21,,,,,,,
69831403,1,,,GPT2 on apple M1 Pro chip,try install gpt accord instructions official github repo end error try use mean not think try gpt m pro chip instructions incomplete do not tell python pip version use install tensorflow just say need tensorflow  official tensorflow website connect dot instructions figure need python macos deadend give beautiful open source ml model discover official apple github page optimize tensorflow version macos allow advantage  neuralengine core m pro cpu no care about gpu support problem tensorflow time a versioned x gpt use  not believe apple care about backward compatibility x version github archive readonly no intensions hope problem versions package remove rat answer day suggest google module without tfcontrib know new location migrate code accordingly correct import statement now access contrib package github repo tensorflow no need google gues error point modelpy line  simply download github repo paste gpt think repeat trick hparamspy ask hparam_pb do not exist not know extensioned file _pbpy run problem kindly advise,2021-11-03 20:33:57,,2021-11-03 21:55:56,2021-11-03 21:55:56,<python><tensorflow><apple-m1><gpt-2>,0,1,2,1531,,,,,,,
71003190,1,8552928.0,,OpenAI API repeats completions with no variation,try implement a chatbot openai javascript use official openai npm dependency way solve array chat message get join newlines send prompt api example push question ask array push ai string openaiendpoint complete result prompt api complete look like response push array conversation continue time send  line array problem bot start repeat seemingly random time start answer like great about send question prompt answer example relevant thing documentation frequency_penalty presence_penalty change doesnt parameters use examples above course try different combinations temperatures penalties just a know problem misunderstand,2022-02-06 00:06:01,,,2023-01-19 19:35:50,<openai-api><gpt-3>,1,1,3,3466,,,,,,,
76489469,1,22083420.0,,Unsupervised fine-tuning on custom documents after the supervised fine tuning on general question-answers dataset. Will it be useful for GPT-2 model?,know formal way train a gpt model custom document semisupervised fine tune text document follow supervise finetuning question answer document sole purpose supervise finetuning acquire style answer question possible supervise finetuning a general dataset perform unsupervised finetuning custom text dataset document way question answer style acquire model advantage have no need make a questionanswer dataset custom document desire result,2023-06-16 10:51:50,,,2023-06-16 10:51:50,<pre-trained-model><gpt-2><large-language-model><semisupervised-learning><generative-pretrained-transformer>,0,0,0,9,,,,,,,
76137940,1,21774119.0,,While trying to generate text using GPT-2 the custom loss function accesses PAD_TOKEN_ID,train custom loss function try access pad_token_id result error pad_token_id vocab size gpt order remove try mask label logitsthe label mask a shape  post mask  logits mask a shape   post mask    code use mask logits follow a primary problem label value  access try remove mask logits label fail different shape probably a dumb question run ideas really helpful a look try mask label logits mention above size label  logits  try apply tfboolean_mask fail shape mismatch error expect calculate loss mention text generation train loop pass label input_ids show,2023-04-29 19:04:58,,2023-04-29 19:08:56,2023-04-29 19:08:56,<tensorflow><loss-function><gpt-2><text-generation>,0,0,0,58,,,,,,,
71333114,1,9557623.0,,OpenAI retrieve file content,unable retrieve content file upload kindly suggest go wrong try type file search classification answer finetune file upload successfully retrieve content show error error,2022-03-03 06:50:00,,2022-03-03 15:24:51,2022-06-04 10:15:35,<python><openai-api><gpt-3>,1,2,0,1190,,,,,,,
71371756,1,18390515.0,,Can i clear up gpu vram in colab,try use aitextgen finetune m gpt  a dataset unfortunately no matter train fail  mb vram available clear vram without restart runtime maybe prevent vram,2022-03-06 15:42:13,,,2022-10-12 14:15:38,<google-colaboratory><gpt-2><fine-tune><vram>,2,0,4,6322,,,,,,,
75699797,1,13714062.0,,"In NLP, how can we use text query to fetch data from Tabular format data with thousands of rows?",let say excel thousands row want fetch data base query enter pure text format google tapas model do not work  row way let say input text box user enter query want buy mercedez benz class c white color price range lac available purchase new york state base query fetch relevant data table user condition tabular data thousands row tapas model not work gpt expensive train use way,2023-03-10 18:29:24,,,2023-03-10 18:29:24,<elasticsearch><search><nlp><transformer-model><gpt-3>,0,0,0,46,,,,,,,
75783029,1,1314732.0,,PyTorch with Transformer - finetune GPT2 throws index out of range Error,jupiter follow code not figure throw a error ist code text file moment look simple error tell wrong train setup update change a pt tensor output pad model length error delete cache,2023-03-19 15:24:24,,2023-03-20 08:30:52,2023-03-20 08:30:52,<python><pytorch><huggingface><gpt-2>,0,2,2,286,,,,,,,
75860080,1,16923163.0,,Can I use a single ChatGPT-3 API key for multiple projects simultaneously?,a noobie programmer college try learn use api key currently use chatgpt api siri personal assistant project work far now develop application a bot utilize resume automatically generate cover letter reach talent acquisition team use chatgpt api key project simultaneously limitations issue aware use a single api key multiple project,2023-03-27 20:25:14,,2023-03-27 20:39:33,2023-03-27 20:57:09,<api><chatgpt-api><gpt-4>,1,0,0,740,,,,,,,
75871333,1,396014.0,,Using GPT-3 to identify relationships in a corpus,a corpus k news article like train a gpt model   ingest texts output locations events action participants things describe texts relate corpus say john smith take a protest like tell people take protest relate specific locations possible point right direction learn search find link about use gpt model extractive abstractive summaries individual texts suppose relate not quite,2023-03-28 21:46:12,,2023-04-17 04:52:47,2023-04-17 04:52:47,<nlp><information-extraction><gpt-3><relation-extraction>,1,0,0,33,,,,,,,
75871649,1,11481515.0,,gpt-35-turbo does not memorize messages in PHP,create a telegram bot respond user message use openai gpt api work fine issue gptturbo model possible add parameters memorize message track conversations do do not work share code assistance code,2023-03-28 22:40:06,,,2023-03-28 22:40:06,<php><telegram-bot><session-variables><openai-api><gpt-4>,0,0,0,187,,,,,,,
75915524,1,15525882.0,,How can I put a python program with specific module dependencies into an HTML page?,try make a modify gpt model design python available question html page try use pyscript not know access modules want python code file gptpy try use pyscript follow html code get error follow link tell make sure modules pynoneanywhl file do possible call do not change error message make html page access modules thank,2023-04-03 01:40:34,,2023-04-03 11:43:31,2023-04-03 11:43:31,<python><html><gpt-3><pyscript><llama-index>,0,3,0,187,,,,,,,
75951366,1,21582404.0,,How to save the gpt-2-simple model after training?,train gptsimple chat bot model unable save important download train model colab download m model time code try various methods save train model like none work not ideas train code,2023-04-06 15:42:27,,2023-04-11 11:37:39,2023-04-11 11:37:39,<machine-learning><nlp><artificial-intelligence><gpt-2><text-generation>,1,0,2,176,,,,,,,
76226113,1,21878965.0,,How to use large prompt for GPT-3 models in python?,go extract information docx file use openai gpt model python total length prompt big gpt provide opinion about problem help thank try split chunk not useful need information chunk not chunk,2023-05-11 09:27:03,,,2023-05-11 09:38:04,<python><gpt-3>,1,0,-3,110,,,,,,,
76278747,1,11725056.0,,"Out of 4 methods for Document Question Answering in LangChain, which one if the fastest and why (ignoring the LLM model used)?",note reference document data change periodically use embed vector space method modify embed say a day want know factor design compensate reference document data generation latency create embed use cron job  methods use retrieve qa document less wrappers use dynamic document time call embed space document wrapper use embed space a memory chat history difference term speed use embed db faster assume make embed document like help time take a  word prompt time take a  word document text prompt speed affect use without,2023-05-18 07:45:43,,2023-05-18 07:52:31,2023-05-31 06:33:07,<python><openai-api><gpt-3><langchain>,1,0,-1,900,,,,,,,
76289498,1,12159826.0,,AttributeError: 'tuple' object has no attribute 'is_single_input,try use langchain agents order answer question ask use api face error attributeerror tuple object no attribute is_single_input follow code error open solution suggestions step  implement api function class step  create a tool step  initialize language model step  initialize agent step  use agent implement face follow error,2023-05-19 13:22:39,,2023-05-20 04:00:24,2023-05-26 01:41:09,<python><artificial-intelligence><openai-api><langchain><gpt-4>,2,0,0,236,,,,,,,
76421921,1,13057722.0,,Using GPT 4 or GPT 3.5 with SQL Database Agent throws OutputParserException: Could not parse LLM output:,use sql database agent query a postgres database want use gpt  gpt  model openai llm pass agent say use chatopenai use chatopenai throw parse errors reason want switch model reduce cost better performance importantly token limit max token size k textdavinci need least double code throw error chain midway say help thank,2023-06-07 09:40:43,,2023-06-07 09:48:48,2023-06-23 10:34:21,<openai-api><gpt-3><langchain><gpt-4>,1,0,1,389,,,,,,,
74262671,1,20378708.0,,"openai using python, returned length problem in ""text-davinci-002"" model",try use textdavinci model use openai return text a single sentence sentence return a text openai official example code use output code hello employer like request a promotion sentence openai website output hello employer like request a promotion position position want company time feel experience qualifications need position thank advance,2022-10-31 11:35:48,,,2022-12-27 18:52:43,<python><openai-api><gpt-3>,1,1,1,885,,,,,,,
61121982,1,13268010.0,64542919.0,Asking gpt-2 to finish sentence with huggingface transformers,currently generate text leave context use example script huggingface transformers library gpt like generate short complete sentence way tell model finish a sentence word note not mind change model prefer autoregressive,2020-04-09 13:12:13,,2020-11-29 11:58:17,2022-04-30 02:20:17,<nlp><pytorch><huggingface-transformers><gpt-2>,2,3,4,1922,,2.0,4777851.0,"<p>Unfortunately there is no way to do so. You can set the <code>length</code> parameter to a greater value and then just discard the incomplete part at the end.</p>
<p>Even GPT3 doesn't support completing a sentence before a specific <code>length</code>. GPT3 support &quot;sequences&quot; though. Sequences force the model to stop when certain condition is fulfilled. You can find more information about in thi <a href=""https://www.twilio.com/blog/ultimate-guide-openai-gpt-3-language-model"" rel=""nofollow noreferrer"">article</a></p>
",2020-10-26 18:25:41,1.0,3.0
68460080,1,15887188.0,68481504.0,how can i use openai's gpt 3 to find alternate spellings of bad words?,make auto mod discord bot find alternate spell bad word try use regex false positives think about use openai gpt saw a screenshot use appear find alternate spell screenshot unfortunately not know exactly gpt like use application like a discord bot tell use gpt alternate spell word help appreciate thank,2021-07-20 19:00:02,,,2023-04-24 10:39:23,<openai-api><gpt-3>,2,0,0,882,,2.0,11333098.0,"<p>I am not sure if you are looking for prompt/settings. However, based on my experience (3-4 months) I would use a few-shot approach prompt such this one:</p>
<pre><code>Check spelling and return the corrected word:
Word: nawty
Returns: naughty
Word: rigt
Returns: right
Word: stakoverflow
Returns: 
</code></pre>
<p>I guess that a high temperature and no penalties will do a good job. Also, keep trying different engines and see how it behaves. Curie-instruct-beta should do it.</p>
",2021-07-22 08:23:31,1.0,3.0
69773687,1,8655577.0,70045434.0,AttributeError: 'GPT2Model' object has no attribute 'gradient_checkpointing',try load a gpt fine tune model flask initially model load init function use perform prediction task follow snippet throw follow error mention question attributeerror gptmodel object no attribute gradient_checkpointing check default set constructor class,2021-10-29 19:04:17,,2021-10-29 23:01:50,2022-03-15 04:33:40,<python><pytorch><gpt-2><openai-api>,2,0,1,1585,,2.0,8655577.0,"<p>This issue is found to be occurring only if the framework is run using venv or deployment frameworks like uWSGI or gunicorn.
It is resolved when transformers version 4.10.0 is used instead of the latest package.</p>
",2021-11-20 11:21:43,0.0,0.0
64722585,1,2330237.0,,GPT-3 Prompts for Sentence-Level and Paragraph-Level Text Summarization / Text Shortening / Text Rewriting,need effective prompt gpt accomplish program task create effective gpt prompt essentially a new form program give a computer instructions complete a task get repositories nascient grow program language gpt prompt a work example work ok do not really address need not adequately reliable important new quickly grow area seek prompt accomplish goal title summarize shorten sentence paragraph high reliability without create nonsense reviewers important question people not narrowminded decide gpt prompt not a traditional computer language not a place thank help example gpt prompt summarize article microsoft talk buy tiktok negotiations bytedanceowned social media group come trump threaten action microsoft hold talk acquire tiktok chinese owner bytedance face mount pressure government sell video share app risk blacklist country say people brief matter rest article q summarize article above sentence,2020-11-06 22:50:25,2023-01-05 13:45:23,2020-11-29 11:47:16,2023-01-03 20:53:58,<text><artificial-intelligence><summarization><gpt-3>,3,1,4,5489,0.0,,,,,,
65097515,1,14270840.0,,How to use GPT-2 for topic modelling?,want generate topics subtopics a corpus great share python code,2020-12-01 19:48:36,,,2020-12-03 07:25:01,<nlp><topic-modeling><bert-language-model><gpt-2>,1,0,2,921,,,,,,,
65159768,1,9824768.0,,"On-the-fly tokenization with datasets, tokenizers, and torch Datasets and Dataloaders",a question regard onthefly tokenization question elicit read train a new language model scratch use transformers tokenizers end sentence dataset large opt load tokenize examples fly a preprocessing step try come a solution combine do not manage a good pattern guess solution entail wrap a dataset a pytorch dataset a concrete example docs implement onthefly tokenization exploit vectorized capabilities tokenizers,2020-12-05 17:15:55,,2020-12-08 21:57:02,2021-03-04 09:49:05,<huggingface-transformers><huggingface-tokenizers><gpt-2>,1,0,5,2589,0.0,,,,,,
76436535,1,9785742.0,,"I try to use GPTJ-lora model to generate txt, but the max-length of the generated text seemed to be 20 tokens. How to make it longer",result like generate text repeat a tokens text try use gptjlora model generate txt wish a tokens generate textbut result not response  tokens not longer repeat sentence length repeat sentence  tokens shorter  tokens problem base model gptj train databricksdollykjsonl modify japanese,2023-06-09 00:49:42,,2023-06-09 02:34:15,2023-06-09 02:34:15,<gpt-3><lora><llm><peft>,0,0,0,26,,,,,,,
76451232,1,22056563.0,,Why does this bundled app not work when the python script does work?,essentially zero cod experience use gpt prompt help a simple print utility generate a barcode add  code print order organize inventory script work great run idle package do not generate a barcode image give error cmd gpt try numerous ways result error conclude pyinstaller incompatible barcode pip script error cmd script package run script do generate last_number txt file barcodes folder just do not generate a barcode thank give post a look,2023-06-11 15:43:09,,,2023-06-11 15:48:41,<python><printing><pyinstaller><barcode><gpt-4>,1,0,0,22,,,,,,,
76451783,1,14552928.0,,AuthenticationError: <empty message> in OpenAPI api,try use langchain library pip instal langchain import run code colab set openai api key now try initialize follow run without error follow result try run throw follow error,2023-06-11 17:55:46,,,2023-06-13 17:38:13,<python><api><openai-api><gpt-3>,1,0,1,256,,,,,,,
76470976,1,21656479.0,,What replacement to gpt-3.5-turbo stream mode in gpt-3.5-turbo-0613?,gptturbo no longer support stream mode popular standart gpt telegram bot fail  replace  model like,2023-06-14 07:13:53,,,2023-06-14 07:13:53,<gpt-3>,0,0,-1,161,,,,,,,
76514041,1,20148726.0,,my gpt 3.5 turbo api is not giving good enough response as i can get from chat gpt,implement chat gpt  turbo api react app app basically like assistant a recruiter a recruiter give a sample job post app send post chat gpt craft now different personas copy response instruct follow personas style example persona lou adler style entice problem problem cht gpt givng good response case api app response not good tell about problem code note user roles not understand actual propt user kindly elaborate problem,2023-06-20 11:02:18,,,2023-06-20 11:02:18,<reactjs><chat><openai-api><chatgpt-api><gpt-4>,0,0,0,39,,,,,,,
76533304,1,13312941.0,,Sentence Transformers Installation Error: legacy install failure,use privategpt a project throw error instal dependencies r just instal privategpt github run follow instructions give privategpt readme instal pytorch throw,2023-06-22 15:09:04,,2023-06-22 15:28:55,2023-06-22 15:28:55,<python><sentence-transformers><gpt-4>,0,4,-3,18,,,,,,,
71539894,1,13510057.0,,CUDA out of memory while fine-tuning GPT2,error get try play batch size no avail train google colab piece code concern error solution,2022-03-19 16:18:09,,2022-03-19 16:19:47,2022-03-19 17:58:28,<python><machine-learning><nlp><training-data><gpt-2>,1,0,1,1555,,,,,,,
58884492,1,11130181.0,,why is encoder.json not found when running GPT2 small model,good even caveat im not a python machine learn expert try run small instance gpt hype want check far download prerequisites python regex tensorflow come run script generate sample model im throw follow error file c fydesktoppythongptsrcencoderpy line  get_encoder open ospathjoin models_dir model_name encoderjson r f filenotfounderror errno  no file directory modelsmencoderjson call script switch directory hold file run generate_unconditional_samplespy top_k  command line script look like advise do wrong im sure really obvious try sort stuff about  hours no luck advice appreciate,2019-11-15 20:50:54,,2020-11-29 12:09:39,2020-11-29 12:09:39,<python><machine-learning><artificial-intelligence><nlg><gpt-2>,1,0,1,1634,0.0,,,,,,
65897844,1,15025011.0,,Why using GPT2Tokenizer convert Arabic characters to symbols?,try use gpt arabic text classification task follow use tokenizer convert arabic character symbols like,2021-01-26 08:16:49,,2021-01-26 18:02:05,2021-01-26 18:02:05,<pytorch><tokenize><huggingface-transformers><huggingface-tokenizers><gpt-2>,0,3,1,266,,,,,,,
76397904,1,11745522.0,,Generate the probabilities of all the next possible word for a given text,follow code want generate word give input sentence want list possible word probabilities llm use gpt example code want choose  word  word suggestion word probabilities suggest word,2023-06-03 20:23:27,,2023-06-03 21:37:46,2023-06-03 21:40:49,<text><pytorch><huggingface-transformers><gpt-2>,2,0,3,119,,,,,,,
66518316,1,13808280.0,,How do I make a paraphrase generation using BERT/ GPT-2,try hard understand make a paraphrase generation use bertgpt understand make provide resources able make a paraphrase generation model input a sentence output a paraphrase sentence,2021-03-07 15:45:38,,,2021-06-18 13:10:05,<nlp><gpt-2>,2,0,4,3065,,,,,,,
62219426,1,13494387.0,,Adding tokens to GPT-2 BPE tokenizer,want add new word bpe tokenizer know symbol ġ mean end a new token majority tokens vocabs pretrained tokenizers start ġ assume want add word salah tokenizer try add salah token ġsalah tokenizeradd_tokens alah ġsalah   value respectively tokenize a sentence salah appear tokenizer never return second number neither use nor instance return question use symbol add new tokens tokenizer do probably specify manually thank advance,2020-06-05 15:56:12,,2020-11-29 12:05:47,2020-11-29 12:05:47,<python><nlp><tokenize><huggingface-transformers><gpt-2>,0,2,2,1021,,,,,,,
67365595,1,3146304.0,,Pytorch inference OOM after some batches,try inference a gptlike model a large dataset k sample speed like batch try go cuda oom batch fact go batch sound strange suppose memory use less constant different batch code problem,2021-05-03 08:15:15,,2021-05-03 13:26:03,2021-05-03 13:26:03,<pytorch><gpt-2>,0,3,0,187,,,,,,,
70577285,1,13440007.0,70580033.0,"""ValueError: You have to specify either input_ids or inputs_embeds"" when training AutoModelWithLMHead Model (GPT-2)",want finetune automodelwithlmhead model repository a german gpt model follow tutorials preprocessing finetuning prepocessed a bunch text passages finetuning begin train receive follow error code reference do know reason help welcome,2022-01-04 10:26:25,,,2022-06-23 14:13:51,<python><pytorch><huggingface-transformers><gpt-2>,2,0,0,2090,,2.0,13440007.0,"<p>I didn't find the concrete answer to this question, but a workaround. For anyone looking for examples on how to fine-tune the GPT models from HuggingFace, you may have a look into this <a href=""https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling"" rel=""nofollow noreferrer"">repo</a>. They listed a couple of examples on how to fine-tune different Transformer models, complemented by documented code examples. I used the <code>run_clm.py</code> script and it achieved what I wanted.</p>
",2022-01-04 14:08:06,0.0,0.0
72580299,1,5361942.0,72586215.0,GPT2 paper clarification,gpt paper section  page  say do not follow line reason logic conclude,2022-06-10 22:16:11,,,2022-06-11 16:49:37,<gpt-2>,1,1,0,119,,2.0,14644941.0,"<p>The underlying principle here is that if <code>f</code> is a function with domain <code>D</code> and <code>S</code> is a subset of <code>D</code>, then if <code>d</code> maximizes <code>f</code> over <code>D</code> and <code>d</code> happens to be in <code>S</code>, then <code>d</code> also maximizes <code>f</code> over <code>S</code>.</p>
<p>In simper words &quot;a global maximum is also a local maximum&quot;.</p>
<p>Now how does this apply to GPT-2? Let's look at how GPT-2 is trained.</p>
<p>First step: GPT-2 uses unsupervised training to learn the distribution of the next letter in a sequence by examining examples in a huge corpus of existing text. By this point, it should be able to output valid words and be able to complete things like &quot;Hello ther&quot; to &quot;Hello there&quot;.</p>
<p>Second step: GPT-2 uses supervised training at specific tasks such as answering specific questions posed to it such as &quot;Who wrote the book the origin of species?&quot; Answer &quot;Charles Darwin&quot;.</p>
<p>Question: Does the second step of supervised training undo general knowledge that GPT-2 learned in the first step?</p>
<p>Answer: No, the question-answer pair &quot;Who wrote the book the origin of species? Charles Darwin.&quot; is itself valid English text that comes from the same distribution that the network is trying to learn in the first place. It may well even appear verbatim in the corpus of text from step 1. Therefore, these supervised examples are elements of the same domain (valid English text) and optimizing the loss function to get these supervised examples correct is working towards the same objective as optimizing the loss function to get the unsupervised examples correct.</p>
<p>In simpler words, supervised question-answer pairs or other specific tasks that GPT-2 was trained to do use examples from the same underlying distribution as the unsupervised corpus text, so they are optimizing towards the same goal and will have the same global optimum.</p>
<p>Caveat: you can still accidentally end up in a local-minimum due to (over)training using these supervised examples that you might not have run into otherwise. However, GPT-2 was revolutionary in its field and whether or not this happened with GPT-2, it still made significant progress from the state-of-the-art before it.</p>
",2022-06-11 16:49:37,0.0,1.0
59150725,1,12466078.0,,Tensorflow not fully utilizing GPU in GPT-2 program,run gpt code large model m use generation text sample interactive_conditional_samplespy link give input file contain prompt automatically select generate output output automatically copy a file short not train use model generate text use a single gpu problem face code not utilize gpu fully use nvidiasmi command able image,2019-12-03 05:34:50,,2020-11-29 12:01:18,2020-11-29 12:01:18,<python><tensorflow><gpt-2>,1,6,2,635,,,,,,,
59997686,1,12419427.0,,"Python gpt-2-simple, load multiple models at once",work a discord bot function want implement respond text generate gptsimple library want model load multiple model available respond message users follow error run function second model try a way gpt instance seperate modules not achieve sandboxing effect advice seperating model instance do ideas,2020-01-31 04:01:21,,2020-11-29 12:01:38,2021-06-17 00:36:29,<python><python-3.x><tensorflow><gpt-2>,3,0,4,752,0.0,,,,,,
66451430,1,6463094.0,,Changes in GPT2/GPT3 model during few shot learning,transfer learn a pretrained network observation pair input label use data finetune weight use backpropagation shotfew shoot learn accord paper language model fewshot learners no gradient update perform change happen model like gpt gpt shotfew shoot learn,2021-03-03 05:46:56,,,2023-03-19 15:58:06,<nlp><gpt-2><gpt-3>,3,0,1,515,,,,,,,
60567168,1,4321521.0,,Use BertTokenizer with HuggingFace GPT-2,a specific generation problem involve a dataset build a small vocabulary ideally use case straightforward simply provide vocabulary a fix set tokens know berttokenizer example provide a file avoid tokenization basic vocabulary wonder a way gpt thing think right now create a hack subclass a better idea thoughts appreciate update okay turn just swap create thank huggingface a welldesigned modular codebase,2020-03-06 15:30:36,,2020-11-29 11:58:32,2020-11-29 11:58:32,<nlp><huggingface-transformers><gpt-2>,0,0,1,189,,,,,,,
60574112,1,5915270.0,,Can we use GPT-2 sentence embedding for classification tasks?,experiment use transformer embeddings sentence classification task without finetuning use bert embeddings experiment give good result now want use gpt embeddings without finetuning question use gpt embeddings like know gpt train leave right example use gpt classification task generation task use gptembeddings,2020-03-07 03:28:06,,2020-11-29 11:50:22,2020-11-29 11:50:22,<nlp><huggingface-transformers><gpt-2>,1,6,4,5710,,,,,,,
66956460,1,11810876.0,,Huggingface GPT transformers layers output,try use a gpt language model weight assign word state text generation model a gpt transformers library pretrained model goal use information layer model a matrix length vocabulary softmax activation use combination model try tensorflowplease share comment think easier convenient ways do pytorch,2021-04-05 16:40:57,,2021-04-06 11:37:47,2021-04-06 11:37:47,<tensorflow><nlp><huggingface-transformers><language-model><gpt-2>,0,3,1,423,,,,,,,
66020205,1,12384851.0,,Huggingface Transformer Priming,try replicate result demo author prim gpt just follow text access gpt huggingface transformer prime gpt large huggingface replicate above examples issue do not prime input correspond output separately author gpt demo do above similarly tutorial describe use huggingface no example clearly show prime use input vs output examples do know desire output use gpt return like input potato output peel slice cook mash bake gpt demo obviously exact list output verbs wo not gpt gpt not identical model,2021-02-03 01:56:48,,,2021-02-03 10:17:22,<python><huggingface-transformers><gpt-2><gpt-3>,1,0,1,223,,,,,,,
72604790,1,14143310.0,,How to train GPT2 with Huggingface trainer,try fine tune gpt huggingface trainer class code error use variables torch_dataset_train torch_dataset_eval trainer arguments error typeerror dataset use wikitext torchtext fix issue,2022-06-13 14:46:15,,,2022-06-13 14:46:15,<python-3.x><pytorch><huggingface-transformers><gpt-2><wikitext>,0,3,2,2585,,,,,,,
71641369,1,17400364.0,,GPT-3 made a mistake using numpy and I can't fix it,use gpt generate a neural network use a simple cell simulator run script follow error know matrices not correctly shape try transpose without success try modify input list without success,2022-03-28 00:38:58,,2022-03-28 10:19:11,2022-03-28 10:19:11,<python><numpy><deep-learning><gpt-3>,0,2,0,125,,,,,,,
71683057,1,17445782.0,,_forward_unimplemented() got an unexpected keyword argument 'input_ids',train a model use huggingface trainer class gpt text classification follow code do a decent job get error _forward_unimplemented get unexpected keyword argument input_ids input_ids label error mg model argiteture,2022-03-30 19:25:57,,2022-04-02 18:17:17,2022-04-02 18:17:17,<pytorch><huggingface-transformers><huggingface-tokenizers><gpt-2><google-publisher-tag>,0,3,0,488,,,,,,,
71737891,1,18701948.0,,Error when using mode.generate() from Transformers - TypeError: forward() got an unexpected keyword argument 'return_dict',try perform inference a finetuned gptheadwithvaluemodel transformers library use modelgenerate method generation_utilspy use function generate method try follow error traceback a bite a rookie hope point do wrong thank a lot,2022-04-04 13:13:29,,2022-04-04 13:52:37,2022-04-04 13:52:37,<huggingface-transformers><gpt-2>,0,1,1,1031,,,,,,,
72724956,1,15095688.0,,openai.error.InvalidRequestError: does not have access to the answers endpoint,try implement qa gpt error occur code myopenaikey secret key allocate openai website,2022-06-23 05:28:56,,,2022-06-23 05:28:56,<gpt-3>,0,0,0,1004,,,,,,,
73972852,1,473923.0,73972895.0,GPT3 completion with insertion - invalid argument :suffix,try completions use insertions suppose use a parameter call inform end insert go payload endpoint try do a ruby implementation gpt invalid argument error,2022-10-06 11:14:08,,,2022-10-06 11:17:43,<openai-api><gpt-3>,1,0,1,691,,2.0,473923.0,"<p>I looked at the Payload from OpenAI vs the payload from the Ruby Library and saw the issue.</p>
<p>My ruby library was setting the model to <code>code-davinci-001</code> while OpenAI was using <code>code-davinci-002</code>.</p>
<p>As soon as I manually altered the model: attribute in debug, the completion started working correctly.</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;id&quot;=&gt;&quot;cmpl-5yJ8b01Cw26W6ZIHoRSOb71Dc4QvH&quot;,
  &quot;object&quot;=&gt;&quot;text_completion&quot;,
  &quot;created&quot;=&gt;1665054929,
  &quot;model&quot;=&gt;&quot;code-davinci-002&quot;,
  &quot;choices&quot;=&gt;
  [{&quot;text&quot;=&gt;&quot;\n    \&quot;firstName\&quot;: \&quot;John\&quot;,\n    \&quot;lastName\&quot;: \&quot;Smith\&quot;&quot;,
    &quot;index&quot;=&gt;0,
    &quot;logprobs&quot;=&gt;nil,
    &quot;finish_reason&quot;=&gt;&quot;stop&quot;}],
  &quot;usage&quot;=&gt;{&quot;prompt_tokens&quot;=&gt;14, &quot;completion_tokens&quot;=&gt;19, 
 &quot;total_tokens&quot;=&gt;33}
}
</code></pre>
",2022-10-06 11:17:43,0.0,1.0
74524530,1,11805611.0,74524554.0,How to get the items inside of an OpenAIobject in Python?,like text inside data structure output gpt openai use python print object text component example object call qa output items above add a text do not get error not sure isolate text read docs,2022-11-21 20:26:49,,2023-04-02 20:17:47,2023-05-31 11:12:34,<python><openai-api><gpt-3>,4,0,2,4254,,2.0,20498988.0,"<pre><code>x = {&amp;quot;choices&amp;quot;: [{&amp;quot;finish_reason&amp;quot;: &amp;quot;length&amp;quot;,
                  &amp;quot;text&amp;quot;: &amp;quot;, everyone, and welcome to the first installment of the new opening&amp;quot;}], }

text = x['choices'][0]['text']
print(text)  # , everyone, and welcome to the first installment of the new opening
</code></pre>
",2022-11-21 20:30:27,0.0,4.0
58991927,1,12205961.0,,Can the HuggingFace GPT2DoubleHeadsModel be used for non-multiple-choice next token prediction?,accord huggingface transformer website gptdoubleheadsmodel not gptlmheadmodel gptdoubleheadsmodel gpt transformer model a language model a multiplechoice classification head rocstoriesswag task do mean use gptdoubleheadsmodel process nonmultiplechoicebased language model task word prediction multiplechoice question without make adjustment head need adjust head gptdoubleheadsmodel want nonmultiplechoicebased word predictions gptdoubleheadsmodel answer multiplechoice type question a bite confuse impression get read gpt paper gpt use language model process process type language task gpt regular language model head gptdoubleheadsmodel suggest need adjust head gpt different type language task thank,2019-11-22 10:08:44,,2020-11-29 12:06:50,2020-11-29 12:06:50,<nlp><huggingface-transformers><transformer-model><gpt-2>,0,0,2,425,0.0,,,,,,
62677651,1,3659250.0,,OpenAI GPT-2 model use with TensorFlow JS,possible generate texts openai gpt use tensorflowjs not limitation like model format,2020-07-01 13:12:01,,2020-11-29 11:52:07,2021-07-29 09:46:12,<tensorflow><machine-learning><nlp><tensorflow.js><gpt-2>,1,5,10,2664,,,,,,,
63543006,1,11263621.0,,How can I find the probability of a sentence using GPT-2?,try write a program give a list sentence return probable want use gpt quite new use not really know plan find probability a word give previous word multiply probabilities overall probability sentence occur not know probability a word occur give previous word psuedo code help,2020-08-23 03:07:19,,2020-11-29 12:05:32,2021-12-13 19:55:04,<python><nlp><probability><gpt-2>,4,0,2,4337,0.0,,,,,,
69602062,1,16451554.0,,Cudnn won't work when I install cudnn64_8.dll,currently work gpt run tensorflow text generation work repo specifically recently decide install cuda cudnn improve gpu capability instal instructions currently use windows  x nvidia geforce gtx  gpu use command prompt terminal follow instructions best download right gpu driver set environment variables copy cudnn file finish instal try generate unconditional sample model train happen not sure happen figure instal cudnn file incorrectly mess a bite remove cudnn_dll c program filesnvidia gpu compute toolkitcudavbin tell copy run unconditional sample gpt work just fine able generate text cudnn file cuda directories not sure inclusion cudnn_dll screw things do install wrong version cuda exactly go edit decide add environment variables terminal suggest above time do not oom error like time terminate program result exactly do wrong gpu run memory,2021-10-17 07:06:41,,2021-10-17 21:23:31,2021-10-17 21:23:31,<python><tensorflow><cudnn><gpt-2>,0,2,0,264,,,,,,,
71580925,1,4467390.0,,Generating 10000 sentences from GptNeo Model results in out of memory error,do work want generate  sentence gptneo model a gpu size gb run model gpu everytime code run memory a limitation number sentence generate a small snippet code,2022-03-23 01:47:29,,,2022-03-23 01:47:29,<nlp><huggingface-transformers><gpt-2>,0,3,0,88,,,,,,,
62799540,1,1019952.0,,Cannot convert GPT-2 model using Tensorflow.JS,try load a gpt model a nodejs project believe use tfjs library try convert gpt model tfjs model follow recommendations answer export gpt model savedmodel run follow code export savedmodel xxpb file run command convert savedmodel tfjs compatible file cause error say unsupported a way solve,2020-07-08 16:41:54,,2020-11-29 11:49:46,2021-02-21 16:25:26,<python><tensorflow><tensorflow.js><gpt-2>,1,0,1,783,,,,,,,
63350105,1,11718897.0,,How to alter gpt-2 code to work with Tensorflow 2.0?,try use gpt text generation compatibility errors run tensorflow  code upgrade script step follow clone repo follow directions developersmd run upgrade script file src terminal run build run enter traceback appear hparams deprecate new version tensorflow  call hparam parameters different params instantiate follow do not appear   translation tensorflow  do know make gpt work tensorflow  gpu nvidia xx thank,2020-08-11 01:09:02,,2020-11-29 12:02:24,2021-03-09 09:45:25,<python><docker><tensorflow><tensorflow2.0><gpt-2>,3,0,2,3613,,,,,,,
72821522,1,11484585.0,,Why does Post request to OpenAI in Unity result in error 400?,try use gpt a game make not able openai api correctly get unity docs code use return  bad request gpt docs idea time make web request unity probably miss obvious thank,2022-06-30 20:13:50,,,2023-02-18 16:06:27,<unity-game-engine><http-post><artificial-intelligence><gpt-3>,1,4,1,1973,,,,,,,
72758187,1,19417188.0,,OpenAI GPT-3 API: Fine tune a fine tuned model?,openai documentation attribute finetune api state a bite confusingly question better finetune a base model a finetuned model create a finetune model file now a bigger file sample want use improve finetuned model second round finetuning better finetune finetune finetuned model assume possible fine tune model create ,2022-06-26 00:35:25,,2023-03-13 13:30:31,2023-03-13 13:30:31,<transformer-model><openai-api><fine-tune><gpt-3>,1,1,9,2193,,,,,,,
67058277,1,15221534.0,,Understanding repository gpt transformer,project need understand able execute github repository about commonsense generation use gpt transformer language model quite extensive not program experience make sense good subject guide ithelp spot post question,2021-04-12 12:21:59,,,2021-04-12 12:21:59,<github><nlp><transformer-model><google-publisher-tag><gpt-2>,0,4,0,64,,,,,,,
73014448,1,6501180.0,,Is there a known workaround for the max token limit on the input to GPT-3?,a bite context recently start work a personal project accept url recipe web page pull html convert html simplify markdown gpt send markdown a thermal receipt printer kitchen print recipe web page a wide variety structure notorious include long irrelevant article recipe sake seo plan use finetuning api davinci fee a bunch straight recipe html input clean recipeonly markdown output notice maximum input token count train inference  html a web page larger like k tokens wonder a workaround train drive gpt tokens  open suggestions instance consider pass just visible text page html tree less context present form model easily confuse link navigational elements present page consider allow project accept printerfriendly versions recipes tend smaller easily come  token limit not sit offer a printerfriendly article not want a limitation,2022-07-17 18:43:06,,,2023-01-23 00:56:34,<machine-learning><gpt-3>,2,0,4,4818,,,,,,,
73117628,1,19564052.0,,How to solve API connection error and SSL certification error while connecting to GPT-3 open AI?,try run a python script jupyter notebook experiment gpt open ai create nlp project understand function use case get error ssl certification api connection try open a json file check solutions internet do not offer remedy simply try connect server api key code not work code execute follow above script throw follow errors do know solve kind problem suggest a solution work,2022-07-26 04:07:12,,,2023-03-21 11:54:26,<python-3.x><api><ssl-certificate><openai-api><gpt-3>,2,0,2,8064,,,,,,,
74978793,1,5038122.0,74992998.0,"OpenAI GPT-3 API error: ""InvalidRequestError: Unrecognized request argument supplied""",conversation_memory parameter not recognize try serveral different model error lastest openai computer not understand error,2023-01-02 03:28:18,,2023-03-13 13:28:06,2023-03-13 13:28:49,<python><artificial-intelligence><openai-api><gpt-3>,1,0,0,2798,,2.0,10347145.0,"<p>The error itself tells you what's wrong.</p>
<p><strong>You're trying to pass <code>conversation_memory</code> as a parameter to the Completions endpoint, which the OpenAI API doesn't recognize as a parameter.</strong></p>
<p>See the <a href=""https://beta.openai.com/docs/api-reference/completions/create"" rel=""nofollow noreferrer"">complete list</a> of parameters you can pass to the Completions endpoint:</p>
<ul>
<li><code>model</code></li>
<li><code>prompt</code></li>
<li><code>suffix</code></li>
<li><code>max_tokens</code></li>
<li><code>temperature</code></li>
<li><code>top_p</code></li>
<li><code>n</code></li>
<li><code>stream</code></li>
<li><code>logprobs</code></li>
<li><code>echo</code></li>
<li><code>stop</code></li>
<li><code>presence_penalty</code></li>
<li><code>frequency_penalty</code></li>
<li><code>best_of</code></li>
<li><code>logit_bias</code></li>
<li><code>user</code></li>
</ul>
",2023-01-03 11:51:49,0.0,1.0
75112672,1,10337134.0,75124884.0,No module named 'openai_secret_manager',ask chatgpt about csv data chatgpt answer example read a csv file use pandas use data train finetune gpt use openai api get error,2023-01-13 17:40:20,,2023-01-13 17:55:42,2023-03-25 14:21:52,<python><pandas><openai-api><gpt-3>,2,5,2,9195,,2.0,12146581.0,"<p>No need to use <strong>openai_secret_manager</strong>. I faced the same problem and deleted it and you need to generate &amp; place an API from your account on OpenAI directly to the code.</p>
<pre><code>import pandas as pd
import openai_secret_manager

# Read the CSV file
df = pd.read_csv(&quot;example.csv&quot;)

# Use the data from the CSV file to train or fine-tune GPT-3
# (Assuming you have the OpenAI API key and the OpenAI Python library installed)
import openai
openai.api_key = openai_api_key
response = openai.Completion.create(
    engine=&quot;text-davinci-002&quot;,
    prompt=(f&quot;train on data from example.csv{df}&quot;),
    max_tokens=2048,
    n = 1,
    stop=None,
    temperature=0.5,
)
print(response[&quot;choices&quot;][0][&quot;text&quot;])
</code></pre>
<p><a href=""https://i.stack.imgur.com/MgxNP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MgxNP.png"" alt=""enter image description here"" /></a></p>
<p>Copy and paste the API and replace <strong>openai_api_key</strong> here</p>
<pre><code>openai.api_key = &quot;PLACE_YOUR_API_IN_HERE&quot;
</code></pre>
",2023-01-15 11:56:45,4.0,4.0
75130116,1,13553999.0,75130180.0,Getting 400 Bad Request from Open AI API using Python Flask,want response use flask openai api get status  bad request browser bad request browser proxy send a request server not understand check postman,2023-01-16 03:56:19,,2023-01-24 18:21:04,2023-01-24 18:21:04,<flask><flask-restful><openai-api><gpt-3>,1,1,-2,941,,2.0,18515689.0,"<p>It would be best if you check the openai documentation to make sure you are using the correct endpoint and data format in your request.
Also, you should check your API key, if it is correct and if you have reached the limit of requests.</p>
<p>Also, it's worth noting that the code you provided is missing the import statement for Flask. You will need to add the following line at the top of your file:</p>
<p>from <code>flask import Flask, request</code>
Also, I see that you're using <code>request.form['text']</code> but you should check if the request is a GET or POST request.</p>
<pre><code>if request.method == 'POST':
    user_input = request.form['text']
else:
    user_input = request.args.get('text')
</code></pre>
<p>This is to avoid a KeyError being raised when the request is a GET request.</p>
",2023-01-16 04:12:52,0.0,1.0
75299615,1,15313661.0,75300061.0,OpenAI API: Can I remove the line break from the response with a parameter?,start use openai api r download package get a double linebreak text response example code a way rid without correct response text use function,2023-01-31 15:46:42,,2023-03-13 14:04:27,2023-05-04 11:02:46,<r><openai-api><gpt-3>,3,0,3,1773,,2.0,10347145.0,"<p>No, it's not possible.</p>
<p>The OpenAI API returns the completion with a starting <code>\n\n</code> by default. There's no parameter for the <a href=""https://platform.openai.com/docs/api-reference/completions"" rel=""nofollow noreferrer"">Completions endpoint</a> to control this.</p>
<p>You need to remove the line break manually.</p>
<p>An example response looks like this:</p>
<pre><code>{
  &quot;id&quot;: &quot;cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7&quot;,
  &quot;object&quot;: &quot;text_completion&quot;,
  &quot;created&quot;: 1589478378,
  &quot;model&quot;: &quot;text-davinci-003&quot;,
  &quot;choices&quot;: [
    {
      &quot;text&quot;: &quot;\n\nThis is indeed a test&quot;,
      &quot;index&quot;: 0,
      &quot;logprobs&quot;: null,
      &quot;finish_reason&quot;: &quot;length&quot;
    }
  ],
  &quot;usage&quot;: {
    &quot;prompt_tokens&quot;: 5,
    &quot;completion_tokens&quot;: 7,
    &quot;total_tokens&quot;: 12
  }
}
</code></pre>
",2023-01-31 16:20:30,1.0,3.0
75322813,1,33522.0,75322907.0,"OpenAI GPT-3 API error: ""That model does not exist""",model do not exist api nodejs,2023-02-02 11:56:10,,2023-03-13 14:09:17,2023-03-13 14:09:17,<node.js><openai-api><gpt-3>,1,0,-1,2011,,2.0,10347145.0,"<p>You have to set the <code>model</code> parameter to <code>text-davinci-003</code>, <code>text-curie-001</code>, <code>text-babbage-001</code> or <code>text-ada-001</code>. It's a <a href=""https://platform.openai.com/docs/api-reference/completions/create"" rel=""nofollow noreferrer"">required parameter</a>.</p>
<p>Also, all <a href=""https://beta.openai.com/docs/api-reference/engines"" rel=""nofollow noreferrer"">Engines endpoints</a> are deprecated.</p>
<p><a href=""https://i.stack.imgur.com/lQu9c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lQu9c.png"" alt=""Deprecated"" /></a></p>
",2023-02-02 12:04:00,1.0,2.0
75376813,1,21105687.0,75397197.0,OpenAI fine-tune with python return null model,try finetune model openai gpt use python follow code openai respond data train file upload check finetune response code get fine_tune_model null use completion question check example loop finetune complete use ft id,2023-02-07 17:15:43,,2023-02-07 18:42:22,2023-03-27 14:33:54,<python><openai-api><gpt-3>,2,1,1,864,,2.0,8949058.0,"<p>Here is data from the OpenAI documentation on fine-tuning:</p>
<blockquote>
<p>After you've started a fine-tune job, it may take some time to complete. Your job may be queued behind other jobs on our system, and training our model can take minutes or hours depending on the model and dataset size.</p>
</blockquote>
<p>Ref: <a href=""https://platform.openai.com/docs/guides/fine-tuning"" rel=""nofollow noreferrer"">https://platform.openai.com/docs/guides/fine-tuning</a></p>
<p>The OpenAI guide uses the CLI tool to create the fine-tuning and then accesses the model programatically once the training has completed.</p>
<p>Therefore, you couldn't run the code in Python as you have laid it out, since you need to wait for the training to complete. Meaning you can't train the model on the fly and use it instantly.</p>
",2023-02-09 10:22:06,1.0,2.0
68783979,1,14067076.0,,Does anyone knows how to input a text content in huggingface gpt2?,want input conversation data input gpt model huggingface transformers example a do meet david b meet central park a not quite strange day predict b not like upper example want input conversation data transformer a reply pretrained model gpt anybody tell,2021-08-14 14:02:04,,,2021-08-14 14:02:04,<huggingface-transformers><gpt-2>,0,2,0,324,,,,,,,
71028228,1,11132563.0,,GPT-3 long input posts for Question Answering,understand gpt train a specific task include label examples desiredtest example question answer include a context a question situation input prompt long people address use hug face gptj implementation input token limit  include multiple qa examples prompt especially contexts quickly reach limit limitting example prompt inputted do know issue handle a gptj set especially qa,2022-02-08 03:26:09,,,2022-03-24 13:08:57,<deep-learning><nlp><huggingface-transformers><nlp-question-answering><gpt-3>,1,1,1,2266,,,,,,,
71040945,1,14272134.0,,GPT-2: How do I speed up/optimize token text generation?,try generate a  token text use gpt simple take  second generate sentence ai dungeon take  second generate size sentence a way fastenoptimize gpt text generation,2022-02-08 21:11:07,,2023-01-16 08:09:49,2023-01-16 08:09:49,<openai-api><gpt-2>,3,0,2,2542,,,,,,,
71130046,1,13679903.0,,GPT-2 pretrained model fails to load when TF v2 behaviour is disabled,try use gpt a codebase write tensorflow x run code tf x installation binaries flag without flag gpt pretrained model load fine model fail load flag use code error use tf  transformers v way make work tf v behaviour disable,2022-02-15 16:31:37,,,2022-02-16 17:59:17,<python><tensorflow><huggingface-transformers><gpt-2>,1,0,0,351,,,,,,,
72008843,1,18567298.0,,TypeError: Cannot subclass <class 'typing._SpecialForm'> while fine tuning GPT-J,try fine tune gptj follow github repository run train command encounter error look like a source code error not sure raise issue github regard help appreciate,2022-04-26 05:34:24,,2022-05-03 04:33:05,2022-06-17 13:36:29,<python><class><gpt-3>,1,0,2,459,,,,,,,
72047597,1,7876035.0,,How can I respond to a CLI prompt in Kaggle?,use kaggle generate poetry sample gpt notebook use datasets gwern poetry generator use nshepperd gpt model work fine notebook generate unconditional sample want generate sample interactive conditional method problem request a model prompt no way enter a prompt do not work enter a prompt kaggle cli run desktop use compute power automatically allow enter text response prompt a way enter a prompt kaggle try auto respond use flag like use y auto accept a yesno prompt install not work far notebook public want test,2022-04-28 16:58:34,,2022-05-20 17:56:18,2022-05-20 17:56:18,<python><jupyter-notebook><command-line-interface><kaggle><gpt-2>,0,4,0,141,0.0,,,,,,
74647792,1,14729820.0,,TrOCR fine-tuning with Text generator model like gpt-2 or Bert,want finetune trocr transformer model model a different decoder like bert gpt dataset image text pair text follow format inside data folder try execute google collab face issue not know use google collab dataset try finetune like iam dataset line segment link code mention repo use run cell get issue,2022-12-01 20:23:32,,2022-12-02 14:56:24,2023-03-07 12:18:19,<bash><pytorch><nlp><huggingface-transformers><gpt-2>,0,1,1,381,,,,,,,
74682597,1,13788466.0,,Fine-Tuning GPT2 - attention mask and pad token id errors,try finetune gpt wikitext dataset just help learn process run a warn message not see attention mask pad token id not set a consequence observe unexpected behavior pass input obtain reliable result set  openend generation strange clearly specify eos token code instantiate tokenizer train complete without crash loss improve epoch inference model output absolute gibberish generate a single word nothing think a link warn message get model not perform get train valid test data use raw file manually add raw txt file datasets result train data look like examples take middle text file follow tutorial closely code,2022-12-05 01:57:10,,,2023-06-25 08:48:35,<machine-learning><tokenize><training-data><gpt-2><fine-tune>,2,1,4,4905,,,,,,,
74715461,1,12341397.0,,"OpenAI Python API is giving gibberish responses for the query ""hi""",use python access openai api use discordpy integrate a discord bot command look like command work properly intend input give a proper output recently discover a simple input hi give gibberish output output completely different time refer image output try  try  try  note command work completely fine query example correct response strangely give a normal response hello reason respond improperly a simple hi query,2022-12-07 10:51:47,,,2022-12-07 10:51:47,<python><discord.py><openai-api><gpt-3>,0,1,2,541,,,,,,,
74654341,1,17971398.0,,How to Answer Subjective/descriptive types of lQuestions using BERT Model?,try implement bert model question answer task a little different exist qa model model give text  page ask question base text expect answer ask short descriptive subjective type try implement bert task problems face input token limit bert  answer long form instance process event,2022-12-02 10:34:26,,,2022-12-02 17:10:23,<nlp><huggingface-transformers><bert-language-model><transformer-model><gpt-2>,1,0,0,22,,,,,,,
74687645,1,20006087.0,,GPT3 conversational Discord bot stops working after 5-6 sentences,create a fun discord bot talk like imaginary crush lol problem work  line give error see error error discord chat message not come try change max_tokens prompt no avail give administrator permissions bot,2022-12-05 11:44:18,,2022-12-05 11:45:10,2022-12-06 22:16:45,<discord><gpt-3>,1,1,0,397,,,,,,,
75400926,1,2686197.0,75401250.0,OpenAI ChatGPT API: CORS policy error when fetching data,try write a simple javascript script use chatgpt api ask a question a response get follow error message enable cors headers server host environment error remain reason issue fix issue code,2023-02-09 15:37:20,,2023-03-13 14:24:02,2023-06-22 05:37:27,<javascript><openai-api><gpt-3>,1,3,1,2205,,2.0,10347145.0,"<p><strong>UPDATE: 1 March 2023</strong></p>
<h3>ChatGPT API is now available</h3>
<p>As stated in the official <a href=""https://openai.com/blog/introducing-chatgpt-and-whisper-apis"" rel=""nofollow noreferrer"">OpenAI blog</a>:</p>
<blockquote>
<p><strong>ChatGPT and Whisper models are now available on our API</strong>, giving
developers access to cutting-edge language (not just chat!) and
speech-to-text capabilities. Through a series of system-wide
optimizations, we’ve achieved 90% cost reduction for ChatGPT since
December; we’re now passing through those savings to API users.
Developers can now use our open-source Whisper large-v2 model in the
API with much faster and cost-effective results. ChatGPT API users can
expect continuous model improvements and the option to choose
dedicated capacity for deeper control over the models. We’ve also
listened closely to feedback from our developers and refined our API
terms of service to better meet their needs.</p>
</blockquote>
<p>See the <a href=""https://platform.openai.com/docs/guides/chat"" rel=""nofollow noreferrer"">documentation</a>.</p>
<hr />
<h4>ChatGPT API is not available yet</h4>
<p>As stated on the official <a href=""https://twitter.com/OpenAI/status/1615160228366147585?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Etweet"" rel=""nofollow noreferrer"">OpenAI Twitter profile</a>:</p>
<blockquote>
<p>We've learned a lot from the ChatGPT research preview and have been
making important updates based on user feedback. <strong>ChatGPT will be
coming to our API and Microsoft's Azure OpenAI Service soon.</strong></p>
</blockquote>
<p>Did you mean the GPT-3 API? If yes, then read the <a href=""https://platform.openai.com/docs/guides/completion"" rel=""nofollow noreferrer"">documentation</a>, see the list of all available <a href=""https://platform.openai.com/docs/models/gpt-3"" rel=""nofollow noreferrer"">GPT-3 models</a>, and learn how to write the code using the <a href=""https://platform.openai.com/docs/api-reference/completions"" rel=""nofollow noreferrer"">Completions endpoint</a>.</p>
",2023-02-09 16:03:27,3.0,3.0
72077048,1,2925716.0,,GPT-3: a medical analogue,analogue answer like excellent doctor einstein ancient greek url list possibilities just discover gpt amaze,2022-05-01 13:15:16,,,2022-07-19 08:49:07,<medical><gpt-3>,1,1,-1,71,,,,,,,
72080207,1,14045986.0,,GPT-3 completions not working with API KEY,sorry a simple problem new stuff i̇ code api return say i̇ not right api key api key a processenv file i̇ not i̇ a file name processenv a variable call openai_api_key do not work return i̇ set apikey equal actual key like a roundabout solution thank,2022-05-01 20:31:58,,,2023-06-19 11:57:30,<node.js><openai-api><gpt-3>,1,0,2,1940,,,,,,,
69889395,1,5136891.0,,implement do_sampling for custom GPT-NEO model,run  output run  output custom model return output stock return different result spend a lot time figure do_sampling work transformers require help guy appreciate code a custom model different result thank,2021-11-08 20:11:40,,,2021-11-09 09:57:05,<python><nlp><torch><huggingface-transformers><gpt-2>,1,0,0,214,,,,,,,
74776748,1,413741.0,,embeddings distribution wrong,have code suppose plot word embeddings create a list embeddings identical word expect a cluster point embeddings point scatter like idea do wrong,2022-12-12 20:08:22,,2022-12-12 20:16:13,2022-12-12 20:16:13,<python><word-embedding><openai-api><gpt-3>,0,0,0,57,,,,,,,
74803152,1,7158458.0,,How to return an image as a response in Django,do a post request gpt order code completion output send input get response expect actually code write gpt response assume actual code write fall solution header life figure view image just return image link nothing open update assume response markdown use a markdown parser return response markdown,2022-12-14 19:07:32,,2022-12-14 19:50:53,2022-12-14 19:50:53,<python><django><gpt-3>,0,0,0,55,,,,,,,
74813629,1,19800686.0,,How to deploy GPT-like model to Triton inference server?,tutorials deployment gptlike model inference triton look like preprocess data fee input triton inference server postprocess output like use finetuned gpt model method give incorrect result correct result obtain method way deploy finetuned gptlike huggingface model triton inference not,2022-12-15 15:09:28,,,2022-12-15 15:09:28,<pytorch><huggingface-transformers><gpt-2><triton>,0,3,2,189,,,,,,,
74822543,1,7339624.0,,"Colab: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory warn(f""Failed to load image Python extension: {e}"")",try use python package google colab finetune gpt instal version package error import help solutions give question pass error downgrade package like now face error import package colab crash mind error import package not a bug code clear error just import like deal error,2022-12-16 09:31:33,,,2022-12-16 09:31:33,<python><import><google-colaboratory><huggingface-transformers><gpt-2>,0,0,4,3106,,,,,,,
74972916,1,1152980.0,,How to ping the ChatGPT via curl and retain the state of conversation,code work curl question chatgpt reply a oneoff basis try engage a conversation require state previous submissions reference chat not follow like know need code need retain context conversation curl run new terminal,2023-01-01 02:48:17,,2023-01-11 20:20:26,2023-04-23 08:26:49,<javascript><node.js><express><curl><gpt-3>,2,1,3,693,,,,,,,
74990552,1,2172547.0,,openai gpt-3 is asking random question in return of user query when using apis,use open ai api parameters prompt send user answer good bot reply like automatically append question like not relative chat open ai playground parameters use behave properly maintain context query context open ai playground use send open ai api issue fix,2023-01-03 08:07:42,,2023-01-09 08:55:23,2023-01-09 08:55:23,<openai-api><gpt-3>,0,0,0,261,,,,,,,
74774018,1,20758268.0,,How to keep the conversation going with OpenAI API PHP sdk,try a conversation go use completion method openai php sdk prompt  prompt  ask ai forget ask reply random answer second prompt code use  call miss session alive call order make ai remember ask,2022-12-12 16:10:02,,,2023-05-02 10:02:02,<php><openai-api><gpt-3>,3,0,3,4735,,,,,,,
74832384,1,16749013.0,,How to train GPT2 with Tensorflow,try train gpt model custom dataset fail error think model dataset correctly define process refer article error show execute tell resolve error proper way train model,2022-12-17 07:13:20,,,2022-12-17 07:13:20,<python><tensorflow><gpt-2>,0,1,0,326,,,,,,,
74926252,1,8124392.0,,How to replace the tokenize() and pad_sequence() functions from transformers?,get follow import get error use function fix problem not docs version roll transformers,2022-12-27 06:19:42,,,2022-12-27 13:26:36,<python><huggingface-transformers><huggingface-tokenizers><gpt-2>,1,1,0,68,,,,,,,
74969653,1,19881860.0,,"OpenAI and Javascript error : Getting 'TypeError: Cannot read properties of undefined (reading 'create') at Object.<anonymous>""",sorry basic question get no a basic piece code npm instal latest version openai get a constant error terminal code grateful help expect terminal response prompt,2022-12-31 12:51:52,,2023-01-11 18:10:03,2023-01-11 18:10:03,<javascript><typeerror><openai-api><gpt-3>,1,1,0,4462,,,,,,,
74976042,1,1152980.0,,How to receive ChatGPT multi-line replies when using CURL?,code work problem chatgpt reply  line render terminal rest text cut not familiar curl command update code multiline reply render edit try consolelog response inside express post request believe curl problem appear curl not problem chatgpt simply cut mid reply curl,2023-01-01 17:04:20,,2023-01-11 20:20:36,2023-01-11 20:20:36,<javascript><node.js><express><curl><gpt-3>,1,2,-2,1933,,,,,,,
75060922,1,8028335.0,,No space left on device error when trying to load GPT2 model,try run experiment gpt use error a traceback lead unlikely error code exact code work fine gptb tlarge course model line think issue try download gpt model run space device homeusername directory a pretty small storage datausername directory storage not quite sure redirect download weight directory help really appreciate help resolve,2023-01-09 17:27:33,,,2023-01-09 23:06:58,<linux><huggingface><oserror><gpt-2>,2,0,0,251,,,,,,,
75401992,1,2686197.0,75402073.0,"OpenAI API error: ""You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth""",create a php script access open ai api ask a query a response get follow error think provide api key variable code,2023-02-09 17:00:01,,2023-03-02 11:54:56,2023-04-06 16:49:45,<php><wordpress><curl><openai-api><gpt-3>,1,1,0,2902,,2.0,10347145.0,"<p>All <a href=""https://beta.openai.com/docs/api-reference/engines"" rel=""nofollow noreferrer"">Engines endpoints</a> are deprecated.</p>
<p><a href=""https://i.stack.imgur.com/lQu9c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lQu9c.png"" alt=""Deprecated"" /></a></p>
<p>This is the correct <a href=""https://platform.openai.com/docs/api-reference/completions"" rel=""nofollow noreferrer"">Completions endpoint</a>:</p>
<pre><code>https://api.openai.com/v1/completions
</code></pre>
<h3>Working example</h3>
<p>If you run <code>test.php</code> the OpenAI API will return the following completion:</p>
<blockquote>
<p>string(23) &quot;</p>
<p>This is indeed a test&quot;</p>
</blockquote>
<p><strong>test.php</strong></p>
<pre><code>&lt;?php
    $ch = curl_init();

    $url = 'https://api.openai.com/v1/completions';

    $api_key = 'sk-xxxxxxxxxxxxxxxxxxxx';

    $post_fields = '{
        &quot;model&quot;: &quot;text-davinci-003&quot;,
        &quot;prompt&quot;: &quot;Say this is a test&quot;,
        &quot;max_tokens&quot;: 7,
        &quot;temperature&quot;: 0
    }';

    $header  = [
        'Content-Type: application/json',
        'Authorization: Bearer ' . $api_key
    ];

    curl_setopt($ch, CURLOPT_URL, $url);
    curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);
    curl_setopt($ch, CURLOPT_POST, 1);
    curl_setopt($ch, CURLOPT_POSTFIELDS, $post_fields);
    curl_setopt($ch, CURLOPT_HTTPHEADER, $header);

    $result = curl_exec($ch);
    if (curl_errno($ch)) {
        echo 'Error: ' . curl_error($ch);
    }
    curl_close($ch);

    $response = json_decode($result);
    var_dump($response-&gt;choices[0]-&gt;text);
?&gt;
</code></pre>
",2023-02-09 17:08:02,8.0,2.0
75454265,1,5832020.0,75458209.0,OpenAI GPT-3 API: Does fine-tuning have a token limit?,documentation gpt api say documentation fine tune model say question do  word limit apply fine tune model do double dataset size mean number train datasets instead size train dataset,2023-02-14 23:38:34,,2023-04-10 10:41:07,2023-04-10 10:41:07,<openai-api><gpt-3>,1,1,2,3173,,2.0,10347145.0,"<p>As far as I understand...</p>
<p><strong>GPT-3 models have token limits</strong> because you can only provide 1 prompt and get 1 completion. Therefore, as stated in the official <a href=""https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them"" rel=""nofollow noreferrer"">OpenAI article</a>:</p>
<blockquote>
<p>Depending on the <a href=""https://platform.openai.com/docs/models/gpt-3"" rel=""nofollow noreferrer"">model</a> used, requests can use up to 4097 tokens shared
between prompt and completion. If your prompt is 4000 tokens, your
completion can be 97 tokens at most.</p>
</blockquote>
<p>Whereas, <strong>fine-tuning as such doesn't have a token limit</strong> (i.e., you can have a million training examples, a million prompt-completion pairs), as stated in the official <a href=""https://platform.openai.com/docs/guides/fine-tuning/prepare-training-data"" rel=""nofollow noreferrer"">OpenAI documentation</a>:</p>
<blockquote>
<p>The more training examples you have, the better. We recommend having
at least a couple hundred examples. In general, we've found that each
doubling of the dataset size leads to a linear increase in model
quality.</p>
</blockquote>
<p>But, <strong>each fine-tuning prompt-completion pair does have a token limit</strong>. Each fine-tuning prompt-completion pair should not exceed the token limit.</p>
",2023-02-15 10:09:58,0.0,6.0
75709199,1,2627777.0,,SimpleDirectoryReader cannot be downloaded via llama_index's download_loader,use llama_index package index document query use gpt work fairly individual pdfs a large anout pdfs like load a single run use simpledirectoryreader get follow error follow command run readersllamahub_modulesfile folder a folder call pdf do not a basepy file try uninstalling reinstall llama_index python module no impact python version  work,2023-03-11 20:04:05,,,2023-05-16 06:00:15,<python><gpt-3>,2,0,2,2222,,,,,,,
75731765,1,19577630.0,,gpt3 - error with the openai api when trying to generate an embedding,a python code create a embed openai try execute code receive error server currently overload request sorry about retry request contact help center helpopenaicom error persist python code,2023-03-14 10:16:04,,2023-03-14 10:31:41,2023-03-14 20:41:29,<openai-api><gpt-3>,1,2,-2,2449,,,,,,,
75744277,1,13887235.0,,How Can I make openAI API respond to requests in specific categories only?,create openai api use python respond type prompt want make api respond request relate ad product description greet request user send a request not relate task api send a message like not suitable task like want update code generate a chat like make bot understand generate ads greet request ignore ex user hello api hello assist today brand user write a social media post follow product run facebook aim parent nnproduct learn room a virtual environment help students kindergarten high school excel school api look a way child a head start school look no learn room virtual environment design help students kindergarten high school excel study unique platform offer personalize learn plan interactive activities realtime feedback ensure child get education child best chance succeed school learn room user unite state locate api not suitable type task update code,2023-03-15 11:47:20,,,2023-03-24 03:24:44,<python><python-3.x><openai-api><gpt-3><chatgpt-api>,2,0,1,1252,,,,,,,
74996908,1,19735730.0,,can't change embedding dimension to pass it through gpt2,practice image caption problems different dimension tensors image embed aka size   gpt use caption generation need size n  n number tokens caption begin not know change dimension image embed pass gpt think a good idea image embed zero size   think negatively affect result caption thank help try image embeddings zero size   think wo not help a lot,2023-01-03 17:50:56,,2023-01-03 20:44:38,2023-01-03 20:44:38,<machine-learning><deep-learning><embedding><gpt-2><multimodal>,0,0,4,96,,,,,,,
75026428,1,7148393.0,,Error in formating the URL for chatGPT's API,try make a program a user ask gpt a question api try gpt assistant design code errors use outdated information  modify code go documentation work generate a javaiofilenotfoundexception error believe problem format completion section url not sure tell wrong greatly appreciate know api key valid change url whats show output appropriate information format vmodelstextcurie output detail model textcurie format vcompletions output a response base give prompt,2023-01-06 02:41:49,,2023-01-09 09:00:09,2023-03-11 21:00:09,<java><gpt-3>,1,8,1,342,,,,,,,
75049140,1,20958759.0,,"OpenAI GPT-3 API error 429: ""Request failed with status code 429""",try connect openai api vuejs project ok time try post request a  status code request do not chance make help response method vuejs server,2023-01-08 15:46:00,,2023-03-13 13:34:29,2023-05-03 09:02:19,<vue.js><openai-api><gpt-3>,1,3,2,3852,,,,,,,
75196414,1,16857335.0,,"OpenAI API error 500: ""The server had an error while processing your request"", 503: ""Service Unavailable"" or 504: ""Gateway Timeout""",create a python script loop a list text string string about  character length summarize string code response prompt a loop work maybe   items text list receive error happen consistently use different api key prompt account try exponential backoff no success idea happen,2023-01-21 20:40:18,,2023-02-21 15:20:27,2023-03-30 10:49:26,<python><openai-api><gpt-3>,1,1,0,3322,,,,,,,
75196859,1,17300847.0,,How to make the bot multithreaded?,write a telegram bot base openai a problem multithreading user ask bot person information example user know konstantin polukhin bot yes begin second user respect bot yes know konstantin polukhin necessary make sure data do not overlap bot say relate request user try methods suggest none help code,2023-01-21 22:00:57,,2023-01-24 18:38:34,2023-01-24 18:38:34,<python><telegram-bot><openai-api><aiogram><gpt-3>,0,6,0,163,,,,,,,
75196860,1,11063729.0,,"What is the OpenAI API warning: To avoid an invalid_request_error, best_of was set to equal n. What is ""best of""?",best warn result use openai api a pc run win context use openai api jupyter lab ir kernel have rgpt library instal notebook api successfully perform a test code completion do not matter api make a single multiple api request return warn follow result use  query perform multiple unsuccessful web search include a search stack overflow information about warn exist no information about warn probably early process openai api relatively new people decide post question answer regard warn find information difficult time consume users boldly go go errors warn message not inspire confidence,2023-01-21 22:01:29,,2023-01-21 23:24:09,2023-01-21 23:50:57,<jupyter-lab><openai-api><gpt-2><gpt-3>,1,0,-1,245,,,,,,,
75248089,1,20372902.0,,How to work with JSON lines GPT-2 database?,download file just a randomly answer json format want train tensorflowjs model use database not a question database need want train model offline version gpt do not a pretrained model yes want use javascript tensorflowjs library note ones file download use download_datasetpy script xlmkvalidjsonl xl mtestjsonl xlmtrainjsonl xlmvalidjsonl use repo,2023-01-26 15:15:34,,2023-01-26 22:04:56,2023-01-26 22:04:56,<javascript><artificial-intelligence><tensorflow.js><gpt-2>,0,0,0,128,,,,,,,
75067851,1,13298551.0,,How to fix Python pip install openai error: subprocess-exited-with-error,try install openai python  windows os pip fully upgrade get error error message no idea solve error anybody a hint message try solution do not work,2023-01-10 09:23:45,,2023-01-19 17:17:20,2023-03-09 19:20:15,<python><python-3.x><windows><openai-api><gpt-3>,2,0,3,6955,,,,,,,
75106599,1,18805643.0,,OpenAI GPT-3 API: Why am I getting different completions on Playground vs. the API?,try use ada language processor openai summarize a piece text try use playground function work a summarization make sense use humans curl playground curl transform php code stop work better say work return complete nonsense nothing similar result playground php code now try use a json code like write a php array convert json result try use a library return nonsense say nonsense text return not read say hey a proffesional synopsis go example a sentence get iterations not pretty no think oh look not republican kid kno things oh a restrictious school assure no mention republicans kid text process question do wrong do openai work differently playground code,2023-01-13 08:28:15,,2023-03-13 13:44:45,2023-03-13 13:44:45,<php><artificial-intelligence><openai-api><gpt-3>,1,0,2,1393,,,,,,,
75254337,1,19968983.0,,"App framework for linking HuggingFace transformers to, using a mobile device (Android)",look create app use a mobile device android accept huggingface api form a simple chatbot question simply efficient method create base application past use termux writedebugcompile app android phone app need cleanly possible allow seamless upgrade embed websites,2023-01-27 05:19:10,,,2023-01-27 05:19:10,<android><api><termux><huggingface><gpt-3>,0,0,0,90,,,,,,,
75302104,1,10897106.0,,How to add encoder's last hidden state to GPT2 as encoder-decoder attention?,a bertbased encoder model encoder want input hide state output a gptbased model decoder no options use encoder hide layer input gpt achieve want like use encoderdecoder attention transformer unit gpt,2023-01-31 19:15:00,,,2023-01-31 19:15:00,<pytorch><nlp><huggingface-transformers><bert-language-model><gpt-2>,0,0,0,184,,,,,,,
75344458,1,21140352.0,,OpenAI GPT API pre-tokenizing?,try make a personal assistant chatbot use gpt ai api answer question about ask things order a lot information about currently do prompt example mean time ask a question prompt include information about mean get tokenized single time a question ask a way pretokenize information about store way ask information about cost suck a lot tokens thank try look online no avail,2023-02-04 10:28:49,,,2023-04-12 15:51:44,<artificial-intelligence><token><tokenize><gpt-3>,1,0,0,306,,,,,,,
75344868,1,19813924.0,,Saving data with the hive package,save retrieve map use hive pressuremodel class look like case wonder do sharedpreferences chatgtp help code save data code data,2023-02-04 11:44:41,,2023-02-05 13:28:16,2023-02-05 13:28:16,<flutter><dart><hive><gpt-3>,0,0,0,70,,,,,,,
75624308,1,19040716.0,75626340.0,OpenAI GPT-3 API errors: 'text' does not exist TS(2339) & 'prompt' does not exist on type 'CreateChatCompletion' TS(2345),property text do not exist type createchatcompletionresponsechoicesinner ts  argument type model string prompt string temperature number top_p number max_tokens number frequency_penalty number presence_penalty number not assignable parameter type createchatcompletionrequest object literal specify know properties prompt do not exist type createchatcompletionrequest ts  video errors a beginner cod try make applications base videos learn thank application respond without return error enter image description,2023-03-03 07:34:41,,2023-03-03 11:23:35,2023-05-15 06:29:57,<next.js><openai-api><gpt-3>,2,0,0,497,,2.0,10347145.0,"<h3>Problem</h3>
<p>You watched a tutorial which used the <a href=""https://platform.openai.com/docs/api-reference/completions/create"" rel=""nofollow noreferrer"">GPT-3 Completions endpoint</a> (you need to provide the prompt and parameters to get a completion). In this case, this is the function which generates a completion:</p>
<pre><code>openai.createCompletion()
</code></pre>
<p>Whereas, you used the code from the tutorial, <strong>but used the <a href=""https://platform.openai.com/docs/api-reference/chat/create"" rel=""nofollow noreferrer"">ChatGPT Completions endpoint</a></strong> (you need to provide the chat message to get a completion). In this case, this is the function which generates a completion:</p>
<pre><code>openai.createChatCompletion()
</code></pre>
<h3>Solution</h3>
<p>So, change this...</p>
<pre><code>openai.createChatCompletion()
</code></pre>
<p>...to this.</p>
<pre><code>openai.createCompletion()
</code></pre>
<p>Both errors will disappear.</p>
<h3>My advice</h3>
<p>However, you want to achieve a chat-like bot using a GPT-3 model. At the time the tutorial was recorded, this was the only way to do it. Since 1 March 2023, the <code>gpt-3.5-turbo</code> model is available. I strongly suggest you to use it. See the official <a href=""https://platform.openai.com/docs/guides/chat"" rel=""nofollow noreferrer"">OpenAI documentation</a>.</p>
",2023-03-03 11:10:57,0.0,1.0
75718913,1,2278116.0,75719777.0,"OpenAI GPT-3 API: Why do I get different, non-related random responses to the same question every time?",use “textdavinci ” model copy code form openai playground bot keep give random response a simple “hello ” everytime code use response simple “hello ” chat  different time time give a hello world program java second time answer correctly ‘hi help today ’ time just dont use simple ‘hello ’ prompt openai playground give accurate response eveytime hi help today,2023-03-13 07:12:36,,2023-03-13 14:55:26,2023-03-13 14:55:26,<python><chatbot><openai-api><gpt-3>,1,0,0,700,,2.0,10347145.0,"<p>As stated in the official <a href=""https://platform.openai.com/docs/guides/completion/prompt-design"" rel=""nofollow noreferrer"">OpenAI documentation</a>:</p>
<blockquote>
<p>The temperature and top_p settings control how deterministic the model
is in generating a response. <strong>If you're asking it for a response where
there's only one right answer, then you'd want to set these lower.</strong> If
you're looking for more diverse responses, then you might want to set
them higher. The number one mistake people use with these settings is
assuming that they're &quot;cleverness&quot; or &quot;creativity&quot; controls.</p>
</blockquote>
<p>Change this...</p>
<pre><code>temperature = 0.9
</code></pre>
<p>...to this.</p>
<pre><code>temperature = 0
</code></pre>
",2023-03-13 09:02:18,0.0,1.0
75773786,1,9680491.0,75774187.0,Can't access gpt-4 model via python API although gpt-3.5 works,able use gptturbo model access chatgpt api not gpt model code use test exclude openai api key code run write replace gptturbo gpt gpt gptk give error openaierrorinvalidrequesterror model do not exist a chatgpt subscription use api key use gpt successfully openai interface error use gpt gptk see a couple article claim similar code work use gpt work model specification code paste do anybody know possible access gpt model python api,2023-03-18 03:59:30,,2023-03-18 19:59:51,2023-05-31 15:17:21,<python><openai-api><gpt-4>,3,4,12,11782,,2.0,14995807.0,"<p>Currently the GPT 4 API is restricted, Even to users with a Chat GPT <strong>+</strong> subscription.</p>
<p>You may need to join the <a href=""https://openai.com/waitlist/gpt-4-api"" rel=""noreferrer"">Waitlist</a> for the API.</p>
",2023-03-18 06:25:34,2.0,15.0
75790862,1,19966847.0,75791101.0,OpenAI GPT-3 API: Why do I get a response that makes no sense in relation to the question?,ask a question parameters request response no sentence question response try temperature response never chatgpt try model like davincicodex davinci curie babbage idea parameters api response  temperature reponse a difficult question question answer,2023-03-20 13:27:50,,2023-03-21 09:20:20,2023-03-21 09:20:20,<api><openai-api><gpt-3>,1,3,-1,847,,2.0,10347145.0,"<p>You're using an old GPT-3 model (i.e., <code>davinci</code>). <strong>Use a newer GPT-3 model.</strong></p>
<p>For example, use the model <code>text-davinci-003</code> instead of <code>davinci</code>.</p>
<p>As stated in the official <a href=""https://help.openai.com/en/articles/6643408-how-do-davinci-and-text-davinci-003-differ"" rel=""nofollow noreferrer"">OpenAI article</a>:</p>
<blockquote>
<h3>How do <code>davinci</code> and <code>text-davinci-003</code> differ?</h3>
<p>While both <code>davinci</code> and <code>text-davinci-003</code> are powerful models, they
differ in a few key ways.</p>
<p><strong><code>text-davinci-003</code> is the newer and more capable model</strong>, designed
specifically for instruction-following tasks. This enables it to
respond concisely and more accurately - even in zero-shot scenarios,
i.e. without the need for any examples given in the prompt.</p>
<p>Additionally, <code>text-davinci-003</code> supports a longer context window (max
prompt+completion length) than davinci - 4097 tokens compared to
davinci's 2049.</p>
<p>Finally, <code>text-davinci-003</code> was trained on a more recent dataset,
containing data up to June 2021. These updates, along with its support
for Inserting text, make <code>text-davinci-003</code> a particularly versatile and
powerful model we recommend for most use-cases.</p>
</blockquote>
",2023-03-20 13:50:18,0.0,1.0
76048714,1,1133919.0,76052203.0,Finetuning a LM vs prompt-engineering an LLM,possible finetune a smaller language model like roberta say a customer service dataset result good prompt gpt part dataset a finetuned roberta model learn follow instructions a conversational manner least a small domain like paper article explore issue empirically check,2023-04-18 20:15:18,,,2023-04-20 14:56:03,<language-model><roberta-language-model><roberta><gpt-4><large-language-model>,1,0,2,1097,,2.0,1133919.0,"<p>I found a medium piece which goes a long way in clarifying this <a href=""https://medium.com/@lucalila/can-prompt-engineering-surpass-fine-tuning-performance-with-pre-trained-large-language-models-eefe107fb60e"" rel=""noreferrer"">here</a>.</p>
<p>Quoting from the conclusion in the above,</p>
<blockquote>
<p>In the low data domain, prompting shows superior performance to the
respective fine-tuning method. To beat the SOTA benchmarks in
fine-tuning, leveraging large frozen language models in combination
with tuning a soft prompt seems to be the way forward.</p>
</blockquote>
<p>It appears prompting an LLM <em>may</em> outperform fine tuning a smaller model on domain-specific tasks if the training data is small and vice versa if otherwise.</p>
<p>Additionally, in my own personal anecdotal experience with ChatGPT, Bard, Bing, Vicuna-3b, Dolly-v2-12b and Illama-13b, it appears models of the size of ChatGPT, Bard and Bing have learned to mimic human understanding of language well enough to be able to extract meaningful answers from context provided at inference time. It seems to me the smaller models do not have that <em>mimicry-mastery</em> and might not perform as well with in-context learning at inference time. They might also be too large to be well suited for fine-tuning in a very limited domain. My hunch is that for very limited domains, if one is going the fine-tuning route, fine-tuning on much smaller models like BERT or Roberta (or smaller variants of GPT-2 or GPT-J, for generative tasks) rather than on these medium-sized models might be the more prudent approach resource-wise.</p>
<p>Another approach to fine tuning the smaller models on domain data could be to use more carefully and rigorously crafted prompts with the medium-sized models. This could be a viable alternative to using the APIs provided by the owners of the very large proprietary models.</p>
",2023-04-19 08:19:12,0.0,5.0
76073607,1,6294538.0,76076750.0,How to change goals after setting up Auto-GPT,set autogpt goals  grow linkedin account  look new innovative linkedin post ideas ai technology  prepare post idea  write post file  shutdown auto gpt not produce result expect now want change goals different result modify goals set earlier way improve result produce try restart auto gpt terminal,2023-04-21 13:25:36,,,2023-04-21 21:14:33,<openai-api><gpt-4><autogpt>,1,0,1,1269,,2.0,3370807.0,"<p>Edit your ai_settings.yaml to change goals.</p>
",2023-04-21 21:14:33,1.0,4.0
76403814,1,22021301.0,76411286.0,What is the best approach to creating a question generation model using GPT and Bert architectures?,want make a question generation model question context make use gpt base model bert base architectures gpt able perform task return vague question not context use wizardlm b able generalize question context sound natural nearly point keep limit ,2023-06-05 05:58:55,,2023-06-05 06:14:45,2023-06-06 03:32:51,<python><open-source><huggingface-transformers><huggingface><gpt-3>,1,1,0,49,,2.0,4882300.0,"<p>When dealing with text generation, it is more straightforward to work with Transformer decoder models such as GPT-* models. Although BERT-like models are also capable of text generation, it is a quite convoluted process and not something that follows naturally from the tasks for which these models have been pretrained.</p>
<p>I assume you are comparing GPT-2 and WizardLM (7B). The performance of the model on this task is expected to improve as you scale up the number of parameters by using larger models. I would recommend you to try LLMs such as <a href=""https://github.com/tloen/alpaca-lora"" rel=""nofollow noreferrer"">Alpaca-LoRA</a>, <a href=""https://huggingface.co/databricks/dolly-v2-12b"" rel=""nofollow noreferrer"">Dolly</a> or <a href=""https://huggingface.co/EleutherAI/gpt-j-6b"" rel=""nofollow noreferrer"">GPT-J</a> ( <a href=""https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/GPT-J-6B/Inference_with_GPT_J_6B.ipynb"" rel=""nofollow noreferrer"">see here</a> how to run GPT-J on Colab Pro ).</p>
",2023-06-06 03:32:51,1.0,1.0
75257323,1,4127155.0,,Fine Tuning an OpenAI GPT-3 model on a collection of documents,accord documentation train data fine tune openai gpt model structure follow a collection document internal knowledge base preprocessed a jsonl file a format like documentation suggest a model fine tune article use command run result not unexpected give document file structure note above run tell right approach try fine tune a gtp model collections document question later ask about content field case not start a place a collection possible question ideal answer fundamentally misunderstand mechanism use fine tune a gtp model do make sense gtp need train possible question answer give base model train process above provide additional datasets not public domain question ask about think want achieve possible a work example ask a question about document follow get answer right now try fine tune model  document do not paste single document time a question ask model able consider content across  just single user provide,2023-01-27 11:10:32,,2023-01-30 07:31:52,2023-03-25 08:57:48,<openai-api><gpt-3>,2,0,7,4316,,,,,,,
75304632,1,16450589.0,,How to Use Edit images in OpenAi Kotlin Client,use openai client android kotlin implementation path wrong library miss see want a path do not succeed path,2023-02-01 00:55:40,,2023-02-05 10:47:37,2023-02-07 13:01:54,<android><android-studio><kotlin><openai-api><gpt-3>,1,0,0,191,,,,,,,
75324242,1,972982.0,,GPT-J (6b): how to properly formulate autocomplete prompts,new ai playground purpose experiment gptj b model amazon sagemaker notebook instance gdnxlarge far manage register endpoint run predictor sure make wrong question not really understand model parameters work probable code problem try a different response use parameters nothing just repeat input response definitely do wrong funny thing actually a pretty understandable text output throw input sample temperature openai playground textdavinci a hint do wrong oh question specify like  word a keyword match,2023-02-02 13:59:41,,,2023-02-02 13:59:41,<jupyter-notebook><amazon-sagemaker><huggingface><gpt-3>,0,4,1,187,,,,,,,
67334513,1,15221534.0,,Is there an 'untrained' gpt model folder?,crazy question maybe want download gpt model framework want weight initialize randomly model finetuned reddit content include json vocab meta amp index file possible kind regard,2021-04-30 13:09:03,,,2021-05-04 14:31:08,<huggingface-transformers><transformer-model><gpt-2>,1,0,1,229,,,,,,,
64635072,1,8713984.0,,huggingface transformers run_clm.py stops early,run run_clmpy finetune gpt form huggingface library follow language_modeling example output process start c appear stop process environment info transformers version  platform linuxx_withubuntubionic python version  tensorflow version  use gpu script yes possible trigger early stop,2020-11-01 17:52:48,,2020-11-29 12:09:14,2020-11-29 12:09:14,<huggingface-transformers><gpt-2>,0,0,1,930,,,,,,,
70482540,1,8744937.0,,What happens if optimal training loss is too high,train a transformer setups obtain validation train loss look like understand stop train epoch  train loss high a problem do value train loss actually mean thank,2021-12-25 20:22:20,,,2022-01-28 12:01:01,<pytorch><huggingface-transformers><transformer-model><gpt-2><trainingloss>,2,0,0,903,,,,,,,
70562362,1,17819675.0,,Getting MemoryError fine-tuning GPT2(355M) model with small datasets (3MB) through aitextgen,use aitextgen finetune m gpt model use train function datasets small txt file consist line like encode texts keywordbased text generation keywords use aitextgen train function like run function output try methods include clear cuda cache use split file smaller ones none work run local machine rtx gb ram check task manager ram usage barely hit  wrong code cause memory errors,2022-01-03 06:34:42,,2022-01-03 08:42:28,2022-01-03 08:42:28,<python><nlp><pytorch><gpt-2>,0,5,2,454,,,,,,,
71911771,1,1802425.0,,Can I create a fine-tuned model for OpenAI API Codex models?,like translate user request ticket sort structure data format json example user want order chair a desk drawers leave output look like gpt not verywell suit task output not form natural language codex not openai api docs possible create a custom finetuned model openai api codex model,2022-04-18 12:22:26,,2023-03-02 00:43:14,2023-05-20 02:52:29,<json><openai-api><gpt-3><fine-tune>,3,0,4,2575,,,,,,,
75384220,1,19548998.0,,[InteractionAlreadyReplied]: The reply to this interaction has already been sent or deferred,say interactionalreadyreplied reply interaction send defer inreality use editreply have issue log error work fine try show catch error try use followup doesnt work keep give error shut bot try use followup result,2023-02-08 10:10:52,,2023-02-08 10:37:32,2023-02-09 04:50:08,<node.js><discord><discord.js><gpt-3>,1,1,0,203,,,,,,,
75787638,1,18628287.0,,"OpenAI GPT-3 API error: ""Request timed out""",error run code try modifiying parameters error occur experience process thank advance,2023-03-20 07:43:47,,2023-03-21 17:45:25,2023-03-21 17:45:25,<python><openai-api><gpt-3>,1,1,3,3966,,,,,,,
75810740,1,12827843.0,,OpenAI GPT-4 API: What is the difference between gpt-4 and gpt-4-0314?,help explain difference appear openai playground dropdown menu check various search engines not clear openai forum result gpt refer a specific version release gpt a particular set update improvements release march th experience differences feedback welcome,2023-03-22 09:57:45,,2023-03-22 17:26:49,2023-06-02 19:20:46,<openai-api><gpt-4>,1,1,8,4697,,,,,,,
75832120,1,245549.0,,Do nodes in List Index come with embedding vectors in LlamaIndex?,run embeddingbased query list index link nod list index supply embed vectors difference list index vector store index think distinctive feature vector store index assign embed vector nod index look like list index do,2023-03-24 09:48:25,,,2023-03-27 20:57:51,<embedding><openai-api><gpt-3><llama-index>,1,0,0,450,,,,,,,
75859074,1,6695297.0,,Getting RateLimitError while implementing openai GPT with Python,start implement openai gpt model python send a single request get ratelimiterror code look like error get development without get error check doc provide a free version limitations initial stage send  request hour thank advance help,2023-03-27 18:14:46,,,2023-06-09 15:56:30,<python><python-3.x><openai-api><gpt-3>,2,5,3,2945,,,,,,,
75379690,1,8068222.0,,Not getting proper response from GPT-3 using SDK in JS,use createcompletion a response do not actual text response textpayload text package comexampledemocontroller code,2023-02-07 22:38:28,,,2023-02-09 09:35:10,<openai-api><gpt-3>,1,0,0,276,,,,,,,
75403409,1,2195440.0,,Context window length in OpenAI API Codex models,completion window length include context window length openai codex model context window length set tokens understand example prompt length tokens remain completion no way use token prompt pretty sure understand helpful confirm knowledgeable,2023-02-09 19:16:21,,2023-03-02 00:35:41,2023-03-02 00:35:41,<openai-api><gpt-3>,1,0,0,806,,,,,,,
75429596,1,2947435.0,,OpenAI GPT-3 API: How to parse the response into an ordered list or dictionary?,gpt amaze parse result a bite a headache miss example ask gpt write about digital market return interest stuff send a list topics relate digital market headline apparently a e line break tabulation reflex split text line break look like format not equal line break second half response make inaccurate like reformatting output a kind list topics headline like maybe a parameter send withing request do not doc guess best bet reformat use a pattern not case consistent number prefix element look like end response example idea use python adapt language,2023-02-12 19:24:01,,2023-03-13 14:28:06,2023-05-18 11:45:28,<python><openai-api><gpt-3>,2,6,1,2010,,,,,,,
75617250,1,19989305.0,,cannot import name 'GPT2ForQuestionAnswering' from 'transformers',,2023-03-02 15:01:41,,,2023-05-13 10:40:13,<pip><torch><gpt-2>,1,1,0,184,,,,,,,
75655947,1,425281.0,,Does openai GPT finetuning consider the prompt in the loss function?,openai api include a finetuning service divide task prompt completion documentation say accuracy metrics calculate respect completion loss say calculate train batch understand train a gpt model happen batch max available size use special token separate contexts ask predict token entries loss function obvious cross entropy output fine tune opportunity learn predict template prompt not decisions sensible learn template amount train a parse mask template avoid overfitting current practice openai,2023-03-06 21:18:20,,,2023-03-06 23:02:11,<openai-api><gpt-3><gpt-2>,1,0,0,462,,,,,,,
75662453,1,20927753.0,,Getting an error 'no file named tf_model.h5 or pytorch_model.bin found in directory gpt2',run above code model download successfully model download define model configuration load model update configuration throw error oserror error no file name tf_modelh pytorch_modelbin directory gptmedium resolve issue,2023-03-07 13:15:28,,,2023-03-07 16:01:31,<tensorflow><text><pytorch><classification><gpt-2>,1,0,0,657,,,,,,,
75664012,1,17522290.0,,"I want to make an AI text classifier using OpenAI API, based on GPT2 but i cannot find the API documentation for the GPT2",want create ai text classifier project college want use gpt api reliable catch content generate gpt  use gpt documentation useful resources welcome try go model section documentation not gpt gpt ,2023-03-07 15:33:17,,2023-03-07 15:36:12,2023-03-24 18:53:58,<machine-learning><artificial-intelligence><openai-api><language-model><gpt-2>,1,0,0,236,,,,,,,
75667860,1,19615881.0,,openai unknown command 'tools',learn gpt finetuning successfully run command pip install upgrade openai not run command export openai_api_key skxxxxxxxxxxxxxx error export term export not recognize a cmdlet function script file operable program run command instead set openai_api_key skxxxxxxxxxxxxxx run setexecutionpolicy executionpolicy unrestricted scope currentuser now try run openai tool fine_tunesprepare_data f c users finetuneuploadxlsx show error unknown command tool idea explain beginner language,2023-03-07 22:37:38,,,2023-03-09 09:32:30,<openai-api><gpt-3><fine-tune>,1,1,0,434,,,,,,,
73454328,1,19825964.0,,A question about using past_key_values generated by gpt2,recently a problem about use past_key_values generate gpt demo maybe fee gpt right,2022-08-23 06:53:24,,2022-08-24 17:43:25,2022-08-24 17:43:25,<python><nlp><gpt-2>,0,0,0,193,,,,,,,
73455802,1,18992575.0,,Search through GPT-3's training data,use gpt experiment prompt language model test cognitive science test form short text snippets now like check gpt encounter text snippets train question way sift gpt train text corpora a certain string text corpora thank help,2022-08-23 08:56:26,,,2022-12-08 00:37:13,<nlp><training-data><gpt-3>,1,0,1,157,,,,,,,
56415280,1,9435410.0,,Fine tune GPT-2 Text Prediction for Conversational AI,experiment gpt model conditional text generation tweak a good chatbot use nsheppard code retrain custom dataset train model a custom dataset conversations pull facebook data change sample length  dialogues interactive conditional generation dataset look like train try chat instead complete sentence instead reply understand interactive_conditional_samplespy build complete sentence base prompt think change dataset work sure do not work trainpy samplepy interactive_conditional_samplespy tweak code work like a chatbot guess context samplepy unsure go work,2019-06-02 12:57:41,,2020-11-29 12:02:02,2020-11-29 12:02:02,<python><tensorflow><nlp><chatbot><gpt-2>,2,0,3,4864,0.0,,,,,,
73629287,1,15279628.0,,OpenAI GPT-3 API: How to extend length of the TL;DR output?,like produce a  sentence summary a  page article use openai tldr paste article text output stay   sentence,2022-09-07 01:53:41,,2023-03-13 13:33:12,2023-03-13 13:33:12,<openai-api><gpt-3>,1,0,6,941,,,,,,,
73657901,1,14156907.0,,Combine GPT3 with RASA,try integrate rasa gpt not get proper response help look code tell issue actionpy not able understand issue help solve thank,2022-09-09 06:00:56,,,2023-03-01 01:08:25,<python><rasa><openai-api><gpt-3>,1,0,0,578,,,,,,,
72484590,1,15181966.0,,Gpt 3 keywords extractor,get accustom gpt want build a keywords extractor book summaries point reference help use case,2022-06-03 04:05:52,,2022-06-03 04:06:14,2022-06-22 09:58:47,<python><nlp><openai-api><gpt-3>,1,2,2,3137,0.0,,,,,,
75865844,1,10176535.0,,Alpaca Large Language Model from Python script,able install alpaca linux start use interactivelly correspond command like run not interactive mode a python jupyter script prompt string parameter possible model time without need reload time write a python script work technically work technically now unfortunately model produce nonsense like fix,2023-03-28 11:46:15,,2023-05-31 11:54:51,2023-05-31 11:55:31,<python><c#><artificial-intelligence><gpt-3><large-language-model>,2,0,2,1139,,,,,,,
75866651,1,13828374.0,,Why does LLM(LLaMA) loss drop staircase-like over epochs?,train a llm llamab notice loss drop a stairlike fashion epochs specifically little loss change epoch suddenly loss drop quite a bite a new epoch curious about cause phenomenon learn rate architecture model insights greatly appreciate loss figure curious about cause phenomenon insights greatly appreciate,2023-03-28 13:05:24,,,2023-04-20 04:07:07,<loss><gpt-3><fine-tune><large-language-model><llama-index>,1,0,0,286,,,,,,,
75902238,1,21538764.0,,"Getting error ""Container localhost does not exist. (Could not find resource: localhost/model/wpe)"" while generating with gpt2-simple",try generate text use gpt language model use gptsimple library train process work fine run error try generate text use generate function error message receive container localhost do not exist not resource localhostmodelwpe not sure cause error not able relevant information documentation online forums help resolve issue greatly appreciate code use generate data expect result expect generate function generate text without errors actual result receive follow error message container localhost do not exist not resource localhostmodelwpe additional information,2023-03-31 19:43:48,,,2023-03-31 19:43:48,<python><tensorflow><artificial-intelligence><gpt-2>,0,0,0,33,,,,,,,
75442916,1,2629034.0,,OpenAI GPT-3 API: How to preserve formatting when pasting content into an Excel cell?,try create a fine tune gpt model end content format word im try bring excel order import train dataset input format copy content excel excel cell convert way preserve original format way better way help appreciate greatly regard galeej,2023-02-14 02:24:52,,2023-03-13 14:38:14,2023-03-13 14:38:14,<excel><openai-api><gpt-3>,1,0,0,128,,,,,,,
75454722,1,14949601.0,,OpenAI GPT-3 API: How does it count tokens for different languages?,know gpt model accept produce kinds languages english french chinese japanese traditional nlp different languages different tokenmaking methods alphabetic languages english use bpe method make tokens like charactaristic languages chinese japanese just use character token like gpt compose different languages produce chinese english sentence really curious model make tokens,2023-02-15 01:26:42,,2023-03-07 14:10:52,2023-03-07 14:10:52,<nlp><tokenize><openai-api><gpt-3>,1,0,-1,1301,,,,,,,
70483937,1,10025412.0,,Fine-tune dialoGPT with a new dataset - loss below 1 and perplexity exploded,follow tutorial finetuning dialogpt gpt a new conversational dataset train fine earlier perplexity about   result dialogue normal enter image description herenow not sure touch plot train loss go  about  perplexity ,2021-12-26 02:52:11,,,2021-12-26 02:52:11,<deep-learning><nlp><gpt-2>,0,0,0,1421,,,,,,,
70534103,1,17798153.0,,Uncaught RangeError [MESSAGE_CONTENT_TYPE]: Message content must be a non-empty string,try make a gpt chatbot get error discordjs v,2021-12-30 16:17:42,,2021-12-31 15:06:26,2021-12-31 15:06:26,<javascript><discord><discord.js><artificial-intelligence><gpt-3>,0,3,2,305,,,,,,,
76073565,1,10759664.0,76424001.0,GPT2 special tokens: Ignore word(s) in input text when predicting next word,just start use gpt a question concern special tokens like predict word give a text input want mask word input chunk use a special token not want gpt predict mask word just not want use prediction want gpt know do not input word example quick brown fox jump lazy input sentence want gpt predict word correct dog case want mask word lazy gpt know end input sentence basically gpt input look like quick brown fox jump _ _ not like quick brown fox jump know not predict word think about use special tokens replace hide word think neither mask nor pad make sense case do idea solve thank advance help,2023-04-21 13:20:32,,2023-04-21 13:26:03,2023-06-07 13:46:36,<python><nlp><token><predict><gpt-2>,1,0,0,41,,2.0,10759664.0,"<p>Solved this, masking the tokens did the trick. I used an attention mask and set all attention mask values of tokens I wanted to ignore to 0, so their attention weights are 0 on all layers.</p>
",2023-06-07 13:46:36,0.0,0.0
73760982,1,12127131.0,,Why is my addEventListener click event only firing once?,use addeventlistener click event trigger a new gpt request take latest database post a prompt text completion data send database a hide html form action form trigger a php script newly generate text send database prompt gpt request problem event fire like able continue generate new text load page whilst data heavy file page load a link site show issue ignore errors dev console relate separate issue sort gltf loaders resolve load php link form action ideas happen,2022-09-18 07:52:38,,2022-09-18 09:08:44,2022-09-18 09:14:00,<javascript><php><addeventlistener><gpt-3>,1,2,0,60,,,,,,,
75417403,1,295944.0,,Using GPT2 to find commonalities in text records,a dataset incidents data free text form row incident a text field happen try train a gpt model free text try prompt person get burn want common cause burn cause write ways think maybe mean work prompt work funny reason not think work want,2023-02-11 00:48:01,,2023-02-11 00:54:29,2023-02-11 00:54:29,<nlp><bert-language-model><gpt-2>,0,0,0,17,,,,,,,
75469128,1,14949601.0,,How does Huggingface's tokenizers tokenize non-English characters?,use tokenize natural language sentence tokens come question examples try use tokenizers code above examples try example english sentence understand correspond correspond not understand tool tokenizes nonenglish sequence tokens not vocab example tokenized result tokenized result search huggingface document website no clue source code write rust hard understand help figure,2023-02-16 07:39:04,,2023-02-16 08:14:43,2023-02-16 08:14:43,<nlp><tokenize><huggingface-tokenizers><gpt-3><gpt-2>,0,0,2,103,,,,,,,
75494945,1,2411311.0,,OpenAi api 429 rate limit error without reaching rate limit,occasion get a rate limit error without rate limit use text completions endpoint pay api a rate limit  request minute use  request minute follow error api status code request open ai error type open ai error message open ai documentation state a  error indicate exceed rate limit clearly not weird thing open ai error message not state give response usually a error service unavailable love hear thoughts theories experience,2023-02-18 17:05:02,,,2023-02-19 08:54:14,<openai-api><gpt-3>,2,1,2,4773,,,,,,,
75913490,1,721998.0,,Conversational Bot with Flan-T5,build a chat bot use flant model bot a text window instructions like summarize big text go dump text chat window say summarize above text similar dump a bunch domain specific facts chat window ask question about question form context data bot knowledge about info pass summarize answer quetsions text pass create a prompt detect intent just case just feed info use ask question create sumarry later,2023-04-02 17:10:21,,,2023-04-13 12:20:46,<chatbot><openai-api><gpt-3><conversational-ai>,0,0,1,307,,,,,,,
75924179,1,6421492.0,,Llama_index unexpected keyword argument error on ChatGPT Model Python,test a couple widely publish gpt model just try feet wet run error solve run code get error have a hard time find documentation llamma index errors hop point right direction,2023-04-03 22:27:15,,,2023-04-19 08:54:54,<python><openai-api><gpt-3><llama-index>,2,6,8,3059,,,,,,,
75942269,1,15478457.0,,How to generate gpt-3 completion beyond max token limit,want ask a way properly use openai api generate complete responses max token limit use official openai python package not way replicate gpt textdavinci do not support chat interface code currently like,2023-04-05 17:11:47,,2023-04-05 20:04:24,2023-04-05 20:04:24,<python><openai-api><gpt-3><chatgpt-api>,1,2,-1,1282,,,,,,,
75946090,1,8668935.0,,Why is GPT-4 giving different answers with same prompt & temperature=0?,code call gpt model keep system_msg amp req constant temperature different answer get  different answer run  time instance answer similar concept differ semantics happen,2023-04-06 05:23:31,,,2023-04-07 05:28:42,<gpt-4>,1,0,0,492,,,,,,,
72294775,1,18244751.0,,How do I know how much tokens a GPT-3 request used?,build app gpt like know tokens request make use possible,2022-05-18 19:11:58,,,2023-04-18 11:17:02,<gpt-3>,4,1,8,6982,,,,,,,
75586733,1,21300655.0,,ChatGPT Token Limit,want chatgpt remember past conversations a consistent stateful conversation see code chatgpt prompt engineer ways design prompt show pseudo code use a single input cheap better possible stack previous history expensive token limitation possible choose way cheap a consistent conversation word do chatgpt remember past history prompt current input,2023-02-28 00:06:49,,2023-04-15 02:54:00,2023-06-15 00:42:19,<text><nlp><prompt><openai-api><gpt-3>,5,1,15,16584,,,,,,,
75672816,1,11512643.0,,How does GPT-like transformers utilize only the decoder to do sequence generation?,want code a gptlike transformer a specific text generation task gptlike model use decoder block stack  know code submodules decoder block show embed softmax layer pytorch not know input say figure output shift right example data sos eos tokens gptlike model train properly not use a encoder input multihead attention block sorry question a little dumb new transformers,2023-03-08 12:04:35,,,2023-03-08 18:58:45,<deep-learning><pytorch><gpt-2><text-generation>,1,0,2,1528,,,,,,,
72418745,1,381436.0,,GPT-J and GPT-Neo generate too long sentences,train a gptj gptneo model fine tune texts try generate new text sentence long  character dataset sentence normal length  character usually try a lot things change adjust temperature top_k half result long phrase neen short try long examples generate result,2022-05-28 19:38:28,,,2022-12-06 21:47:18,<text><artificial-intelligence><gpt-2><fine-tune>,1,0,2,614,,,,,,,
72467610,1,14272134.0,,OOM while fine-tuning medium sized model with DialoGPT on colab,try finetune dialogpt a mediumsized model get cuda error train phase reduce batch size  error persist parameters gpu allocate tesla ppcie gb memory kindly let know resolve issue suggestion appreciate,2022-06-01 20:20:08,,2022-06-01 22:52:21,2022-08-21 20:16:32,<google-colaboratory><huggingface-transformers><language-model><gpt-2>,1,0,0,306,,,,,,,
73770730,1,10456211.0,,How can I implement text classification for the purpose of matching using GPT-3?,try fine tune a gpt model purpose text classification classify name match william jonathan william j label yesno yes indicate name match no indicate not create a large number examples relate different scenarios name spell differently abbreviations miss token finetuning model gpt use examples look like a jsonl format not able binary classification output a large number label instance prompt follow name samenwilliam jonathan nwilliam j completion yesyesnoyesnoyesyesnoyesnoyesnoyesnoyesyes idea perform binary text classification use gpt example similar above,2022-09-19 08:44:29,,,2022-09-19 08:44:29,<text><classification><matching><openai-api><gpt-3>,0,1,0,34,,,,,,,
73919757,1,1743703.0,,GPT-3 Twitter Bot,try finetune a gpt model tweet want model generate tweet no prompt possible dataset reqired prompt completion columns just couple word tweet make a prompt,2022-10-01 15:54:03,,,2023-01-10 07:20:50,<openai-api><gpt-3>,2,1,1,459,,,,,,,
74000154,1,1461800.0,,OpenAI GPT-3 API: How do I make sure answers are from a customized (fine-tuning) dataset?,use customize text prompt completion train new model tutorial use create customize model data betaopenaicomdocsguidesfinetuningadvancedusage train model send prompt text model get generic result not suitable make sure completion result prompt text use model not generic openai model use flag eliminate result generic model,2022-10-08 19:58:07,,2023-05-05 21:06:56,2023-06-21 19:51:43,<nlp><customization><openai-api><gpt-3><fine-tune>,1,0,7,4521,,,,,,,
75946877,1,11487426.0,,"How to fix ""TypeError: dataclass_transform() got an unexpected keyword argument 'field_specifiers'"" when using gpt-index/llama-index?",try use gptindexllamaindex fee chatgpt custom data build a custom chatbot try import gptindex llamaindex jupyter follow error try uninstalling reinstall problem persist use python  jupyter notebook ,2023-04-06 07:27:37,,,2023-04-06 07:27:37,<python-3.x><jupyter-notebook><openai-api><gpt-3><chatgpt-api>,0,0,0,175,,,,,,,
75952444,1,20038123.0,,Huggingface Transformers (PyTorch) - Custom training loop doubles speed?,quite strange use huggingface transformers a custom train loop pytorch context currently try fine tune a pretrained gpt small gptlmheadmodel m param version multiple nod use huggingface accelerate use huggingface library train course step process accelerate write a custom pytorch train loop do help official tutorial huggingface naturally decide test model new train loop implement accelerate ensure actually work relevant code original model correspond code new train loop note equal  model code not show exactly model original custom train loop test node course gpus gb work suspiciously original model average about  iterationss custom loop hand average about  iterationss absolutely bizarre possible simply add train loop just a couple line code not faster official provide huggingface nearly twice fast do write train loop incorrectly completely miss,2023-04-06 17:58:30,,,2023-04-25 23:53:02,<pytorch><huggingface-transformers><huggingface><gpt-2><huggingface-datasets>,1,0,0,209,,,,,,,
75501276,1,3186094.0,,OpenAI GPT-3 API: How to make a model remember past conversations?,a way train a large language model llm store a specific context example a long story want ask question about not want story prompt make llm remember story,2023-02-19 15:38:07,,2023-03-13 14:40:05,2023-04-10 10:38:36,<openai-api><gpt-3>,1,2,3,3916,,,,,,,
73338768,1,15256429.0,,How can I keep my discord bot from remembering old messages? (NodeJS),work a discord bot use openai answer question chat bring previous message prompt grab eventually cause repeat answer adjust antirepetition value max a way make message read account instead remember previous message code,2022-08-12 19:10:08,,2022-08-13 07:29:44,2022-08-13 07:29:44,<node.js><discord><discord.js><gpt-3>,0,0,0,210,0.0,,,,,,
75555647,1,11375592.0,,Train GPT-2 on custom data,look a way train textual data use gpt amp a blog post work fine model build dataset build show weird texts output corpus data take wikipedia output say completely new nlp section issue,2023-02-24 10:50:59,,2023-02-24 16:09:13,2023-02-24 16:09:13,<python><nlp><gpt-2>,0,0,0,518,,,,,,,
75970356,1,354067.0,,"Comparing methods for a QA system on a 1,000-document Markdown dataset: Indexes and embeddings with GPT-4 vs. retraining GPT4ALL (or similar)",work a project build a questionanswering a documentation portal contain  markdown document document consist approximately  tokens consider follow options use index embeddings gpt retrain a model like gptall a similar model specifically handle dataset approach likely produce better result use case,2023-04-09 11:58:57,,2023-05-09 09:08:55,2023-05-09 09:08:55,<deep-learning><openai-api><gpt-4><large-language-model><gpt4all>,1,2,3,214,,,,,,,
75974345,1,5282493.0,,What is language model will fit in my 7950x 16core & rtx3090 with 64 gb of ddr 5 ram,best language model fit hardware longer context guide noobe,2023-04-10 04:54:50,,,2023-04-10 04:54:50,<nlp><gpt-3><alpaca>,0,0,0,19,,,,,,,
76053920,1,14031645.0,,How do I extract only code content from chat gpt response?,use generate sql query use api model face difficulty extract sql query response chatgpt provide explanation query not try regex expressions not reliable approach extract code section response,2023-04-19 11:25:11,,,2023-05-05 22:21:57,<sql><code-generation><openai-api><gpt-3><chatgpt-api>,2,4,0,1367,,,,,,,
76101635,1,12902027.0,,"Kaggle Code doesn't download ""gpt2"" language model",use kaggle code download gpt language model intend download gptxl model huggingface hub line raise localentrynotfounderror detais do not kaggle code connect huggingface hub do happen fix error,2023-04-25 13:20:15,,,2023-04-25 13:20:15,<python><huggingface-transformers><kaggle><gpt-2>,1,0,0,43,,,,,,,
76199709,1,10687259.0,,Image to text using Azure OpenAI GPT4,azure open ai account gpt model deploy use api imagetotext description yes image use code throw error error like apierror invalid response object api unsupported data typen http response code  mention inside explanation,2023-05-08 10:32:50,,,2023-05-31 10:54:16,<python><openai-api><azure-openai><gpt-4>,1,0,2,234,,,,,,,
76222070,1,1354514.0,,Getting an error while trying to train my model in train_function (Empty Logs),try train a gpt model wikipedia text do follow error error happen code call code,2023-05-10 19:42:56,,2023-05-11 01:30:23,2023-05-11 01:30:23,<tensorflow><machine-learning><keras><huggingface-transformers><gpt-2>,0,0,0,107,,,,,,,
76233070,1,14587120.0,,Generic Answer when Fine Tuning OpenAI Model,prepare a dataset train a davinci model use finetuning give correct answer variant question exist dataset fine tune model like a sorry not know answer question ask not dataset example ask  olympics host a generic not know answer question do not exist dataset,2023-05-12 04:10:33,,2023-05-12 04:15:38,2023-05-15 07:01:52,<openai-api><gpt-3><chatgpt-api><text-davinci-003>,0,0,1,58,,,,,,,
76255342,1,1019129.0,,Figuring out general specs for running LLM models,question give count llm parameters billions figure gpu ram need run model cpuram no gpu run model slow run llm model like hogpt openassistant mix gpuram cpuram,2023-05-15 14:57:12,,,2023-05-18 08:11:23,<deep-learning><artificial-intelligence><gpt-3><large-language-model>,1,0,1,642,,,,,,,
76266379,1,10225070.0,,fine tune gpt3 with tabular data,title say fine tune a llm tabular data initial sense llm not suit learn tabular data unless tabular data restructure grammatical form reservations about approach basically idea able ask question specific domain data responses include specific datapoints database thoughts,2023-05-16 19:12:07,,,2023-05-16 19:12:07,<openai-api><gpt-3><fine-tune><large-language-model>,0,0,0,87,,,,,,,
76269666,1,16154213.0,,I ran into an error when I try to use YoutubeLoader.from_youtube_url,code snippet error class youtubeloader recently update methods from_youtube_channel from_youtube_urlbut use from_youtube_url happen error valueerror expect metadata value a str int float get none want know dothank,2023-05-17 07:45:48,,2023-05-17 14:15:11,2023-06-04 10:45:26,<python><youtube-api><openai-api><gpt-3><langchain>,1,8,0,273,,,,,,,
76345550,1,13605626.0,,"I want to ask about llama_index, the response take too long when get full and get truncated when not",want ask about llama_index use vietnamese response long about  second response truncate not try errors no result try old version gpt_index llama_index  run fine want use mongo help thank a lot,2023-05-27 06:03:36,,2023-05-27 09:05:05,2023-05-27 09:05:05,<python><gpt-3><chatgpt-api><llama-index>,0,0,-2,65,,,,,,,
74255038,1,982636.0,,What's your approach to extracting a summarized paragraph from multiple articles using GPT-3?,follow scenario best approach use gpt api need come a short paragraph about a specific subject base paragraph a set article  article write unknown structure work main constraint open ai token limit prompt constraint ask opt parse unstructured data use specific subject prompt request iterate article save  string variable repeat time use new string variable article long cut smaller chunk curse finetune model specific subject produce better result set make sure gpt use facts data source example let say want write a paragraph about subject a subject b subject c  article reference open ai playground look like,2022-10-30 17:00:45,,2023-01-17 04:32:06,2023-01-17 04:32:06,<machine-learning><nlp><summarization><openai-api><gpt-3>,1,0,1,134,,,,,,,
75329518,1,1479849.0,,Fine Tuning GPT-3 for Consistent Output Format,try use open ai api create quiz question incorrect answer correct answer prompt use like parse chatgpt answer format inconsistent instance quiz_topic literature follow response correct format use history follow output a different format goal finetune use article create  line a train set a sample run validation tool command follow message not understand remove write a quiz misunderstand finetune a model consistent format anybody shed a light make sure format prompt,2023-02-02 22:18:50,,,2023-03-01 14:01:41,<openai-api><gpt-3>,1,0,1,937,,,,,,,
76376524,1,10229072.0,,"Expected input batch_size (28) to match target batch_size (456), Changing batch size increase the target batch size with GPT2 model",practise finetuning a gpt model a simple questionanswer dataset encounter error study answer input dataset shape look fine train loop,2023-05-31 18:56:43,,2023-06-12 09:47:32,2023-06-12 09:47:32,<dataframe><deep-learning><nlp><transformer-model><gpt-2>,0,0,1,42,,,,,,,
76420507,1,22033979.0,,"How to restrict the open AI API responses to only Physics, Chemistry, Mathematics and Biology? If user asks non tech question then say 'Not related'",restrict open ai api respond certain field example physics chemistry mathematics biology relate query need answer respond not relate physics chemistry mathematics biology know api generic answer query way restrict work give unwanted responses prevent alternate method available achieve functionality,2023-06-07 06:42:21,2023-06-07 06:46:01,2023-06-07 06:45:01,2023-06-07 06:45:01,<node.js><openai-api><gpt-3><chatgpt-api>,0,0,-2,13,,,,,,,
56770831,1,5440125.0,,Using GPT-2 with your own dictionary of words,train gpt custom encode custom vocabbpe file generate text use gpt output tokens range exceed range new encode make gpt work,2019-06-26 10:37:46,,2020-11-29 12:08:30,2020-11-29 12:08:30,<python-3.x><nlp><gpt-2>,0,1,3,589,0.0,,,,,,
74350123,1,20442081.0,,ValueError while trying to finetune a gpt-2 model,try finetune gpt error valueerror can´t cause error use max wool´s google colab finetune gpt follow instructions notebook try start cell error can´t solutions problem anybody help,2022-11-07 16:52:54,,2022-11-07 22:48:56,2022-11-07 22:48:56,<python><google-colaboratory><gpt-2><fine-tune>,0,0,0,48,,,,,,,
63380543,1,6815505.0,63391545.0,"How many characters can be input into the ""prompt"" for GPT-2",use openai gpt model github think top_k parameter dictate tokens sample parameter dictate large a prompt give top_k  large prompt,2020-08-12 16:09:45,,2020-11-29 12:03:15,2020-11-29 12:03:15,<python><nlp><openai-api><gpt-2>,1,0,3,5470,,2.0,5652313.0,"<p>GPT-2 does not work on character-level but on the subword level. The maximum length of text segments in was trained on was 1,024 subwords.</p>
<p>It uses a vocabulary based on <a href=""https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10"" rel=""nofollow noreferrer"">byte-pair-encoding</a>. Under such encoding, frequent words remain intact, infrequent words get split into several units, eventually down to the byte level. In practice, the segmentation looks like this (69 characters, 17 subwords):</p>
<pre><code>Hello , ▁Stack Over flow ! ▁This ▁is ▁an ▁example ▁how _a ▁string ▁gets ▁segment ed .
</code></pre>
<p>At the training time, there is no difference between the prompt and the answer, so the only limitation is that the prompt and answer cannot be longer than 1,024 subwords in total. In theory, you can continue generating beyond this, but the history model considers can never be longer.</p>
<p>The selection of <code>top_k</code> only influences memory requirements. A long query also needs more memory, but it is probably not the main limitation</p>
",2020-08-13 08:54:07,2.0,3.0
64130834,1,6786996.0,66070991.0,Build a model that answers question from dataset using GPT3,try build a chat bot give text corpus answer question ask text hear gpt a beast require minimum train link tutorialgithub repo help start,2020-09-30 04:23:45,,2023-01-21 20:40:54,2023-01-21 20:40:54,<nlp><nlp-question-answering><gpt-3>,1,3,1,1700,,2.0,14852784.0,"<p>Sure, if you got a beta access to the <a href=""https://beta.openai.com/"" rel=""nofollow noreferrer"">OpenAI GPT-3 API</a> you're easily able to do so. In case you don't, you can apply for it - you should get accepted fairly quickly <em>(in my specific case it took about 24 hours)</em>.</p>
<p>Depending whether you look for speed or precision you should choose between Davinci, Cushman or Curie (<a href=""https://beta.openai.com/docs/engines"" rel=""nofollow noreferrer"">list of engines</a>), whereas Davinci is the best (precision-wise).</p>
<p>You can use the Playground to enter a text corpus and a question - here is an example:
<a href=""https://i.stack.imgur.com/TSaZz.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/TSaZz.png"" alt=""Example picture"" /></a>
<em>I used <code>davinci-instruct-beta</code> with a temperature of <code>0.25</code> and response length of <code>10</code>. A pretty basic setup.</em></p>
<p>For demonstration purposes, here is the API request made via <strong>Python</strong>. <code>response</code> returns <em>&quot;Anna hates doing research the most.&quot;</em></p>
<pre><code>import openai

openai.api_key = 'KEY'

response = openai.Completion.create(
  engine=&quot;davinci-instruct-beta&quot;,
  prompt=&quot;Anna loves programming in Python and C++, though she absolutely despises doing research.\nWhat does Anna hate the most?\n\nAnna hates doing research the most.Example&quot;,
  temperature=0.25,
  max_tokens=10,
  top_p=1
)
</code></pre>
",2021-02-05 21:42:20,3.0,1.0
76424390,1,19871283.0,,How to change the QA_PROMPT for my own usecase?,chain follow description not understand qa_prompt mean change usecase check pinecone index not about qa_prompt help,2023-06-07 14:25:46,,,2023-06-08 07:39:06,<prompt><openai-api><chain><langchain><gpt-4>,1,1,0,32,,,,,,,
76457935,1,12689038.0,,"TypeError: argmax(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",traning a model gpt use curated dataset get follow error try debug issue a new error come helpme fix script run traing process start later get error donot know fix erros help fix script helpful,2023-06-12 15:01:55,,2023-06-12 17:45:44,2023-06-12 17:45:44,<nlp><gpt-2>,0,0,0,12,,,,,,,
76462707,1,22065227.0,,Building a GPT-3 Enabled Research Assistant with LangChain & Pinecone,like a chatbot handle large csv file answer question about data contain implement pinecone store vector data connect langchain current setup not provide accurate answer base give data potential error configuration,2023-06-13 07:46:46,,,2023-06-13 07:46:46,<csv><chatbot><gpt-3><langchain><pinecone>,0,3,0,39,,,,,,,
76463184,1,21824356.0,,Using OpenAI LLMs for classification. Asking for classification vs. asking for probabilities,use llms classify products specific categories multiclass way ask a yesno a specific category loop categories way ask a probability certain product belong class second option allow adjust prediction thresholds post overunderclassify certain class word street rlhftrained openai model weak guess probabilities relative text completion model like rlhf train make model think like a human bad guess probabilities literature read know about ahead run a  test not try give test timecost intensive like a baseline understand tackle problem start,2023-06-13 08:49:01,,,2023-06-13 08:49:01,<text-classification><openai-api><multilabel-classification><gpt-4><llm>,0,0,0,10,,,,,,,
76106366,1,20779237.0,,how to use tiktoken in offline mode computer,keep show error call requestsexceptionssslerror cause sslerror sslcertverificationerror  ssl certificate_verify_failed certificate verify fail self sign certificate certificate chain _sslc  like want run code number tokens keep show error mention earlier miss code,2023-04-26 00:36:11,,,2023-06-04 20:41:26,<python><tokenize><gpt-3>,1,1,2,1232,,,,,,,
76106760,1,21740587.0,,Azure OpenaAI GPT-4 Review version cannot be found in the list of models,receive email  april confirm onboarded azure openai service gpt preview gpt review version list openai model gpt deploy resource group choice eastern south central grateful guru help use a variety methods a payasyougo subscriber,2023-04-26 02:34:38,,2023-04-26 10:35:42,2023-04-26 10:35:42,<azure-openai><gpt-4>,0,0,0,59,,,,,,,
57782409,1,4544413.0,57830166.0,Set the number of iterations gpt-2,fine tune a gpt model follow tutorial associate github repository able replicate examples issue not find a parameter set number iterations basically train script show a sample  iterations save a model version  iterations not find a parameter train say  iterations close script train edit suggest cronoik try replace a loop add change add additional argument parseradd_argument training_steps metavar teps typeint default help a number represent train step model shall train change loop use new argument python trainpy training_steps  get error,2019-09-04 06:09:29,,2020-11-29 11:51:15,2020-11-29 11:51:15,<python><tensorflow><nlp><gpt-2>,1,1,0,795,,2.0,6664872.0,"<p>All you have to do is to modify the <code>while True</code> loop to a <code>for</code> loop:</p>

<pre class=""lang-py prettyprint-override""><code>try:
    #replaced
    #while True:
    for i in range(5000):
        if counter % args.save_every == 0:
            save()
        if counter % args.sample_every == 0:
            generate_samples()
        if args.val_every &gt; 0 and (counter % args.val_every == 0 or counter == 1):
            validation()

        if args.accumulate_gradients &gt; 1:
            sess.run(opt_reset)
            for _ in range(args.accumulate_gradients):
                sess.run(
                    opt_compute, feed_dict={context: sample_batch()})
            (v_loss, v_summary) = sess.run((opt_apply, summaries))
        else:
            (_, v_loss, v_summary) = sess.run(
                (opt_apply, loss, summaries),
                feed_dict={context: sample_batch()})

        summary_log.add_summary(v_summary, counter)

        avg_loss = (avg_loss[0] * 0.99 + v_loss,
                    avg_loss[1] * 0.99 + 1.0)

        print(
            '[{counter} | {time:2.2f}] loss={loss:2.2f} avg={avg:2.2f}'
            .format(
                counter=counter,
                time=time.time() - start_time,
                loss=v_loss,
                avg=avg_loss[0] / avg_loss[1]))

        counter += 1
except KeyboardInterrupt:
    print('interrupted')
    save()
</code></pre>
",2019-09-07 02:11:11,1.0,1.0
58093426,1,10429635.0,59687442.0,"Train GPT-2 on local machine, load dataset",try run gpt local machine google restrict resources train long colab load dataset original colab notebook command gptcopy_file_from_gdrive use local machine github repo simply file shakespearetxt function gptfinetune work do not work help appreciate,2019-09-25 07:37:39,,2020-11-29 11:53:05,2020-11-29 11:53:05,<python><jupyter-notebook><google-colaboratory><gpt-2>,1,0,1,1886,,2.0,2896004.0,"<p>If I read the <a href=""https://github.com/minimaxir/gpt-2-simple#usage"" rel=""nofollow noreferrer"">example</a> correctly on GitHub, it loads <code>shakespeare.txt</code> if it is present on the machine and downloads it if it isn't. For a local dataset, I simply drop a txt file in the same folder and call it in <code>file_name =</code>.</p>

<p>You should be able to remove the logic around <code>if not os.path.isfile(file_name):</code>—it shouldn't be needed if you use a local file. </p>
",2020-01-10 18:51:24,0.0,1.0
59944537,1,6289601.0,59945547.0,Is there a GPT-2 implementation that allows me to fine-tune and prompt for text completion?,wish finetune a gpt implementation text data want use model complete a text prompt easily use max woolf gptsimple implementation neil shepherd fork openai allow gpt train new data complete text corpus small train not gibberish way combine function ideally like able a python interface oppose cli like use pandas data clean whathaveyou thank,2020-01-28 08:13:29,,2020-11-29 12:02:45,2020-11-29 12:02:45,<python-3.x><deep-learning><nlp><openai-gym><gpt-2>,1,0,3,1970,0.0,2.0,5652313.0,"<p><a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">Huggingface's Transformers</a> package has a GPT-2 implementation (including pre-trained models) for PyTorch and TensorFlow. You can easily work with them in Python.</p>

<p>Fine-tuning of GPT-2, however, requires a lot of memory and I am not sure is you will be able to do the full backpropagation on that. In that case, you fine-tune just a few highest layers.</p>
",2020-01-28 09:23:06,2.0,3.0
65987683,1,1793799.0,65991030.0,Modifying the Learning Rate in the middle of the Model Training in Deep Learning,code configure trainingarguments consume huggingface transformers library finetune gpt language model number epochs  learning_rate  early_stopping configure patience value  model run  epochs notice difference loss_value negligible latest checkpoint save now modify resume train latest save checkpoint do efficient train new value start train begin,2021-02-01 05:42:01,,2021-02-01 05:48:22,2021-02-01 10:30:40,<deep-learning><pytorch><huggingface-transformers><language-model><gpt-2>,2,1,2,1184,,2.0,8047535.0,"<p><strong>No, you don't have to restart your training.</strong></p>
<p>Changing the learning rate is like changing how big a step your model take in the <strong>direction determined by your loss function</strong>.</p>
<p>You can also think of it as transfer learning where the model has <strong>some experience</strong> (no matter how little or irrelevant) and the <code>weights</code> are in a state <strong>most likely better than a randomly initialised one</strong>.</p>
<p>As a matter of fact, changing the learning rate mid-training is considered an art in deep learning and you should change it if you have a <strong>very very good reason</strong> to do it.</p>
<p>You would probably want to write down when (why, what, etc) you did it if you or someone else wants to &quot;reproduce&quot; the result of your model.</p>
",2021-02-01 10:30:40,0.0,3.0
67089849,1,15221534.0,67089951.0,AttributeError: 'GPT2TokenizerFast' object has no attribute 'max_len',just use huggingface transformer library follow message run run_lm_finetuningpy attributeerror gpttokenizerfast object no attribute max_len problem idea fix thank experiment run mkdir experiment epoch  python run_lm_finetuningpy model_name_or_path distilgpt model_type gpt train_data_file small_dataset_train_preprocessedtxt output_dir experimentsepochs_ epoch do_train overwrite_output_dir per_device_train_batch_size  num_train_epochs epoch,2021-04-14 10:20:20,,,2023-06-15 09:00:04,<tokenize><huggingface-transformers><transformer-model><huggingface-tokenizers><gpt-2>,2,0,2,7151,0.0,2.0,3832970.0,"<p>The <a href=""https://github.com/huggingface/transformers/issues/8739"" rel=""nofollow noreferrer"">&quot;AttributeError: 'BertTokenizerFast' object has no attribute 'max_len'&quot; Github issue</a> contains the fix:</p>
<blockquote>
<p>The <code>run_language_modeling.py</code> script is deprecated in favor of <code>language-modeling/run_{clm, plm, mlm}.py</code>.</p>
<p>If not, the fix is to change <code>max_len</code> to <code>model_max_length</code>.</p>
</blockquote>
<p>Also, <code>pip install transformers==3.0.2</code> might fix the issue since it has been reported to work for some people.</p>
",2021-04-14 10:27:39,0.0,11.0
67735561,1,4438203.0,67736155.0,Fine-tuning GPT-2/3 on new data,try wrap head train openai language model new data set experience regard idea fee gpt  not api access  a textbook train able discuss content book language model not think change hyperparameters just need data model possible thank a lot conceptual help,2021-05-28 08:35:33,2021-05-28 12:10:37,,2021-07-14 14:00:00,<machine-learning><training-data><gpt-2><gpt-3>,2,2,0,2143,,2.0,1362200.0,"<p>Presently GPT-3 has no way to be finetuned as we can do with GPT-2, or GPT-Neo / Neo-X. This is because the model is kept on their server and requests has to be made via API. A Hackernews <a href=""https://news.ycombinator.com/item?id=23725834"" rel=""nofollow noreferrer"">post</a> says that finetuning GPT-3 is planned or in process of construction.</p>
<p>Having said that, OpenAI's GPT-3 provide <a href=""https://beta.openai.com/docs/guides/search"" rel=""nofollow noreferrer"">Answer API</a> which you could provide with context documents (up to 200 files/1GB). The API could then be used as a way for discussion with it.</p>
<p>EDIT:
Open AI has recently introduced Fine Tuning beta.
<a href=""https://beta.openai.com/docs/guides/fine-tuning"" rel=""nofollow noreferrer"">https://beta.openai.com/docs/guides/fine-tuning</a>
Thus it will be best answer to the question to follow through description on that link.</p>
",2021-05-28 09:18:38,2.0,2.0
67299510,1,2178942.0,67906030.0,Understanding how gpt-2 tokenizes the strings,use tutorials write follow cod realize input consist tokenized items sentence value tokenized items example hello dog cute ask think separetes a word word not dictionary a word language want check cod,2021-04-28 11:38:49,,2021-05-12 22:09:27,2021-06-09 14:19:35,<python><huggingface-transformers><transformer-model><gpt-2>,1,1,1,888,,2.0,8301609.0,"<p>You can call <code>tokenizer.decode</code> on the output of the tokenizer to get the words from its vocabulary under given indices:</p>
<pre><code>&gt;&gt;&gt; inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)
&gt;&gt;&gt; list(map(tokenizer.decode, inputs.input_ids[0]))
['Hello', ',', ' my', ' dog', ' is', ' cute']
</code></pre>
",2021-06-09 14:19:35,0.0,3.0
76172889,1,21815490.0,,Chatgpt api url questions: Chatgpt3.5 error,try use openai api do not work curl result watch openai docs url right do not pass thank help,2023-05-04 11:54:16,,,2023-06-03 04:00:28,<openai-api><gpt-3><chatgpt-api>,1,1,-3,206,,,,,,,
76502113,1,21018812.0,,Error with Few-shot prompting using gpt 3.5,try train gpt  model fewshot prompt use message argument instead prompt argument throw error clearly mention openai documentation train a model way error try conversations model inside responses soes not work,2023-06-18 18:58:45,,2023-06-18 19:01:09,2023-06-19 07:38:48,<chatbot><openai-api><gpt-3><chatgpt-api>,1,1,0,29,,,,,,,
76525391,1,5204859.0,,How to train gpt2 model to learn from the training text I have given?,try train fine tune gpt model sample train document use code similar text generate not relate text train document solution approach,2023-06-21 16:28:20,,,2023-06-21 16:28:20,<huggingface-transformers><training-data><huggingface><gpt-2><fine-tune>,0,0,0,18,,,,,,,
66873983,1,4458718.0,66874815.0,Flask app serving GPT2 on Google Cloud Run not persisting downloaded files?,a flask app run google cloud run need download a large model gpt huggingface take a download try set download deployment just serve subsequent visit follow code a script import main flask app apppy basically try load download model fail download a new copy model autoscaling set a minimum  think mean run download file persist activity keep have redownload model freeze app people try use try recreate like app do not appear load time issue flask app follow redownload model hours avoid have app redownloading model app freeze want try,2021-03-30 15:33:03,,2021-03-30 16:26:45,2021-03-30 16:27:12,<flask><google-cloud-platform><pytorch><google-cloud-run><gpt-2>,1,0,2,198,0.0,2.0,8016720.0,"<p>Data written to the filesystem does not persist when the container instance is stopped.</p>
<p>Cloud Run lifetime is the time between an HTTP Request and the HTTP response. Overlapped requests extend this lifetime. Once the final HTTP response is sent your container can be stopped.</p>
<p>Cloud Run instances can run on different hardware (clusters). One instance will not have the same temporary data as another instance. Instances can be moved. Your strategy of downloading a large file and saving it to the in-memory file system will not work consistently.</p>
<p><a href=""https://cloud.google.com/run/docs/reference/container-contract#filesystem"" rel=""nofollow noreferrer"">Filesystem access</a></p>
<p>Also note that the file system is in-memory, which means you need to have additional memory to store files.</p>
",2021-03-30 16:27:12,3.0,3.0
67403271,1,6065246.0,67446168.0,Using AI generators to ask questions to provoke thinking instead of giving answers?,a use case want use help independent creators talk about interest twitter use experience go like want talk about entrepreneurship experience like pain a way ai like gpt generate prompt use word create a list openended question provoke thoughts entrepreneurship not painful look like know about entrepreneurship painful starters know lower barrier entrepreneurship a less painful opportunity a person work need explore open ai documentation gpt unclear solve problem generate prompt thank,2021-05-05 14:25:52,2021-05-08 13:26:53,2021-05-13 19:35:51,2021-05-13 19:35:51,<artificial-intelligence><gpt-2><gpt-3>,1,0,0,110,,2.0,1362200.0,"<p>You should provide some samples so that the GPT-3 can see the pattern and produce a sensible response from your prompt.
For example, see the following screenshot from your case. Note that the bold text is my prompt. The regular text is the response from GPT-3. In that example, I was  &quot;priming&quot; the GPT-3 with relevant pattern: First line, the general description, then the Topics, followed by Questions. This should be enough for booting up your ideas and customizations.</p>
<p><a href=""https://i.stack.imgur.com/kgYpx.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kgYpx.png"" alt=""sample of prompt to generate questions from topics"" /></a></p>
",2021-05-08 09:46:19,2.0,0.0
76526344,1,18937923.0,,How to queue API calls to Azure OpenAI service (with a token per minute rate limit) the most efficiently?,implement efficient queue use azure serverless technologies azure servicebus azure openai service concurrently guarantee earlier message process complexity rate limit not base x request minute base a roll window instead about tokens minute azure implement a  minute timer not know reset explanation rate limit policy assume follow queue a rate limit  tpm request   expect tokens request   expect tokens request   expect tokens request   expect tokens request   expect tokens like queue concurrently process request   alize request  overshoot token limit chedule minute wait request  amp  concurrently schedule minute wait process request  theory not need chedule just hit rate limit a retry policy maybe better schedule not know moment timer reset exact tokens azure estimate request cost case make sure not end a race condition request  fail retry  get  theory intelligent solution process  parallel wait a minute process  wait a minute process   allow  fit minute limit waste,2023-06-21 18:57:51,,2023-06-21 19:05:49,2023-06-21 19:05:49,<azure><message-queue><azureservicebus><azure-openai><gpt-4>,0,1,1,50,,,,,,,
76529971,1,22113227.0,,LLM Content Generation in Non-English Languages,try use gpt generate content nonenglish languages include lowresource languages inherently small train data think approach challenge use translation api gpt gpt interact english see a translate prompt produce english content retranslate original language ask gpt produce nonenglish content directly response swahili do knowledge know approach likely succeed,2023-06-22 08:31:54,,,2023-06-22 08:31:54,<translation><openai-api><linguistics><gpt-3><llm>,0,0,0,11,,,,,,,
76535292,1,15071578.0,,I am not getting exact response from my api as i am getting from chat gpt,explain not get good response  api generate content good gpt response app about help recruiters refine job post not work fine improve response tell optimize,2023-06-22 20:04:40,,2023-06-22 22:41:47,2023-06-22 22:41:47,<reactjs><artificial-intelligence><openai-api><chatgpt-api><gpt-4>,0,1,0,24,,,,,,,
76543141,1,13209030.0,,GPT-4 doesn't follow output format instruction occasionally,write a custom wrapper openai gpt api prompt similarly react model think action observation final answer output format instruction agent scratchpad idea tell gpt include end answer end response parse output regex approximately  time fail include keyword mess output parser improve prompt result consistent try set temperature  add delimeters openai suggest result keep inconsistent,2023-06-23 19:58:54,,,2023-06-23 19:58:54,<openai-api><gpt-4>,0,0,0,12,,,,,,,
76173736,1,10759664.0,,GPT-2: Setting biases as -1 billion,currently try predict upcoming word give input text chunk want mask n word input text set attention weight  small try try modify bias layer gpt model set a small value tokens want mask read add bias value dotproduct query key vectors figure negative ideally small order make result attention weight small possible a blog post selfattention read use inf  billion gpt use value  errors possibly produce value small produce underflow think odd  basically minimum cutoff value use not small need advice a do change bias like make sense a newbie gptuser a little unsure correct b approach make sense not use value  a way use smaller value c not use  work reduce attention weight  code thank advance helpideascomments,2023-05-04 13:26:10,,2023-05-04 13:36:41,2023-05-04 13:39:31,<python><nlp><gpt-2>,0,1,0,46,,,,,,,
65145526,1,9291922.0,,Why new lines aren't generated with my fine-tuned DistilGPT2 model?,currently try finetune distilgpt pytorch huggingface transformers library a code completion task corpus arrange like follow example attempt run follow script transformers library do generation test realize model not predict give context imagine preprocess stage similar miss predict expect hf forum question thank,2020-12-04 14:37:53,,,2020-12-13 05:29:01,<pytorch><huggingface-transformers><gpt-2>,1,2,1,1079,,,,,,,
65341363,1,14844030.0,,What memory does Transformer Decoder Only use?,read a lot about transformers self attention see bert gpt a newer version use encoder transformer bert decoder transformer gpt try build a decoder model sequence prediction confuse thing use pytorch look thereseqseq tutorial look transformer decoder block transformer decoder layer confusion come memory need pass documentation say memory layer encoder block make sense a seqseq model want make a decoder model question pass a decoder model like gpt memory not encoder,2020-12-17 13:08:59,,,2023-05-09 14:37:17,<python><pytorch><decoder><transformer-model><gpt-2>,1,0,2,1728,,,,,,,
65551516,1,843036.0,,Cannot convert from a fine-tuned GPT-2 model to a Tensorflow Lite model,fine tune a distilgpt model use text use work fine train script produce expect result now want convert a tensorflow lite model do use follow dont think do right conversion write go ahead tflite conversion use do not work use generate model error sure model not convert properly get inputoutput do idea fix thank,2021-01-03 15:26:40,,,2021-01-03 15:26:40,<tensorflow><tensorflow-lite><huggingface-transformers><gpt-2>,0,3,1,546,,,,,,,
68442098,1,13531125.0,68442279.0,How to store API keys in environment variable? and call the same in google colab,not sure make a json file gpt api keyenvironment variable like utilize google colab automatic code generation want api key thejson file use code,2021-07-19 14:20:48,,2021-07-19 14:28:15,2021-07-19 14:41:18,<python><environment-variables><gpt-3>,1,4,0,3723,,2.0,13151915.0,"<p>To read from a json file you do the following:</p>
<pre><code>import json

my_key = ''
with open('GPT_SECRET_KEY.json', 'r') as file_to_read:
    json_data = json.load(file_to_read)
    my_key = json_data[&quot;API_KEY&quot;]
</code></pre>
<p>The structure of your json file should look something like this.</p>
<pre><code>{
    &quot;API_KEY&quot;: &quot;xxxxthis_is_the_key_you_are_targetingxxxx&quot;, 
    &quot;API_KEY1&quot;: &quot;xxxxxxxxxxxxxxxxxxxxxxx&quot;,
    &quot;API_KEY2&quot;: &quot;xxxxxxxxxxxxxxxxxxxxxxx&quot;
}
</code></pre>
<p><a href=""https://stackoverflow.com/questions/20199126/reading-json-from-a-file"">Here is a link</a> to a similar question if my answer was not clear enough.</p>
",2021-07-19 14:32:40,1.0,0.0
68604289,1,10605020.0,68656887.0,AttributeError: module transformers has no attribute TFGPTNeoForCausalLM,clone repositorydocumentation error run google collab locally instal transformers use sure configuration file name configjson code transformerscli env result version dev platform linuxmicrosoftx_withglibc python version  pytorch version gpu cpu false tensorflow version gpu  false flax version cpugputpu not instal na jax version not instal jaxlib version not instal use gpu script use distribute parallel setup script collab locally tensorflow  version,2021-07-31 17:14:49,,2021-08-01 09:27:54,2021-08-04 19:14:11,<python><pytorch><huggingface-transformers><google-publisher-tag><gpt-3>,2,0,2,7265,,2.0,10605020.0,"<p>My solution was to first edit the source code to remove the line that adds &quot;TF&quot; in front of the package as the correct transformers module is GPTNeoForCausalLM
, but somewhere in the source code it manually added a &quot;TF&quot; in front of it.</p>
<p>Secondly, before cloning the repository it is a must to run</p>
<pre><code> git lfs install. 
</code></pre>
<p>This link helped me install git lfs properly <a href=""https://askubuntu.com/questions/799341/how-to-install-git-lfs-on-ubuntu-16-04"">https://askubuntu.com/questions/799341/how-to-install-git-lfs-on-ubuntu-16-04</a></p>
",2021-08-04 19:14:11,2.0,0.0
69931987,1,4124584.0,69932414.0,GPT-3 davinci gives different results with the same prompt,not sure access gpt particularly davinci completeasentence tool api info try tool past hour time hit api use prompt input receive a different response happen encounter situation expect happen know reason examples request header try use example provide output  output  output ,2021-11-11 16:49:41,,,2023-01-13 22:52:41,<text><nlp><autocomplete><gpt-3>,2,0,4,2981,,2.0,4124584.0,"<p>I just talked to OpenAI and they said that their response is not deterministic. It's probabilistic so that it can be creative. In order to make it deterministic or reduce the risk of being probabilistic, they suggest adjusting the <code>temperature</code> parameter. By default, it is 1 (i.e. 100% taking risks). If we want to make it completely deterministic, set it to 0.</p>
<p>Another parameter is <code>top_p</code> (default=1) that can be used to set the state of being deterministic. But they don't recommend tweaking both <code>temperature</code> and <code>top_p</code>. Only one of them would do the job.</p>
",2021-11-11 17:21:08,0.0,2.0
66852791,1,13868065.0,,Key difference between BERT and GPT2?,read lot article people say bert good nlu gpt good nlg key difference structure just add a mask not selfattention train model different ways code understand correctly free choose add attention mask not come conclusion pretrained parameters bert good nlu pretrained parameters gpt good nlg critical difference make people come conclusion mention begin,2021-03-29 10:45:17,,,2021-03-30 09:03:33,<bert-language-model><gpt-2>,1,1,1,1084,,,,,,,
66991360,1,15252562.0,,GPT-2's encoder.py and train.py are not working,try train gpt use provide a text file napoleontxt run encoder work command prompt do not actually create napoleonnpz problem larger issue trainpy actually need order train gpt spit error single time try single solution internet think stick help,2021-04-07 17:41:11,,,2021-04-07 17:47:38,<python><artificial-intelligence><gpt-2>,1,0,0,280,,,,,,,
61510865,1,2058221.0,,"Tensorflow has no Attribute ""sort"" in GPT 2 Git Release?",download git repo follow python instructions developersmd installation kubuntu lts box run instead error far run culminate error strange not sure proceed think instructions lead successful installation appear not uninstalling reinstall no effect final result tensorflow execute gptii,2020-04-29 20:14:22,,2020-11-29 11:59:08,2021-02-15 18:31:19,<tensorflow><gpt-2>,2,1,3,1586,,,,,,,
69403613,1,2632462.0,,How to early-stop autoregressive model with a list of stop words?,use gptneo model generate text prompt use start like stop sentence par generate a method source code without instructions use do a way earlystop model generation thank try,2021-10-01 09:30:21,,2021-10-10 16:57:10,2022-04-25 17:29:51,<python><huggingface-transformers><autoregressive-models><gpt-2>,1,2,3,1181,,,,,,,
67379533,1,10575373.0,,Why some weights of GPT2Model are not initialized?,use gpt pretrained model a research project load pretrained model follow code follow warn message understand say weight above layer not initialize pretrained model know attention layer attn important gpt not actual weight pretrained model point use a pretrained model really appreciate explain tell fix,2021-05-04 05:59:46,,,2021-05-12 21:43:34,<pytorch><huggingface-transformers><gpt-2>,1,2,3,980,,,,,,,
68038662,1,16262479.0,68038692.0,How to get th content of a string inside a request response?,cod a webapp base gpt not good decide switch official openai gpt make request print response want print text print text value response list thank advance a good day,2021-06-18 16:34:08,,,2022-10-11 03:42:50,<python><python-requests><openai-api><gpt-3>,3,0,0,5316,,2.0,7212686.0,"<p>Using the dict indexing by key, and the list indexing by index</p>
<pre><code>x = {&quot;choices&quot;: [{&quot;finish_reason&quot;: &quot;length&quot;,
                  &quot;text&quot;: &quot;, everyone, and welcome to the first installment of the new opening&quot;}], }

text = x['choices'][0]['text']
print(text)  # , everyone, and welcome to the first installment of the new opening
</code></pre>
",2021-06-18 16:36:38,0.0,4.0
69549494,1,17139319.0,69549620.0,"A way to make GPT-3's ""davinci"" converse with a user(s) through a bot in discord using discord.js?",try give completions like default preset chat preset gpt playground use openainode code javascript python openai api,2021-10-13 03:43:17,,,2021-10-13 04:03:48,<javascript><discord.js><openai-api><gpt-3>,1,0,1,408,,2.0,4384238.0,"<p>Your <code>prompt</code> needs to be given more information for GPT-3 to understand what you want. You're providing a prompt of the message, such as</p>
<pre><code>My message!
</code></pre>
<p>But what you really should be giving it is something like:</p>
<pre><code>The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.

Human: Hello, who are you?
AI: I am an AI created by OpenAI. How can I help you today?
Human: My message!
AI:
</code></pre>
<p>In addition, if you it to be contextually aware, you need to continue adding information to the prompt, such as:</p>
<pre><code>The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.

Human: Hello, who are you?
AI: I am an AI created by OpenAI. How can I help you today?
Human: My message!
AI: Response here
Human: Another message here
AI:
</code></pre>
<p><strong>Be aware of the token limits and costs.</strong> You may to choose to make it <em>not</em> contextual, or at some point start cutting out previous messages.</p>
",2021-10-13 04:03:48,0.0,1.0
72554328,1,2195440.0,72718976.0,How to fine tune fine tune GitHub Copilot?,fine tune language model like fine tune model look examples detail really appreciate fine tune github copilot,2022-06-09 03:12:02,,2023-01-18 21:35:08,2023-01-18 21:35:08,<github><deep-learning><openai-api><gpt-3><github-copilot>,3,0,3,2020,0.0,2.0,6309.0,"<p>There does not seem to be a client-facing feature allowing you to fine-tune Copilot directly.</p>
<p>Here are two illustration as to why this feature is, for now (Q2 2022) missing.</p>
<p>The <a href=""https://github.com/features/copilot"" rel=""nofollow noreferrer"">Copilot feature page</a> initially included this:</p>
<blockquote>
<h2>How will GitHub Copilot get better over time?</h2>
<p>GitHub Copilot doesn’t actually test the code it suggests, so the code may not even compile or run. GitHub Copilot can only hold a very limited context, so even single source files longer than a few hundred lines are clipped and only the immediately preceding context is used. And GitHub Copilot may suggest old or deprecated uses of libraries and languages. You can use the code anywhere, but you do so at your own risk.</p>
</blockquote>
<p>As <a href=""https://twitter.com/tomekkorbak"" rel=""nofollow noreferrer"">Tomek Korbak</a> explains <a href=""https://twitter.com/tomekkorbak/status/1410554250514636805"" rel=""nofollow noreferrer"">on Twitter</a>:</p>
<blockquote>
<p>Actually, Copilot's completions will always be optimised for human's liking, not necessarily compiler's liking.</p>
<p>That's because the language model training objective (predicting the next token in text) is great at capturing short-term dependencies (which explains the human feel of generated snippets).</p>
<p>But it struggles to capture long-term, global, semantic properties of generated sequences such as compilability. And there's no easy way of including compilability as a signal for their training.</p>
<p>The standard way -- fine-tuning language models using RL with compilability as a reward -- notoriously leads to catastrophic forgetting: less diverse and less accurate completions.</p>
</blockquote>
<p>Tomek references &quot;<a href=""https://arxiv.org/pdf/2106.04985.pdf"" rel=""nofollow noreferrer"">Energy-Based Models for Code Generation under Compilability Constraints (pdf)</a>&quot;</p>
<blockquote>
<p><a href=""https://i.stack.imgur.com/ulfPr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ulfPr.png"" alt=""https://pbs.twimg.com/media/E5NHqGjXIAYRtwa?format=png&amp;name=small"" /></a></p>
<p>Our solution (KL-DPG) boosts compilability rate of generated sequences from 55% to 70%.<br />
RL fine-tuning can do better but at a cost of catastrophic forgetting.</p>
<p>Overall, energy-based models (EBMs) turn out to be great at expressing weird, sequence-level constraints that would be super hard as to express as normalised priors for autoregressive language models.</p>
<p>EBMs provide a way of injecting our structured, symbolic knowledge into large language models without breaking them down or sacrificing their uncanny abilities.<br />
The space of further applications in controllable generation is huge.</p>
</blockquote>
<p>So not so easy.</p>
<p><a href=""https://tmabraham.github.io/"" rel=""nofollow noreferrer"">Tanishq Mathew Abraham</a> explains in &quot;<a href=""https://tmabraham.github.io/blog/github_copilot"" rel=""nofollow noreferrer"">Coding with GitHub Copilot</a>&quot;</p>
<blockquote>
<p>I wonder if the GitHub team might also develop a way of perhaps fine-tuning GitHub Copilot to specific use-cases.</p>
<p>For example, there may be a specific GitHub Copilot models for fastai, JAX, etc. They would be fine-tuned on the source code of of these libraries and codebases that use these libraries.</p>
<p>But making sure that the tool does not provide outdated suggestions would still be a challenge.<br />
I don’t think it would be possible to provide suggestions for a brand-new library that does not have enough codebases using it to train on.</p>
<p>Additionally, for situations like fastai where there are older APIs and newer APIs, when fine-tuning a model, the codebases using the older APIs would have to be filtered out.</p>
</blockquote>
",2022-06-22 16:25:11,0.0,1.0
67444616,1,10575373.0,,How to increase batch size in GPT2 training for translation task?,develop a code use pretrained gpt model a machine translation task length data wordtoid  develop follow code model code work pretty batch size  slow want increase batch size   dimension compatibility problems increase batch size without errors data consist pair sentence a sentence language second translation second language example assume xshape batch_size  mean batch_size sentence length  input yshape batch_size  translations a wordtoid dictionary length  match word a sentence index,2021-05-08 06:12:06,,2021-05-11 07:30:58,2021-05-13 17:09:35,<nlp><pytorch><gpt-2>,1,2,2,1088,0.0,,,,,,
68444704,1,13531125.0,,"How to fix the error : ""cannot import name 'GPT' from ""gpt""",run code google colab follow error note instal gpt use pip pip install gpt code error help fix issue,2021-07-19 17:29:47,,,2022-02-04 13:31:38,<python><gpt-3>,1,3,1,2231,,,,,,,
72663133,1,955883.0,,GPT-3 fine tuning Error: Incorrect API key provided,follow tutorial finetune a gpt model run code error curious thing api key correct use make a prompt work perfectly example cause error think maybe gpt train require a pay account do not restriction openai website code use thank advance help,2022-06-17 17:53:53,,,2023-03-20 07:06:30,<python><api><openai-api><gpt-3>,1,1,3,10110,,,,,,,
71690297,1,11410327.0,,Large Language Model Perplexity,currently use gpt try compare capabilities relate language model master thesis unfortunatly gpt api base application not really able extract metrics perplexity api acces metrics course model output training_loss loss train batch training_sequence_accuracy percentage completions train batch model predict tokens match true completion tokens exactly example a batch_size  data contain completions       model predict       accuracy   training_token_accuracy percentage tokens train batch correctly predict model example a batch_size  data contain completions       model predict       accuracy   possibility calculate perplexity model use python thank,2022-03-31 09:41:36,,,2022-03-31 09:41:36,<python><nlp><nltk><gpt-3><perplexity>,0,2,1,378,,,,,,,
65822014,1,1793799.0,,"RuntimeError: Input tensor at index 3 has invalid shape [2, 2, 16, 128, 64] but expected [2, 4, 16, 128, 64]",runtime error finetuning a pretrained gptmedium model use huggingface library sagemaker mlpxlarge instance contain libraries pretrained model train test data construction amp unstructured text data file size  million k line data train arguments train arguments construct train model trainer train train  epoch  gpus use mlpxlarge instance train torchdistribution like train end epoch observe error way construct use do wrong,2021-01-21 06:11:18,,,2022-02-28 05:45:52,<python><pytorch><amazon-sagemaker><huggingface-transformers><gpt-2>,1,0,1,1142,,,,,,,
66647600,1,15404079.0,,Speeding up Inference time on GPT2 - optimizing tf.sess.run(),try optimize inference time gpt current time generate a sample call script  secs google colab timestamps try isolate bottleneck code line complexity lie do way improve piece code thank,2021-03-16 00:39:22,,,2021-07-27 01:54:51,<tensorflow><gpt-2><sess.run>,1,0,0,281,,,,,,,
72922146,1,19453849.0,,Why do 'callback' and 'tweet' fail in my Twitter GPT-3 OpenAI bot?,try develop a twitter bot use openai gpt library follow tutorial get error click auth link authorize bot click link get error recently get elevate access twitter bot try use bot help thank code,2022-07-09 14:24:19,,2022-07-09 14:26:36,2022-07-09 14:26:36,<javascript><twitter><twitter-oauth><openai-api><gpt-3>,0,0,1,93,,,,,,,
73797902,1,11108470.0,73853263.0,GPT-3 API invalid_request_error: you must provide a model parameter,new apis try understand a response a prompt use openai gpt api use apiopenaicomvcompletions use postman documentation say require parameter model error say provide a model parameter provide do wrong,2022-09-21 08:49:56,,2023-01-11 20:14:51,2023-05-11 08:16:18,<rest><postman><http-post><openai-api><gpt-3>,4,2,13,19728,,2.0,34170.0,"<p>You can get this to work the following way in Postman with the POST setting:</p>
<ol>
<li><p>Leave all items in the Params tab empty</p>
</li>
<li><p>In the Authorization tab, paste your OpenAI API token as the Type Bearer Token (as you likely already did)</p>
</li>
<li><p>In the Headers tab, add key &quot;Content-Type&quot; with value &quot;application/json&quot;</p>
</li>
<li><p>In the Body tab, switch to Raw, and add e.g.</p>
<pre><code> {  
     &quot;model&quot;:&quot;text-davinci-002&quot;,
     &quot;prompt&quot;:&quot;Albert Einstein was&quot;
 }
</code></pre>
</li>
<li><p>Hit Send. You'll get back the completions for your prompt.</p>
</li>
</ol>
<p>Note alternatively, you can add the model into the Post URL, like <code>https://api.openai.com/v1/engines/text-davinci-002/completions</code></p>
<p>While above works, it might not be using the Postman UI to its full potential -- after all, we're raw-editing JSON instead of utilizing nice key-value input boxes. If you find out how to do the latter, let us know.</p>
<p><a href=""https://i.stack.imgur.com/3MKez.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3MKez.png"" alt=""enter image description here"" /></a></p>
",2022-09-26 11:06:35,2.0,16.0
75091786,1,20972936.0,75092193.0,OpenAI Unity - POST Request not working properly (400 status),connect gpt openai just manage make a proper post request follow guide work code return a bad request aka,2023-01-12 04:58:07,,2023-01-12 06:45:51,2023-01-30 07:17:48,<unity-game-engine><httprequest><openai-api><gpt-3>,1,0,4,282,,2.0,20971780.0,"<p>Remove the &quot;Content-Type&quot; from the headers. The content is not JSON, it's form data.</p>
<p>i.e.</p>
<pre class=""lang-cs prettyprint-override""><code>using ( UnityWebRequest wR = UnityWebRequest.Post ( &quot;https://api.openai.com/v1/completions&quot;, form ) )
{
    wR.SetRequestHeader ( &quot;Authorization&quot;, &quot;Bearer &quot; + apiKey );
    //wR.SetRequestHeader ( &quot;Content-Type&quot;, &quot;json&quot; );
    yield return wR.SendWebRequest ( );
    if ( wR.result != UnityWebRequest.Result.Success )
    {
        Debug.Log ( &quot;ERROR:\n&quot; + wR.error );
    }
    else
    {
        Debug.Log ( &quot;Success:\n&quot; + wR.result + &quot;\nUpload Completed!);
    }
}
</code></pre>
",2023-01-12 06:05:33,0.0,3.0
72992885,1,19554677.0,,Is there a way I can build a language model as described below?,wanna collect data a txt file lyric  odd songs artist want use train ai language model want kind output input a small phrase try use gptsimple max woolf github train a txt file create give a completely unrelated random output want code use output get file lyric lead zeppelin doubt output relevant train want,2022-07-15 10:53:20,,,2022-07-15 10:53:20,<tensorflow><machine-learning><gpt-2>,0,0,0,39,,,,,,,
73094271,1,2195440.0,,What is suffix and prefix prompt in openai Codex?,try understand suffix prompt addition prefix prompt codex provide example example not clear create a suffix prompt understand suffix prompt code insert model use case mode code need update middle a code snippet provide a snippet show use suffix prompt codex work insert mode,2022-07-23 21:21:41,,2023-01-18 21:33:15,2023-01-18 21:33:15,<python><deep-learning><openai-api><gpt-2><gpt-3>,1,0,1,1177,,,,,,,
73136017,1,4544236.0,,GPT-2 fails when passing in multiple context tokens,set anaconda environment run gpt model run generate_unconditional_samplespy script gpu without issue run interactive_conditional_samplespy script crash context token interactive_conditional_samplespy script work fine long model prompt produce context token instance use prompt produce list tokens  correctly generate text crash model prompt produce context tokens instance use prompt pig produce list tokens   crash immediately try a number different prompt pretty confident key difference work do not number context tokens crash get error a little insight go wrong fix really appreciate help,2022-07-27 10:06:56,,,2022-07-27 10:06:56,<python><tensorflow><gpt-2>,0,0,0,85,,,,,,,
75155794,1,239427.0,75209141.0,Why does GPT-2 vocab contain weird words?,look vocabulary gpt surprise weird tokens do not expect example contain token index  ãâãâãâãâãâãâãâãâãâãâãâãâãâãâãâãâãâãâãâãâãâãâãâãâãâãâãâãâãâãâãâãâ do happen token common gpt train data general vocabulary gpt build a problem,2023-01-18 07:21:49,,2023-01-18 07:27:51,2023-01-23 11:55:36,<machine-learning><gpt-2>,1,0,0,262,,2.0,12750353.0,"<p>Information about the model available here <a href=""https://huggingface.co/gpt2"" rel=""nofollow noreferrer"">https://huggingface.co/gpt2</a></p>
<blockquote>
<p>The OpenAI team wanted to train this model on a corpus as large as possible. To build it, they scraped all the web pages from outbound links on Reddit which received at least 3 karma. Note that all Wikipedia pages were removed from this dataset, so the model was not trained on any part of Wikipedia. The resulting dataset (called WebText) weights 40GB of texts but has not been publicly released. You can find a list of the top 1,000 domains present in WebText <a href=""https://github.com/openai/gpt-2/blob/master/domains.txt"" rel=""nofollow noreferrer"">here</a>.</p>
</blockquote>
<p>Accordingly to hugging face <a href=""https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2Tokenizer"" rel=""nofollow noreferrer"">GPT2Tokenizer</a>, the tokenizer is based on <a href=""https://en.wikipedia.org/wiki/Byte_pair_encoding"" rel=""nofollow noreferrer"">BPE</a>, such token could have ended up there due to an encoding issue.</p>
<p>You can see that this is the the char codes for <code>ÃÂ</code> are <code>195</code>, and <code>194</code>, <code>C3 C2</code> that could be a two-byte encoded character in a different encoding? Or part of binary data that leaked into the corpus?</p>
<p>If that token was not frequent it is likely that it will never be relevant at the output. But it is an issue in the sense that the model wastes resources describing the behavior for that token.</p>
",2023-01-23 11:55:36,4.0,1.0
75390542,1,21159312.0,75390600.0,Discordbot.py:NotFound: 404 Not Found (error code: 10062): Unknown interaction,code try slash command tell error code really not solution try time help change delay try embedem responsedefer try override embed nothing do not work,2023-02-08 19:01:25,,2023-02-08 19:03:46,2023-02-08 19:07:31,<python><discord.py><gpt-3>,1,0,0,268,,2.0,4680300.0,"<p>I think the issue is that your interaction is timing out before you respond - hence the <code>404</code> error. You should put the <code>defer()</code> further up - ideally as soon as possible and <em>before</em> you make any API calls.</p>
<pre class=""lang-py prettyprint-override""><code>@bot.tree.command(description=&quot;create a txt with chat-gpt&quot;)
async def gpt(interaction: discord.Interaction, *, prompt:str):
    await interaction.response.defer()
    async with aiohttp.ClientSession() as session:
        payload = {
            &quot;model&quot;:&quot;text-davinci-003&quot;,
            &quot;prompt&quot;:prompt,
            &quot;temperature&quot;: 0.5,
            &quot;max_tokens&quot;: 50,
            &quot;presence_penalty&quot;: 0,
            &quot;frequency_penalty&quot;: 0,
            &quot;best_of&quot;: 1,
        }
        headers = {&quot;Authorization&quot;: f&quot;Bearer {API_KEY}&quot;}
        async with session.post(&quot;https://api.openai.com/v1/completions&quot;, json=payload, headers=headers) as resp:
            response = await resp.json()
            em = discord.Embed(title=&quot;Chat gpt’s responce:&quot;, description=response)
            await asyncio.sleep(delay=0)
            await interaction.followup.send(embed=em)
</code></pre>
",2023-02-08 19:07:31,0.0,2.0
73158634,1,19564052.0,,"Can someone help me understand -""InvalidRequestError"" in Gpt3 open ai using python as I did not find any solution?",experiment open ai run error run a python script use gpt open ai library despite look good error persist code error get explain error suggest a way,2022-07-28 20:07:44,,2022-07-29 04:08:50,2022-07-29 04:08:50,<python-3.x><openai-api><gpt-3>,0,1,0,288,,,,,,,
73207664,1,13297517.0,,Issues running GPT-J-6B demo inference on colab,try run gptj b demo available unfortunatelay issue block far firstly run cell colab notebook not understand try download versions tensorflow specify tensorflow code instal versions a long time a screenshot a output output image end execution excecution end message image try import libraries follow code cells receive miss modules errors miss modules vary depend result execution cell miss module image believe colab run disk memory try download model dependencies demo exist colab not run,2022-08-02 12:35:40,,2022-08-02 13:08:49,2022-08-02 13:08:49,<python><tensorflow><deep-learning><google-colaboratory><gpt-3>,0,0,1,616,,,,,,,
74712335,1,6288172.0,,how to fine tune a GPT-2 model?,use huggingface transformers package load a pretrained gpt model want use gpt text generation pretrained version not want fine tune a bunch personal text data not sure prepare data train model tokenized text data train gpt not sure label text generation not a classification problem train gpt data use keras api model tokenizer tokenized dataset use tokenized dataset fine tune gpt model,2022-12-07 05:55:51,,,2023-06-12 19:04:34,<python><tensorflow><dataset><huggingface-transformers><gpt-2>,2,2,1,1530,,,,,,,
75396481,1,8949058.0,75397187.0,"OpenAI GPT-3 API error: ""This model's maximum context length is 4097 tokens""",make a request completions endpoint prompt  tokens verify playground tokenizer wo not prompt a little long question request openai nodejs use openai npm package test playground total tokens response  submit prompt completions api get follow error able solve love hear do,2023-02-09 09:18:40,,2023-03-13 14:20:48,2023-05-15 06:09:51,<openai-api><gpt-3>,3,1,15,33120,,2.0,10347145.0,"<p>The <code>max_tokens</code> parameter is <strong>shared</strong> between the prompt and the completion. Tokens from the prompt and the completion all together should not exceed the token limit of a particular GPT-3 model.</p>
<p>As stated in the official <a href=""https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them"" rel=""noreferrer"">OpenAI article</a>:</p>
<blockquote>
<p>Depending on the model used, requests can use up to <code>4097</code> tokens shared
between prompt and completion. If your prompt is <code>4000</code> tokens, your
completion can be <code>97</code> tokens at most.</p>
<p>The limit is currently a technical limitation, but there are often
creative ways to solve problems within the limit, e.g. condensing your
prompt, breaking the text into smaller pieces, etc.</p>
</blockquote>
<p><em>Note: For counting tokens <strong>before(!)</strong> sending an API request, see <a href=""https://stackoverflow.com/questions/75804599/openai-api-how-do-i-count-tokens-before-i-send-an-api-request/75804651#75804651"">this answer</a>.</em></p>
<p><a href=""https://platform.openai.com/docs/models/gpt-3"" rel=""noreferrer"">GPT-3 models</a>:</p>
<p><a href=""https://i.stack.imgur.com/RExzL.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/RExzL.png"" alt=""Table"" /></a></p>
",2023-02-09 10:21:27,3.0,15.0
75349226,1,9610309.0,75418423.0,How to avoid word limit in OpenAI API in R?,register link a key openai api use package r set environment use function output miss word example do not complete text solution similar question reddit edit try increase process time function chat finish type content without stop midway like editors note prior march st  no official chatgpt api,2023-02-05 00:01:42,,2023-03-01 22:29:22,2023-03-01 22:29:22,<r><chatbot><openai-api><gpt-3>,2,3,0,1661,,2.0,8949058.0,"<p>What you are running into is prompt engineering. GPT is a sophisticated autocomplete engine.</p>
<p>If you are not getting the right response you need to recraft your prompt.</p>
<p>You can always test your prompts in the OpenAI playground:
<a href=""https://platform.openai.com/playground"" rel=""nofollow noreferrer"">https://platform.openai.com/playground</a></p>
<p>I was able to get a list of ingredients and cooking steps by using the following prompt:</p>
<blockquote>
<p>How can I make a strawberry pie to donate to my wife? Please provide first a numbered list of ingredients, and secondly a numbered lists of steps.</p>
</blockquote>
<p>Here is the output I got in the playground:</p>
<pre><code>Ingredients:
1. 2 ½ cups of fresh or frozen strawberries
2. 1 9-inch pre-made pie crust
3. ¾ cup of granulated sugar
4. 2 tablespoons of cornstarch
5. ¼ teaspoon of salt
6. 1 tablespoon of fresh lemon juice

Steps:
 1. Preheat oven to 425 degrees F.
 2. Place the pre-made pie crust in a 9-inch pie dish and set aside.
 3. In a medium bowl, combine the strawberries, sugar, cornstarch, salt, and lemon juice. Stir until the mixture is combined.
 4. Pour the strawberry mixture into the pre-made pie crust.
 5. Place the pie dish on a baking sheet and bake for 15 minutes.
 6. Reduce the oven temperature to 375 degrees F and bake for an additional 25 minutes.
 7. Allow the pie to cool completely before serving.
</code></pre>
<p>Another thing to note, per the Github repo for the chatgpt R library it says &quot;The {chatgpt} R package provides a set of features to assist in R coding.&quot;</p>
<p>Ref: <a href=""https://github.com/jcrodriguez1989/chatgpt"" rel=""nofollow noreferrer"">https://github.com/jcrodriguez1989/chatgpt</a></p>
<p>I would use the OpenAI APIs directly, this way you will have a lot more control over your response. I am not an R specialist, but this is how the OpenAI Playground showed me how to do it.</p>
<pre><code>library(httr)

response &lt;- GET(&quot;https://api.openai.com/v1/completions&quot;, 
   query = list(
      prompt = &quot;How can I make a strawberry pie to donate to my wife? Please provide first a numbered list of ingredients, and secondly a numbered lists of steps.&quot;,
      max_tokens = 200,
      model = 'text-davinci-003'
   ),
   add_headers(Authorization = &quot;bearer YOUR_OPENAI_API_KEY&quot;)
)

content(response)
</code></pre>
<p>Ref: OpenAI playground</p>
",2023-02-11 06:13:05,4.0,4.0
74717631,1,9224324.0,,how to finetune gpt-2 in hugging-face's pytorch transformer library,want finetune gpt link work correctly google colab run lab gpu encounter follow error x torchaddmm selfbias xview  xsize  selfweight runtimeerror cuda error cublas_status_alloc_failed call cublascreate handle confuse help thank,2022-12-07 13:43:18,,,2022-12-07 13:43:18,<gpt-2>,0,0,0,104,,,,,,,
74721925,1,20717038.0,,Problems getting GPT-3 to conduct statistical analysis of a dataframe or json file,have problems get gpt simple statistical summaries a dataframejson file use python pandas suggest prompt oddly do categorical analysis df value_counts hallucinate face number sure just error not sure way feed df cheer try use different versions file different ways call no effect change engines,2022-12-07 19:19:50,,2023-01-06 10:12:35,2023-01-06 10:12:35,<javascript><python><pandas><openai-api><gpt-3>,0,3,3,562,,,,,,,
74860930,1,20821790.0,,Is it possible to use continue in the Open AI API?,test use open ai api endpoint website simplely use continue ask ai answer context use continue mulittime expand max token limit possible use continue api effect try use field api not work,2022-12-20 09:28:32,,2022-12-26 02:25:15,2023-02-11 02:19:48,<openai-api><gpt-3>,1,1,0,1799,,,,,,,
74907860,1,1101221.0,,torch parameter is going to nan when after first optim.step() while doing GPT's downstream task,foundation do finetuning gptjforcausallm pretrained b fp environment ubuntu  nvidiadocker cuda docker version  rtx  g vram torch__version__  transformers__version__  jupyter lab version  python version python  pretrained model kakaokogpt b fp problem freeze layer parameters do not freeze parameters go step make question error internet search result help bug reproduction code information work really want use downstream do time code,2022-12-24 13:11:27,,2022-12-24 16:15:36,2022-12-24 16:15:36,<python><pytorch><huggingface-transformers><gpt-3>,0,0,0,152,,,,,,,
74915677,1,20853939.0,,Encountering an error while making an essay app with gpt 3,make a essay app get error indexhtml  post  code essay generator essay generator topic word count generate essay try build essay generator,2022-12-25 20:15:54,,,2023-06-23 12:51:09,<gpt-3>,0,2,0,50,,,,,,,
75567331,1,21095790.0,75571367.0,"OpenAI GPT-3 API error: ""You must provide a model parameter""",try post a question openai api swift work fine use payload postman xcodecondole get follow response openai code print http request fine try use payload postman request work fine try use different encode throw error not sure do wrong maybe help thank advance bet tobi,2023-02-25 17:52:50,,2023-03-13 14:47:19,2023-03-13 14:47:19,<swift><openai-api><gpt-3>,1,1,1,1806,,2.0,10347145.0,"<p>Your HTTP request reveals the problem. You need to add <code>'Content-Type: application/json'</code>.</p>
<p>According to <a href=""https://www.geeksforgeeks.org/what-is-the-correct-json-content-type/"" rel=""nofollow noreferrer"">GeeksforGeeks</a>:</p>
<blockquote>
<p><code>Content-Type</code> is an HTTP header that is used to indicate the media type
of the resource and in the case of responses, it tells the browser
about what actually content type of the returned content is.</p>
</blockquote>
",2023-02-26 10:31:57,0.0,1.0
75710776,1,429476.0,75712209.0,Huggingface GPT2 loss understanding,post get stick understand gpt loss want model label have target generate loss zero a input text current model predict word loss never zero label input_text simulate give label welcome new york city internal neural net irrespective model a loss zero near explain mean snippet note read forum document label input text model shift leave label loss not calculate token loss zero not output try proper a loss about  generate  token update base answer jindfitch put moderators delted try add answer train gpt particular text train  layer freeze take model loss lowest use test sure loss lower like follow train code note train small text way not really fully sure proper train loss jump a bite increase epochs case epoch  train script train ouput log train data space end eval script remove token correspond city inputids give model generate eval script output,2023-03-12 02:34:07,,2023-03-31 14:26:44,2023-03-31 14:26:44,<pytorch><huggingface-transformers><gpt-2>,1,0,1,1148,,2.0,5652313.0,"<p>The default loss function is negative log-likelihood. The actual model output is not the token <code>City</code> but a categorical distribution over the entire 50k vocabulary. Depending on the generation strategy, you either sample from these distributions or take the most probable token.</p>
<p>The token <code>City</code>, apparently the most probable one, gets some probability, and the loss is then minus the logarithm of this probability. Loss close to zero would mean the token would get a probability close to one. However, the token distribution also considers many plausible but less likely follow-ups. Loss 3.26 corresponds to the probability of <code>exp(-3.26)</code>, approximately 3.8%. It seems small, but in a 50k vocabulary, it is approximately 2000 times more probable than a random guess.</p>
<p>You can try to fine-tune the model to be absolutely sure that <code>City</code> will follow with 100% probability, but it would probably break other language modeling capabilities.</p>
",2023-03-12 09:32:59,1.0,4.0
70069026,1,3408256.0,70157536.0,How to use files in the Answer api of OpenAI,finally openai open gpt relate api publicly play explore discover potential try answer api simple example documentation upload file indicate succesfully upload api try use unfortunately error wait hours not think content deserve a long wait know a normal behaviour miss thank,2021-11-22 16:19:49,,2023-01-15 17:50:20,2023-01-15 17:50:20,<python><openai-api><gpt-3>,1,0,2,4247,0.0,2.0,3408256.0,"<p>After a few hours (the day after) the file metadata status changed from <code>uploaded</code> to <code>processed</code> and the file could be used in the Answer API as stated in the documentation.</p>
<p>I think this need to be better documented in the original OpenAI API reference.</p>
",2021-11-29 15:55:58,0.0,2.0
70408322,1,13222954.0,70422274.0,OpenAI GPT3 Search API not working locally,use python client gpt  search model jsonlines file run code google colab notebook test purpose work fine return search responses run code local machine mac m a web application run localhost use flask web service functionalities give follow error error occur implement exact example give openai documentation link search example give run perfectly fine local machine colab notebook use completion api use gpt playground code link code follow idea strange behaviour occur solve thank,2021-12-19 00:56:26,,,2021-12-20 13:05:32,<python><openai-api><gpt-3>,1,0,2,1902,,2.0,13222954.0,"<p>The problem was on this line:</p>
<p><code>file = openai.File.create(file=open(jsonFileName), purpose=&quot;search&quot;)</code></p>
<p>It returns the call with a file ID and status uploaded which makes it seem like the upload and file processing is complete. I then passed that fileID to the search API, but in reality it had not completed processing and so the search API threw the error <code>openai.error.InvalidRequestError: File is still processing.  Check back later.</code></p>
<p>The returned file object looks like this (misleading):</p>
<p><a href=""https://i.stack.imgur.com/6xDNh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/6xDNh.png"" alt=""enter image description here"" /></a></p>
<p>It worked in google colab because the openai.File.create call and the search call were in 2 different cells, which gave it the time to finish processing as I executed the cells one by one. If I write all of the same code in one cell, it gave me the same error there.</p>
<p>So, I had to introduce a wait time for 4-7 seconds depending on the size of your data, <code>time.sleep(5)</code> after openai.File.create call before calling the openai.Engine(&quot;davinci&quot;).search call and that solved the issue. :)</p>
",2021-12-20 13:05:32,0.0,1.0
74924661,1,6843825.0,,"OpenAI GPT-3 API error: ""TypeError: openai.completions is not a function""",try run test code tutorial follow code myjs run node myjs git bash window windows  try various alternate code snippets openai docs suggest question not able work,2022-12-26 23:24:29,,2023-03-13 13:18:13,2023-03-13 13:18:13,<typescript><next.js><openai-api><gpt-3>,1,3,3,2434,,,,,,,
74981741,1,657477.0,,GPT-3 codex doc sample for explanations doesn't provide explanation,docs example try copy playground codedavinci just spew query chatgptdavinci work fine actual codex handle sample docs codedavinci capable write explanations code,2023-01-02 11:16:13,,2023-01-18 21:36:16,2023-01-18 21:36:16,<sql><nlp><openai-api><gpt-3>,0,1,0,62,,,,,,,
74988265,1,294260.0,,Multiple Requests until it's complete?,gpt project pull basic json a body text use nodejs playground completion work great press submit button finish least  time guess token limitations request nodejs use prompt work playground require multiple submit equivalent way a programmatic nodejs environment just submit request like maybe partial request maybe stringifying concat completion a valid json object a recommend way make multiple request track a completion use openai gpt,2023-01-03 01:03:14,,2023-01-03 02:11:44,2023-01-03 02:11:44,<node.js><gpt-3>,0,0,0,48,,,,,,,
74988365,1,20915380.0,,"OpenAI GPT-3 API error: ""AttributeError: module 'openai' has no attribute 'GPT'""",latest version openai attribute miss try reinstall do not solve gpt chat ones discover not work bare mind new python basic knowledge language code take github code tell help appreciate edit answer error away a new thank work do not work error try resolve appreciate help tip probably not error encounter,2023-01-03 01:29:18,,2023-03-13 13:20:06,2023-03-13 13:20:06,<python><module><attributeerror><openai-api><gpt-3>,1,0,0,1708,,,,,,,
74996136,1,20920901.0,,extend dialogflow webhook deadline time for gpt api call,try use a script internet extend maximum time a webhook request google dialogflow max  second timeout need extend time make api openai take longer  second idea start  function parallel broadbridge_webhook_results function extend time trigger a followupeventinput dialogflow  second a new come dialogflow  second start new go apparently  time meantime api openai soon api successful answer send dialogflow unfortunately currently not get think thread functionality set understand incorrectly follow code far,2023-01-03 16:36:55,,,2023-01-25 13:56:24,<python><dialogflow-es><gpt-3>,1,0,0,148,,,,,,,
75022045,1,1303952.0,,ML.Net : How to define Shape Vectors for GPT2 ONNX model?,try use mlnet consume onnx gpt model stick define shape dicionary follow input output model properties extract netron correct value shape dictionary represente input tryied totally guess pass shape dictionary applyonnxmodel article thank,2023-01-05 17:11:15,,,2023-01-05 17:11:15,<onnx><ml.net><gpt-2>,0,0,0,172,,,,,,,
75050453,1,19119619.0,,TFGPT2LMHeadModel to TFLite changes the input and output shape,tfgptlmheadmodel convertion tflite render unexpected input output shape oppoed pre train model gpttflite fix give output expect try convert tfgptlmheadmodel tflite different output output fix,2023-01-08 18:50:28,,2023-01-08 18:52:34,2023-01-08 18:52:34,<tensorflow><huggingface-transformers><huggingface><tflite><gpt-2>,0,0,0,151,,,,,,,
75127878,1,15811628.0,,Python string column iteration,work openai stick try sort issue do not resolution want code run sentence generation operation row input_description_oai column output column openai_description help completion task new python dataset look like get error execution row,2023-01-15 19:33:53,,2023-01-16 12:24:23,2023-01-16 12:24:23,<python><pandas><deep-learning><data-science><gpt-3>,0,0,0,75,,,,,,,
75198769,1,12906445.0,,fine tuning GPT2 on Colab gives error: Your session crashed after using all available RAM,new ml try create a ml model fine tune gpt dataset preprocessed actually try run code fine tune gpt colab say session crash use available ram colab pro version ram size gb file size about mb try lower train step batch size error happen idea stop behavior,2023-01-22 07:12:35,,,2023-01-22 07:12:35,<python><gpt-2>,0,0,0,84,,,,,,,
75210265,1,11932718.0,,OpenAI Prompt in Python,just want sure write prompt correctly a text include a journal title abstract keyword information want extract name mlai methods usedproposed problem code snippet test_text text read a txt file use mention best practice sample result get follow bound box algorithm faster rcnn artificial intelligence machine learn python try change structure prompt example instead name use try name propose mean sentence remain result irrelevant result return think above prompt correct extract aiml methods use journal update prompt accurate robust result addition think “ ” “ ” usages correct thank,2023-01-23 13:35:15,,,2023-01-23 13:35:15,<python><machine-learning><artificial-intelligence><openai-api><gpt-3>,0,0,0,418,,,,,,,
76001873,1,20920040.0,76001924.0,Invalid URL (POST /v1/chat/completions) error in Python,a tts python program interpret speechtotext data ask prompt gpt davinci api answer just switch gpt  turbo do not work invalid url post vchatcompletions error try check model endpoint compatibility web page try ask gpt gpt didn ’ t answer check reddit didn ’ t check stack overflow didn ’ t api endpoint url least try current gpt turbo engine code,2023-04-13 05:18:25,,2023-04-16 11:26:53,2023-04-16 11:28:19,<python><gpt-3><chatgpt-api>,1,2,-1,165,,2.0,15446995.0,"<p><a href=""https://platform.openai.com/docs/api-reference/engines"" rel=""nofollow noreferrer"">Using <em>engines</em> is deprecated</a>. Use <em>model</em> instead.</p>
<pre><code>response = openai.ChatCompletion.create(
    model=&quot;gpt-3.5-turbo&quot;,
    messages=messages,
    max_tokens=1000,
    n=1,
    temperature=0.3,
)
</code></pre>
",2023-04-13 05:26:33,0.0,0.0
76160057,1,21801792.0,76161653.0,OpenAI GPT-3.5 and GPT-4 API: How do I customize answers from a GPT-3.5 or GPT-4 model if I can't fine-tune them?,see company use gpt gpt model train data provide customize answer gpt gpt model not available finetuning see document openai about issue see openai allow finetuning example customize answer a gpt gpt model not finetune,2023-05-03 02:27:37,,2023-05-05 21:10:26,2023-05-05 21:10:26,<openai-api><chatgpt-api><fine-tune><gpt-4>,1,1,2,855,,2.0,10347145.0,"<p><strong>They don't fine-tune GPT-3.5 or GPT-4 models.</strong></p>
<p>What they do is use <a href=""https://gpt-index.readthedocs.io/en/stable/"" rel=""nofollow noreferrer"">LlamaIndex</a> (formerly GPT-Index) or <a href=""https://python.langchain.com/en/latest/index.html"" rel=""nofollow noreferrer"">LangChain</a>. Both of them enable you to connect OpenAI models with your existing data sources.</p>
",2023-05-03 08:02:41,1.0,2.0
76138660,1,17945984.0,76187971.0,problem with running OpenAI Cookbook's chatbot,have trouble run chatbot app openai cookbook repository try instal necessary package pip install r requirementstxt env file openai api key insert code chatbotpy line  setup above guess doc totally unclear about set run app local command streamlit run appschatbotkickstarterchatpy do not work properly app run enter text press ubmit button app get error use python  appreciate help guidance resolve issue,2023-04-29 22:05:30,,2023-05-02 12:22:49,2023-05-06 08:47:43,<python><streamlit><openai-api><gpt-3><chatgpt-api>,2,5,-1,102,,2.0,17945984.0,"<p>Putting the key directly in chatbot.py just worked. It shouldn't be taken from environment variables.</p>
",2023-05-06 08:47:43,0.0,0.0
75444023,1,16920990.0,,Max prompt token not working when using GPT-3 text-davinci-003,integrate gpt textdavinci algorithm node js work good pass token time different time give reply gpt textdavinci solution pass  token reply max token  like want large token reply,2023-02-14 06:05:04,,2023-02-14 09:27:49,2023-02-24 19:19:33,<javascript><node.js><typescript><openai-api><gpt-3>,1,0,2,1347,,,,,,,
75497540,1,2317643.0,,Does Visual Studio Code support Environment Variables for the ChatGPT extension?,like open editor able use chatgpt extension without need log browser store openai api key a workspace file ideally able use environment variable store know visual studio code allow environment variables set not not work file,2023-02-19 01:55:50,,,2023-02-19 08:59:43,<security><visual-studio-code><openai-api><gpt-3>,1,0,0,241,,,,,,,
75536615,1,5239482.0,,nanoGPT with custom dataset,try use nanogpt custom input file post issue repo issue  not get response lookin advice stackoverflow follow step describe shakespere input file try a custom input file contain multiple paragraph different head file content look like contain  paragraph preparepy trainpy file execute successfully input file try generate sample output incorrect english like input dataset format correct specific need,2023-02-22 18:12:11,,,2023-02-22 18:12:11,<gpt-2>,0,0,0,369,,,,,,,
75547772,1,9720696.0,,Recovering input IDs from input embeddings using GPT-2,suppose follow text want use gpt produce input_ids produce embed embeddings recover input_ids input_ids define decode make sure reproduce aim expect produce embed convert input_ids tensor procude embeddings now mention earlier aim use input_embeddings recover input_ids do level wonder do wrong recover input_ids use embed produce,2023-02-23 16:41:17,,,2023-02-23 22:54:04,<python><pytorch><huggingface-transformers><gpt-2>,1,0,1,348,,,,,,,
67221901,1,3837352.0,,Mismatched tensor size error when generating text with beam_search (huggingface library),use huggingface library generate text use pretrained distilgpt model particular make use beam_search function like include a logitsprocessorlist not use generate function relevant portion code look like beam_search function throw error try generate use a max_length less  not figure  come input length longer shorter use a different batch size number beam nothing define length  nor default max length sequence do effect result beam search like figure able set a shorter max length,2021-04-22 23:09:25,,,2021-04-23 10:29:16,<python><nlp><pytorch><huggingface-transformers><gpt-2>,1,0,2,744,,,,,,,
67930127,1,15847023.0,,No module named 'tensorflow.contrib' even on tensorflow 1.13.2,import a gpt__simple package error instal python  try install tensorflow    gain mistake im use windows,2021-06-11 01:06:55,,,2021-06-11 18:48:37,<tensorflow><gpt-2>,1,0,1,214,,,,,,,
67959077,1,16215008.0,,How to get onnx format from pretrained GPT2 models?,try transform kogpt model pretrained gpt onnx format order change model tensorflow format use do not work reason not know error imply possible make onnx format model code implement error thank,2021-06-13 14:01:15,,,2021-11-14 09:40:24,<python><tensorflow><pytorch><onnx><gpt-2>,1,0,2,781,,,,,,,
68233646,1,6272434.0,,ValueError when trying to fine-tune GPT-2 model in TensorFlow,encounter a python code try finetune hug face distribution gpt model specifically  text file concatenate a string variable call pass follow function create train test tensorflow datasets try train model call trigger variables allcaps settings pull a json file currently set information regard error mean ideas resolve greatly appreciate thank,2021-07-03 06:02:01,,,2021-07-29 19:55:56,<tensorflow><huggingface-transformers><transformer-model><pre-trained-model><gpt-2>,1,0,0,791,0.0,,,,,,
75084272,1,16461166.0,76444852.0,How to keep the format of the OpenAI API response when using the OpenAI GPT-3 API?,use gpt playground result format number list paragraph like directly use api extract response json result cram text version hard read like question people format gpt result display a neater readable way,2023-01-11 14:02:46,,2023-04-11 21:55:42,2023-06-10 05:31:51,<openai-api><gpt-3>,3,2,2,3189,,2.0,16461166.0,"<p>The problem was on my side's frontend. The openAI API was returning the correct response, and I was rending the result with the wrong whitespace CSS settings</p>
",2023-06-10 05:31:51,0.0,1.0
71215965,1,10853823.0,71227037.0,How to save checkpoints for thie transformer gpt2 to continue training?,retrain gpt language model follow blog train a network gpt try recreate a dataset large mb want continue train intervals word want checkpoint model train help a piece code implement checkpoint continue train help a great deal thank,2022-02-22 04:36:01,,,2022-02-22 19:10:58,<tensorflow><nlp><gpt-2>,1,0,0,570,,2.0,4762466.0,"<pre><code>training_args = TrainingArguments(
    output_dir=model_checkpoint,
    # other hyper-params
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_set,
    eval_dataset=dev_set,
    tokenizer=tokenizer
)

trainer.train()
# Save the model to model_dir
trainer.save_model()

def prepare_model(tokenizer, model_name_path):
    model = AutoModelForCausalLM.from_pretrained(model_name_path)
    model.resize_token_embeddings(len(tokenizer))
    return model

# Assume tokenizer is defined, You can simply pass the saved model directory path.
model = prepare_model(tokenizer, model_checkpoint)
</code></pre>
",2022-02-22 19:10:58,0.0,1.0
70834489,1,17067836.0,70881747.0,Choosing a good prompt for GPT-3,try generate a quiz a text look like work perfectly not keywords multiple prompt answer options correct answer problem want generate question correct answer keyword try kinds prompt give examples like do above do not work,2022-01-24 13:16:53,2022-12-13 15:42:12,2022-12-13 15:42:17,2022-12-13 15:42:17,<nlp><openai-api><gpt-3>,1,0,0,570,,2.0,17132650.0,"<p>Using GPT-J I tested the following:</p>
<p>Input:</p>
<pre><code>Create a quiz about the following text according to the keywords: Mary has little lamb and John has a cow. The lamb is one month old. It eats grass and milk, which Mary brings from the the farm.

Keywords: &quot;lamb&quot;, &quot;cow&quot;, &quot;one month old&quot;, &quot;farm.&quot;

Question 1: What does Mary have?
A. Lamb
B. Cow
C. Dog
D. Cat

Answer: A. Lamb

Question 2:
</code></pre>
<p>Output:</p>
<pre><code>Create a quiz about the following text according to the keywords: Mary has little lamb and John has a cow. The lamb is one month old. It eats grass and milk, which Mary brings from the the farm.

Keywords: &quot;lamb&quot;, &quot;cow&quot;, &quot;one month old&quot;, &quot;farm.&quot;

Question 1: What does Mary have?
A. Lamb
B. Cow
C. Dog
D. Cat

Answer: A. Lamb

Question 2: What does John have?
A. Lamb
B. Cow
C. Dog
D. Cat

Answer: B. Cow

Question 3: How old is the lamb?
A. 1 month
B. 1 year
C. 2 months
D. 1 year and a half

Answer: C. 2 months
</code></pre>
<p>Giving an example of what you want in the input (1 question + answer) can help GPT to understand the structure of the desired output. Explicitly explaining the GPT what the desired task is in the input (see line 1) can help it to understand the task in my experience.</p>
<p>It does not execute the task perfectly, but using GPT-3 might help and I hope this is a step in the right direction for you.</p>
",2022-01-27 16:00:13,0.0,1.0
68312300,1,16412367.0,,How to push trained NLP model to huggingface.co via git-lfs?,use windows wrong pas error message text screenshot,2021-07-09 06:39:58,,2021-07-09 07:17:53,2021-07-09 07:17:53,<python><git><nlp><huggingface-transformers><gpt-2>,0,3,0,378,0.0,,,,,,
68729645,1,10836319.0,,How to get the language modeling loss by passing 'labels' while using ONNX inference session?,use gpt simply pass label parameter loss follow not able loss onnx inference session use code return last_hidden_state,2021-08-10 15:30:00,,2021-08-13 18:48:01,2021-08-13 18:48:01,<pytorch><huggingface-transformers><language-model><onnxruntime><gpt-2>,1,0,0,446,,,,,,,
69928517,1,17200786.0,,Encoding issues on OpenAI predictions after fine-tuning,follow openai tutorial about finetuning generate dataset openai tool problem output encode inference result mix utf non utf character generate model look like instance ask ¿ cómo estás a train completion sentence estoy bien ¿ y tú inference return exactly good add nonencoded word estoy bien ¿ y tú cuã © ntame algo ti add ã © instead é return exactly sentence train no encode issue not know inference take nonencoded character model encode dataset utf leave dataset utf decode bad encode char response openai docs finetuning not include about encode,2021-11-11 12:44:06,,,2021-12-09 00:36:53,<utf-8><character-encoding><openai-api><gpt-3><fine-tune>,1,0,3,2259,,,,,,,
70784728,1,17067836.0,70802282.0,GPT-3 question answering based on keywords,currently get accustom gpt try generate question a text inputting keywords text ideally answer question try input text simply write just enumerate word input question examples obviously not use structure wonder possible like,2022-01-20 10:40:50,,,2022-01-21 13:43:27,<nlp><openai-api><gpt-3>,1,0,1,594,,2.0,17132650.0,"<p>I am currently working on something similar (but I'm using GPT-J). One thing I find helpful is describing in the input what you want GPT to do.</p>
<p>e.g.: I want to generate a sentence containing all given key-words:</p>
<p>input:</p>
<pre><code>Generate a sentence which contains all of the keywords.
Keywords: dog, cat, mouse
Sentence: 
</code></pre>
<p>output:</p>
<pre><code>Generate a sentence which contains all of the keywords.
Keywords: dog, cat, mouse
Sentence: I have a dog, a cat and a mouse.
</code></pre>
<p>Let me know if this helps you, or if you find any other solutions which might help me as well!</p>
",2022-01-21 13:43:27,0.0,2.0
63626014,1,13624094.0,63636417.0,How to Get Rid of GPT-2 Warning Message?,time run gpt receive message a way away,2020-08-28 00:57:18,,2020-11-29 11:52:40,2020-11-29 11:52:40,<python><huggingface-transformers><gpt-2>,1,1,2,895,,2.0,6664872.0,"<p>Yes you need to change the <a href=""https://docs.python.org/2/library/logging.html#levels"" rel=""nofollow noreferrer"">loglevel</a> <strong>before you import anything</strong> from the transformers library:</p>
<pre class=""lang-py prettyprint-override""><code>import logging
logging.basicConfig(level='ERROR')

from transformers import GPT2LMHeadModel

model = GPT2LMHeadModel.from_pretrained('gpt2')
</code></pre>
",2020-08-28 15:06:19,0.0,2.0
76500504,1,7037351.0,76502140.0,What is the cause of HFValidationError in this code and how do I resolve this error?,python code chaquopy android studio project now show follow error file m gpt model checkpoint encoderjson hparamsjson modelckptdataof modelckptindex modelckptmeta vocabbpe file inside assets folder,2023-06-18 12:20:29,,2023-06-19 07:05:19,2023-06-19 07:05:19,<android-studio><python-3.8><chaquopy><gpt-2>,1,0,1,42,,2.0,220765.0,"<p>The <a href=""https://huggingface.co/docs/transformers/v4.30.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained"" rel=""nofollow noreferrer""><code>from_pretrained</code></a> documentation is not entirely clear about how it distinguishes huggingface repository names from local paths, although all the local path examples end with a slash. In any case, when loading data files with Chaquopy, you must always use absolute paths, as it says in <a href=""https://chaquo.com/chaquopy/doc/current/android.html#android-data"" rel=""nofollow noreferrer"">the FAQ</a>.</p>
<p>So assuming your &quot;assets&quot; directory is at the same level as the Python code, you can do this:</p>
<pre><code>from os.path import dirname
tokenizer = GPT2Tokenizer.from_pretrained(f'{dirname(__file__)}/assets/')
</code></pre>
",2023-06-18 19:08:47,1.0,1.0
70672460,1,8613875.0,70728130.0,Hugging face - Efficient tokenization of unknown token in GPT2,try train a dialog use gpt tokenization use follow configuration add special tokens try tokenize a sequence dialog utterance later convert ids important tokens sequence get map unknown tokens ids important tokens bos eos map gpt source code a work example important_tokens map id   endoftext  model fail learn important tokens generate poor hallucinate responses a quick efficient fix issue,2022-01-11 19:35:14,,,2023-06-08 21:05:57,<python><nlp><huggingface-transformers><huggingface-tokenizers><gpt-2>,1,0,2,2802,,2.0,8704180.0,"<p>For the important_tokens which contain several actual words (like <code>frankie_and_bennys</code>), you can replace <code>underscore</code> with the <code>space</code> and feed them normally, Or add them as a special token. I prefer the first option because this way you can use pre-trained embedding for their subtokens. For the ones which aren't actual words (like <code>cb17dy</code>), you must add them as special tokens.</p>
<pre><code>from transformers import GPT2TokenizerFast
tokenizer = GPT2TokenizerFast.from_pretrained(&quot;gpt2&quot;)
your_string = '[PRED] name [SUB] frankie and bennys frankie_and_bennys [PRED]  cb17dy'
SPECIAL_TOKENS = {
    &quot;bos_token&quot;: &quot;&lt;|endoftext|&gt;&quot;,
    &quot;eos_token&quot;: &quot;&lt;|endoftext|&gt;&quot;,
    &quot;pad_token&quot;: &quot;[PAD]&quot;,
    &quot;additional_special_tokens&quot;: [&quot;[SYS]&quot;, &quot;[USR]&quot;, &quot;[KG]&quot;, &quot;[SUB]&quot;, &quot;[PRED]&quot;, &quot;[OBJ]&quot;, &quot;[TRIPLE]&quot;, &quot;[SEP]&quot;, &quot;[Q]&quot;,&quot;[DOM]&quot;, 'frankie_and_bennys', 'cb17dy']
}
tokenizer.add_special_tokens(SPECIAL_TOKENS)
print(tokenizer(your_string)['input_ids'])
print(tokenizer.convert_ids_to_tokens(tokenizer(your_string)['input_ids']))
</code></pre>
<p>the output</p>
<pre><code>[50262, 1438, 220, 50261, 14346, 494, 290, 275, 1697, 893, 220, 50268, 220, 50262, 220, 220, 50269]
['[PRED]', 'Ġname', 'Ġ', '[SUB]', 'Ġfrank', 'ie', 'Ġand', 'Ġb', 'enn', 'ys', 'Ġ', 'frankie_and_bennys', 'Ġ', '[PRED]', 'Ġ', 'Ġ', 'cb17dy']
</code></pre>
",2022-01-16 07:28:04,0.0,3.0
75256485,1,3111375.0,,Training / using OpenAI GPT-3 for translations,try use openai translation products descriptions language languages en cz sk hu pl si translations especially skczhupl languages mainly gramatically quite bad use model get idea a thousands similar products fully translate languages professional translators possible use exist correct translations train gpt use model translate new texts anybody try similar,2023-01-27 09:56:11,,2023-01-27 18:35:54,2023-02-15 12:19:52,<nlp><translate><openai-api><machine-translation><gpt-3>,0,4,1,347,,,,,,,
75266549,1,257493.0,,Fine-tune a davinci model to be similar to InstructGPT,a fewshot gpt textdavinci prompt produce pretty good result quickly run tokens request interest use case a data set n like train model no way finetune instructgpt model base gpt model understand a a way harvest x data not easy option b a way finetune davinci capable simpler instructgpt behaviours let know a option attempt increase epochs   quality really nowhere near good way finetune davinci point model things instruct do not need capabilities make narrow use case ideal way a common misconception finetuning a gpt model a base davinci ada babbage train latest textdavinci not gpt work explain gpt blog post support post not claim create a model base textdavinci not true use base davinci,2023-01-28 09:09:22,,,2023-04-09 01:15:38,<gpt-3><fine-tune>,2,1,2,1491,,,,,,,
75348532,1,4352114.0,,"How to use the OpenAI stream=true property with a Django Rest Framework response, and still save the content returned?",try use streamtrue property follow unfortunately not know return react frontend typically use standard response object set a status serializerdata data read online use not sure integrate iterator object actually save output data stream view end return iterator endpoint help,2023-02-04 21:34:09,,,2023-04-02 11:24:59,<reactjs><django><gpt-3>,2,0,4,981,,,,,,,
75559316,1,16961408.0,,Using sliding windows to evaluate sentiment for text groupings,try create a slide window evaluate sentiment group utterances a conversation goal evaluate sentiment a single utterance a conversational group evaluate a group sentiment base statement item  above add a new utterance string predictor utterance conversation predictor evaluate previous string context new string note individual additive statement step receive a sentiment score repeat item   add a new utterance string data evaluate new rd string utterance get evaluate evaluate context previous  utterances now utterances evaluate addition individual newly add string example transform classifications integer value simply come mean entire statement group a final conversation sentiment classification list utterances routine unfortunately return not correct apparently not layer utterances way describe above use debug print see appear logic evaluate statements transform classifications integer value simply come mean entire statement group a final conversation sentiment classification thoughts assistance greatly appreciate,2023-02-24 16:54:29,,,2023-02-27 14:45:59,<python><data-science><openai-api><gpt-3>,1,0,0,39,,,,,,,
73411215,1,19739568.0,,Getting logits from T5 Hugging Face model using forward() method without labels,use case need obtain logits t forward method without inputting label know forward generate different see post logits obtain label generate possible obtain logits forward method without inputting label,2022-08-19 02:15:34,,,2022-08-20 09:37:09,<nlp><huggingface-transformers><bert-language-model><gpt-2>,1,0,2,540,,,,,,,
75621041,1,20784837.0,,How can i migrate from text-davinci-003 model to gpt-3.5-turbo model using OpenAI API?,try change code able use new openai model application stop work bold part code change work use textdavinci model bold part code change arent work gptturbo model change part bold now do nto work application stop,2023-03-02 21:35:36,,2023-03-04 20:16:40,2023-03-04 20:16:40,<android><kotlin><openai-api><gpt-3><chatgpt-api>,0,2,2,975,,,,,,,
75650841,1,15800270.0,,How to train or fine-tune GPT-2 / GPT-J model for generative question answering?,new use huggingface model basic understand model tokenizers train look a way leverage generative model like gpt gptj huggingface community tune question close generative question answer train model specific domain data medical ask question relate possible walk process thank 🤗,2023-03-06 12:27:08,,2023-03-06 14:23:03,2023-04-23 16:02:17,<python><machine-learning><nlp><huggingface-transformers><gpt-2>,1,1,0,573,,,,,,,
75680684,1,3563517.0,,how to read tokens from huggingface GPT2 tflite model using Interpreter,recently convert a pre train gpt model tflite try use interpreter generate text prompt code do follow convert pretrained model tflite work fine create interpreter save tflite model work fine use tokenizer generate tokens set tensors input invoke interpreter extract output detail now accurately convert output tokens input tokenizer decode method,2023-03-09 05:17:22,,2023-03-23 11:36:33,2023-03-23 11:36:33,<huggingface-transformers><tensorflow-lite><transformer-model><tflite><gpt-2>,0,0,0,141,,,,,,,
73467393,1,,,gpt3 fine tuning with openai not learning,fine tune jsonl file want a model predict gender speaker give a statement instance prompt go buy a skirt today completion female create examples give gpt finetune feed sentence go pick wife shop result model expect a gender response get a story about pick wife shop gpt do not learn fine tune a question fine tune equivalent write a examples openai playground get gpt guess come fine tune pay tokens promptcompletion subsequent run spend  train a model a million examples pay individual promptcompletion subsequent call chat bot instance come a context sentence forth exchange  chat participants like a conversation a rude man name john a young girl name sarah incorporate context fine tune structure prompt completion,2022-08-24 04:20:47,,2022-12-13 15:35:02,2022-12-13 15:35:02,<python-3.x><nlp><openai-api><gpt-3>,1,0,2,805,,,,,,,
72199570,1,5851623.0,,Fine tuning GPT2 for generative question anwering,try finetune gpt a generative question answer task basically data a format similar context matt wreck car today question matt day answer bad look huggingface documentation finetune gpt a custom dataset do instructions finetuning address issue not provide guidance data prepare model learn different datasets available none a format fit task really appreciate experience help a nice day,2022-05-11 10:38:19,,,2023-06-20 10:10:16,<machine-learning><gpt-2>,2,0,3,3785,,,,,,,
72479175,1,14735451.0,,How to force GPT2 to generate specific tokens in each sentence?,input a string output vector representations correspond generate tokens try force output specific tokens  commas word generate sentence a potential loss component force gpt generate specific tokens approach easier robust not sure possible similar mask tokens bert instead force gpt generate sentence unique tokens predefined tokens sentence issue approach not a predefined number tokens generatedmasked nor a predefined number sentence generate give input use bert code,2022-06-02 16:07:42,,,2022-06-05 22:07:46,<machine-learning><pytorch><huggingface-transformers><language-model><gpt-2>,0,0,1,411,,,,,,,
73779456,1,9951.0,,Why do generating text with gpt2 keep increasing memory consumption?,a python script run infinite loop call run cpu not gpu model load spike memory usage ram consumption increase about mo minute nothing loop store no typical python memory leak trap python memory profiler not show a specific culprit wonder not c code search web notice not limit memory consumption cpu gpu tensorflow clue cause,2022-09-19 21:26:28,,,2022-09-19 21:26:28,<python><tensorflow><gpt-2>,0,0,0,91,,,,,,,
73938457,1,18992575.0,,Reproducibility when using best_of in GPT-3 settings,want test use gpt instead set temperature  like use best_of function give nonreproducible result differ iteration code execution do idea achieve reproducibledeterministic result use best_of a way use a random seed api gpt thank,2022-10-03 16:46:24,,,2022-12-05 18:24:37,<random-seed><gpt-3>,1,1,1,893,,,,,,,
73999135,1,5157277.0,,Open AI. Response from openai.Completion.create incomplete,code python like use openaicompletioncreate function completion not complete,2022-10-08 17:28:49,,,2022-10-08 17:28:49,<python><openai-api><gpt-3>,0,2,1,2322,,,,,,,
75389044,1,16415800.0,,How can I correctly implement the OpenAI API in swiftUI using a REST API? closed,currently a rest api use alamofire swiftui request a post method url basically send a question openai api suppose answer json format api key openai include code problem face get input recieve follow error try make sure api key correct try a curl statement work check error url use curl statement work fine give a correct output help code context about code send a post request url get output save array alongside original input input output display a chat do know recieving error do miss a crucial code thank advance btw read comment realise not exactly implement server apoligize waste time,2023-02-08 16:47:12,,2023-03-24 12:50:52,2023-03-24 12:50:52,<curl><swiftui><alamofire><openai-api><gpt-3>,1,5,-2,616,,,,,,,
75404485,1,20994974.0,,GPT3 fine tuned model returns additional questions and answers,fine tune a custom dataset use gpt create a simple program user input a question return correct response program work return additional question answer dataset upload model try reduce max tokens cap set temperature  figure stop program return additional question answer encounter problem fix code,2023-02-09 21:17:01,,2023-02-14 11:53:14,2023-02-14 11:53:14,<openai-api><gpt-3>,1,0,1,615,,,,,,,
71335585,1,16852041.0,71336842.0,"HuggingFace | ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet con",not occasionally run code error appear doubt a connectivity issue cash issue discuss older git issue clear cache do not help runtime traceback reference nltk get  line cache clear code traceback fail attempt close ide bash terminal run powershell relaunched ide bash terminal error disconnect different vpn clear cache,2022-03-03 10:28:24,,2022-03-03 13:51:06,2023-05-15 19:12:21,<python-3.x><tensorflow><huggingface-transformers><valueerror><gpt-2>,4,3,5,9848,,2.0,16852041.0,"<p>Since I am working in a <strong>conda venv</strong> and using <strong>Poetry</strong> for handling dependencies, I needed to re-<strong>install torch</strong> - a dependency for Hugging Face 🤗 Transformers.</p>
<hr />
<p>First, install torch:
<a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">PyTorch's website</a> lets you chose your exact setup/ specification for install. In my case, the command was</p>
<pre class=""lang-bash prettyprint-override""><code>conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch
</code></pre>
<p>Then add to Poetry:</p>
<pre><code>poetry add torch
</code></pre>
<p>Both take ages to process. Runtime was back to normal :)</p>
",2022-03-03 11:59:39,0.0,0.0
71618602,1,12872310.0,71824786.0,Finetuning GPT-3 on Windows?,read documentation finetuning gpt not understand propose cli command not work windows cmd interface not documentation finetune gpt use a regular python script try understand function define package not make sense information miss just not possible finetune gpt a windows machine,2022-03-25 14:28:52,,,2022-04-11 08:29:25,<python><machine-learning><nlp><openai-api><gpt-3>,2,0,1,617,,2.0,12872310.0,"<p>For anybody having a similar problem: We solved the problem by using the anaconda prompt cmd. There everything worked flawlessly.</p>
",2022-04-11 08:29:25,0.0,1.0
73642618,1,589921.0,73853380.0,GPT-3 cannot mix two actions into one prompt (summarisation and tense changing),just a head feel like a weird question ask not really code involve not sure right place ask try summarise a journal entry convert second person past tense go shop go shop follow prompt gpt da vinci params normal give a summary expect a good summary now convert summary second person pov like excellent combine prompt like do not work fact just ignore summarisation prompt just convert passage second person past tense fix,2022-09-08 00:26:15,,,2022-09-26 11:17:53,<gpt-3>,1,0,0,431,,2.0,34170.0,"<p>Try the following format and see if it works good enough for you; I'm using this approach for a whole lot of scenarios to solve the issue you described. Use zero-temperature (unless you want to risk variations).</p>
<p>Our prompt:</p>
<pre><code>Story Text: &quot;We took to the streets of London on the London hire bikes aka Boris Bikes / BoJo Bikes; previously Barclays Bikes and now Santander Bikes – bloomin heck this is complicated.  I knew the direction where I wanted to get to and knew how to get there except I didn’t really.

We started our journey at one of bike hire station in St John’s Wood and continued around Regents Park (the wrong way) Simon got us to one of the gateways to the path along the Regents Canal.  Sometimes they can be quite difficult to find; this was one of those times. This particular one was located at the back of a housing estate; only that Simon knew where it was there was no way I would have found it.

Story Text: &quot;We took to the streets of London on the London hire bikes aka Boris Bikes / BoJo Bikes; previously Barclays Bikes and now Santander Bikes – bloomin heck this is complicated.  I knew the direction where I wanted to get to and knew how to get there except I didn’t really.

We started our journey at one of bike hire station in St John’s Wood and continued around Regents Park (the wrong way) Simon got us to one of the gateways to the path along the Regents Canal.  Sometimes they can be quite difficult to find; this was one of those times. This particular one was located at the back of a housing estate; only that Simon knew where it was there was no way I would have found it.

Off down the canal we went. Sunday afternoons are a busy time along the canal with local people mixed in with tourists from all over the world; so cycling along a narrow path is not easy as everyone walks on different sides of the path (according to where they come from)! We got towards Camden Market and the path got very busy, to the point that I almost went into the canal but with a wibble and a wobble I managed to stay in.  At that point the decision was easily made to get off that bike and walk it. The Santander App showed us where the nearest parking station was and that there was space available to park up.

Coffee time! Forget the major chains, we found a small local place called T &amp; G for some cups of coffee and a sarnie before we went out to find out next bike to get us to Granary Square in Kings Cross for our next stop. From the canal path there is a grassed set of steps going up to the Square but first we parked up the bikes on the other side of the canal. So many places to choose from to hang out, for drinks and for food or trains to Paris, Lille, Edinburgh or Manchester to start off with.&quot;

Following is the Summary of the Story Text (1) and Second Person Past Tense of that Summary (2): 
1) 
</code></pre>
<p>So, by using &quot;Following is the Summary of the Story Text (1) and Second Person Past Tense of that Summary (2): 1) &quot; we're biasing GPT-3 in a simple and syntactically strongly outlined way, and this bias is the very last thing in the prompt; we also help it by already providing the &quot;1) &quot; (but leaving its content empty).</p>
<p>GPT-3's zero-temperature result (model text-davinci-002):</p>
<pre><code> We took the London hire bikes for a ride and ended up at a coffee shop near Camden Market.
2) You took the London hire bikes for a ride and ended up at a coffee shop near Camden Market.
</code></pre>
<p>I suggest you also add &quot;3)&quot; as stop sequence in case GPT-3 adds too much. The result is now easily parsable by splitting alongside newlines, removing any &quot;2) &quot;, trimming, and then grabbing lines[0] and [1].</p>
",2022-09-26 11:17:53,0.0,1.0
55531061,1,11315879.0,56754191.0,How can I create and fit vocab.bpe file (GPT and GPT2 OpenAI models) with my own corpus text?,question familiar gpt gpt openai model particular encode task bytepair encode problem like know create vocabbpe file a spanish corpus text like use fit bpe encoder succeedeed create encoderjson pythonbpe library no idea obtain vocabbpe file review code gptsrcencoderpy not able hint help idea thank advance,2019-04-05 08:15:51,,2020-11-29 12:07:28,2020-11-29 12:07:28,<python><encoding><nlp><gpt-2>,2,2,6,2490,0.0,2.0,11697642.0,"<p>check out <a href=""https://github.com/rsennrich/subword-nmt/"" rel=""nofollow noreferrer"">here</a>, you can easily create the same vocab.bpe using the following command:</p>

<pre><code>python learn_bpe -o ./vocab.bpe -i dataset.txt --symbols 50000
</code></pre>
",2019-06-25 12:34:15,0.0,4.0
60833301,1,7896124.0,60833512.0,Train huggingface's GPT2 from scratch : assert n_state % config.n_head == 0 error,try use a gpt architecture musical applications consequently need train scratch a bite google issue  huggingface github solve question try run propose solution follow error do mean solve generally a documentation a forward gpt define function use model buildin function force use a train fee individual tensors look not answer doc maybe miss ps read blogpost fron huggingfaceco omit informations detail usefull application,2020-03-24 14:42:13,,2020-11-29 12:10:24,2020-11-29 12:10:24,<python><nlp><huggingface-transformers><transformer-model><gpt-2>,1,0,0,690,,2.0,3607203.0,"<p>I think the error message is pretty clear:</p>

<blockquote>
  <p><code>assert n_state % config.n_head == 0</code></p>
</blockquote>

<p>Tracing it back through <a href=""https://github.com/huggingface/transformers/blob/v2.5.1/src/transformers/modeling_gpt2.py#L99"" rel=""nofollow noreferrer"">the code</a>, we can see</p>

<blockquote>
  <p><code>n_state = nx  # in Attention: n_state=768</code></p>
</blockquote>

<p>which indicates that <code>n_state</code> represents the embedding dimension (which is generally 768 by default in BERT-like models). When we then look at the <a href=""https://huggingface.co/transformers/model_doc/gpt2.html#gpt2config"" rel=""nofollow noreferrer"">GPT-2 documentation</a>, it seems the parameter specifying this is <code>n_embd</code>, which you are setting to <code>5</code>. As the error indicates, the embedding dimension <strong>has to be evenly divisible through the number of attention heads</strong>, which were specified as <code>4</code>. So, choosing a different embedding dimension as a multiple of <code>4</code> should solve the problem. Of course, you can also change the number of heads to begin with, but it seems that odd embedding dimensions are not supported.</p>
",2020-03-24 14:53:57,3.0,2.0
62830783,1,7710572.0,62830856.0,"Scripts missing for GPT-2 fine tune, and inference in Hugging-face GitHub?",follow documentation hug face website say finetune gpt use script run_lm_finetuningpy finetuning script run_generationpy inference script not actually exist github anymore do anybody know documentation outdated script thank,2020-07-10 08:59:58,,2020-11-29 12:03:01,2022-05-25 07:40:03,<python><huggingface-transformers><language-model><gpt-2>,2,0,1,359,,2.0,5266133.0,"<p>It looks like they've been moved around a couple times and the docs are indeed out of date, the current version can be found in <code>run_language_modeling.py</code> here <a href=""https://github.com/huggingface/transformers/tree/master/examples/language-modeling"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/tree/master/examples/language-modeling</a></p>
",2020-07-10 09:03:54,1.0,1.0
63321892,1,3593041.0,63442865.0,How can I use GPT 3 for my text classification?,wonder able use openai gpt transfer learn a text classification problem start use tensorflow keras,2020-08-09 02:17:17,,2020-11-29 11:46:41,2020-11-29 11:46:41,<keras><text-classification><transfer-learning><openai-api><gpt-3>,1,7,6,9646,0.0,2.0,3488735.0,"<p>(i substituted hateful language with ******** in the following samples)</p>
<p>Given samples like:</p>
<pre><code>(&quot;You look like ****** *** to me *******&quot;, true)
(&quot;**** you *********&quot;, true)
(&quot;**** my ****&quot;, true)
(&quot;hey my name is John can you help me?&quot;, false)
(&quot;hey my name is John, i think you ****** ***!&quot;, true)
(&quot;i have a problem with my network driver hpz-3332d&quot;, false)
</code></pre>
<p>GPT-3 can indeed then decide if a given input is hateful or not. GPT-3 actually is implementing filters that will very effectively tell if an arbitrary comment is hatefull or not. You would just enter the msg and let GPT3 autcomplete the  <code>, true|false)</code> part at the end, setting tokens to about ~6 and temperature setting 90%.</p>
<p>Boolean-ish classification that also relies on more complex context (you can insult someone without using foul-language) id doeable with GPT3 and can also be done with GPT2.</p>
",2020-08-16 23:31:25,5.0,6.0
65529156,1,1793799.0,65563077.0,Huggingface Transformer - GPT2 resume training from saved checkpoint,resume finetuning implement do gpt huggingface a parameter resume train save checkpoint instead train begin suppose python notebook crash train checkpoints save train model start train begin source finetuning code above code a script provide huggingface finetune gpt train customize dataset,2021-01-01 11:07:28,,,2022-08-24 08:32:02,<python><pytorch><huggingface-transformers><language-model><gpt-2>,2,0,3,2826,,2.0,843036.0,"<p>To resume training from checkpoint you use the <code>--model_name_or_path</code> parameter. So instead of giving the default <code>gpt2</code> you direct this to your latest checkpoint folder.</p>
<p>So your command becomes:</p>
<pre><code>!python3 run_clm.py \
    --train_file source.txt \
    --do_train \
    --output_dir gpt-finetuned \
    --overwrite_output_dir \
    --per_device_train_batch_size 2 \
    --model_name_or_path=/content/models/checkpoint-5000 \
    --save_steps 100 \
    --num_train_epochs=1 \
    --block_size=200 \
    --tokenizer_name=gpt2
</code></pre>
",2021-01-04 12:55:30,0.0,4.0
74656790,1,18002337.0,74657190.0,Prepare json file for GPT,like create a dataset use finetuning gpt read follow site dataset look like reason create dataset follow way try load look like not want let know face problem,2022-12-02 13:48:53,,,2022-12-02 14:59:31,<python><nlp><gpt-3>,1,0,1,1952,,2.0,15568504.0,"<p>it's more like, writing <code>\n</code> a new line character after each json. so each line is JSON. somehow the link <a href=""https://jsonlines.org"" rel=""nofollow noreferrer"">jsonlines</a> throw server not found error on me.</p>
<p>you can have these options:</p>
<ol>
<li>write <code>\n</code> after each line:</li>
</ol>
<pre><code>import json
with open(&quot;sample2_op1.json&quot;, &quot;w&quot;) as outfile:
    for e_json in dictionary:
        json.dump(e_json, outfile)
        outfile.write('\n')
#read file, as it has \n, read line by line and load as json
with open(&quot;sample2_op1.json&quot;,&quot;r&quot;) as file:
    for line in file:
        print(json.loads(line),type(json.loads(line)))
</code></pre>
<ol start=""2"">
<li>which have way to read file too, its <a href=""https://jsonlines.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">jsonlines</a>
install the module <code>!pip install jsonlines</code></li>
</ol>
<pre><code>import jsonlines
#write to file
with jsonlines.open('sample2_op2.jsonl', 'w') as outfile:
    outfile.write_all(dictionary)
#read the file
with jsonlines.open('sample2_op2.jsonl') as reader:
    for obj in reader:
        print(obj)
</code></pre>
",2022-12-02 14:17:37,2.0,1.0
75313457,1,4831435.0,75313682.0,OpenAI GPT-3 API: openai.api_key = os.getenv() not working,just try simple function python openai apis run error a valid api secret key use code,2023-02-01 16:44:32,,2023-03-13 14:08:23,2023-05-16 15:17:34,<python><openai-api><gpt-3>,1,3,3,5404,,2.0,10347145.0,"<h3>Option 1: OpenAI API key NOT as an environmental variable</h3>
<p>Change this...</p>
<p><code>openai.api_key = os.getenv('sk-xxxxxxxxxxxxxxxxxxxx')</code></p>
<p>...to this.</p>
<p><code>openai.api_key = 'sk-xxxxxxxxxxxxxxxxxxxx'</code></p>
<br>
<h3>Option 2: OpenAI API key as an environmental variable (recommended)</h3>
<p>Change this...</p>
<p><code>openai.api_key = os.getenv('sk-xxxxxxxxxxxxxxxxxxxx'</code>)</p>
<p>...to this...</p>
<p><code>openai.api_key = os.getenv('OPENAI_API_KEY')</code></p>
<br>
<h4><a href=""https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety"" rel=""nofollow noreferrer"">How do I set the OpenAI API key as an environmental variable?</a></h4>
<p>STEP 1: Open <em>System</em> properties and select <em>Advanced system settings</em></p>
<p>STEP 2: Select <em>Environment Variables</em></p>
<p>STEP 3: Select <em>New</em></p>
<p>STEP 4: Add your name/key value pair</p>
<pre><code>Variable name: OPENAI_API_KEY

Variable value: sk-xxxxxxxxxxxxxxxxxxxx
</code></pre>
<p>STEP 5: Restart your computer</p>
",2023-02-01 17:01:26,2.0,8.0
75335523,1,21140352.0,75335600.0,Error 400 when using GPT API (in JavaScript),get a  error try run basic chatbot use gpt api error attach code do wrong api key thank try consult websites solutions no luck,2023-02-03 12:08:08,,2023-02-04 06:24:14,2023-02-09 09:22:10,<javascript><error-handling><openai-api><gpt-3>,2,1,1,1149,,2.0,1584167.0,"<p>400 (Bad Request) error code typically means that client request's data is incorrect. So yes, must be something with your auth headers/body of request. Quite often response contains a reason, please try to print the text of response (before trying to get json output), e.g.</p>
<pre><code>console.log(response.text());
</code></pre>
<p>or just check Network Tab in Dev Console</p>
",2023-02-03 12:14:02,0.0,0.0
75762087,1,13908629.0,75771480.0,Trying to finetune GPT-2 in Vertex AI but it just freezes,follow tutorials train gpt scrap code work google colab vertex ai workbench just sit nothing run train code gpu quotas set a bill account enable relevant apis code use tokenizer code use model code use model path train args data collator trainer run start warn use code just sit tell run check nvidiasmi terminal tell use gpu just sit use a tesla ppciegb gpu use gpt small model make quick work  row data hopeful just a dumb mistake experience department greatly appreciate,2023-03-16 22:13:47,,,2023-03-17 19:27:02,<python><pytorch><huggingface-transformers><google-cloud-vertex-ai><gpt-2>,1,1,0,164,,2.0,13908629.0,"<p>I got around this by using a workbook with these settings:</p>
<ul>
<li>Zone: US-Central1-b</li>
<li>Environment: NumPy/SciPy/scikit-learn (when making the workbook I chose the Python Cuda 11.0 option)</li>
<li>Machine Type: 8 vCPUS, 30GB RAM</li>
<li>GPUs: Nvidia V100 x1</li>
</ul>
<p>And in the workbook itself, I used this command to install PyTorch:</p>
<pre><code>!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117
</code></pre>
<p>After that, everything worked just fine, just like in Google Colab!</p>
",2023-03-17 19:27:02,0.0,0.0
75529578,1,21263614.0,75797335.0,"Stream interrupted (client disconnected). To resume the stream, run: openai api fine_tunes.follow -i ft-***, while fine tuning openai",ctrlc interrupt stream not cancel finetune     create finetune ftlaofntddufqbmxthkviccuc stream interrupt client disconnect resume stream run openai api fine_tunesfollow ftlaofntddufqbmxthkviccuc issue use version openai   version get error help solve above problem,2023-02-22 07:48:04,,,2023-03-21 04:31:51,<python><openai-api><gpt-3>,1,1,1,861,,2.0,1151189.0,"<p><a href=""https://i.stack.imgur.com/kI0JP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kI0JP.png"" alt=""enter image description here"" /></a></p>
<p><strong>It was a temporary issue of OpenAI</strong></p>
",2023-03-21 04:31:51,0.0,0.0
75804599,1,4505301.0,75804651.0,OpenAI API: How do I count tokens before(!) I send an API request?,openai text model a context length curie a context length  tokens provide max_tokens stop parameters control length generate sequence generation stop stop token obtain max_tokens reach issue generate a text not know tokens prompt contain not know set max_tokens  number_tokens_in_prompt prevent generate text dynamically a wide range text term length need continue generate stop token question count number tokens python api set max_tokens parameter accordingly a way set max_tokens max cap wo not need count number prompt tokens,2023-03-21 17:35:10,,2023-03-21 17:50:19,2023-05-22 10:54:08,<openai-api><gpt-3><chatgpt-api>,2,0,17,11694,,2.0,10347145.0,"<p>As stated in the official <a href=""https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them"" rel=""noreferrer"">OpenAI article</a>:</p>
<blockquote>
<p>To further explore tokenization, you can use our interactive <a href=""https://platform.openai.com/tokenizer"" rel=""noreferrer"">Tokenizer</a>
tool, which allows you to calculate the number of tokens and see how
text is broken into tokens. <strong>Alternatively, if you'd like to tokenize
text programmatically, use <a href=""https://github.com/openai/tiktoken"" rel=""noreferrer"">Tiktoken</a> as a fast BPE tokenizer
specifically used for OpenAI models.</strong> Other such libraries you can
explore as well include <a href=""https://huggingface.co/docs/transformers/model_doc/gpt2#gpt2tokenizerfast"" rel=""noreferrer"">transformers package</a> for Python or the
<a href=""https://www.npmjs.com/package/gpt-3-encoder"" rel=""noreferrer"">gpt-3-encoder package</a> for NodeJS.</p>
</blockquote>
<p>A tokenizer can split the text string into a list of tokens, as stated in the official <a href=""https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb"" rel=""noreferrer"">OpenAI example</a> on counting tokens with Tiktoken:</p>
<blockquote>
<p>Tiktoken is a fast open-source tokenizer by OpenAI.</p>
<p>Given a text string (e.g., <code>&quot;tiktoken is great!&quot;</code>) and an encoding
(e.g., <code>&quot;cl100k_base&quot;</code>), a tokenizer can split the text string into a
list of tokens (e.g., <code>[&quot;t&quot;, &quot;ik&quot;, &quot;token&quot;, &quot; is&quot;, &quot; great&quot;, &quot;!&quot;]</code>).</p>
<p>Splitting text strings into tokens is useful because GPT models see
text in the form of tokens. Knowing how many tokens are in a text
string can tell you:</p>
<ul>
<li>whether the string is too long for a text model to process and</li>
<li>how much an OpenAI API call costs (as usage is priced by token).</li>
</ul>
</blockquote>
<p>Tiktoken supports 3 encodings used by OpenAI models (<a href=""https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb"" rel=""noreferrer"">source</a>):</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Encoding name</th>
<th>OpenAI models</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>cl100k_base</code></td>
<td><code>gpt-4</code>, <code>gpt-3.5-turbo</code>, <code>text-embedding-ada-002</code></td>
</tr>
<tr>
<td><code>p50k_base</code></td>
<td>Codex models, <code>text-davinci-002</code>, <code>text-davinci-003</code></td>
</tr>
<tr>
<td><code>r50k_base</code> (<code>gpt2</code>)</td>
<td>GPT-3 models like <code>davinci</code></td>
</tr>
</tbody>
</table>
</div>
<p>For <code>cl100k_base</code> and <code>p50k_base</code> encodings:</p>
<ul>
<li>Python: <a href=""https://github.com/openai/tiktoken/blob/main/README.md"" rel=""noreferrer"">tiktoken</a></li>
<li>.NET / C#: <a href=""https://github.com/dmitry-brazhenko/SharpToken"" rel=""noreferrer"">SharpToken</a></li>
<li>Java: <a href=""https://github.com/knuddelsgmbh/jtokkit"" rel=""noreferrer"">jtokkit</a></li>
</ul>
<p>For <code>r50k_base</code> (<code>gpt2</code>) encodings, tokenizers are available in many languages:</p>
<ul>
<li>Python: <a href=""https://github.com/openai/tiktoken/blob/main/README.md"" rel=""noreferrer"">tiktoken</a> (or alternatively <a href=""https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2TokenizerFast"" rel=""noreferrer"">GPT2TokenizerFast</a>)</li>
<li>JavaScript: <a href=""https://www.npmjs.com/package/gpt-3-encoder"" rel=""noreferrer"">gpt-3-encoder</a></li>
<li>.NET / C#: <a href=""https://github.com/dluc/openai-tools"" rel=""noreferrer"">GPT Tokenizer</a></li>
<li>Java: <a href=""https://github.com/hyunwoongko/gpt2-tokenizer-java"" rel=""noreferrer"">gpt2-tokenizer-java</a></li>
<li>PHP: <a href=""https://github.com/CodeRevolutionPlugins/GPT-3-Encoder-PHP"" rel=""noreferrer"">GPT-3-Encoder-PHP</a></li>
</ul>
<p>Note that <code>gpt-3.5-turbo</code> and <code>gpt-4</code> use tokens in the same way as other models as stated in the official <a href=""https://platform.openai.com/docs/guides/chat/managing-tokens"" rel=""noreferrer"">OpenAI documentation</a>:</p>
<blockquote>
<p>Chat models like <code>gpt-3.5-turbo</code> and <code>gpt-4</code> use tokens in the same way as
other models, but because of their message-based formatting, it's more
difficult to count how many tokens will be used by a conversation.</p>
<p>If a conversation has too many tokens to fit within a model’s maximum
limit (e.g., more than 4096 tokens for <code>gpt-3.5-turbo</code>), you will have
to truncate, omit, or otherwise shrink your text until it fits. Beware
that if a message is removed from the messages input, the model will
lose all knowledge of it.</p>
<p>Note too that very long conversations are more likely to receive
incomplete replies. For example, a <code>gpt-3.5-turbo</code> conversation that is
4090 tokens long will have its reply cut off after just 6 tokens.</p>
</blockquote>
",2023-03-21 17:39:41,2.0,19.0
75882988,1,21522645.0,75886728.0,"OpenAI GPT-3 API error: ""Invalid URL (POST /v1/chat/completions)""",code snippet run run issue nodejs v look internet try solutions nothing work help usually link attach code look problem online a beginner stuff help appreciate,2023-03-29 23:32:12,,2023-03-31 07:31:22,2023-05-16 08:40:40,<node.js><openai-api><gpt-3>,2,3,3,3339,,2.0,10347145.0,"<p><strong>TL;DR: Treat the <code>text-davinci-003</code> as a GPT-3 model. See the code under OPTION 1.</strong></p>
<h2>Introduction</h2>
<p>At first glance, as someone who's been using the OpenAI API for the past few months, I thought the answer was straight and simple if you read the official OpenAI documentation. Well, I read the documentation once again, and now I understand why you're confused.</p>
<h3>Confusion number 1</h3>
<p>You want to use the <code>text-davinci-003</code> model. This model is originally from the GPT-3 model family. But if you take a look at the <a href=""https://platform.openai.com/docs/models/overview"" rel=""nofollow noreferrer"">OpenAI models overview</a> and click <a href=""https://platform.openai.com/docs/models/gpt-3"" rel=""nofollow noreferrer"">GPT-3</a>, you won't find <code>text-davinci-003</code> listed as a GPT-3 model. This is unexpected.</p>
<p><a href=""https://i.stack.imgur.com/MFI3u.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MFI3u.png"" alt=""Screenshot 1"" /></a></p>
<h3>Confusion number 2</h3>
<p>Moreover, the <code>text-davinci-003</code> is listed as a <a href=""https://platform.openai.com/docs/models/gpt-3-5"" rel=""nofollow noreferrer"">GPT-3.5</a> model.</p>
<p><a href=""https://i.stack.imgur.com/CXQJd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CXQJd.png"" alt=""Screenshot 2"" /></a></p>
<h3>Confusion number 3</h3>
<p>As if this isn't confusing enough, if you take a look at the <a href=""https://platform.openai.com/docs/models/model-endpoint-compatibility"" rel=""nofollow noreferrer"">OpenAI model endpoint compatibility</a>, you'll find the <code>text-davinci-003</code> listed under the <code>/v1/completions</code> endpoint. This API endpoint is used for the GPT-3 model family.</p>
<p><a href=""https://i.stack.imgur.com/XBt3B.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/XBt3B.png"" alt=""Screenshot 3"" /></a></p>
<br>
<h2>Wait, what?</h2>
<p><strong>The <code>text-davinci-003</code> isn't listed as a GPT-3 model. It's listed as a GPT-3.5 model but is compatible with the GPT-3 API endpoint. This doesn't make any sense.</strong></p>
<br>
<h2>Test</h2>
<p>Either the <code>text-davinci-003</code> could be treated as a GPT-3 model or a GPT-3.5 model, or perhaps both? Let's make a test.</p>
<h5>OPTION 1: Treat the <code>text-davinci-003</code> as a GPT-3 model --&gt; IT WORKS ✓</h5>
<p>If you treat the <code>text-davinci-003</code> as a GPT-3 model, then run <code>test-1.js</code>, and the OpenAI will return the following completion:</p>
<blockquote>
<p>This is indeed a test</p>
</blockquote>
<p><strong>test-1.js</strong></p>
<pre><code>const { Configuration, OpenAIApi } = require('openai');

const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});

const openai = new OpenAIApi(configuration);

async function getCompletionFromOpenAI() {
  const completion = await openai.createCompletion({
    model: 'text-davinci-003',
    prompt: 'Say this is a test',
    max_tokens: 7,
    temperature: 0,
  });

  console.log(completion.data.choices[0].text);
}

getCompletionFromOpenAI();
</code></pre>
<h5>OPTION 2: Treat the <code>text-davinci-003</code> as a GPT-3.5 model --&gt; IT DOESN'T WORK ✗</h5>
<p>If you treat the <code>text-davinci-003</code> as a GPT-3.5 model, then run <code>test-2.js</code>, and the OpenAI will return the following error:</p>
<pre><code>data: {
  error: {
    message: 'Invalid URL (POST /v1/chat/completions)',
    type: 'invalid_request_error',
    param: null,
    code: null
  }
}
</code></pre>
<p><strong>test-2.js</strong></p>
<pre><code>const { Configuration, OpenAIApi } = require('openai');

const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});

const openai = new OpenAIApi(configuration);

async function getCompletionFromOpenAI() {
  const completion = await openai.createChatCompletion({
    model: 'text-davinci-003',
    messages: [{ role: 'user', content: 'Hello!' }],
    temperature: 0,
  });

  console.log(completion.data.choices[0].message.content);
}

getCompletionFromOpenAI();
</code></pre>
<h2>Conclusion</h2>
<p>Treat the <code>text-davinci-003</code> as a GPT-3 model. See the code under OPTION 1.</p>
",2023-03-30 09:58:07,1.0,4.0
75904923,1,10560942.0,75904953.0,"I am getting error here torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse) when I call trainer.train() function of GPT2 model",new nlp try gpt train data code check train data load correclty get convert embeddings train data look like hello do today whats md im do good do im alright just take a nap nap doesnt help just make worse question life choices oh wow haha feel tire huh yeah do bed late call trainertrain function get error indexerror index range self line torchembedding weight input padding_idx scale_grad_by_freq sparse problem not check value weight input internal function help try change parameters like batch size per_device_train_batch_size stick,2023-04-01 08:01:40,,,2023-04-01 08:10:03,<python><nlp><huggingface-transformers><torch><gpt-2>,1,1,0,88,,2.0,21524483.0,"<p>The error you are experiencing is most likely due to the size of the vocabulary you have set in your GPT2Config.</p>
<p>You have set the vocab_size to 10000, but the actual size of the vocabulary in the GPT-2 model is 50257. Therefore, the model is expecting input token IDs to be between 0 and 50256, but some of the token IDs in your training data are outside this range.</p>
<p>To fix this, you should set the vocab_size in your GPT2Config to 50257. Also, make sure that the tokenizer you are using is the same as the one used to tokenize your training data. If the tokenizer is different, the token IDs in your training data may not match the expected token IDs of the model.</p>
<pre><code>from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments

config = GPT2Config(vocab_size=50257, n_positions=256, n_ctx=256, n_embd=512, n_layer=12, n_head=8)

model = GPT2LMHeadModel(config=config)

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
train_text = &quot;&quot;&quot;Hello How are you doing today? whats up MD im doing good     how are you doing? Im alright, I just took a nap. But it was one of those naps that doesnt help anything. It just makes everything worse and you question all your life choices oh wow haha so you still feel tired huh? Yeah did you go to bed late?&quot;&quot;&quot;
train_data = TextDataset(tokenizer=tokenizer, file_path=None,             split_text=train_text, block_size=256)

training_args = TrainingArguments(
    output_dir='./models',
    overwrite_output_dir=True,
num_train_epochs=1,
per_device_train_batch_size=4,
save_steps=1000,
save_total_limit=2,
prediction_loss_only=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
   prediction_loss_only=True,
)
trainer.train()
</code></pre>
",2023-04-01 08:10:03,1.0,2.0
67372903,1,468305.0,75949356.0,Access OpenAI (json) API from R,want access openai api follow curl command r think curl package cran best option never use package help get start simple,2021-05-03 17:01:19,,2023-01-24 18:23:08,2023-04-06 12:08:29,<r><json><curl><openai-api><gpt-3>,2,8,1,1465,0.0,2.0,8163634.0,"<p>I created an R package named &quot;openapi&quot; (<a href=""https://github.com/zhanghao-njmu/openapi"" rel=""nofollow noreferrer"">https://github.com/zhanghao-njmu/openapi</a>), which supports all OpenAI APIs and can generate streaming returns (currently, other packages do not have good solutions), chatGPT app, and various RStudio add-ins. Welcome to use it.</p>
",2023-04-06 12:08:29,0.0,1.0
76153016,1,21794193.0,76166261.0,gpt chatbot not working after using open ai imports and langchain,hi try build a chatbot use openai api basic function read a pdf txtfile answer base follow a tutorial use follow code instal neccessary dependencies run code follow error fix,2023-05-02 08:24:15,,,2023-05-03 16:40:36,<python><window><openai-api><gpt-3><langchain>,2,4,0,3150,,2.0,21807871.0,"<p>fixed this with</p>
<pre><code>pip install langchain==0.0.118
</code></pre>
<p>and</p>
<pre><code>pip install gpt_index==0.4.24
</code></pre>
<p>not ideal but got this code to work after these changes</p>
",2023-05-03 16:40:36,0.0,7.0
76067091,1,5832020.0,76320287.0,GPU out of memory fine tune flan-ul2,standard_ncs_v single node gpu gb memory  gpus error message say total capacity gib fine tune not use  gpus  gpus use fine tune flanul use huggingface transformers,2023-04-20 18:13:02,,2023-04-20 18:33:12,2023-05-24 05:23:00,<gpu><huggingface-transformers><huggingface-tokenizers><gpt-3><fine-tune>,1,2,1,224,,2.0,5832020.0,"<p>I solve the issue by using the following package versions.</p>
<pre><code>!pip install transformers==4.28.1
!pip install sentencepiece==0.1.97
!pip install accelerate==0.18.0
!pip install bitsandbytes==0.37.2
!pip install torch==1.13.1
</code></pre>
",2023-05-24 05:23:00,0.0,0.0
75882872,1,12226377.0,76323834.0,How to overcome Rate limit error while working with GPT3 Models using Tenacity,situation try pass a prompt use a helper function actual gpt model case textada eventually apply a pandas column use follow code recover follow error eventually apply pandas column error overcome error look link tenacity help resolve issue not sure structure code do follow moment use code suggest link overcome rate limit error,2023-03-29 23:04:29,,2023-04-01 07:42:04,2023-05-28 11:16:01,<pandas><openai-api><gpt-3><tenacity>,1,1,0,229,,2.0,8789910.0,"<p>Import tenacity at the beginning of your code and then add its decoration where you are calling the OpenAI library with create. So your code would look like this:</p>
<pre><code>from tenacity import (
    retry,
    stop_after_attempt,
    wait_random_exponential,
) 

@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))
def sentiment_text(text):
        your_prompt = &quot;&quot;&quot;Is the sentiment Positive, Negative or Neutral for the 
                         following text:
    
                         &quot;{}&quot;
                      &quot;&quot;&quot;.format(text)
        response = openai.Completion.create(
           engine=&quot;text-ada-001&quot;,
           prompt=your_prompt ,
           max_tokens=1000,
           temperature=0,
           top_p=1,
           frequency_penalty=0,
           presence_penalty=0
        )
        sentiment = response.choices[0].text
        return sentiment
</code></pre>
",2023-05-24 13:00:31,0.0,1.0
76363168,1,19107509.0,76371360.0,OpenAI API: How do I handle errors in Python?,try use code openai api do not method library effectively handle error,2023-05-30 08:54:55,,2023-06-02 13:55:45,2023-06-02 14:02:24,<python><openai-api><gpt-3><chatgpt-api><gpt-4>,1,1,1,195,,2.0,10347145.0,"<p><strong>Your code isn't correct.</strong></p>
<p>Change this...</p>
<pre><code>except openai.AuthenticationError
</code></pre>
<p>...to this.</p>
<pre><code>except openai.error.AuthenticationError
</code></pre>
<p>Try the following, as shown in the official <a href=""https://help.openai.com/en/articles/6897213-openai-library-error-types-guidance"" rel=""nofollow noreferrer"">OpenAI article</a>:</p>
<pre><code>try:
  #Make your OpenAI API request here
  response = openai.Completion.create(model = &quot;text-davinci-003&quot;, prompt = &quot;Hello world&quot;)
except openai.error.Timeout as e:
  #Handle timeout error, e.g. retry or log
  print(f&quot;OpenAI API request timed out: {e}&quot;)
  pass
except openai.error.APIError as e:
  #Handle API error, e.g. retry or log
  print(f&quot;OpenAI API returned an API Error: {e}&quot;)
  pass
except openai.error.APIConnectionError as e:
  #Handle connection error, e.g. check network or log
  print(f&quot;OpenAI API request failed to connect: {e}&quot;)
  pass
except openai.error.InvalidRequestError as e:
  #Handle invalid request error, e.g. validate parameters or log
  print(f&quot;OpenAI API request was invalid: {e}&quot;)
  pass
except openai.error.AuthenticationError as e:
  #Handle authentication error, e.g. check credentials or log
  print(f&quot;OpenAI API request was not authorized: {e}&quot;)
  pass
except openai.error.PermissionError as e:
  #Handle permission error, e.g. check scope or log
  print(f&quot;OpenAI API request was not permitted: {e}&quot;)
  pass
except openai.error.RateLimitError as e:
  #Handle rate limit error, e.g. wait or log
  print(f&quot;OpenAI API request exceeded rate limit: {e}&quot;)
  pass
</code></pre>
",2023-05-31 08:00:20,0.0,0.0
76398258,1,18217301.0,76398870.0,"OpenAI GPT-3.5 ""prompt"" argument not working",try make a flutter app openai api work like a chatbot want add a prompt responses specialize like openai playground website test api post function postman work perfectly fine try add a prompt assume add a prompt just add a prompt line body like work textdavinci model message return a different way need gpt model do prompt argument just not exist,2023-06-03 22:26:20,,2023-06-04 22:14:05,2023-06-04 22:14:05,<http><openai-api><gpt-3>,1,0,0,72,,2.0,23235.0,"<p>The OPEN AI 3.5-turbo model only supports the newer <a href=""https://platform.openai.com/docs/api-reference/chat/create"" rel=""nofollow noreferrer"">chat completion API</a> which does not have a 'prompt' json body field.</p>
<p>I would assume you are using the older <a href=""https://platform.openai.com/docs/api-reference/completions/create"" rel=""nofollow noreferrer"">competion API</a> json body format aginst the newer chat completion API endpoint and that is the reason for your error.</p>
<p>--- update ---</p>
<p>I can reproduce your exact error response with:</p>
<blockquote>
<p>curl <a href=""https://api.openai.com/v1/chat/completions"" rel=""nofollow noreferrer"">https://api.openai.com/v1/chat/completions</a> -H &quot;Content-Type:
application/json&quot;  -H &quot;Authorization: Bearer $OPENAI_API_KEY&quot;  -d '{
&quot;model&quot;: &quot;gpt-3.5-turbo&quot;, &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;:
&quot;Hello!&quot;}], &quot;prompt&quot;: &quot;testing&quot; }'</p>
</blockquote>
<p>This is just a modified example from the api link above. Removing the added &quot;prompt&quot; field from the json makes it work fine.</p>
<p>So my advice in the comment stands.  Figure out what you are sending in the body of your http GET request.  It will have a &quot;prompt&quot; field, remove it and any other field that should not be there and it should work.</p>
",2023-06-04 03:55:30,4.0,0.0
76411359,1,22027390.0,76412710.0,OpenAI API: How do I migrate from text-davinci-003 to gpt-3.5-turbo in NodeJS?,migrate try follow change change problem do not work code give unmodified code help change want upgrade irritate completion like send hello give entire letter not a greet live sample github page github repository,2023-06-06 03:56:25,,2023-06-07 10:50:21,2023-06-07 10:50:21,<openai-api><gpt-3><chatgpt-api><text-davinci-003>,1,3,0,149,,2.0,10347145.0,"<p>You want to use the <code>gpt-3.5-turbo</code> model.</p>
<p>There are three main differences between the ChatGPT API (i.e., the GPT-3.5 API) and the GPT-3 API:</p>
<ol>
<li>API endpoint
<ul>
<li>GPT-3 API: <code>https://api.openai.com/v1/completions</code></li>
<li>ChatGPT API: <code>https://api.openai.com/v1/chat/completions</code></li>
</ul>
</li>
<li>The <code>prompt</code> parameter (GPT-3 API) is replaced by the <code>messages</code> parameter (ChatGPT API)</li>
<li>Response access
<ul>
<li>GPT-3 API: <code>response.choices[0].text.trim()</code></li>
<li>ChatGPT API: <code>response.choices[0].message.content.trim()</code></li>
</ul>
</li>
</ol>
<p>Try this:</p>
<pre><code>const getChatResponse = async (incomingChatDiv) =&gt; {
    const API_URL = &quot;https://api.openai.com/v1/chat/completions&quot;; /* Changed */
    const pElement = document.createElement(&quot;p&quot;);

    // Define the properties and data for the API request
    const requestOptions = {
        method: &quot;POST&quot;,
        headers: {
            &quot;Content-Type&quot;: &quot;application/json&quot;,
            &quot;Authorization&quot;: `Bearer ${API_KEY}`
        },
        body: JSON.stringify({
            model: &quot;gpt-3.5-turbo&quot;,
            messages: [{role: &quot;user&quot;, content: `${userText}`}], /* Changed */
            max_tokens: 2048,
            temperature: 0.2,
            n: 1,
            stop: null
        })
    }

    // Send POST request to API, get response and set the reponse as paragraph element text
    try {
        const response = await (await fetch(API_URL, requestOptions)).json();
        pElement.textContent = response.choices[0].message.content.trim(); /* Changed */
    } catch (error) { // Add error class to the paragraph element and set error text
        pElement.classList.add(&quot;error&quot;);
        pElement.textContent = &quot;Oops! Something went wrong while retrieving the response. Please try again.&quot;;
    }

    // Remove the typing animation, append the paragraph element and save the chats to local storage
    incomingChatDiv.querySelector(&quot;.typing-animation&quot;).remove();
    incomingChatDiv.querySelector(&quot;.chat-details&quot;).appendChild(pElement);
    localStorage.setItem(&quot;all-chats-thedoggybrad&quot;, chatContainer.innerHTML);
    chatContainer.scrollTo(0, chatContainer.scrollHeight);
}
</code></pre>
",2023-06-06 08:22:50,0.0,0.0
66276186,1,1793799.0,66278433.0,HuggingFace - GPT2 Tokenizer configuration in config.json,gpt finetuned model upload huggingfacemodels inferencing error observe inference not load tokenizer use from_pretrained update configuration not load tokenizer balamodel__test make sure balamodel__test a correct model identifier list balamodel__test correct path a directory contain relevant tokenizer file configuration configjson file finetuned huggingface model configure gpt tokenizer just like configjson file,2021-02-19 10:53:15,,,2021-02-19 13:25:37,<pytorch><huggingface-transformers><language-model><huggingface-tokenizers><gpt-2>,1,0,1,2211,,2.0,6664872.0,"<p>Your repository does not contain the required files to create a tokenizer. It seems like you have only uploaded the files for your model. Create an object of your tokenizer that you have used for training the model and save the required files with <a href=""https://huggingface.co/transformers/internal/tokenization_utils.html?highlight=save_pretrained#transformers.tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained"" rel=""nofollow noreferrer"">save_pretrained()</a>:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import GPT2Tokenizer

t = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
t.save_pretrained('/SOMEFOLDER/')
</code></pre>
<p>Output:</p>
<pre><code>('/SOMEFOLDER/tokenizer_config.json',
 '/SOMEFOLDER/special_tokens_map.json',
 '/SOMEFOLDER/vocab.json',
 '/SOMEFOLDER/merges.txt',
 '/SOMEFOLDER/added_tokens.json')
</code></pre>
",2021-02-19 13:25:37,0.0,1.0
66901602,1,6010395.0,66905568.0,What is tokenizer.max len doing in this class definition?,follow rostylav tutorial runnning error dont quite understand class believe cause error not able understand tokenizemax_len suppose try fix thank read,2021-04-01 09:07:34,,,2021-04-01 13:38:17,<python><google-colaboratory><huggingface-transformers><huggingface-tokenizers><gpt-2>,1,0,1,888,,2.0,6664872.0,"<p>The attribute <code>max_len</code> was <a href=""https://huggingface.co/transformers/migration.html?highlight=max_len#removed-some-deprecated-attributes"" rel=""nofollow noreferrer"">migrated</a> to <code>model_max_length</code>. It represents the maximum number of tokens a model can handle (i.e. including special tokens) (<a href=""https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=model_max_length#transformers.PreTrainedTokenizerFast"" rel=""nofollow noreferrer"">documentation</a>).</p>
<p><code>max_len_single_sentence</code> on the other side represents the maximum number of tokens a single sentence can have (i.e. without special tokens) (<a href=""https://huggingface.co/transformers/internal/tokenization_utils.html?highlight=max_len_single_sentence#transformers.tokenization_utils_base.PreTrainedTokenizerBase.max_len_single_sentence"" rel=""nofollow noreferrer"">documentation</a>).</p>
",2021-04-01 13:38:17,1.0,1.0
67325687,1,9693537.0,67653823.0,How to save pre-trained API on GPT-3?,a question about gpt know examples network adjust model examples model save examples reuse apis now use examples model follow code not able save pretrained api time retrain way reuse,2021-04-29 22:07:10,,2021-05-22 19:39:56,2023-01-12 01:55:51,<python><gpt-3>,2,0,5,637,,2.0,14852784.0,"<blockquote>
<p>Every time I have to retrain it - is there any way to reuse it?</p>
</blockquote>
<p>No, there isn't any way to reuse it. You are mixing up the terms: You don't need to train GPT-3, you need to pass in examples to the prompt. As you don't have any kind of container in which you could store previous results (and thus &quot;train&quot; your model), it's required to pass examples including your task each and every time.</p>
<p>To perfect the engineering process (and therefore reduce the cost per request) is a difficult process and will take a long time with trial and error.</p>
<p>Though let's be honest: Even with passing the examples every time, GPT-3 is extremely cost efficient. Depending on your specific situation, you (on average) only spend a few hundred tokens for a complex completion with Davinci.</p>
",2021-05-22 20:19:07,3.0,2.0
67707374,1,15365513.0,67845671.0,How can text completion using the GPT-2 language model generate a full URL?,auto text completion mr fabrice bellard website ask like picture question respond text generate randomly control text type control text type not relate question type possible generate a link a web page access new ai neutral network sort thing forgive a stupid question curious about,2021-05-26 14:49:19,,,2021-06-05 02:05:49,<artificial-intelligence><gpt-2>,1,1,0,477,0.0,2.0,2444877.0,"<p>Gtp-2 was trained on massive amounts of text all around the internet and is able to generate text by predicting the next word in a sequence of tokens. In theory, the content generated should be driven by the input you provide. Beware that the URLs generated are not real, the model is inventing them.</p>
<p>You might also to check GPT-3 as it does a much better job at generating text following the context of the input.</p>
",2021-06-05 02:05:49,0.0,1.0
68946827,1,364966.0,68961549.0,Spacy-Transformers: Access GPT-2?,use spacytransformers build nlp model spacytransformers docs say sample code page show base learn video en_core_web_trf appear package use a bert model search spacytransformers docs not see equivalent package access gpt a specific package load order use a gpt model,2021-08-27 00:48:37,,,2021-08-28 05:16:12,<machine-learning><nlp><spacy><gpt-2>,1,0,1,429,,2.0,355715.0,"<p>The <code>en_core_web_trf</code> uses a specific Transformers model, but you can specify arbitrary ones using the <code>TransformerModel</code> wrapper class from <code>spacy-transformers</code>. See <a href=""https://spacy.io/api/architectures#TransformerModel"" rel=""nofollow noreferrer"">the docs</a> for that. An example config:</p>
<pre><code>[model]
@architectures = &quot;spacy-transformers.TransformerModel.v1&quot;
name = &quot;roberta-base&quot; # this can be the name of any hub model
tokenizer_config = {&quot;use_fast&quot;: true}
</code></pre>
",2021-08-28 05:16:12,2.0,1.0
69182644,1,16814329.0,69183100.0,GPT 2 - TypeError: Cannot cast array data from dtype('O') to dtype('int64') according to the rule 'safe',work gpt python  tensorflow  connect flask flask run terminal a follow message code generatorpy help appreciate ps add entire traceback,2021-09-14 18:08:06,,2021-09-14 18:32:38,2021-09-14 18:44:59,<python><gpt-2>,1,1,0,398,,2.0,10199456.0,"<p>The problem is the line <code>None,</code> in your code. This is causing the tuple <code>(None,)</code> as the input to the <code>np.random.seed(seed)</code>. It accepts integer, but you are sending the tuple.</p>
",2021-09-14 18:44:59,1.0,0.0
70531364,1,14879655.0,70710672.0,Structuring dataset for OpenAI's GPT-3 fine tuning,fine tune endpoint openai api fairly new not examples fine tune datasets online charge a voicebot test performance gpt general openconversation question like train model fix intentresponse pair currently use probably end perform better term company voice style ready a long json file data extract current conversational engine match user input intents return specify response like train a gpt model data now quick test set call api just like suggest a fix intro text form prepended query a small python class keep track context start turn api response append way keep track say a question query prompt string send look like question provide format train data advisable docs indicate train set format do prompt need include intro text description time simply provide a series userbot exchange a end completion answer expect a best practice case fear want slightly change intro prompt a month now retrain thing response train specific block text prepended,2021-12-30 12:00:37,,2022-01-10 11:45:53,2022-01-14 12:37:01,<python><machine-learning><training-data><openai-api><gpt-3>,1,0,3,2278,0.0,2.0,14879655.0,"<p>I contacted OpenAI's support and they were extremely helpful: I'll leave their answer here.</p>
<blockquote>
<p>the prompt does not need the fixed intro every time. Instead, you'll just want to provide at least a few hundred prompt-completion pairs of user/bot exchanges.
We have a sample of a chatbot fine-tuning dataset <a href=""https://beta.openai.com/docs/guides/fine-tuning/case-study-customer-support-chatbot"" rel=""nofollow noreferrer"">here</a>.</p>
</blockquote>
",2022-01-14 12:37:01,0.0,2.0
71203411,1,2293224.0,71368459.0,"Openai: `prompt` column/key is missing. Please make sure you name your columns/keys appropriately, then retry",want run gpt text classification step prepare data use openai cli get a csv file look like follow write follow command prepare data get follow error not sure about columnskeys appropriately convention follow help really appreciate fix error,2022-02-21 08:46:04,,2022-02-21 09:17:09,2023-01-18 12:07:02,<python><openai-api><gpt-3>,2,0,1,2265,0.0,2.0,18387639.0,"<p>You may convert your csv/tsv file to json, rename the header as prompt and completion.</p>
<p>Like this:
| prompt | completion |
| -------- | -------------- |
| text1    | result1        |
| text2    | result2        |</p>
",2022-03-06 07:32:52,1.0,2.0
73370817,1,15152059.0,73371474.0,How to use GPT-3 for fill-mask tasks?,use follow code likely replacements a mask word example likely replacement mask sequence sun mask rise  shin   question a way achieve gpt a complete insert preset openai playground give sentence instead single word no probabilities help,2022-08-16 08:15:15,,,2022-08-16 09:08:10,<python><nlp><gpt-3>,1,3,1,854,,2.0,14719844.0,"<p>First of all, I don't think you can access properties like token or scores in GPT-3, all you have is the generated text.</p>
<p>Second of all, in my experience GPT-3 is ALL about the correct prompt. You just have to give it instructions like you were talking to a human being.</p>
<p>In you specific case, I would use a prompt like this:</p>
<p>Prompt:</p>
<blockquote>
<p>The sun is [MASK].</p>
<p>Replace [MASK] with the most probable 5 words to replace, and give me
their probabilities.</p>
</blockquote>
<p>Result:</p>
<blockquote>
<p>The sun is shining.</p>
<ol>
<li>shining - 0.47</li>
<li>bright - 0.18</li>
<li>sunny - 0.13</li>
<li>hot - 0.10</li>
<li>beautiful - 0.09</li>
</ol>
</blockquote>
<p>If you want to do that programmatically, here's the code:</p>
<pre><code>import openai
openai.organization = &quot;your org key, if you have one&quot;
openai.api_key = &quot;you api key&quot;
openai.Engine.list()

my_prompt = '''The sun is [MASK].
    
    Replace [MASK] with the most probable 5 words to replace, and give me their probabilities.'''

# Here set parameters as you like
response = openai.Completion.create(
  engine=&quot;text-davinci-002&quot;,
  prompt=my_prompt,
  temperature=0,
  max_tokens=500,
  # top_p=1,
  # frequency_penalty=0.0,
  # presence_penalty=0.0,
  # stop=[&quot;\n&quot;]
)

print(response['choices'][0]['text'])
</code></pre>
",2022-08-16 09:08:10,1.0,1.0
74666268,1,702977.0,74744388.0,OpenAI: Stream interrupted (client disconnected),try openai prepare train data use minutes later show try minutes later fail show return help,2022-12-03 11:24:58,,,2023-05-30 19:06:58,<openai-api><gpt-3>,2,1,9,4882,,2.0,702977.0,"<p>It was a temporary issue of OpenAI, the team fixed that.</p>
",2022-12-09 14:20:22,1.0,1.0
74823070,1,4211617.0,74823327.0,Can you create a custom model using GPT-3 to answer questions only about a specific topic?,use gpt create a chatbot answer question relate a specific topic gpt train detect question irrelevant topic refuse answer example let say want create a chatbot answer question about javascript ask list seven wonder world refuse answer,2022-12-16 10:21:20,,,2023-06-20 18:45:02,<nlp><chatbot><gpt-3>,1,0,1,560,,2.0,15592565.0,"<p>This has been successful for me, you may wish to try it as well.</p>
<p>I want you to act as a javascript guide. You are here to help answer any questions I may have about the language. If I have any questions,  you will do your best to provide a helpful response. Please note that if my question is not related to Javascript, you have to write only &quot;Error&quot;. Let's get started.</p>
",2022-12-16 10:45:12,1.0,3.0
75041247,1,287316.0,75043933.0,What's the correct URL to test OpenAI API?,try test gpt api a request use curl windows cmd give do change my_key key get try model invalid url error correct url not docs chatgpt,2023-01-07 14:50:32,,2023-01-24 18:16:59,2023-05-04 17:46:28,<curl><openai-api><gpt-3>,2,1,0,3920,,2.0,20819591.0,"<p>Sending a POST request to <code>/v1/conversations/text-davinci-003/messages</code> will not return the result you want, because this URL is not used by the OpenAI API.</p>
<p>Here's an example of a cURL request which completes the message <code>Say this is a test</code></p>
<pre class=""lang-bash prettyprint-override""><code>curl https://api.openai.com/v1/completions \
-H &quot;Content-Type: application/json&quot; \
-H &quot;Authorization: Bearer YOUR_API_KEY&quot; \
-d '{&quot;model&quot;: &quot;text-davinci-003&quot;, &quot;prompt&quot;: &quot;Say this is a test&quot;, &quot;temperature&quot;: 0, &quot;max_tokens&quot;: 7}'
</code></pre>
<p>And this is an example of what the API will respond with:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;id&quot;: &quot;cmpl-GERzeJQ4lvqPk8SkZu4XMIuR&quot;,
    &quot;object&quot;: &quot;text_completion&quot;,
    &quot;created&quot;: 1586839808,
    &quot;model&quot;: &quot;text-davinci:003&quot;,
    &quot;choices&quot;: [
        {
            &quot;text&quot;: &quot;This is indeed a test&quot;,
            &quot;index&quot;: 0,
            &quot;logprobs&quot;: null,
            &quot;finish_reason&quot;: &quot;length&quot;
        }
    ],
    &quot;usage&quot;: {
        &quot;prompt_tokens&quot;: 5,
        &quot;completion_tokens&quot;: 7,
        &quot;total_tokens&quot;: 12
    }
}
</code></pre>
<p>This is the full list of API paths:</p>
<p>Instead, you can use the URLs listed in the <a href=""https://beta.openai.com/docs/api-reference/"" rel=""noreferrer"">OpenAI documentation</a>:</p>
<ul>
<li><h4>List models</h4>
GET <code>https://api.openai.com/v1/models</code></li>
<li><h4>Retrieve model</h4>
GET <code>https://api.openai.com/v1/models/{model}</code></li>
<li><h4>Create completion</h4>
POST <code>https://api.openai.com/v1/completions</code></li>
<li><h4>Create edit</h4>
POST <code>https://api.openai.com/v1/edits</code></li>
<li><h4>Create image</h4>
POST <code>https://api.openai.com/v1/images/generations</code></li>
<li><h4>Create image edit</h4>
POST <code>https://api.openai.com/v1/images/edits</code></li>
<li><h4>Create image variation</h4>
POST <code>https://api.openai.com/v1/images/variations</code></li>
<li><h4>Create embeddings</h4>
POST <code>https://api.openai.com/v1/embeddings</code></li>
</ul>
<p>More found in the <a href=""https://beta.openai.com/docs/api-reference/"" rel=""noreferrer"">OpenAI documentation</a>.</p>
",2023-01-07 21:34:25,2.0,8.0
75051126,1,9990572.0,75104866.0,"Open AI's GPT Davinci - Asking it questions, but it's returning gibberish?",ask covid use follow code return baffle do wrong not emulate similar result chatgpt use follow nuget openai access,2023-01-08 20:41:38,,2023-01-09 09:06:01,2023-01-13 04:30:39,<c#><.net><openai-api><gpt-3>,1,1,0,358,,2.0,9990572.0,"<p>I solved this by using <code>OpenAIAPI api = new OpenAIAPI(key, &quot;text-davinci-003&quot;);</code> rather than <code>Engine.Davinci</code>.</p>
",2023-01-13 04:30:39,0.0,1.0
75176667,1,20930898.0,75182746.0,"OpenAI GPT-3 API error: ""Cannot specify both model and engine""",work python code work chatgpt do send a request a prompt get reply get errors error code api key valid json code not json code try different api key do not work look different model chatgpt do not work,2023-01-19 18:19:45,,2023-03-13 13:49:34,2023-06-23 15:22:41,<python><json><python-3.x><openai-api><gpt-3>,1,2,0,4009,,2.0,10347145.0,"<p>All <a href=""https://platform.openai.com/docs/deprecations"" rel=""nofollow noreferrer"">Engines API endpoints</a> are deprecated.</p>
<p><a href=""https://i.stack.imgur.com/lQu9c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lQu9c.png"" alt=""Deprecated"" /></a></p>
<p>Change the URL from this...</p>
<pre><code>https://api.openai.com/v1/engines/davinci/completions
</code></pre>
<p>...to this.</p>
<pre><code>https://api.openai.com/v1/completions
</code></pre>
<p>If you run <code>test.py</code> the OpenAI API will return a completion. You'll get a different completion because the <code>temperature</code> parameter is not set to <code>0</code>. I got the following completion:</p>
<blockquote>
<p>The meaning of life is to find out and fulfil the purpose and meaning...</p>
</blockquote>
<p><strong>test.py</strong></p>
<pre><code>import json
import requests
import os

data = {
    &quot;prompt&quot;: &quot;What is the meaning of life?&quot;,
    &quot;model&quot;: &quot;text-davinci-003&quot;
}

response = requests.post(&quot;https://api.openai.com/v1/completions&quot;, json=data, headers={
    &quot;Content-Type&quot;: &quot;application/json&quot;,
    &quot;Authorization&quot;: f&quot;Bearer {apikey}&quot;
})

response_json = json.loads(response.text)

print(response_json[&quot;choices&quot;][0][&quot;text&quot;])
</code></pre>
",2023-01-20 10:28:06,1.0,2.0
75210324,1,7541847.0,75210429.0,"OpenAI GPT-3 API error: ""You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth""",get error follow php code correctly provide api key get error,2023-01-23 13:39:13,,2023-03-13 13:55:37,2023-03-13 13:55:37,<php><openai-api><gpt-3>,1,0,1,1987,,2.0,10347145.0,"<p>All <a href=""https://beta.openai.com/docs/api-reference/engines"" rel=""nofollow noreferrer"">Engines endpoints</a> are deprecated.</p>
<p><a href=""https://i.stack.imgur.com/lQu9c.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/lQu9c.png"" alt=""Deprecated"" /></a></p>
<p>This is the correct <a href=""https://platform.openai.com/docs/api-reference/completions"" rel=""nofollow noreferrer"">Completions endpoint</a>:</p>
<pre><code>https://api.openai.com/v1/completions
</code></pre>
<h3>Working example</h3>
<p>If you run <code>test.php</code> the OpenAI API will return the following completion:</p>
<blockquote>
<p>string(23) &quot;</p>
<p>This is indeed a test&quot;</p>
</blockquote>
<p><strong>test.php</strong></p>
<pre><code>&lt;?php
    $ch = curl_init();

    $url = 'https://api.openai.com/v1/completions';

    $api_key = 'sk-xxxxxxxxxxxxxxxxxxxx';

    $post_fields = '{
        &quot;model&quot;: &quot;text-davinci-003&quot;,
        &quot;prompt&quot;: &quot;Say this is a test&quot;,
        &quot;max_tokens&quot;: 7,
        &quot;temperature&quot;: 0
    }';

    $header  = [
        'Content-Type: application/json',
        'Authorization: Bearer ' . $api_key
    ];

    curl_setopt($ch, CURLOPT_URL, $url);
    curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);
    curl_setopt($ch, CURLOPT_POST, 1);
    curl_setopt($ch, CURLOPT_POSTFIELDS, $post_fields);
    curl_setopt($ch, CURLOPT_HTTPHEADER, $header);

    $result = curl_exec($ch);
    if (curl_errno($ch)) {
        echo 'Error: ' . curl_error($ch);
    }
    curl_close($ch);

    $response = json_decode($result);
    var_dump($response-&gt;choices[0]-&gt;text);
?&gt;
</code></pre>
",2023-01-23 13:48:07,5.0,0.0
75237628,1,1816135.0,75249088.0,tokenizer.save_pretrained TypeError: Object of type property is not JSON serializable,try save gpt tokenizer follow get follow error typeerror object type property not json serializable detail solve issue,2023-01-25 17:21:03,,,2023-01-26 16:38:08,<python><huggingface-transformers><gpt-2>,1,3,0,124,,2.0,8395595.0,"<p>The Problem is on the line:</p>
<p><code>tokenizer.pad_token = GPT2Tokenizer.eos_token</code></p>
<p>Here the initializer is wrong, that's why this error occurred.
A simple solution is to modify this line to:
<code>tokenizer.pad_token = tokenizer.eos_token</code></p>
<p>For the reference purpose, your final code will look like this:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import GPT2Tokenizer, GPT2LMHeadModel
tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
tokenizer.pad_token = tokenizer.eos_token
dataset_file = &quot;x.csv&quot;
df = pd.read_csv(dataset_file, sep=&quot;,&quot;)
input_ids = tokenizer.batch_encode_plus(list(df[&quot;x&quot;]), max_length=1024,padding='max_length',truncation=True)[&quot;input_ids&quot;]

# saving the tokenizer
tokenizer.save_pretrained(&quot;tokenfile&quot;)
</code></pre>
",2023-01-26 16:38:08,0.0,1.0
75251168,1,10897106.0,75258801.0,BCELoss between logits and labels not working,use a gpt model output softmax shape need compare label shape calculate bceloss calculate dimension not match contract shape expand shape,2023-01-26 20:12:44,,,2023-01-27 13:32:34,<pytorch><nlp><loss-function><text-classification><gpt-2>,1,2,0,286,,2.0,5652313.0,"<p><strong>Binary cross-entropy</strong> is used when the final classification layer is a <strong>sigmoid layer</strong>, i.e., for each output dimension, only a true/false output is possible. You can imagine it as assigning some tags to the input. This also means that the <code>labels</code> need to have the same dimension as the <code>logits</code>, having 0/1 for each logit. Statistically speaking, for 592 output dimensions, you predict 592 Bernoulli (= binary) distributions. The expected shape is 32 × 56 × 592.</p>
<p>When using the <strong>softmax layer</strong>, you assume only one target class is possible; you predict a single categorical distribution over 592 possible output classes. However, in this case, the correct loss function is not binary cross-entropy but <strong>categorical cross-entropy</strong>, implemented by the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"" rel=""nofollow noreferrer""><code>CrossEntropyLoss</code></a> class in PyTorch. Note that it takes the logits directly before the softmax normalization and does the normalization internally. The expected shape is 32 × 56, as in the code snippet.</p>
",2023-01-27 13:32:34,2.0,0.0
75361743,1,7343560.0,75362164.0,How do I make an API call to GPT-3 correctly?,try make api gpt get error bad request  code,2023-02-06 13:09:22,,2023-02-06 13:14:08,2023-02-06 14:05:29,<python><python-requests><gpt-3>,1,1,0,430,,2.0,17749677.0,"<p>Try changing the url and fixing the Authorization header...</p>
<pre><code>url = &quot;https://api.openai.com/v1/completions&quot;

headers = {
    &quot;Content-Type&quot;: &quot;application/json&quot;,
    &quot;Authorization&quot;: f&quot;Bearer {api_key}&quot;
}

data = {
    &quot;model&quot;: &quot;text-davinci-002&quot;,
    &quot;prompt&quot;: &quot;Correct this to standard English : Who are you \n&quot;,
    &quot;max_tokens&quot;: 60        
}

response = requests.post(url, headers=headers, data=json.dumps(data))
response.json()
</code></pre>
",2023-02-06 13:50:01,1.0,1.0
75373129,1,14752540.0,75373214.0,"OpenAI GPT-3 API error: ""This model's maximum context length is 2049 tokens""", issue relate response result openai completion follow result do not return text a content  word prompt fix grammar mistake tokens issue second issue text double quote single quote mess json format delete type quote content not sure best solution prefer js not php,2023-02-07 12:07:58,,2023-03-13 14:10:31,2023-04-26 07:42:06,<javascript><php><json><openai-api><gpt-3>,1,3,0,1659,,2.0,10347145.0,"<h3>Regarding token limits</h3>
<p>First of all, I think you don't understand how tokens work: <strong>500 words is more than 500 tokens</strong>. Use the <a href=""https://platform.openai.com/tokenizer"" rel=""nofollow noreferrer"">Tokenizer</a> to calculate the number of tokens.</p>
<p>As stated in the official <a href=""https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them"" rel=""nofollow noreferrer"">OpenAI article</a>:</p>
<blockquote>
<p>Depending on the model used, requests can use up to <code>4097</code> tokens <strong>shared</strong>
between prompt and completion. If your prompt is <code>4000</code> tokens, your
completion can be <code>97</code> tokens at most.</p>
<p>The limit is currently a technical limitation, but there are often
creative ways to solve problems within the limit, e.g. condensing your
prompt, breaking the text into smaller pieces, etc.</p>
</blockquote>
<p>Switch <code>text-davinci-001</code> for a GPT-3 model because the token limits are higher.</p>
<p><a href=""https://platform.openai.com/docs/models/gpt-3"" rel=""nofollow noreferrer"">GPT-3 models</a>:</p>
<p><a href=""https://i.stack.imgur.com/RExzL.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RExzL.png"" alt=""Table"" /></a></p>
<br>
<h3>Regarding double quotes in JSON</h3>
<p>You can escape double quotes in JSON by using <code>\</code> in front of double quotes like this:</p>
<pre><code>&quot;This is how you can escape \&quot;double quotes\&quot; in JSON.&quot;
</code></pre>
<p>But... This is more of a quick fix. For proper solution, see @ADyson's comment above:</p>
<blockquote>
<p>Don't build your JSON by hand like that. Make a PHP object / array
with the correct structure, and then use <code>json_encode()</code> to turn it into
valid JSON, it will automatically handle any escaping etc which is
needed, and you can also use the options to tweak certain things about
the output - check the PHP documentation.</p>
</blockquote>
<hr />
<p><strong>EDIT 1</strong></p>
<p>You need to set the <a href=""https://platform.openai.com/docs/api-reference/completions/create#completions/create-max_tokens"" rel=""nofollow noreferrer""><code>max_tokens</code></a> parameter higher. Otherwise, the output will be shorter than your input. You will not get the whole fixed text back, but just a part of it.</p>
<hr />
<p><strong>EDIT 2</strong></p>
<p>Now you set the <code>max_tokens</code> parameter too high! If you set <code>max_tokens = 5000</code>, this is too much even for the most capable GPT-3 model (i.e., <code>text-davinci-003</code>). The prompt and the completion <strong>together</strong> can be <code>4097</code> tokens.</p>
<p>You can figure this out if you take a look at the error you got:</p>
<pre><code>&quot;error&quot;: {&quot;message&quot;: &quot;This model's maximum context length is 4097 tokens, however you requested 6450 tokens (1450 in your prompt; 5000 for the completion). Please reduce your prompt; or completion length.&quot;}
</code></pre>
",2023-02-07 12:15:14,10.0,3.0
75405943,1,21015092.0,75406005.0,I do not know why I can't access open ai's api for use in a react app,try access openai api a react application get unsafe header error error  time send a prompt about not provide api key provide api key a env file not know wonder exactly do wrong react function use get a unsafe header useragent error error  browser console run react app prompt get exactly wrong code hop count a minimal reproducible example pretty new stack overflow,2023-02-10 00:52:22,,2023-02-10 05:24:30,2023-02-10 05:24:30,<javascript><reactjs><openai-api><gpt-3>,1,5,-2,667,,2.0,904956.0,"<p>You should be making the request from a server not your client.</p>
<p><a href=""https://stackoverflow.com/questions/33143776/ajax-request-refused-to-set-unsafe-header"">Ajax request: Refused to set unsafe header</a></p>
<p>I highly recommend checking out Next.js 13 as it uses React under-the-hood and let's you create &quot;Server&quot; components that are essentially isomorphic.</p>
<p>Here's an example Next.js 13 <code>app/pages.tsx</code> file:</p>
<pre><code>const App = async () =&gt; {
  console.log(&quot;App.js&quot;);

  const results = await fetch(
    `http://api.weatherapi.com/v1/forecast.json?key=&lt;API_KEY&gt;&amp;q=Stockholm&amp;days=6&amp;aqi=no&amp;alerts=no`
  );

  const json = await results.json();
  console.log(&quot;json&quot;, json);

  return (
    &lt;&gt;
      &lt;h3&gt;{json.location.name}&lt;/h3&gt;
      &lt;p&gt;{json.location.temp_c}&lt;/p&gt;
      &lt;p&gt;{json.location.localtime}&lt;/p&gt;
    &lt;/&gt;
  );
};

export default App;
</code></pre>
<p><a href=""https://codesandbox.io/p/sandbox/next-js-v12-v13-weatherapi-example-4nsp1p?file=%252Fapp%252Fpage.tsx"" rel=""nofollow noreferrer"">Check out this working Next.js 13 / React18 sandbox</a> which hits the Weather API - If you'd like fork it and see if your API calls work on the server inside this <code>app.pages.tsx</code> file. Otherwise you will need to use a Firebase Function or some backend server.</p>
",2023-02-10 01:03:36,1.0,0.0
75648132,1,139150.0,75671537.0,OpenAI GPT-3 API: Why do I get only partial completion? Why is the completion cut off?,try follow code get partial result like expect json suggest page,2023-03-06 07:29:41,,2023-03-13 14:52:25,2023-03-29 08:26:43,<openai-api><gpt-3>,2,1,3,1710,,2.0,10347145.0,"<h3>In general</h3>
<p>If you get partial completion (i.e., if the completion is cut off), it's because <strong>the <code>max_tokens</code> parameter is set too low or you didn't set it at all</strong> (in this case, it defaults to <code>16</code>). You need to set it higher, <strong>but the token count of your prompt and completion together cannot exceed the model's context length</strong>.</p>
<p>See the official <a href=""https://platform.openai.com/docs/api-reference/completions/create#completions/create-max_tokens"" rel=""nofollow noreferrer"">OpenAI documentation</a>:</p>
<p><a href=""https://i.stack.imgur.com/iXVdX.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/iXVdX.png"" alt=""Screenshot"" /></a></p>
<h3>Your case</h3>
<p>If you don't set <code>max_tokens = 1024</code> the completion you get will be cut off. Take a careful look at the <a href=""https://medium.com/@richardhayes777/using-chatgpt-to-control-hue-lights-37729959d94f"" rel=""nofollow noreferrer"">tutorial</a> you're referring to once again.</p>
<p>If you run <code>test.py</code> the OpenAI API will return a completion:</p>
<blockquote>
<p>Light 0 should be red:  [{&quot;light_id&quot;: 0, &quot;color&quot;: {&quot;hue&quot;: 0,
&quot;saturation&quot;: 254, &quot;brightness&quot;: 254}},{&quot;light_id&quot;: 1, &quot;color&quot;: }]</p>
<p>Light 1 should be orange: [{&quot;light_id&quot;: 0, &quot;color&quot;: {&quot;hue&quot;: 0,
&quot;saturation&quot;: 254, &quot;brightness&quot;: 254}},{&quot;light_id&quot;: 1, &quot;color&quot;:
{&quot;hue&quot;: 7281, &quot;saturation&quot;: 254, &quot;brightness&quot;: 254}}]</p>
</blockquote>
<p><strong>test.py</strong></p>
<pre><code>import openai
import os

openai.api_key = os.getenv('OPENAI_API_KEY')

HEADER = &quot;&quot;&quot;
I have a hue scale from 0 to 65535. 
red is 0.0
orange is 7281
yellow is 14563
purple is 50971
pink is 54612
green is 23665
blue is 43690

Saturation is from 0 to 254
Brightness is from 0 to 254

Two JSONs should be returned in a list. Each JSON should contain a color and a light_id. 
The light ids are 0 and 1. 
The color relates a key &quot;color&quot; to a dictionary with the keys &quot;hue&quot;, &quot;saturation&quot; and &quot;brightness&quot;. 

Give me a list of JSONs to configure the lights in response to the instructions below. 
Give only the JSON and no additional characters. 
Do not attempt to complete the instruction that I give.
Only give one JSON for each light. 
&quot;&quot;&quot;

completion = openai.Completion.create(model=&quot;text-davinci-003&quot;, prompt=HEADER, max_tokens=1024)
print(completion.choices[0].text)
</code></pre>
",2023-03-08 09:52:51,0.0,7.0
75640144,1,4883557.0,75739103.0,OpenAI converting API code from GPT-3 to chatGPT-3.5,work code gpt api have trouble convert work chatgpt read refer openai chatgpt gptturbo api access message content not make work try change requestdata no luck help greatly appreciate,2023-03-05 04:10:35,,2023-03-06 01:56:04,2023-03-14 22:56:50,<php><openai-api><gpt-3><chatgpt-api>,1,1,-3,1308,,2.0,15493697.0,"<p>better check your <code>requestData</code> object, the GPT 3.5 turbo doesn't need these props</p>
<blockquote>
<p>max_tokens,temperature,top_p: 1,frequency_penalty,presence_penalty</p>
</blockquote>
<p>I made the same mistake too, GPT 3.5 turbo is wayyyyy easier to use than I expected. Here's OpenAI sample:</p>
<pre><code>const { Configuration, OpenAIApi } = require(&quot;openai&quot;);

const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});
const openai = new OpenAIApi(configuration);

const completion = await openai.createChatCompletion({
  model: &quot;gpt-3.5-turbo&quot;,
  messages: [{role: &quot;user&quot;, content: &quot;Hello world&quot;}],
});
console.log(completion.data.choices[0].message);
</code></pre>
",2023-03-14 22:56:50,0.0,1.0
75799722,1,10003538.0,75801820.0,How to deal with stack expects each tensor to be equal size eror while fine tuning GPT-2 model?,try fine tune a model personal information create a chat box people learn about chat gpt get error different length input  sample input try far dont a solid background ai like read reference try implement,2023-03-21 10:01:28,,2023-04-10 08:03:00,2023-04-10 08:05:32,<python><tensorflow><artificial-intelligence><huggingface-transformers><gpt-2>,2,0,2,275,,2.0,610569.0,"<p>Yes seems like you didn't pad your inputs. The model expects the size to be the same for each text. So if it's too short, you pad it, and if it's too long, it should be truncated.</p>
<p>See also</p>
<ul>
<li><a href=""https://huggingface.co/docs/transformers/pad_truncation"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/pad_truncation</a></li>
<li><a href=""https://stackoverflow.com/questions/65246703/how-does-max-length-padding-and-truncation-arguments-work-in-huggingface-bertt"">How does max_length, padding and truncation arguments work in HuggingFace&#39; BertTokenizerFast.from_pretrained(&#39;bert-base-uncased&#39;)?</a></li>
<li><a href=""https://ai.stackexchange.com/questions/37624/why-do-transformers-have-a-fixed-input-length"">https://ai.stackexchange.com/questions/37624/why-do-transformers-have-a-fixed-input-length</a></li>
</ul>
<p>Try changing how the tokenizer process the inputs:</p>
<pre><code>
# Define the data loading class
class MyDataset(Dataset):
    def __init__(self, data_path, tokenizer):
        self.data_path = data_path
        self.tokenizer = tokenizer

        with open(self.data_path, 'r') as f:
            self.data = f.read().split('\n')

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        text = self.data[index]
        inputs = self.tokenizer.encode(text, add_special_tokens=True, 
            truncation=True, max_length=80, padding=&quot;max_length&quot;)
        return torch.tensor(inputs)

</code></pre>
",2023-03-21 13:24:11,3.0,2.0
75931067,1,11669260.0,75931129.0,"Convert Python Script to Work with ""GPT-3.5-turbo"" model",follow python code work textdavinci life not work gptturbo try follow code a github repo errors fail error kindly help thank,2023-04-04 15:15:40,,,2023-04-04 15:22:56,<python><chat><openai-api><gpt-3>,1,0,0,693,,2.0,2121074.0,"<p>It sounds like you might have an old version of the OpenAI package. You could try this:</p>
<pre><code>pip install --upgrade openai
</code></pre>
",2023-04-04 15:22:56,1.0,3.0
75935538,1,4761307.0,75937188.0,"OpenAI GPT-3 API error: ""AttributeError: 'builtin_function_or_method' object has no attribute 'text'""",look help extract text chatgpt openaicompletioncreate function function use generate response article case text feed chatgpt response print provide follow output want extract text data structure run,2023-04-05 04:01:44,,2023-04-06 16:10:54,2023-04-06 16:10:54,<python><openai-api><gpt-3>,1,1,0,194,,2.0,10347145.0,"<p>Return just the <code>text</code> from the completion like this:</p>
<pre><code>def generate_keywords(article):
    response = openai.Completion.create(
        model = 'text-davinci-003',
        prompt = article,
        temperature = 0.7,
        max_tokens = 60,
        top_p = 1.0,
        frequency_penalty = 0.0,
        presence_penalty = 1
    )
    return response['choices'][0]['text'] # Change this
</code></pre>
<p>Then just print <code>keywords</code> like this:</p>
<pre><code>keywords = generate_keywords(article)
print(keywords)
</code></pre>
",2023-04-05 08:20:35,0.0,2.0
75969974,1,1335473.0,75977040.0,OpenAI GPT-3 API: Why do I get an unexpected response?,connect gpt api a jupyter notebook code attempt debug code lead play figure do wrong not make work a simple prompt want example unexpected responses get simple prompt above begin a expect a simple hi assist incorrect,2023-04-09 10:26:58,,2023-04-15 10:10:21,2023-04-15 10:10:21,<python><jupyter-notebook><openai-api><gpt-3>,1,0,-4,117,,2.0,10347145.0,"<p>You are using a very old GPT-3 <code>davinci</code> model. The performance of the OpenAI API is model-related. Newer models are more capable.</p>
<ul>
<li><code>text-davinci-003</code> &lt;-- use this one</li>
<li><code>text-davinci-002</code></li>
<li><code>davinci</code></li>
</ul>
",2023-04-10 12:16:40,0.0,1.0
76102276,1,10759664.0,76105153.0,Disable layers in GPT-2 model,currently use a gpt model train german texts like generate word a text give a context chunk instead use model predict word want  layer predict word separately  predictions word differently want lesion layer not involve prediction word model example a context chunk think maybe set attention weight  layer want exclude not a clue correct modify weight model do idea solve amp explain need never use gpt super confuse thank advance help ideas,2023-04-25 14:24:00,,,2023-04-25 20:23:46,<python><nlp><gpt-2>,2,0,1,78,,2.0,1949646.0,"<p>This is technically possibly, but probably won't give you anything useful in understanding your network. You can think of a network like this as computing
y = layer(layer(... (layer(layer(x,theta[0]),theta[1]) ...),theta[n-2]),theta[n-1]), where theta[i] are the weights of the ith layer. Setting the weights for a particular layer to 0 would make the input to layer i+1 garbage. There are residual connections between layers, so maybe something non-garbage would happen, but I wouldn't trust it to be useful.</p>
<p>Nonetheless, if you want to see what happens when you zero out all the weights for a layer, you could set weights to 0 using the model's state_dict</p>
<pre><code>from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForCausalLM
import torch
import re

# download pre-trained German GPT-2 model &amp; tokenizer
tokenizer = AutoTokenizer.from_pretrained(&quot;dbmdz/german-gpt2&quot;)

# initialise the model
model = AutoModelForCausalLM.from_pretrained(&quot;dbmdz/german-gpt2&quot;, pad_token_id = tokenizer.eos_token_id)

input_text = [&quot;Orlando liebte von Natur aus einsame Orte, weite Ausblicke und das Gefühl, für immer und ewig&quot;, # correct next word: &quot;allein&quot;
              &quot;Wo sich Fuchs und Hase gute Nacht&quot;, # correct next word sagen.
              ]
prompt = [torch.tensor(tokenizer.encode(s)).unsqueeze(0) for s in input_text]

ngenerate = 20

sample_output0 = [tokenizer.decode(model.generate(s,max_length=s.shape[-1]+ngenerate)[0,:]) for s in prompt]
print('\n***Before zeroing***')
for i,s in enumerate(sample_output0):
  print(f'{i}: {s}\n')

# zero-out layer 5

layeri = 5

# find weight names for this layer, will include the string 'transformer.h5.'
paramnames = filter(lambda s: re.search(f'transformer.h\.{layeri}\.',s) is not None,model.state_dict().keys())

# set these weights to 0
for paramname in paramnames:
  w = model.state_dict()[paramname]
  if w.ndim &gt; 0:
    w[:] = 0

# generate some sample output
print('\n***After zeroing***')
sample_output1 = [tokenizer.decode(model.generate(s,max_length=s.shape[-1]+ngenerate)[0,:]) for s in prompt]
print('Before zeroing')
for i,s in enumerate(sample_output1):
  print(f'{i}: {s}\n')
</code></pre>
<p>The output of this is:</p>
<pre><code>***Before zeroing***
0: Orlando liebte von Natur aus einsame Orte, weite Ausblicke und das Gefühl, für immer und ewig in der Nähe zu sein.
Er war ein großer Künstler, ein Künstler, der sich in der

1: Wo sich Fuchs und Hase gute Nacht sagen.
Die beiden sind seit Jahren befreundet.
Sie sind ein Paar.
Sie sind ein


***After zeroing***
Before zeroing
0: Orlando liebte von Natur aus einsame Orte, weite Ausblicke und das Gefühl, für immer und ewig zu sein.
Die Natur ist ein Paradies für sich.
Die Natur ist ein Paradies für sich

1: Wo sich Fuchs und Hase gute Nacht, die Sonne, die Sonne, die Sonne, die Sonne, die Sonne, die Sonne, die
</code></pre>
",2023-04-25 20:23:46,0.0,2.0
76173414,1,10311672.0,76173794.0,OpenAI GPT-3 API: What is the difference between davinci and text-davinci-003?,test different model openai notice not develop train a reliable response model test follow need understand difference improve responses match response use chatgpt,2023-05-04 12:51:25,,2023-05-05 21:16:38,2023-05-05 22:22:46,<openai-api><gpt-3>,2,0,2,1519,,2.0,10347145.0,"<p>TL;DR</p>
<ul>
<li><code>text-davinci-003</code> is the newer and more capable model</li>
<li><code>text-davinci-003</code> supports a longer context window (i.e., 4097 tokens)</li>
<li><code>text-davinci-003</code> was trained on a more recent dataset</li>
<li><code>gpt-3.5-turbo</code> and <code>gpt-4</code> are even more capable than <code>text-davinci-003</code></li>
</ul>
<p>As stated in the official <a href=""https://help.openai.com/en/articles/6643408-how-do-davinci-and-text-davinci-003-differ"" rel=""nofollow noreferrer"">OpenAI article</a>:</p>
<blockquote>
<p>While both <code>davinci</code> and <code>text-davinci-003</code> are powerful <a href=""https://platform.openai.com/docs/models"" rel=""nofollow noreferrer"">models</a>, they
differ in a few key ways.</p>
<p><code>text-davinci-003</code> is the newer and more capable model, designed
specifically for <a href=""https://openai.com/research/instruction-following"" rel=""nofollow noreferrer"">instruction-following</a> tasks. This enables it to
respond concisely and more accurately - even in zero-shot scenarios,
i.e. without the need for any examples given in the prompt. <code>davinci</code>,
on the other hand, can be fine-tuned on a specific task, which can
make it very effective if you have access to at least a few hundred
training examples.</p>
<p>Additionally, <code>text-davinci-003</code> supports a longer context window (max
prompt+completion length) than davinci - 4097 tokens compared to
<code>davinci</code>'s 2049.</p>
<p>Finally, <code>text-davinci-003</code> was trained on a more recent dataset,
containing data up to June 2021. These updates, along with its support
for <a href=""https://platform.openai.com/docs/guides/completion/inserting-text"" rel=""nofollow noreferrer"">Inserting text</a>, make <code>text-davinci-003</code> a particularly versatile and
powerful model we recommend for most use-cases.</p>
</blockquote>
<p>Use <code>text-davinci-003</code> because other models you mentioned in your question are less capable.</p>
<p>ChatGPT uses <code>text-davinci-002</code> at the moment for non-subscribed users. If you buy a ChatGPT Plus subscription, you can also use <code>gpt-3.5-turbo</code> or <code>gpt-4</code>. So, to get similar responses as you get from ChatGPT, it depends on whether you are subscribed or not. For sure, <code>gpt-3.5-turbo</code> and <code>gpt-4</code> are even more capable than <code>text-davinci-003</code>.</p>
",2023-05-04 13:32:39,0.0,1.0
76197173,1,15246353.0,76197360.0,"Chrome Extension with Chat GPT-3.5 - ""you must provide a model parameter""",make a chrome extension use chat gpt  cod a simple prompt send api use openai api return a value console code get error event a model parameter,2023-05-08 02:29:40,,,2023-05-08 03:35:25,<javascript><google-chrome-extension><openai-api><gpt-3>,1,1,3,320,,2.0,21148174.0,"<p>I used your code and experienced the same error. I investigated the network request and saw that the payload was malformed:</p>
<pre><code>{
   prompt: {
      model: &quot;gpt-3.5-turbo&quot;, 
      prompt: &quot;Hello, world!&quot;, 
      temperature: 0.5}, 
   max_completions: 1
}
</code></pre>
<p>So, as far as ChatGPT's api is concerned, you're only sending <code>prompt</code> and <code>max_completions</code>. The reason you request is formed this way is because you're passing an object filled with other objects into <code>JSON.stringify()</code>.</p>
<p>Also, I'm not sure where you're getting the <code>max_completions</code> property from, as it is not in the API doc, so I left that out. Here is the change you need to make:</p>
<pre><code>// in your fetch call:
fetch(endpointUrl, {
  method: &quot;POST&quot;,
  headers,
  body: JSON.stringify(prompt), // prompt is an object, so no need to wrap it in another object.
}).then...
</code></pre>
<p>Another issue is that you're calling the <a href=""https://platform.openai.com/docs/api-reference/chat/create"" rel=""nofollow noreferrer"">create chat completion</a> endpoint, but the properties you are sending are not correct. You are required to send:</p>
<ul>
<li><strong>model</strong>: string</li>
<li><strong>messages</strong>: [{role: string, content: string}]</li>
</ul>
<p>So, you would also need to make an edit here:</p>
<pre><code>// in your prompt variable:
const prompt = {
   model: &quot;gpt-3.5-turbo&quot;,
   messages: [{role: &quot;user&quot;, content: &quot;Hello, World!&quot;}], // change prompt to messages array
   temperature: 0.5,
};
</code></pre>
<p>Cheers!</p>
",2023-05-08 03:35:25,1.0,1.0
76482024,1,10760045.0,76483595.0,How to get more detailed results sources with Langchain,try a simple qa source use langchain a specific url source data url consist a single page quite a lot information problem give entire url source result not useful case a way detail source info head specific section page a clickable url correct section page helpful slightly unsure generate a function language model url loader simply try use hope detail read input data help sadly not relevant code excerpt,2023-06-15 11:31:53,,2023-06-15 13:52:57,2023-06-15 16:12:45,<python><openai-api><gpt-3><langchain><chatgpt-api>,1,2,1,329,,2.0,2392087.0,"<p>ChatGPT is very flexible, and the more explicit you are better results you can get. <a href=""https://python.langchain.com/en/latest/reference/modules/chains.html#langchain.chains.ConstitutionalChain"" rel=""nofollow noreferrer"">This link show the docs for the function you are using</a>. there is a parameter for langchain.prompts.BasePromptTemplate that allows you to give ChatGPT more explicit instructions.</p>
<p>It looks like the base prompt template is this</p>
<blockquote>
<p>Use the following knowledge triplets to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n{context}\n\nQuestion: {question}\nHelpful Answer:</p>
</blockquote>
<p>You can add in another sentence giving ChatGPT more clear instructions</p>
<blockquote>
<p>Please format the answer with JSON of the form { &quot;answer&quot;: &quot;{your_answer}&quot;, &quot;relevant_quotes&quot;: [&quot;list of quotes&quot;] }. Substitutde your_answer as the answer to the question, but also include relevant quotes from the source material in the list.</p>
</blockquote>
<p>You may need to tweak it a little bit to get ChatGPT responding well. Then you should be able to parse it.</p>
<p>ChatGPT has 3 message types in the API</p>
<ul>
<li>User - a message from an end user to the model</li>
<li>model - a message from the model to the end user</li>
<li>system - a message from the prompt engineer to model to add instructions. Lang chain doesn't use this since it's a one-shot prompt</li>
</ul>
<p>I strongly recommend these <a href=""https://www.deeplearning.ai/short-courses/"" rel=""nofollow noreferrer"">courses</a> on ChatGPT since they are from Andrew Ng and very high quality.</p>
",2023-06-15 16:12:45,1.0,0.0
75777566,1,12935415.0,,ChatGPT: How to use long texts of unknown content in a prompt?,like website chatpdfcom a lot upload a pdf file discuss textual content file file use chatgpt like program similar wonder use content long pdf file a chatgpt prompt chatgpt accept  tokens conversation reduce number tokens need important consider unknown document use ant goal not summarize document conversation about content test  page pdf file  word try delete less important word string fee prompt lead decrease   tokens accord openai ’ s tiktoken library try mask word unk tag lead increase  tokens,2023-03-18 17:33:41,,,2023-03-18 23:23:36,<token><chatbot><tokenize><openai-api><chatgpt-api>,2,3,-1,1101,,,,,,,
75828282,1,2013689.0,,How to format a JSON array?,try add chat gpt minecraft villagers a web request send denizen plugin denizen script language access completion api code work try access chat api code do not api return a  error state line definemap data model gptturbo return correct model lead believe message parameter line correct try variation quoteunquotecurlysquare bracket think read role user content hello array format line,2023-03-23 21:51:10,,,2023-03-24 09:55:37,<json><minecraft><openai-api><chatgpt-api>,0,0,0,75,,,,,,,
75829543,1,21477627.0,,How to loop through nested (two dimensional?) arrays in python,currently build a bot grab user media a web app take caption media comment respective post a summary caption use chatgpt grab media web app return a dimensional nest array dimension array contain post second dimension array contain information about post show problem believe have loop loop not just dimension array caption item array array loop input output time instead just loop post post just loop infinitely stop set a maximum number tokens chatgpt api code use loop output print request believe reason end set a maximum token chatgpt api actually just indefinitely do not set help go crazy,2023-03-24 02:24:55,,,2023-03-24 14:27:59,<python><for-loop><openai-api><chatgpt-api>,1,1,0,49,,,,,,,
75849897,1,19494077.0,,Format code block or code lines in ChatGPT,possible format a code block a code line chatgpt like do ask tell  backticks begin end doesnt work try format code block accord chatgpt show exaclty try without format,2023-03-26 18:58:18,,,2023-03-26 18:58:18,<formatting><format><codeblocks><openai-api><chatgpt-api>,0,1,0,703,,,,,,,
75714108,1,19252320.0,,"Uncaught (in promise) SyntaxError: Unexpected token '<', ""<!DOCTYPE ""... is not valid JSON and internal server 500",get internal  server error uncaught promise syntaxerror unexpected token doctype not valid json face error request fail status code  generate end point chatgpt completion do not understand error expect true response,2023-03-12 15:12:31,,2023-03-12 15:13:14,2023-03-12 15:13:14,<next.js><fetch><openai-api><chatgpt-api>,0,2,0,269,,,,,,,
76006647,1,4253946.0,,Payload clarification for Langchain Embeddings with OpenAI and Chroma,create follow piece code use jupyter notebook case come code take a csv file load chroma use openai embeddings csv jupyter notebook check request use wireshark obtain follow request reply want understand happen scenes kind debug useful troubleshoot payload input field a matrix number do not make sense do not match documentation question do input field matrix number decode answer not create vector suppose receive decode embed field answer use base look like python client openai use older version api reason do not use api chatgpt mention do not provide reference like,2023-04-13 14:29:59,,2023-04-16 11:31:07,2023-04-16 11:31:07,<python><wireshark><openai-api><chatgpt-api><langchain>,0,0,1,748,,,,,,,
76028346,1,14302879.0,,How to parse the OpenAIStream so there are no spaces and lists are correctly formatted?,openai api set stream response work problem result extra space format not work eliminate space issue nor parse list code correctly parse chunk receive message component react app,2023-04-16 14:30:04,,2023-04-17 00:04:28,2023-04-17 00:04:28,<streaming><openai-api><chatgpt-api>,0,0,1,174,,,,,,,
76037154,1,21374700.0,,Unable to use Llama Index with AWS Lambda,use llama index create a custom bot use aws lambda try create layer upload aws layer get package not errors different libraries try delete package llama index package use aws package like pandas numpy get a max size error create package use wsl ubuntu error get try use layer sample code try fix issue remove libraries llama index package use aws custom libraries errors keep occur error numpy add aws numpy package fix get error pandas try use aws pandas library try use numexpr start get a max size layer error explanations llamaindex package originally contain numpy package encounter error mention question remove numpy llamaindex package try use awslambdapythonscipyx package aws layer fix numpy error a new error appear no module name pandas_libsinterval resolve add awspandassdk layer aws combine layer size exceed aws limit overcome limitation remove previously add numpy package add awspandassdk layer instead fix numpy pandas issue encounter a new error no module name numexpr libraries present aws llamaindex package,2023-04-17 16:10:17,,2023-04-17 16:33:15,2023-04-17 16:33:15,<python><aws-lambda><openai-api><chatgpt-api><llama-index>,0,4,1,381,,,,,,,
76044299,1,14811811.0,,How can I use the CHATGPT chat completion API in Java to retrieve previous context in a new question and obtain the corresponding result?,use chatgpt chat completion api java retrieve previous context a new question review previous context provide a summary obtain correspond result use api use chat completion api use edit api response,2023-04-18 11:56:14,,2023-04-18 15:27:46,2023-05-04 05:59:51,<openai-api><chatgpt-api>,1,0,-1,371,,,,,,,
76057076,1,4222261.0,,How to stream Agent's response in Langchain?,use langchain gradio interface python a conversational agent try stream responses gradio chatbot interface a look langchain docs not example implement stream agents part code currently stream terminal output look stream gradio interface help,2023-04-19 16:58:22,,2023-04-19 17:10:30,2023-06-02 03:37:25,<python><chatgpt-api><gradio><langchain>,2,0,6,2606,,,,,,,
76063058,1,21146487.0,,How to seperate data for multiple chatbots in pinecone vector database service?,build a platform users upload custom data build a chatbot think use lanchain open ai embeddings chat gpt api pinecone manage service check pinecone documentation unable figure organise database different chatbots mean different data set single chatbot a different index multiple index store a single pod index store a seperate pod,2023-04-20 10:25:23,,,2023-04-20 10:25:23,<chatgpt-api><langchain><vector-database>,0,0,1,208,,,,,,,
76063600,1,14912240.0,,How do we call AzureOpenAI Chat Playground through HTTP method,create a flow power automate call azureopenai chat playground http post method run say solve issue,2023-04-20 11:34:41,,2023-04-21 08:23:45,2023-05-31 11:00:55,<azure><power-automate><chatgpt-api><azure-openai>,1,1,0,45,,,,,,,
76070777,1,2123099.0,,PowerBI Custom Visual with ChatGPT,develop a custom visual power bi use typescript input type text user input prompt input type text chatgpt answer idea user ask about report data report visual answer visual current stage look like scenes user prompt send azureopenai service process chatgpt deployment response miss able pass report data see a similar video do powerautomate visual video video able pass report data power automate visual user prompt order analyze question data manage pass visual data a structure json format prompt work question possible report data typescript custom visual without have dataset visual self try a library call powerbi client inside custom visual use library visual stop work think use powerbi embed base article not possible use a custom visual access data page report scope level ideas,2023-04-21 07:13:03,,2023-05-02 09:39:38,2023-05-02 09:39:38,<typescript><powerbi><openai-api><powerbi-custom-visuals><chatgpt-api>,0,0,1,172,,,,,,,
76197116,1,21847272.0,,OpenAI API: AxiosError: Request failed with status code 401,try use openai api generate recipes a website get error title code get api key account not figure api key not work try regenerate key,2023-05-08 02:12:48,,2023-05-08 02:13:22,2023-05-08 02:13:22,<javascript><typescript><openai-api><chatgpt-api>,0,0,1,137,,,,,,,
76198077,1,15708401.0,,Nodejs OpenAI ChatGPT API error 400 without error,try use official openai nodejs backend get  error check api key get error code request point error,2023-05-08 06:46:10,,2023-05-09 19:52:08,2023-05-09 19:52:08,<node.js><openai-api><chatgpt-api><chatgpt-plugin>,0,5,1,169,,,,,,,
76205337,1,2056005.0,,Completion using OpenAI and Php | Chatgpt,able openai completion dunno correct btw work want thing chatgpt variations  cod chatgpt give ask chatgpt a summary give answer ask tell parameters use work playground try use php do not work anymore basically return text try configurations tokens temperatures search site try things basically try tl dr end text length  text length  tokens vary   result use php text start summarise text,2023-05-09 01:43:07,,,2023-05-09 01:43:07,<php><openai-api><chatgpt-api>,0,1,0,56,,,,,,,
76212050,1,21864076.0,,Markdown or formatting text in ChatGPT response,just confirm use api chat completion response plain text format text response least new line table bullet point head like,2023-05-09 17:36:19,,,2023-05-09 17:36:19,<openai-api><chatgpt-api>,0,0,3,751,,,,,,,
76230979,1,6591677.0,,OpenAI turbo-3.5 API returning error with complex prompts,simple prompt like hey tell summarize work fine run complex prompt like list explain break no change complexity error message run jupyterlab really appreciate help,2023-05-11 19:19:02,,,2023-05-11 19:19:02,<python><python-3.x><openai-api><chatgpt-api>,0,3,0,140,,,,,,,
76273237,1,21914090.0,,Langchain and Github - Converting Code File to Text!! I'm doing right?,basically try a code allow script c a repository github use langchain enter bot query things project far able fetch script repository use loader able convert code text generate chunk text generate use llama index langchain enter image description try run follow query follow error solve text data use text data api,2023-05-17 14:30:59,,,2023-05-26 14:54:41,<python><chatgpt-api><langchain>,1,1,0,343,,,,,,,
76288488,1,19480934.0,,Error when using Streamlit and Langchain to build an online AutoGPT app,error try use langchain streamlit build online autogpt app point right direction appreciate best david,2023-05-19 11:09:57,,,2023-05-19 11:16:59,<python><streamlit><chatgpt-api><langchain><autogpt>,1,0,0,121,,,,,,,
76288648,1,20434115.0,,Why I got this error: POST https://api.openai.com/v1/chat/completions 400?,code get error post  use function,2023-05-19 11:32:30,,,2023-05-22 10:16:11,<json><api><openai-api><chatgpt-api>,1,3,-2,155,,,,,,,
76288897,1,21919091.0,,REACT NATIVE - Anyone know how to use Streaming for the ChatGPT 3.5/4 API in React Native?,know implement stream feature gptturbo api work code release entire answer look client unless absolutely set a server,2023-05-19 12:06:00,,,2023-06-20 20:13:35,<react-native><openai-api><chatgpt-api>,0,0,0,35,,,,,,,
76295530,1,2008597.0,,"In llamaindex / gptindex, how do i control number of responses to a query",follow code  responses right now query a parameter change,2023-05-20 13:41:21,,,2023-05-20 13:41:21,<chatgpt-api><langchain><gpt-index>,0,2,0,106,,,,,,,
76323622,1,9285078.0,,How to send json data created by nextjs api to a new route?,a complete web development noob a bite new javascript try make a quizlet style website users input subject specific topic not good practice question render a flashcardjs file write api nextjs generate practise question know api work print question not figure parse data flashcardjs file handler api look like like try use userouter not use handler sort open ideas stick months help greatly appreciate,2023-05-24 12:36:35,,,2023-05-24 12:56:04,<json><next.js><api-design><openai-api><chatgpt-api>,1,0,0,33,,,,,,,
76331009,1,21959101.0,,I have usede GPT generated API queries to fetch JSON data from the JIRA API,code use fetch json data display a flask base web app generate api query correct check use postman fetch json data relevant api query generate api query json response get json body api query use postman reason above help clarify,2023-05-25 09:54:23,,2023-05-25 12:01:16,2023-05-25 12:01:16,<jira><jira-rest-api><python-jira><chatgpt-api>,0,11,-2,80,,,,,,,
76337276,1,21963475.0,,How do I resolve the 'Import openai could not be resolved' error in Visual Studio Code when creating a custom chat GPT bot?,try make a custom chatgpt bot visual studio code get error import openai not resolve pylance reportmissingimports error code reportmissingimports boolean string optional generate suppress diagnostics import no correspond import python file type stub file default value set error relatively new cod a nudge right direction try switch file a custom file download file do not work try open a terminal type do not work,2023-05-26 02:18:45,,2023-05-26 02:21:58,2023-05-26 02:21:58,<python><chatgpt-api>,0,6,0,69,,,,,,,
76342525,1,21961201.0,,Re-training OpenAI Fine-Tuned Chat-Bot Model: From Scratch or Can I Add New Data?,currently work a finetuned model a partner ask build website order use a ai power chatbot question model build retrain new information website products change train model old new data set data preparation train model,2023-05-26 16:02:30,,,2023-05-26 16:02:30,<python><chatbot><openai-api><chatgpt-api>,0,1,0,20,,,,,,,
76363018,1,4001754.0,,Preventing omitted text in translations due to OpenAI server errors,best approach handle openai errors result server load issue develop a small translation plugin wordpress never mind issue main issue sheer errors message server error process request sorry about best approach handle errors give fact not want translations riddle omit text course try result fail attempt best think build error handler do exactly like not really help openai migitate load thoughts save omit block till end try fix hardly different not quite not error list possible errors probably just inability read documentation properly,2023-05-30 08:36:45,,,2023-05-30 08:36:45,<openai-api><chatgpt-api>,0,0,0,30,,,,,,,
75920597,1,21557233.0,,openai.error.APIConnectionError: Error communicating with OpenAI,project run code return try work hope use async make,2023-04-03 14:29:35,,2023-04-03 14:32:24,2023-06-21 02:52:28,<python><asynchronous><embedding><openai-api><chatgpt-api>,1,0,2,1930,,,,,,,
75956610,1,12138506.0,,Running Databricks Dolly locally on my Mac M1,try deploy run databricks dolly a latest release opensource llm model alternate option gpt doc try run hug dace transformers code get follow error assertionerror torch not compile cuda enable look internet pytorch support cuda x_ architectures cuda support not available apple m macs shoud,2023-04-07 08:07:30,,,2023-04-16 06:06:56,<python><open-source><chatgpt-api><alpaca>,2,0,0,1681,,,,,,,
75971578,1,21603412.0,,"OpenAI ChatGPT (GPT-3.5) API error: ""'messages' is a required property"" when testing the API with Postman",successfully a completion model try postman post body headers follow error,2023-04-09 16:15:22,,2023-04-24 08:25:13,2023-05-24 11:19:18,<openai-api><chatgpt-api>,2,0,0,2485,,,,,,,
75987872,1,14495002.0,,How does LlaMA index select nodes based on the query text?,query a simple vector index create use a llama index return a json object response query source nod score use generate answer do calculate nod use guess semantic search a way just return nod do not use openai api cost money use gptturbo answer query try search llama index documentation not,2023-04-11 15:55:29,,2023-04-16 14:06:55,2023-05-24 00:14:35,<openai-api><chatgpt-api><langchain><llama-index>,1,0,3,686,,,,,,,
76112949,1,12108041.0,,ChatCompletion function gone from openai module,run code use openaichatcompletion give error run check modules give no sight chatcompletion use latest version openai module  uninstalled reinstall module pip chatcompletion not,2023-04-26 16:12:57,,,2023-06-18 22:47:01,<python><module><openai-api><chatgpt-api>,3,0,0,445,,,,,,,
76377384,1,17281101.0,,how do i have different chats in chatgpt API?,not sure possible different chat a api chatgpt goal a user a conversation chatgpt user access conversation issue differentiate users moment help try look docs stackoverflow use chatgpt nothing work differentiate chat code work fine,2023-05-31 21:20:29,,,2023-06-05 11:36:59,<flutter><http><openai-api><chatgpt-api>,0,1,0,60,,,,,,,
76382423,1,22000747.0,,Creating a Chatbot using the data stored in my huge database,want build a custom chat bot answer question base data databse try problems face open suggestions help try without use langchain code establish a connection a postgresql database prompt user information want obtain generate sql query base user input table name db use openai gpt language model code extract table name generate query fetch column information connect database generate a prompt include table name column detail use gpt model generate a final sql query base prompt final sql query execute database result print currently overall code utilize natural language process database interactions generate execute sql query base user input try use langchain problem face prompt size get  tokens unable figure happen try custom prompt templates do not decrease prompt size felt just add custom prompt data prompt send open ai api instead just send custom prompt help way pls methods see know fine tune embeddings know fine tune use open ai don ’ t idea embeddings want know better use far search come know case database information not secure think organization user privacy risk finally a better way build a bot answer question base information db,2023-06-01 13:29:24,,,2023-06-01 13:29:24,<python><python-3.x><openai-api><chatgpt-api><azure-openai>,0,1,-1,120,,,,,,,
76387712,1,11910879.0,,"How to generate dialogflow intent, training phrases and text response from the given document using LLM and chatgpt",work a project scrape user website scrap content need generate personalise dialogflow intent train phrase text response use llm pinecone chatgpt right now generate intent train phrase text response content text response generalise not personalise need give source way generate personalise text response intent chatgpt pinecone llm,2023-06-02 06:40:50,,,2023-06-02 06:40:50,<dialogflow-es><dialogflow-cx><chatgpt-api><vector-database>,0,0,0,33,,,,,,,
76407244,1,22006119.0,,How to support OpenAI's Chat Completions API format in LlamaIndex?,currently use llamaindex a project try a way support complex prompt format use openai chat completions api chat engine llamaindex openai api use a list message prompt message a role ystem user assistant content text message example use function llamaindex documentation parameter do not support context string format limitation affect ability complex interactions model especially conversational ai applications do experience issue provide guidance support openai chat completions api format llamaindex help greatly appreciate,2023-06-05 14:05:12,,,2023-06-05 14:05:12,<openai-api><chatgpt-api><llama-index><gpt-index>,0,0,0,84,,,,,,,
76407415,1,9087175.0,,How to create a multi-user chatbot with langchain,hope do good ’ ve prepare a chatbot base langchain documentation langchain chatbot documentation above langchain documenation prompt template input variables history human input ’ ve variables userid sessionid ’ m store userid sessionid usermessage llmresponse a csv file use python pandas module read csv filter data frame give userid sessionid prepare chathistory specific user session ’ m pass chathistory ‘history ’ input langchain prompt template discuss above link set verbosetrue langchain print prompt template console api ’ ve start conversation user session send  human_inputs later start second user session now session id user id change observe prompt template console ’ ve observe langchain not take chathistory second user session ’ s take chathistory previous user session ’ ve write correct code prepare chathistory give user session code chathistory teach langchain consider give user session chathistory ’ s prompt help,2023-06-05 14:24:00,,,2023-06-09 08:01:21,<chatbot><openai-api><chatgpt-api><langchain><llm>,1,0,0,253,,,,,,,
76420235,1,16640995.0,,Laravel OpenAI client doesn't work with List result,try request throw openai laravel library model gptturbo result list library throw exception try request test,2023-06-07 05:53:17,,2023-06-08 06:33:06,2023-06-08 06:33:06,<php><laravel><openai-api><chatgpt-api>,0,2,0,42,,,,,,,
76432106,1,3232335.0,,Extracting and Displaying Prompts Sent to OpenAI API via Various Frameworks,currently work debug application use langchain library a pythonbased language model libraryframework application use openai python client library send request openai api debug process want view raw prompt generate application send openai library subsequently request library assume prompt generate a method function langchain library unsure access print prompt review interest a general approach allow extract display prompt send openai api application regardless underlie framework particularly useful develop future applications use different frameworks langchain leverage openai library suggest effective ways achieve goals possible modify openai python library use tool like wireshark fiddler python log library intercept http request view prompt look approach comprehensive compliant openai usage policies help greatly appreciate addition above like share approach attempt extract prompt openai library debuglevel log log entries look like parse log implement a python script follow order capture debuglevel log modify application log settings follow finally start application commandline use feel approach shortcomings introduce a significant additional code require modifications original application code adjust log level commandline invocation complex hard manage give challenge look alternative ways achieve goal suggestions improvements approach welcome,2023-06-08 12:59:06,,,2023-06-08 12:59:06,<python><openai-api><chatgpt-api><langchain>,0,0,0,51,,,,,,,
76475956,1,22069606.0,,Implementing ChatGPT Prompts for Efficient and Creative Output,experiment openai chatgpt find quite impressive tool text generation efficiency output creativity involve largely influence prompt design use understand prompt engineer play a significant role determine usefulness output chatgpt like a trial error process come best prompt different type task recently come across a platform name flowgpt a usergenerated content platform chatgpt prompt users share rate different prompt design various use case interest resource wonder use platform experience integrate prompt project better result resources strategies recommend effective prompt engineer chatgpt chatgpt promopt expect better,2023-06-14 17:01:44,,,2023-06-14 17:01:44,<openai-api><chatgpt-api><chatgpt-plugin>,0,0,-1,23,,,,,,,
76506664,1,13860286.0,,How to feed data as a preserved info in open AI chat-completions?,use openai chatcompletionsapi want create a software preserve data augment knowledge reproduce text not simply mean a chat history a data structure specific entity like a json data a return query a database need use info message limit course not know data example want suppose employee employee specific data years experience role a company salary accord info background want software able create a suitable response question possible simple request documentation follow,2023-06-19 12:32:10,,2023-06-20 13:05:02,2023-06-20 13:05:02,<openai-api><chatgpt-api>,0,2,0,15,,,,,,,
76513288,1,17759509.0,,Too many requests in 1 hour. Try again later,open use a minutes error solve problem,2023-06-20 09:30:17,,,2023-06-20 09:30:17,<openai-api><chatgpt-api>,0,0,-3,19,,,,,,,
76537595,1,1123140.0,,how to use chatgpt in visual studio to fix buggy code?,know a chatgpt extension vs code want a similar functionality visual studio  extension vs,2023-06-23 06:29:34,,,2023-06-23 06:29:34,<visual-studio><chatgpt-api>,0,0,-3,16,,,,,,,
76543581,1,19270319.0,,What is the easiest way to integrate chatgpt api in Laravel 10?,make a todo list app school thesis want add ai functionality give a way task efficient possible view try display chatgpt response a textbox controller route want click chatbtn send prompt return response display textbox a better way a simpler way like hear long work free suffer,2023-06-23 21:30:38,,,2023-06-23 21:30:38,<javascript><php><openai-api><laravel-10><chatgpt-api>,0,0,0,21,,,,,,,
76548062,1,6641693.0,,Grouping Keywords based on semantic value of the word,try a way grouop multiple keywords base mean possible relation group base semantics word not word similarity not know solutions use word similarity try chatgpt do group a sample list correctly keep forget sample multiple occasion make response unreliable issue use chatgpt directly service expose apis not useful option right now run opensource llm locally reliably use group keywords decide group add keyword bffore like insight similar issue solutions applicable opensource solution hemp exmaple want,2023-06-24 21:31:00,,,2023-06-24 21:31:00,<keyword><chatgpt-api><llm>,0,0,0,7,,,,,,,
76438937,1,22046807.0,,What is the process to get access of ChatGPT API?,do a mini project generate synthetic data use chatgpt python try chatgpt api access flask framework get stick kindly guide process stick begin,2023-06-09 09:25:44,,,2023-06-09 13:31:59,<python><flask><openai-api><chatgpt-api>,1,2,-3,36,,,,,,,
76444688,1,6144372.0,,"How can I add the ""system"" message into my prompt?",try integrate app maintain chat history use generate boilerplate code download code edit successfully integrate model maintain chat history unable figure add message api code function code function submit button function function try add message function like return a error code indicate,2023-06-10 04:19:52,,,2023-06-18 23:21:31,<flutter><chatgpt-api><flutterflow>,3,1,2,181,,,,,,,
75647638,1,1872194.0,,How to send longer text inputs to ChatGPT API?,a use case chatgpt summarize long piece text speechtotext conversations hour k token limit tend lead a truncation input text say half token limit process part do not retain history previous part options submit a longer request k tokens,2023-03-06 06:23:36,,,2023-05-27 08:15:45,<openai-api><chatgpt-api>,4,5,17,22641,,,,,,,
75622285,1,19315721.0,75626662.0,"OpenAI ChatGPT (GPT-3.5) API error: ""openai.createChatCompletion is not a function""",mern stack code file work change api request use new api chat completion do not work api code openai website work postman error message post  not,2023-03-03 00:59:29,,2023-03-21 17:56:05,2023-06-02 10:20:34,<axios><openai-api><chatgpt-api>,2,0,5,3658,,2.0,8949058.0,"<p>You need to reinstall the openai npm package. It has only just been updated with the createChatCompletion in the past 2 days.</p>
<p>When I reinstalled the package and ran your code it worked successfully.</p>
",2023-03-03 11:43:57,1.0,3.0
75650840,1,2323372.0,75650860.0,"OpenAI ChatGPT (GPT-3.5) API error 400: ""Bad Request"" (migrating from GPT-3 API to GPT-3.5 API)",try gotturbo api just release chatgpt get a bad request error code api documentation sample im get error edit error message,2023-03-06 12:26:59,,2023-03-21 17:55:41,2023-05-21 15:51:44,<post><openai-api><chatgpt-api>,3,0,3,5948,,2.0,10347145.0,"<p>You're using the <code>gpt-3.5-turbo</code> model.</p>
<p>There are three main differences between the ChatGPT API (i.e., the GPT-3.5 API) and the GPT-3 API:</p>
<ol>
<li>API endpoint
<ul>
<li>GPT-3 API: <code>https://api.openai.com/v1/completions</code></li>
<li>ChatGPT API: <code>https://api.openai.com/v1/chat/completions</code></li>
</ul>
</li>
<li>The <code>prompt</code> parameter (GPT-3 API) is replaced by the <code>messages</code> parameter (ChatGPT API)</li>
<li>Response access
<ul>
<li>GPT-3 API: <BR><code>response.getJSONArray(&quot;choices&quot;).getJSONObject(0).getString(&quot;text&quot;)</code></li>
<li>ChatGPT API: <code>response.getJSONArray(&quot;choices&quot;).getJSONObject(0).getString(&quot;message&quot;)</code></li>
</ul>
</li>
</ol>
<br>
<p><strong>PROBLEM 1: You're using the wrong API endpoint</strong></p>
<p>Change this...</p>
<pre><code>https://api.openai.com/v1/completions
</code></pre>
<p>...to this.</p>
<pre><code>https://api.openai.com/v1/chat/completions
</code></pre>
<br>
<p><strong>PROBLEM 2: Make sure the JSON for the <code>messages</code> parameter is valid</strong></p>
<ul>
<li><p>cURL: <code>&quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}]</code></p>
</li>
<li><p>Python: <code>messages = [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello!&quot;}]</code></p>
</li>
<li><p>NodeJS: <code>messages: [{role: &quot;user&quot;, content: &quot;Hello world&quot;}]</code></p>
</li>
</ul>
<br>
<p><strong>PROBLEM 3: You're accessing the response incorrectly</strong></p>
<p>Change this...</p>
<pre><code>string choices = responseObj.choices[0].text;
</code></pre>
<p>...to this.</p>
<pre><code>string choices = responseObj.choices[0].message.content;
</code></pre>
<br>
<p><strong>PROBLEM 4: You didn't set the <code>Content-Type</code> header</strong></p>
<p>Add this:</p>
<pre><code>requestMessage.Headers.Add(&quot;Content-Type&quot;, &quot;application/json&quot;);
</code></pre>
<p>Be careful, <code>&quot;application/json, UTF-8&quot;</code> won't work like @Srishti mentioned in the comment below.</p>
",2023-03-06 12:29:19,8.0,5.0
76432637,1,5753925.0,76442292.0,VS Code: Azure workspace folders do not appear after I run a function,vs code windows  use python  run a function just return paths terminal stack appear instant leave margin disappear no local workspace resources exist appear workspace folder run function time previous days without issue want deploy function not nothing workspaces image a screenshot do run function,2023-06-08 13:58:16,,2023-06-08 14:10:48,2023-06-09 16:52:17,<azure><visual-studio-code><openai-api><chatgpt-api>,1,2,0,73,,2.0,17623802.0,"<ul>
<li>Open <strong>BasicAzureFunction</strong> in an <strong>Integrated terminal</strong> in VS Code and run &gt; <strong>func host start</strong> in your terminal for your Function to run.</li>
<li>Or else, Just open BasicAzureFunction folder in VS Code &gt; And click on fn + f5 or Run &gt; start Debugging.</li>
<li>Your function will be triggered&gt; Do not open entire Udemy folder, Just open BasicAzureFunction in your Vs Code.</li>
<li>If that doesnot work run the function from the child folder where the function is present and then run function from <strong>BasicAzureFunction</strong> and it will run the function.</li>
</ul>
",2023-06-09 16:52:17,0.0,1.0
75717683,1,21386362.0,,How to get data back from OpenAI API using JavaScript and display it on my website,a simple form website text input want make a openai api ask chatgpt similar company base a job description a user paste text box far not able return data work correctly send job description data not able list a list company fix,2023-03-13 02:48:53,,2023-04-18 23:03:12,2023-04-18 23:03:12,<javascript><openai-api><chatgpt-api>,2,2,0,520,,,,,,,
75768676,1,618067.0,,Classification with ChatGPT: how to match bank/credit card transactions with the accounting expense category,a client manage book hundreds small medium size businesses staff bank credit card statements match transactions a specific list expense categories example transaction description match try a things not get good result good ideas,2023-03-17 14:17:17,,2023-03-18 16:06:17,2023-03-18 16:06:17,<chatgpt-api>,0,3,0,177,,,,,,,
75774552,1,2396198.0,,Problems updating my code from text-davinci-003 to gpt-3.5-turbo,just learn cod try figure replicate little chat gpt website work davinci try upgrade  break work link code tip try just replace textdavinci line  gptturbo break think a different api endpoint not experience apis understand fix page api update think need change prompt message maybe change endpoint url not sure,2023-03-18 07:59:22,,2023-03-20 00:09:20,2023-04-20 17:17:44,<openai-api><chatgpt-api>,2,1,0,781,,,,,,,
75774873,1,13266105.0,,"OpenAI ChatGPT (GPT-3.5) API error: ""This is a chat model and not supported in the v1/completions endpoint""",try use diferent way access api key include add enviroment variables try go wrong pretty new program error message edit solve no api key provide error now follow error message,2023-03-18 09:11:27,,2023-03-21 17:59:41,2023-06-23 09:25:24,<python><discord><openai-api><chatgpt-api>,4,3,6,10803,,,,,,,
75781974,1,16267095.0,,Can anyone tell me I'm getting `error invalid_request_error` on createChatCompletion of OpenAI,code thank help advance,2023-03-19 12:27:25,,,2023-03-19 12:53:18,<openai-api><chatgpt-api>,1,0,-1,190,,,,,,,
75783440,1,769449.0,,Pass local HTML file to ChatGPT API and ask multiple questions,code work really want pass a local html file save disk ask multiple question reduce api call like array product image price product brandname product url video videos relate products product description,2023-03-19 16:30:36,,,2023-03-19 16:30:36,<openai-api><chatgpt-api>,0,0,0,321,,,,,,,
75848481,1,12908887.0,,I need some help to fix a python script that gives a humanoid voice to chatgpt and that allows me to talk with it using my own voice,python script able a realistic humanoid voice chat gpt convert text produce a humanoid voice use voice a mic talk short term want thing “amazon echo alexa ” voice assistant do without buy use … jetson nano jetson nano a place home like a voice assistant spend money buy want use video tutorial code try instal openai pip pip do not work help trouble shoot problem jetson nano run ubuntu  versions python,2023-03-26 14:58:31,,,2023-03-26 21:45:46,<python><python-3.x><chatbot><openai-api><chatgpt-api>,1,0,-2,97,,,,,,,
75945472,1,160059.0,,Receive instant chatgpt response with typing text effect,use chatgpt browser extensions receive chatgpt responses instantly chunk classic type effect use official chatgpt rest api responses come longer delay answer understand extensions not use api just grab data directly website iframe examples,2023-04-06 02:58:34,,,2023-04-18 10:16:39,<javascript><google-chrome-extension><browser-extension><chatgpt-api>,1,0,0,400,,,,,,,
75946337,1,16815915.0,,ChatGPT API integration in a WordPress custom HTML block,a website build a wordpress business account page want include chatgpt api inside a custom html block attempt do fail try different html script integration api nothing work publish output latest work test solution integrate api wordpress inside a custom html block note not interest use chatgpt ai plugins available different html script try inside custom html code block   throw output previewpublish expect output preview a placeholder user input text a response generate chatgpt api base,2023-04-06 06:04:20,,2023-04-15 03:19:15,2023-04-15 03:19:15,<html><wordpress><web><chatgpt-api>,0,4,0,166,,,,,,,
75985331,1,21128862.0,,Python OpenAI API TypeError,try use openai api run a problem use standard example code documentation output problem try update requirements urrlib request openai python actual version,2023-04-11 11:25:50,,2023-04-16 14:21:16,2023-04-16 14:22:03,<python><typeerror><urllib3><openai-api><chatgpt-api>,1,1,0,70,,,,,,,
76040306,1,2348503.0,,ModuleNotFoundError: No module named 'llama_index.langchain_helpers.chatgpt',like use get error m macbook air code look like line  problem python v requirementstxt thank,2023-04-18 00:54:02,,,2023-04-18 00:54:02,<python><python-3.x><openai-api><chatgpt-api><llama-index>,0,1,0,2417,,,,,,,
76047390,1,16733744.0,,How to Reference a Column \by Header Name Using Data Set Named Range in Google Sheets,example suppose a data set google sheet create a name range call suppose a column header call chatgpt claim reference column use name range column chatgpt claim use reference range sumif formulas example suggest test functionality formula way try experiment generate a formula parse error not reference functionality online not think work say example,2023-04-18 17:18:35,,,2023-04-18 21:36:46,<google-sheets><named-ranges><chatgpt-api>,2,3,0,89,,,,,,,
76070176,1,21696128.0,,"Why does when installing chromadb, I'm stuck with preparing wheel metadata? How do I fix this?",try use openai alongside chromadb langchain install chromadb stick prepare wheel metadata hours error a way image reference enter image description try use get stick prepare wheel data hours try install use feature project,2023-04-21 05:34:34,,,2023-05-16 08:07:19,<python><openai-api><chatgpt-api><langchain>,0,1,0,451,,,,,,,
76124757,1,772481.0,,ChatGPT plugin render markdown syntax,ask chatgpt plugin properly request render markdown speak plugin do request return customize markdown do chatgpt know request render customize markdown response like,2023-04-27 21:20:35,,,2023-04-27 21:20:35,<chatgpt-api><chatgpt-plugin>,0,2,1,229,,,,,,,
76175798,1,21410906.0,,Using Custom JSON data for context in Langchain and ConversationChain() in ChatGPT OpenAI,a custom json file create excel sheet contain certain data want question base require answer openai now a piece code follow able a response question base input json file supply openaichatcompletioncreate now want track previous conversations provide context openai answer question base previous question conversation thread langchain have trouble provide json dataset chatopenai conversationchain work like write use python kindly help,2023-05-04 17:18:38,,2023-05-04 17:21:03,2023-05-04 17:21:03,<python-3.x><openai-api><chatgpt-api><langchain><py-langchain>,0,0,2,1953,,,,,,,
76187040,1,12139975.0,,LangChain python - ability to abstract chunk of confidential text before submitting to LLM,confidential document organization like leverage llm openai chatgpt just precaution like abstract confidential information automatically possible use langchain api without loose context company just replace company a look option available generic method like embed understand semantic mean word,2023-05-06 03:45:21,,,2023-05-17 19:34:15,<python><chatgpt-api><py-langchain>,0,2,0,155,,,,,,,
76195150,1,10671406.0,,How can I create a memory storage to use as a context information for my OpenAI-ChatGPT Python Script?,want make a smart assistant capable store historical conversations idea able long discussions store retrieve time expand topics want research a directory follow file code use recurrently place memory begin prompt prior enter model do not responsive interactions stop give answer idea improve,2023-05-07 16:46:02,,2023-05-07 16:56:39,2023-05-07 18:52:56,<python><openai-api><chatgpt-api>,1,1,0,130,,,,,,,
76215950,1,2194805.0,,Different answers from chatgpt-api and web interface,try integrate openai  work kinda fine reply get api totally worse answer web interface way use api python version not use parameters like temperature previously version  use guy idea set example code similar answer web interface parameters set do not documentation about use value web interface thank,2023-05-10 07:32:42,,,2023-06-07 11:16:26,<python-3.x><openai-api><chatgpt-api>,1,0,2,399,,,,,,,
76222270,1,4887393.0,,how to read and write to a folder on my computer using chatgpt,know chatgpt not access file a computer need a plugin api wait list want implement now example a file google cloud create a share link chatgpt read not practical example use api run computer like work fine want type python filepathfilenamepy load filepathfilename inside chatgpt like a codelet demo saw load a panda df file run data vizualization simply type load filecsv run data visualiztion filecsv,2023-05-10 20:18:06,,2023-05-11 16:35:10,2023-05-11 16:35:10,<openai-api><chatgpt-api><chatgpt-plugin>,1,0,-1,470,,,,,,,
76229590,1,20182228.0,,gpt-3.5-turbo post request problem on node.js,try a response a json send a post request api outcome just success true prompt object prompt anorexia nervosa restrict type code,2023-05-11 16:04:18,,,2023-06-07 08:40:06,<node.js><json><http-post><openai-api><chatgpt-api>,1,0,-1,140,,,,,,,
76456041,1,160718.0,76544043.0,map_reduce not working as expected using langchain,try extract information about a csv use langchain chatgpt just a line code use tuff method work perfectly use csv map_reduce fail question current code follow fail answer german drivers driver number  oldest birthdate cost huge  code,2023-06-12 11:05:25,,,2023-06-23 23:59:55,<openai-api><chatgpt-api><langchain>,1,0,0,54,,2.0,2178863.0,"<p>The way how &quot;map_reduce&quot; works, is that it first calls llm function on each Document (the &quot;map&quot; part), and then collect the answers of each call to produce a final answer (the &quot;reduce&quot; part). see <a href=""https://python.langchain.com/docs/modules/chains/document/map_reduce"" rel=""nofollow noreferrer"">LangChain Map Reduce type</a></p>
<p>LangChain's CSVLoader splits the CSV data source in such a way that each row becomes a separate document. This means if your CSV has 10000 rows, then it will call OpenAI API 10001 times (10000 for map, and 1 for reduce). And also, not all questions can be answered in the map-reduce way such as &quot;How many&quot;, &quot;What is the largest&quot; etc. which requires data aggregation.</p>
<p>I think you have to use the &quot;stuff&quot; chain type. &quot;gpt-3.5-turbo-16k&quot; is good to go, which supports 16K context window and also much cheaper than OpenAI you choose.</p>
<p>Note gpt-3.5-turbo-16k is a chat model so you have to use ChatOpenAI instead of OpenAI.</p>
",2023-06-23 23:59:55,0.0,1.0
76162470,1,8402887.0,76163985.0,"OpenAI ChatGPT (GPT-3.5) API error 400: ""Request failed with status code 400""",want upgrade gpt  gpt  turbo node js a problem aiservicejs code try input message return error api correctly,2023-05-03 09:48:16,,2023-05-03 20:54:35,2023-05-03 20:54:35,<node.js><openai-api><chatgpt-api>,1,3,0,600,,2.0,10347145.0,"<p><strong>You used the wrong Completions function.</strong></p>
<p>Change <code>createCompletion</code> (GPT-3 API) to <code>createChatCompletion</code> (GPT-3.5 API).</p>
<p>Try this:</p>
<pre><code>const askAi = async (message) =&gt; {
  try {
    const openAIInstance = await _createOpenAIInstance()

    const response = await openAIInstance.createChatCompletion({
      model: 'gpt-3.5-turbo',
      messages: [{ role: 'user', content: message }]
    })

    const repliedMessage = response.data.choices[0].message.content

    return repliedMessage
  } catch (err) {
    logger.error('', '', 'Ask AI Error: ' + err.message)
    return sendInternalError(err)
  }
}
</code></pre>
",2023-05-03 12:45:24,0.0,2.0
76249463,1,21897947.0,,Error in the creation of a chat GPT app (API connection successfull but no response),create a chatgpt android app ask question app error message invalid response format field content not available spam question message question limit reach buy premium wait a minute api connection work fine json response object attach  picture app maybe idea problem thank help try debug a long time read open ai docs use chat gpt debug hint current error message invalid response format field content not available maybe idea thank answer,2023-05-14 20:05:00,,2023-05-15 05:27:05,2023-05-15 05:27:05,<android><debugging><openai-api><chatgpt-api>,0,0,0,79,,,,,,,
76250276,1,15788211.0,,Is there a way to stream OpenAI (chatGPT) responsse when using firebase cloud functions as a backend?,currently build a chatbot use openai chatgpt firebase cloud function backend want create a realtime chat experience responses chatgpt stream client generate face challenge achieve successfully integrate chatgpt firebase cloud function make api call generate responses problem responses return entire response generate result a delay user receive output a way stream responses chatgpt realtime generate wait complete response want user partial response soon available a simplify version current code a way modify code use a different approach achieve desire realtime stream chatgpt responses suggestions insights greatly appreciate thank,2023-05-15 00:52:56,2023-05-15 01:37:12,2023-05-15 01:36:41,2023-05-15 01:38:09,<firebase><google-cloud-functions><openai-api><chatgpt-api>,0,3,2,232,,,,,,,
76257168,1,18594575.0,,Cache OpenAI system messages to improve latency,use gptturbo model hard cod restrictions not change call user message change hop not resend message cache latency improve suggestions basically hop not send message time,2023-05-15 18:52:02,,,2023-05-15 18:52:02,<openai-api><chatgpt-api>,0,0,0,93,,,,,,,
76278508,1,4120326.0,,Where is the character '\u201c'?,try run finetuning chatgpt keep erroring jsonl file follow error content jsonl file pass script search turn uc leave double quotation mark file erroring file store utf charset try kinds transformations json pandas libraries no avail,2023-05-18 07:07:53,,,2023-05-18 07:37:31,<openai-api><chatgpt-api><jsonlines>,1,5,0,42,,,,,,,
76280180,1,21851013.0,,Bard in react-native,implement bard reactnative project search google chatgpt use bardapi reactnative project do not article things relate explain way implement,2023-05-18 11:03:25,,,2023-05-18 11:51:09,<react-native><data-science><artificial-intelligence><alphabet><chatgpt-api>,1,0,-2,66,,,,,,,
76328160,1,21956793.0,,Getting same X509_V_FLAG_CB_ISSUER_CHECK error each time trying to install openai package,time try pip install openai receive error message try instal various versions cryptography uninstalling reinstall use homebrew not work ideas step follow successfully install openai chatgpt api use a mac os context file optanacondalibpythonsitepackagesopensslcryptopy line  xstoreflags cb_issuer_check _libx_v_flag_cb_issuer_check attributeerror module lib no attribute x_v_flag_cb_issuer_check note need restart kernel use update package thank step follow try debug mention above instal uninstalling versions crpytography use homebrew terminal,2023-05-25 00:22:48,,2023-05-25 00:24:47,2023-05-25 00:24:47,<x509certificate><x509><openai-api><chatgpt-api>,0,0,0,16,,,,,,,
76333309,1,8102886.0,,Multiple sources of data in LangChain,let say  csv file source airline data not merge unfortunately nature data use case pretty simple specify question english example passengers travel july langchain agent suppose figure table query join aggregate data assume course message agent receive necessary meta info about file need read documentation not puzzle,2023-05-25 14:18:31,,,2023-05-25 14:18:31,<python><openai-api><chatgpt-api><langchain>,0,1,-1,228,,,,,,,
75987139,1,11672206.0,75987183.0,OpenAI ChatGPT (GPT-3.5) API: Why does it take so long to get a completion?,try use gpt  flutter app get answer take  second a response code follow right now a personal account don ’ t a pay subscription openai site wrong code select a pay plan solve issue response faster,2023-04-11 14:38:06,,2023-04-16 14:16:32,2023-04-16 14:16:32,<flutter><dart><openai-api><chatgpt-api>,1,0,-3,2092,,2.0,10347145.0,"<p>This is probably due to the OpenAI server being overloaded.</p>
<p>As explained on the official <a href=""https://community.openai.com/t/openai-why-are-the-api-calls-so-slow-when-will-it-be-fixed/148339/9"" rel=""nofollow noreferrer"">OpenAI forum by @rob.wheatley</a>:</p>
<blockquote>
<p>The last few days have been really quite bad. Even with streaming, a
response could take a long time to start. But last night, as I was
testing my new streaming interface, I noticed some odd, but promising,
behavior. Randomly, I would get very quick responses. They were rare
at first. /.../ This morning, all responses have been quick so far.</p>
<p>So, the whole thing looks like a capacity issue to me. Not great if
you are building a commercial app.</p>
</blockquote>
<p>Sources:</p>
<ul>
<li><a href=""https://community.openai.com/t/gpt-3-5-api-is-very-slow-any-fix/141056"" rel=""nofollow noreferrer"">Discusion 1</a></li>
<li><a href=""https://community.openai.com/t/openai-why-are-the-api-calls-so-slow-when-will-it-be-fixed/148339"" rel=""nofollow noreferrer"">Discusion 2</a></li>
</ul>
",2023-04-11 14:43:07,0.0,4.0
75906752,1,1112083.0,75906907.0,How to implement ChatGPT into Unity?,currently try implement capabilities chatgpt unity project use c json class wrap request unwrap successfully manage implement send a request get a response problem responses totally random example ask a verb a response tell about factor contribute a successful podcast not sure configuration wrong exactly go post class request class way send response finally take string text field read api configure best abilities miss,2023-04-01 14:12:26,,2023-04-03 14:53:56,2023-04-03 14:53:56,<c#><unity-game-engine><chatgpt-api>,1,1,-1,380,,2.0,1938558.0,"<p>You need to provide a prompt to tell the AI model what kind of conversation you want it to have with you. At the moment you're just sending the ChatGPT API a single message at a time:</p>
<pre class=""lang-csharp prettyprint-override""><code>Message chatAgentResponse = await OpenAIAPIManager.SendMessageToChatGPT(new Message[]{userMessage}, 0.7f, 256, 1f, 0f, 0f);
</code></pre>
<p>You're initialising a new list of messages as part of each request and it only contains the message you want to send to the chat model. You should craft a message with the &quot;system&quot; role explaining what kind of conversation you want the AI to complete your chat with, e.g.</p>
<pre class=""lang-csharp prettyprint-override""><code>Message promptMessage = new Message(&quot;system&quot;, &quot;You are an AI participant in a chess game. Your opponent is having a conversation with you. Respond professionally as though you are taking part in a renowned international chess tournament, and there is a significant amount of publicity surrounding this match. The whole world is watching.&quot;);
Message[] messages = {promptMessage, userMessage};
Message chatAgentResponse = await OpenAIAPIManager.SendMessageToChatGPT(messages, 0.7f, 256, 1f, 0f, 0f);
</code></pre>
<p>To get the AI model to continue the conversation and remember what has already been said, you'll need to append the response message from the API to this list, then append your user's reply, and keep sending the full list with every request. Have a look at the chat completion guide here, the example API call demonstrates this well:
<a href=""https://platform.openai.com/docs/guides/chat/introduction"" rel=""nofollow noreferrer"">https://platform.openai.com/docs/guides/chat/introduction</a></p>
<p>However, you'll also need to be aware of the maximum number of tokens (basically words, but not exactly) you can use with the model you have chosen. This will grow as the conversation evolves, and eventually you'll run out: <a href=""https://platform.openai.com/docs/api-reference/chat/create#chat/create-max_tokens"" rel=""nofollow noreferrer"">https://platform.openai.com/docs/api-reference/chat/create#chat/create-max_tokens</a>. For example, <code>gpt-4-32k</code> supports 8 times as many tokens as <code>gpt-3.5-turbo</code>, but isn't publicly available yet, and will be much more expensive when it is initially released. <a href=""https://platform.openai.com/docs/models/gpt-4"" rel=""nofollow noreferrer"">https://platform.openai.com/docs/models/gpt-4</a></p>
",2023-04-01 14:41:57,0.0,3.0
76347639,1,13560486.0,,OpenAI ChatGPT (GPT-3.5) API: Why does the API report more prompt_tokens used for the messages parameter than I thought it would?,use model send follow message openai api send a json object think return like tokens do not return tokens send just return like tokens correct,2023-05-27 15:29:37,,2023-06-02 14:20:34,2023-06-02 14:25:26,<openai-api><chatgpt-api>,1,0,0,126,,,,,,,
76347800,1,12223536.0,,SvelteKit: Display chat stream tokens from Langchain,work a project use sveltekit langchain want implement a feature press a button ui display tokens a chat stream come face difficulties current implementation use form action implement far pageserverts pagesvelte need assistance display tokens chat stream come specifically not sure handle handlellmnewtoken callback langchain chatopenai model appreciate guidance suggestions achieve thank advance help,2023-05-27 16:08:50,,,2023-06-24 16:32:44,<svelte><sveltekit><openai-api><chatgpt-api><langchain>,1,0,0,86,,,,,,,
76348006,1,1711271.0,,How to write a GPT prompt programmatically?,a template prompt similar a list topics like query openai api base prompt topic work a bite inelegant redefine fstring iteration loop,2023-05-27 17:01:29,,,2023-05-28 01:06:22,<prompt><openai-api><f-string><chatgpt-api>,1,0,0,61,,,,,,,
75899189,1,21536393.0,75899296.0,"OpenAI ChatGPT (GPT-3.5) API error: ""Invalid URL (POST /chat/v1/completions)""",follow a tutorial make chatgpt app error code try change model remove chat url send prompt directly url new app make java cod no beginner cod understand maybe code not great copy paste code tutorial thank help,2023-03-31 13:37:37,,2023-04-03 09:36:20,2023-04-07 09:06:21,<java><android><okhttp><openai-api><chatgpt-api>,1,1,0,1617,,2.0,10347145.0,"<p>You have a typo. Change this...</p>
<pre><code>https://api.openai.com/chat/v1/completions
</code></pre>
<p>...to this.</p>
<pre><code>https://api.openai.com/v1/chat/completions
</code></pre>
<p>See the <a href=""https://platform.openai.com/docs/api-reference/chat/create"" rel=""nofollow noreferrer"">documentation</a>.</p>
",2023-03-31 13:48:33,1.0,0.0
75734684,1,15616433.0,75743636.0,ApiChatGPT Cutting Text,chatgpt api clip response text a way resolve no way solve remove paragraph text cut help,2023-03-14 14:46:00,,2023-03-14 14:54:56,2023-03-20 11:09:02,<javascript><node.js><chatgpt-api>,1,5,-1,428,,2.0,15616433.0,"<p>OpenAI language model processes text by dividing it into tokens. The API response was getting clipped because the text sent was going over the 100 token limit. To avoid this problem, I set the max_tokens property to its maximum value.</p>
<p>This was my solution:</p>
<pre><code>const settings = {
  prompt: newMessage,
  model: 'text-davinci-003',
  temperature: 0.5,
  max_tokens: 2048,
  frequency_penalty: 0.5,
  presence_penalty: 0,
 }
</code></pre>
<p>Here is the documentation I used: <strong>platform.openai.com/docs/api-reference/completions/create</strong></p>
",2023-03-15 10:46:30,0.0,1.0
76373349,1,19276214.0,,Integrating Google Firebase Firestore with ChatGPT API,possible user ask chatgpt respectively collect data firebase firestore database show data user output,2023-05-31 12:09:46,,2023-05-31 12:24:03,2023-05-31 13:39:21,<firebase><google-cloud-platform><google-cloud-firestore><chatbot><chatgpt-api>,1,0,0,102,,,,,,,
76442693,1,3476463.0,,use llama index to create embeddings for commercial pipeline,python  code code use llama_index meta create index object text corpus pass query index object responses openai chatgpt use additional text corpus index provide openai api key pay openai account index create responses assumption llama_index basically chop text corpus chunk chatgpt create embeddings chop corpus create index object pass a query chatgpt create a similar embed query do inner product index create corpus return a response hear llama_index available research use wonder use scenario a commercial app pay openai account api key far tell llama_index a library instal env help chop corpus pass llm do know llama_index use a commercial pipeline like miss about process hit rate limit lately surprise not do wonder comming llama_index not openai code,2023-06-09 18:00:07,,,2023-06-12 14:54:32,<python-3.x><openai-api><chatgpt-api><llama-index><llm>,1,0,0,96,,,,,,,
76444119,1,21932981.0,,Automatic Latex formatting of python strings (ChatGPT or otherwise),a large set small string math exam question like automatically latex mathjax format insert html file mean a string like want note quite a string a lot issue fraction like dfrac   write   expression  look like  initial think use chatgpt automatically format depite slow expensive api know sort task online use prompt work great try use api python do not nearly nice result like expect chatgpt keep try solve question instead just format python test file run get response not api keep want solve problems instead format honestly slow run sure a far better way about sort format need standalone number variables mathematical expressions place inside tag think regex work not figure regex recognize variables equations regex do not really help weird format issue miss character string help advice thoughts chatgpt work alternative ways automatically format latex appreciate immensely thank,2023-06-09 23:23:23,,2023-06-10 12:08:10,2023-06-10 12:08:10,<python><automation><mathjax><openai-api><chatgpt-api>,0,0,-2,37,,,,,,,
76447776,1,22053467.0,,Failed to load message due to okhttp3.internal.http.RealResponseBody,try implement chatgpt android application send a message chatgpt reply error fail load message okhttpinternalhttprealresponsebody code use try look a post error suggest fix do not work,2023-06-10 19:08:00,,,2023-06-10 19:08:00,<android><okhttp><chatgpt-api>,0,0,0,28,,,,,,,
76496521,1,22088597.0,,codeGPT extension in vscode does not work,try use codegpt extension vs code do say tutorial instal extension copy paste key openai ask explanation display error undefined  suggestions,2023-06-17 13:57:36,,,2023-06-17 13:57:36,<visual-studio-code><chatgpt-api>,0,0,-2,25,,,,,,,
76525263,1,22109667.0,,Langchain's SQLDatabaseSequentialChain to query database,try create a chatbot langchain openai query database large number table base user query use sqldatabasesequentialchain say best large number table database problem run code take forever establish connection end error code internet confirm openai api key run help suggestions method consider let know currently do rnd project do not satisfactory solution thank try check openai api key available yes expect a response gpt model,2023-06-21 16:13:00,,,2023-06-21 16:13:00,<azure-sql-database><openai-api><langchain><chatgpt-api><py-langchain>,0,3,0,39,,,,,,,
75743057,1,7024802.0,75743229.0,OpenAI ChatGPT (GPT-3.5) API: How to implement a for loop with a list of questions in Python?,try run a loop run openai chatcompletion api not make work puzzle goal a list responses basically a list sentence let list example look like try loop input run responses not append answer list case fix like responses thank advance help,2023-03-15 09:54:45,,2023-03-21 17:59:14,2023-03-21 17:59:14,<python><list><for-loop><openai-api><chatgpt-api>,1,0,0,1383,,2.0,10347145.0,"<p>You need to print the <code>output</code>.</p>
<p>If you run <code>test.py</code> the OpenAI API will return a completion:</p>
<blockquote>
<p>['The winner of the UEFA Champions League in 2017 was Real Madrid.',
'The 2014 FIFA World Cup was won by Germany.']</p>
</blockquote>
<p><strong>test.py</strong></p>
<pre><code>import openai
import os

openai.api_key = os.getenv('OPENAI_API_KEY')

input_list = ['Who won the Champions League in 2017?', 'Who won the World Cup in 2014?']

output = []

for i in range(len(input_list)):
  response = openai.ChatCompletion.create(
    model = 'gpt-3.5-turbo',
    messages = [
      {'role': 'system', 'content': 'You are a chatbot.'},
      {'role': 'user', 'content': input_list[i]},
    ]
  )
  
  chat_response = response['choices'][0]['message']['content']
  output.append(chat_response)

print(output)
</code></pre>
",2023-03-15 10:08:17,0.0,2.0
76100086,1,5715258.0,76100421.0,OpenAI ChatGPT (GPT-3.5) API: Why am I not getting a response if the stream parameter is set to false?,value application type promp value chatgpt do not result params chatgpt give result question chatgpt do not result result,2023-04-25 10:19:53,,2023-04-26 19:00:26,2023-04-26 19:00:26,<php><openai-api><chatgpt-api>,1,0,0,1223,,2.0,10347145.0,"<p>The reason why you don't get a response back if you set <code>&quot;stream&quot; =&gt; false</code> is that the whole code is designed to return a response in a streaming fashion when the <code>stream</code> parameter is set to <code>true</code>.</p>
<p>With the following modification, the response will be processed as a whole, regardless of the value of the <code>stream</code> parameter.</p>
<p>Try this:</p>
<pre><code>$API_KEY = &quot;API_KEY_HERE&quot;;

$model = 'gpt-3.5-turbo';
$header = [
    &quot;Authorization: Bearer &quot; . $API_KEY,
    &quot;Content-type: application/json&quot;,
];

$temperature = 0.6;
$frequency_penalty = 0;
$presence_penalty= 0;
$prompt = 'What can you help me with? For example: What do you suggest to keep me motivated?';

$messages = array(
    array(
        &quot;role&quot; =&gt; &quot;system&quot;,
        &quot;content&quot; =&gt; &quot;Your name is 'JOHN DOE'. I want you to act as a motivational coach. I will provide you with some information about someone's goals and challenges, and it will be your job to come up with strategies that can help this person achieve their goals. This could involve providing positive affirmations, giving helpful advice or suggesting activities they can do to reach their end goal. if you don't understand the question, don't think too much, tell the user to be more specific with more details&quot;
        ),
    array(
        &quot;role&quot; =&gt; &quot;assistant&quot;,
        &quot;content&quot; =&gt; &quot;Hello, I'm JOHN DOE, and I'm a motivational coach who loves helping people find their drive and achieve their goals. With years of experience in coaching and personal development, I've developed a unique approach to motivation that combines mindset, energy, and action.&quot;
    ),
    array(
        &quot;role&quot; =&gt; &quot;user&quot;,
        &quot;content&quot; =&gt; $prompt
    )
);

$url = &quot;https://api.openai.com/v1/chat/completions&quot;;

$params = json_encode([
    &quot;messages&quot; =&gt; $messages,
    &quot;model&quot; =&gt; $model,
    &quot;temperature&quot; =&gt; $temperature,
    &quot;max_tokens&quot; =&gt; 1024,
    &quot;frequency_penalty&quot; =&gt; $frequency_penalty,
    &quot;presence_penalty&quot; =&gt; $presence_penalty,
    &quot;stream&quot; =&gt; false
]);

$curl = curl_init($url);
$options = [
    CURLOPT_POST =&gt; true,
    CURLOPT_HTTPHEADER =&gt; $header,
    CURLOPT_POSTFIELDS =&gt; $params,
    CURLOPT_RETURNTRANSFER =&gt; true,
    CURLOPT_SSL_VERIFYPEER =&gt; false,
    CURLOPT_SSL_VERIFYHOST =&gt; 0,
];

curl_setopt_array($curl, $options);
$response = curl_exec($curl);

if ($response === false) {
    echo 'data: {&quot;error&quot;: &quot;[ERROR]&quot;,&quot;message&quot;:&quot;'.curl_error($curl).'&quot;}' . PHP_EOL;
}else{
    $response_array = json_decode($response, true);
    $content = $response_array['choices'][0]['message']['content'];
    echo $content;
}
</code></pre>
",2023-04-25 10:59:34,2.0,2.0
76532101,1,1131714.0,,ChatGPT streaming integration with Flutter package chat_gpt_flutter,use flutterflow develop a mobile app app provide a chat interface chatgpt completions api response ai stream a ui text field similar experience official chatgpt website integrate flutter package achieve call api receive response work app ui update realtime response word receive not a problem flutter package log actually provide new incoming word milliseconds problem update ui stream word arrive current code use a flutterflow custom action run code ffappstate chatwithai a list chat message structs hold app state listview hold chat message link app state variable add a new child ffappstate chatwithai listview update display latest chat message stream end ui get properly update chat message response form chatgpt appear want appear wordbyword reponse stream live not final token receive,2023-06-22 12:49:36,,,2023-06-22 12:49:36,<flutter><dart><chatgpt-api><flutterflow>,0,0,0,17,,,,,,,
76544429,1,9489365.0,,How to train OpenAi model to generate rules,a board game players characteristics rule randomise generate ’ ve a document teach model game work characteristics generate examples teach model try gptturbo limit conversation try finetuned do not understand teach thank advance use finetuned model think send info remember use prompt completion,2023-06-24 03:13:04,,2023-06-24 03:20:53,2023-06-24 03:20:53,<openai-api><chatgpt-api>,0,2,0,15,,,,,,,
76545652,1,9489365.0,,How to tune OpenAi model to generate data,a board game players characteristics rule randomise generate ’ ve a document teach model game work characteristics generate examples teach model try gptturbo limit conversation use finetuned model think send info remember use prompt completion,2023-06-24 10:41:03,,,2023-06-24 10:41:03,<openai-api><chatgpt-api>,0,0,0,8,,,,,,,
76154305,1,10229072.0,,"You exceeded your current quota, please check your plan and billing details with new ChatGPT account?",saw people create use lock country restrictions just like google bard project allow certain countries read a couple question about this_anser problem a new account use time get quota do not right,2023-05-02 11:15:53,,2023-05-03 10:33:34,2023-06-04 10:30:32,<openai-api><chatgpt-api>,1,5,0,1758,,,,,,,
75658025,1,8041823.0,,"Wix's Velo requires the repeater component to have a unique identifier for each item in its data, gpt-turbo errors when given this extra parameter",do a devpost competition build saas use wix velo project do plan utilize newly release gptturbo model run a problems order update repeater simulate a forth conversation item a unique identifier chat array push function backend give output errors additional properties not allow _id unexpected messages code add extra parameter satisfy wix condition a unique identifier without give model extra parameter frontend backend,2023-03-07 04:24:27,,2023-03-07 04:26:47,2023-03-07 04:26:47,<velo><openai-api><chatgpt-api>,0,0,0,89,,,,,,,
75680776,1,7386688.0,,Translating text in PDF using OpenAI,pdf file a folder write korean text text image do extract text pdf translate english use openai save translation result txt file encounter problems try fix max tokens error openai limit number tokens request process time do divde text chunk translate chunk integrate result reason result contain korean sentence mean translation task not complete far know chatgpt use openai api type untranslated korean sentence chatgpt work just fine return perfect translation reason skip translation openapi translation result contain notrelated information let say ask translation document relate engineer translation result include sentence like lucky today eat a chicken japanase sentence no idea come read document time do not contain similar code really appreciate help,2023-03-09 05:35:43,,,2023-03-09 05:35:43,<python><pdf><openai-api><chatgpt-api>,0,1,0,694,,,,,,,
75614444,1,2686197.0,75615117.0,OpenAI ChatGPT (GPT-3.5) API: Why do I get NULL response?,try carry api call newly release model follow code send a query variable api extract content a respond message api get null responses ideas incorrectly,2023-03-02 10:51:41,,2023-03-21 17:53:47,2023-03-21 17:53:47,<php><curl><openai-api><chatgpt-api>,1,0,1,4029,,2.0,10347145.0,"<p>The reason why you're getting <code>NULL</code> response is because the JSON body could not be parsed.</p>
<p>You get the following error: <code>&quot;We could not parse the JSON body of your request. (HINT: This likely means you aren't using your HTTP library correctly. The OpenAI API expects a JSON payload, but what was sent was not valid JSON. If you have trouble figuring out how to fix this, please send an email to support@openai.com and include any relevant code you'd like help with.)&quot;</code>.</p>
<p>Change this...</p>
<pre><code>$post_fields = [
    &quot;model&quot; =&gt; &quot;gpt-3.5-turbo&quot;,
    &quot;messages&quot; =&gt; [&quot;role&quot; =&gt; &quot;user&quot;,&quot;content&quot; =&gt; $query],
    &quot;max_tokens&quot; =&gt; 12,
    &quot;temperature&quot; =&gt; 0
];
</code></pre>
<p>...to this.</p>
<pre><code>$post_fields = array(
    &quot;model&quot; =&gt; &quot;gpt-3.5-turbo&quot;,
    &quot;messages&quot; =&gt; array(
        array(
            &quot;role&quot; =&gt; &quot;user&quot;,
            &quot;content&quot; =&gt; $query
        )
    ),
    &quot;max_tokens&quot; =&gt; 12,
    &quot;temperature&quot; =&gt; 0
);
</code></pre>
<h3>Working example</h3>
<p>If you run <code>php test.php</code> in CMD, the OpenAI API will return the following completion:</p>
<blockquote>
<p>string(40) &quot;</p>
<p>The capital city of England is London.&quot;</p>
</blockquote>
<p><strong>test.php</strong></p>
<pre><code>&lt;?php
    $ch = curl_init();

    $url = 'https://api.openai.com/v1/chat/completions';

    $api_key = 'sk-xxxxxxxxxxxxxxxxxxxx';

    $query = 'What is the capital city of England?';

    $post_fields = array(
        &quot;model&quot; =&gt; &quot;gpt-3.5-turbo&quot;,
        &quot;messages&quot; =&gt; array(
            array(
                &quot;role&quot; =&gt; &quot;user&quot;,
                &quot;content&quot; =&gt; $query
            )
        ),
        &quot;max_tokens&quot; =&gt; 12,
        &quot;temperature&quot; =&gt; 0
    );

    $header  = [
        'Content-Type: application/json',
        'Authorization: Bearer ' . $api_key
    ];

    curl_setopt($ch, CURLOPT_URL, $url);
    curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);
    curl_setopt($ch, CURLOPT_POST, 1);
    curl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($post_fields));
    curl_setopt($ch, CURLOPT_HTTPHEADER, $header);

    $result = curl_exec($ch);
    if (curl_errno($ch)) {
        echo 'Error: ' . curl_error($ch);
    }
    curl_close($ch);

    $response = json_decode($result);
    var_dump($response-&gt;choices[0]-&gt;message-&gt;content);
?&gt;
</code></pre>
",2023-03-02 11:58:03,2.0,5.0
75625675,1,21325092.0,75626044.0,"OpenAI ChatGPT (GPT-3.5) API error 400: ""'user' is not of type 'object'""",share code bellow a response a post request r openai chatgpt api result user content not work model work thank a lot postman json work not make work r,2023-03-03 10:07:56,,2023-03-21 17:55:02,2023-03-21 17:55:02,<r><openai-api><chatgpt-api>,1,0,0,3153,,2.0,10347145.0,"<p>If you run <code>test.r</code> the OpenAI API will return the following completion:</p>
<blockquote>
<p>[1] &quot;\n\nHello! How may I assist you today?&quot;</p>
</blockquote>
<p><strong>test.r</strong></p>
<pre><code>library(httr)
library(jsonlite)

OPENAI_API_KEY &lt;- &quot;sk-xxxxxxxxxxxxxxxxxxxx&quot;

param &lt;- list(model = &quot;gpt-3.5-turbo&quot;,
              messages = list(list(role = &quot;user&quot;, content = &quot;Hello&quot;))
         )
    
result &lt;- POST(&quot;https://api.openai.com/v1/chat/completions&quot;,
               body = param,
               add_headers(&quot;Authorization&quot; = paste(&quot;Bearer&quot;, OPENAI_API_KEY)),
               encode = &quot;json&quot;)

response_content &lt;- fromJSON(rawToChar(result$content))
print(response_content$choices[[1]]$content)
</code></pre>
",2023-03-03 10:43:18,8.0,0.0
75712694,1,21382610.0,,How to fix incorrect html tags inside a string using python?,generate article contain html tag openai api use python article long correct result html tag not correct example fix html tag use bs separate tag separate line not want solution use python try bs not good result,2023-03-12 11:08:06,,2023-03-13 20:13:50,2023-03-13 20:13:50,<python><html><chatgpt-api>,1,2,0,74,,,,,,,
75721401,1,21388821.0,,Chat GPT3.5-turbo API not printing chat response. No code errors,build a basic chat tutor api repl get no chat response run secret key set correctly openai set personal issue no code errors unsure go wrong issue code chat ask question want learn about do not respond question format,2023-03-13 11:41:24,,2023-03-13 11:46:25,2023-04-29 05:40:02,<python><openai-api><chatgpt-api>,1,0,1,353,,,,,,,
75763453,1,13782372.0,,OpenAI Rate Limit 429 Bug,try use repository create semantic search youtube videos use openai pinecone hit a  error step run command npx tsx srcbinprocessytplaylistts preprocess transcripts fetch embeddings openai insert a pinecone search index help appreciate attach openaits file try increase time api call limit get error,2023-03-17 03:14:20,,,2023-04-09 14:29:02,<typescript><next.js><openai-api><chatgpt-api><semantic-search>,1,5,0,563,,,,,,,
75817797,1,21459932.0,,Problum auto redirected to the original script/indicator,just edit exist indicator script add alert function without disrupt part script add modify script chart tradingview immediately redirect original scriptindicator do know issue hope solution,2023-03-22 22:28:06,,,2023-03-23 06:09:10,<javascript><pine-script><chatgpt-api>,1,1,0,24,,,,,,,
75826303,1,13401408.0,,is there any way to stream response word by word of chatgpt api directly in react native (with javascript),want use chat gpt turbo api directly react native expo word word stream work example without stream change stream data word word,2023-03-23 17:37:57,,,2023-06-20 20:17:25,<javascript><react-native><expo><openai-api><chatgpt-api>,2,12,1,5107,,,,,,,
75827468,1,21476148.0,,Why am I getting a 401 error even though I am getting a response when linking my Next.js site with ChatGPT?,try incorporate chatgpt practice ecommerce site use a chatbot import openai add a function send a message chatgpt console log response run site use npm run dev receive a response terminal browser get a  error a  error lack authentication credentials receive a response try render response page use a usestate do not work response terminal use credentials nonnextjs apps do work api key a envlocal file root folder site  error browser state unhandled runtime error error request fail status code  stack createerror node_modulesaxioslibcorecreateerrorjs   settle node_modulesaxioslibcoresettlejs   xmlhttprequestonloadend node_modulesaxioslibadaptersxhrjs   confuse not use axios,2023-03-23 19:54:00,,2023-03-23 20:01:25,2023-03-23 21:30:15,<next.js><openai-api><chatgpt-api>,1,0,3,882,,,,,,,
75869648,1,21512446.0,,Discord Bot Help for gpt-3.5-turbo,know not good practice come  line code look help unfortunately tap gpt not help point likely  be knowledge cap feed article try fix stump code error help appreciate try a things mainly switch axios openai npm node new apologies unfortunately gpt tell use axios a response point use gptturbo goal obviously just make a discord bot respond users use turbo want include learn reaction thumb thumb button discord embed eventually use gpt help finish portion now just want code work environment variables properly store env key discord bot token correct,2023-03-28 18:07:31,,,2023-04-03 02:43:41,<discord.js><openai-api><chatgpt-api>,1,0,-1,367,,,,,,,
75870494,1,21512446.0,,Discord GPT-3.5-Turbo Throwing an Undefined Error,have trouble implement embed response bot luckily code run reference try use embedbuilder discordjs run latest version  believe issue run use ask response embed terminal tell data undefined kind create embed code look good discord greatly appreciate sorry guy not a programmer just interest ai get prompt design months,2023-03-28 19:48:36,,2023-03-29 19:23:37,2023-03-29 19:23:37,<discord.js><embed><openai-api><chatgpt-api>,1,0,-1,180,,,,,,,
75912076,1,15520615.0,,How to write a decent ChatGPT prompt to return mock exam questions based on a link,not sure chatgpt question permit forgive advance like help a chatgpt prompt produce test mock question a link question need multiple choice example a prompt say like write above prompt sound rubbish help a better provide chatgpt,2023-04-02 12:41:03,,2023-04-02 13:15:32,2023-04-02 13:15:32,<chatbot><chatgpt-api>,0,1,0,155,,,,,,,
75968314,1,19601104.0,,"I'm trying to run the llama index model, but when I get to the index building step - it fails time and time again, how can I fix this?",try use llama_index model build index personal document allow ask question about information gpt chat code course api run index build accord step documentation fail step follow error mention try docx file inside a specific folder contain file folders inside subfolders,2023-04-09 00:55:09,,2023-04-22 19:20:53,2023-04-28 18:50:22,<openai-api><chatgpt-api>,1,8,0,1096,,,,,,,
75999180,1,13290761.0,,Accessing OpenAI's CLI on Windows (through a Jupyter Notebook document),try follow tutorial fine tune classification example not able access openai cli document specifically not run code follow right step install update openai package do not work search site answer not help run windows ,2023-04-12 19:36:47,,2023-04-16 11:39:18,2023-04-16 11:39:18,<python><jupyter-notebook><openai-api><chatgpt-api>,1,2,0,159,,,,,,,
76000325,1,21629580.0,,"Creating the load_summarize_chain for Langchain,specified chain_type=map_reduce. get an error when using the prompts",try create load_summarize_chain langchain use prompt create able successfully create chain chain_type set stuff try specify map_reduce refine error message like follow wrong think map_reduce refine directly specify custom prompt reason,2023-04-12 22:36:36,,2023-04-16 11:41:50,2023-04-20 09:40:17,<python><openai-api><chatgpt-api><langchain>,1,1,2,2095,,,,,,,
76027516,1,11033951.0,,"ChatGPT in Python API: No dialogue considered, isolated Q&A without history",use code connect chatgpt python a terminal similar web version problem chatgpt do not learn conversation instance ask high eiffel tower paris answer correctly follow high  chatgpt do not understand context do not know talk about eiffel tower additionally have trouble output part dialogue,2023-04-16 11:37:03,,2023-04-16 11:47:11,2023-04-17 19:17:21,<python-3.x><chatgpt-api>,1,0,0,176,,,,,,,
76216113,1,8430787.0,76416463.0,how can I count tokens before making api call?,try create chat bot provide datafeed start business information conversation history chat api want calculate tokens conversation reduce prompt exceed limit make api try use gpt encoder count tokens array object not string prompt,2023-05-10 07:54:54,,2023-05-10 07:56:10,2023-06-12 07:50:23,<node.js><chatgpt-api>,1,2,0,337,,2.0,1289171.0,"<h1>Exact Method</h1>
<p>A precise way is to use <a href=""https://pypi.org/project/tiktoken/"" rel=""nofollow noreferrer"">tiktoken</a>, which is a python library. Taken from the <a href=""https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb"" rel=""nofollow noreferrer"">openAI cookbook</a>:</p>
<pre class=""lang-py prettyprint-override""><code>    import tiktoken
    encoding = tiktoken.encoding_for_model(&quot;gpt-3.5-turbo&quot;)
    num_tokens = len(encoding.encode(&quot;Look at all them pretty tokens&quot;))
    print(num_tokens)
</code></pre>
<p>More generally, you can use</p>
<pre class=""lang-py prettyprint-override""><code>encoding = tiktoken.get_encoding(&quot;cl100k_base&quot;)
</code></pre>
<p><a href=""https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb"" rel=""nofollow noreferrer"">where</a> <code>cl100k_base</code> is used in gpt-4, gpt-3.5-turbo, text-embedding-ada-002;
<code>p50k_base</code> is used in Codex models, text-davinci-002, text-davinci-003; and <code>r50k_base</code> is what's used in gpt2, and GPT-3 models like davinci.  <code>r50k_base</code> and <code>p50k_base</code> and often (but not always) gives the same results.</p>
<h1>Approximation Method</h1>
<p>You usually just want you program to not crash due to exceeding the token limit, and just need a character count cutoff such that you won't exceed the token limit. Testing with tiktoken reveals that token count is usually linear, particularly with newer models, and that 1/e seems to be a robust conservative constant of proportionality. So, we can write a trivial equation for conservatively relating tokens to characters:</p>
<p>'#tokens &lt;? #characters * (1/e) + safety_margin'</p>
<p>where &lt;? means this is very likely true, and 1/e = 0.36787944117144232159552377016146.
an adaquate choice for safety_margin seems to be 2. In some cases when using with r50k_base this needed to be 8 after 2000 characters. There are two cases where the safety margin comes into play: first for very low character count; there a value of 2 is enough and needed for all models. Second is if the model fails to reason about what it's looking at, resulting in a wobbly/noisy relationship between character count and # tokens with a constant of proportionality closer to 1/e, that may meander over the 1/e limit.</p>
<h2>Main Approximation Result</h2>
<p>Now reverse this to get a maximum number of characters to fit within a token limit:</p>
<p>'max_characters = (#tokens_limit - safety_margin) * e'</p>
<p>where e = 2.7182818284590... Now you've got an instant, language and platform independent, and dependency-free solution for not exceeding the token limit.</p>
<h2>Show Your Work</h2>
<h3>With a paragraph of English</h3>
<p>For model cl100k_base with English text, #tokens = #chars<em>0.2016568976249748 + -5.277472848558375
For model p50k_base with English text, #tokens = #chars</em>0.20820463015644564 + -4.697668008159241
For model r50k_base with English text, #tokens = #chars*0.20820463015644564 + -4.697668008159241</p>
<p><a href=""https://i.stack.imgur.com/09RWM.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/09RWM.png"" alt=""Tokens vs character count for English text"" /></a>
<a href=""https://i.stack.imgur.com/DDBcH.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/DDBcH.png"" alt=""1/e approximation underestimate"" /></a></p>
<h3>With a paragraph of Lorem ipsum</h3>
<p>For model cl100k_base with Lorem ipsum, #tokens = #chars<em>0.325712437966849 + -5.186204883743613
For model p50k_base with Lorem ipsum, #tokens = #chars</em>0.3622005352481815 + 2.4256199405020595
For model r50k_base with Lorem ipsum, #tokens = #chars*0.3622005352481815 + 2.4256199405020595</p>
<p><a href=""https://i.stack.imgur.com/pRxv1.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/pRxv1.png"" alt=""Tokens vs character count for Lorem ipsum text"" /></a>
<a href=""https://i.stack.imgur.com/ui9Uh.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ui9Uh.png"" alt=""lorep ipsum approx worst case"" /></a></p>
<h3>With a paragraph of python code:</h3>
<p>For model cl100k_base with sampletext2, #tokens = #chars<em>0.2658446137873485 + -0.9057612056294033
For model p50k_base with sampletext2, #tokens = #chars</em>0.3240730228908291 + -5.740016444496973
For model r50k_base with sampletext2, #tokens = #chars*0.3754121847018643 + -19.96012603693265</p>
<p><a href=""https://i.stack.imgur.com/wRacC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wRacC.png"" alt=""python token vs char"" /></a>
<a href=""https://i.stack.imgur.com/ZVJhd.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/ZVJhd.png"" alt=""underestimate python"" /></a></p>
",2023-06-06 16:04:49,2.0,1.0
76491056,1,13011830.0,76496823.0,I get HttpClient.Timeout Error in C# OpenAI library,use openai library c project follow error do not receive a response  second add a custom httpclient element solve problem thank advance ‘system thread taskstaskcanceledexception request cancel configure httpclienttimeout  second elapse ‘ library use code,2023-06-16 14:16:10,,2023-06-16 14:21:18,2023-06-17 15:10:35,<c#><timeout><httpclient><openai-api><chatgpt-api>,1,0,1,20,,2.0,13011830.0,"<p>Solution for those who have other problems:</p>
<pre><code>using System.Net.Http;

public class CustomHttpClientFactory : IHttpClientFactory
{
    public HttpClient CreateClient(string name)
    {
        var httpClient = new HttpClient();
        httpClient.Timeout = TimeSpan.FromSeconds(200);

        return httpClient;
    }
}
</code></pre>
<hr />
<pre><code>OpenAIAPI api = new OpenAIAPI(apiKey);  
api.HttpClientFactory = new CustomHttpClientFactory();
var result = await api.Chat.CreateChatCompletionAsync(new ChatRequest()
{
    Model = Model.ChatGPTTurbo,
    Temperature = 0.5,
    Messages = new ChatMessage[]
{
    new ChatMessage(ChatMessageRole.System, &quot;&quot;),
    new ChatMessage(ChatMessageRole.User, prompt)
}
});
</code></pre>
",2023-06-17 15:10:35,1.0,0.0
76272624,1,21872860.0,76273345.0,What is the use case of System role,official documentation chatgpt chat completion understand role a message just greet user do not make sense greet user a helpful assistant explain write behavior ai role like want ai a pro write role like like a interest ai simply just a greet message,2023-05-17 13:25:36,,2023-05-17 16:22:52,2023-05-17 16:22:52,<openai-api><chatgpt-api>,1,0,2,449,,2.0,12914172.0,"<p>It's not for greeting the user but to say how ChatGPT should act. There are a lot of sample prompts in the web. Here you'll find some samples for those system prompts: <a href=""https://github.com/f/awesome-chatgpt-prompts"" rel=""nofollow noreferrer"">https://github.com/f/awesome-chatgpt-prompts</a></p>
",2023-05-17 14:42:41,0.0,2.0
76031421,1,16277480.0,,Invoke Chatgpt in action_default_fallback,try chatgpt answer behalf rasa bot event fallback code result no recognise ‘action_default_fallback ’ register action domainsyml help configyml actionspy,2023-04-17 02:54:15,,,2023-04-17 12:01:44,<python><rasa><chatgpt-api>,1,0,0,84,,,,,,,
76053766,1,5798201.0,,Run AutoGPT in Google Colab. Chrome not reachable,want run autogpt colab fail test install chrome like check tell quite unclear debug idea firefox fail,2023-04-19 11:05:47,,,2023-04-19 11:05:47,<google-colaboratory><chromium><chatgpt-api><autogpt>,0,0,0,955,,,,,,,
76080653,1,10717064.0,,I am not able to get response printed back to text area,use chatgpt api make chrome extension work input area u write help question chatgpt respond input area now problem create a contentjs file install chrome extension type input area help prompt give error console log error line give error code,2023-04-22 16:16:18,,2023-04-25 06:30:38,2023-04-25 06:30:38,<javascript><openai-api><chatgpt-api>,1,0,0,57,,,,,,,
76034314,1,5379584.0,,Valid characters for ChatGPT prompt?,follow request payload generate without error get follow chatgpt api error response change solve problem not restrictions character openai documentation character valid chatgpt api not restrict character escapeencode just filter character edit now pretty sure encode problem nonascii character result error chinese character edit  code a aws lambda function runtime nodejs x payload look correct log code,2023-04-17 10:45:27,,2023-04-17 12:44:03,2023-04-24 04:22:00,<openai-api><chatgpt-api>,2,7,1,709,,,,,,,
76019941,1,772481.0,76021717.0,"OpenAI ChatGPT (GPT-3.5) API: Can I use a fine-tuned GPT-3 model with the GPT-3.5 API endpoint (error: ""Invalid URL (POST /v1/chat/completions)"")?",create a finetuned model use vchatcompletions try give error,2023-04-15 01:24:43,,2023-04-17 07:49:20,2023-06-15 13:29:53,<openai-api><chatgpt-api>,1,0,1,575,,2.0,10347145.0,"<p>It seems like you wanted to fine-tune the GPT-3 <code>davinci</code> model and use it with the GPT-3.5 API endpoint.</p>
<p>You can fine-tune the <code>davinci</code> model as stated in the official <a href=""https://platform.openai.com/docs/guides/fine-tuning/what-models-can-be-fine-tuned"" rel=""nofollow noreferrer"">OpenAI documentation</a>:</p>
<blockquote>
<p>Fine-tuning is currently only available for the following base models:
<code>davinci</code>, <code>curie</code>, <code>babbage</code>, and <code>ada</code>. These are the original models that
do not have any instruction following training (like <code>text-davinci-003</code>
does for example). You are also able to <a href=""https://platform.openai.com/docs/guides/fine-tuning/continue-fine-tuning-from-a-fine-tuned-model"" rel=""nofollow noreferrer"">continue fine-tuning a
fine-tuned model</a> to add additional data without having to start from
scratch.</p>
</blockquote>
<p>But... <strong>The <code>davinci</code> model is not compatible with the <code>/v1/chat/completions</code> API endpoint</strong> as stated in the official <a href=""https://platform.openai.com/docs/models/model-endpoint-compatibility"" rel=""nofollow noreferrer"">OpenAI documentation</a>:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>ENDPOINT</th>
<th>MODEL NAME</th>
</tr>
</thead>
<tbody>
<tr>
<td>/v1/chat/completions</td>
<td>gpt-4, gpt-4-0613, gpt-4-32k, gpt-4-32k-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613</td>
</tr>
<tr>
<td>/v1/completions</td>
<td>text-davinci-003, text-davinci-002, text-curie-001, text-babbage-001, text-ada-001</td>
</tr>
<tr>
<td>/v1/edits</td>
<td>text-davinci-edit-001, code-davinci-edit-001</td>
</tr>
<tr>
<td>/v1/audio/transcriptions</td>
<td>whisper-1</td>
</tr>
<tr>
<td>/v1/audio/translations</td>
<td>whisper-1</td>
</tr>
<tr>
<td>/v1/fine-tunes</td>
<td>davinci, curie, babbage, ada</td>
</tr>
<tr>
<td>/v1/embeddings</td>
<td>text-embedding-ada-002, text-search-ada-doc-001</td>
</tr>
<tr>
<td>/v1/moderations</td>
<td>text-moderation-stable, text-moderation-latest</td>
</tr>
</tbody>
</table>
</div>",2023-04-15 10:38:20,6.0,2.0
76088696,1,408137.0,,Azure Open AI Studio uploading Help Guide for data,want help guide use build train data upload azure open ai studio azure openai studio file management examples take a helpuser guide build data fee azure openai studio train model use azure ai endpoint plenty examples basic prompt completion nothing complex past,2023-04-24 05:00:54,,2023-04-25 06:59:07,2023-05-03 05:02:31,<openai-api><chatgpt-api><azure-openai><azure-ai>,0,0,3,309,,,,,,,
76094671,1,16122184.0,,"Auto-GPT's ""Summarizing chunk"" always fails",retrieve document long summarize chunk use time autogpt try summarize chunk fail follow error usually summarize chunk time like  memory memory memory send chatgpt do not work settings review autogpt version  just mention above settings question code require tell update question information thank,2023-04-24 17:43:01,,2023-04-24 17:53:25,2023-04-24 17:53:25,<openai-api><chatgpt-api><autogpt>,0,1,0,331,,,,,,,
76101760,1,8389716.0,,LlamaIndex with ChatGPT taking too long to retrieve answers,currently work a chatbot website provide domain knowledge use llamaindex chatgpt chatbot use  document  page long contain tutorials information site answer get great performance slow average take  second retrieve answer not practical website try use optimizers suggest documentation not see improvement currently use gptsimplevectorindex not test index try run bot different machine not see a significant improvement performance not think a hardware limitation look suggestions improve performance bot provide answer quickly thank code,2023-04-25 13:34:11,,2023-04-25 16:01:49,2023-05-02 17:06:23,<python><openai-api><chatgpt-api><llama-index>,0,2,3,1135,,,,,,,
76107455,1,15541169.0,,Can i create an API endpoint from nodejs and nextjs web application?,online repo use pinecone gpt  create vectors pdfs transform vectors store pinecone user access app ask question chatgpt response pdfs file repo link note multiple pdfs brnach no experiance nodejs javascript need create api application recive pdfs api user store docs folder show repo multiple pdfs branch endpoint api chat self create possible try make application fastapi flask take time result not good repo save lot time miss api,2023-04-26 05:29:36,,,2023-04-26 05:29:36,<javascript><node.js><typescript><next.js><chatgpt-api>,0,3,0,56,,,,,,,
76133067,1,21767590.0,,"OpenAI ChatGPT (GPT-3.5) API error: ""Invalid URL (POST /v1/engines/gpt-3.5-turbo/chat/completions)"" (migrating GPT-3 to GPT-3.5 API)",fight hours no expert clearly get far api set run end input chat prompt get error gunicorn return big long error ai_chatpy latest source about  variations nearly failure apparently not understand api documentation troubleshoot work long feel like a rabbit hole ai_chatpy error gunicorn user chat submit frontend website run apache server w gunicorn let know need info build ai chat bot web frontend use apache gunicorn host ubuntu server api frontend work submit chat prompt wrong engine chat script fight,2023-04-28 20:27:37,,2023-05-05 21:31:22,2023-05-05 21:33:47,<python-3.x><chatbot><openai-api><chatgpt-api>,1,1,-2,584,,,,,,,
76142673,1,21781509.0,,formatting of chat gpt responses,use chat gpt api react application problem face format response come chat gpt ask a response table format provide weird response use pre tag display text response appear way image attach want proper table just like chat gpt way ask list items display a form paragraph not different line proper format chat gpt response want proper table list chat gpt show receive data data appear use pre tag want proper table,2023-04-30 17:40:41,,,2023-05-24 08:06:03,<javascript><reactjs><formatting><openai-api><chatgpt-api>,3,1,1,4702,,,,,,,
76150014,1,12149285.0,,Is it possible to add a delay between server-sent events?,use flag openai endpoint currently process server send events assign incoming chunk a timestamp use add a document firestore client sort chunk base timestamp append example example document look like problem approach chunk send quickly example a timestamp send quickly timestamp result a collision work think possible add a small delay server send events order prevent collisions not sure fix issue https,2023-05-01 20:35:46,,2023-05-01 20:52:34,2023-05-04 08:02:18,<typescript><google-cloud-firestore><google-cloud-functions><chatgpt-api>,1,2,0,42,,,,,,,
75898276,1,3018860.0,75898717.0,"OpenAI ChatGPT (GPT-3.5) API error 429: ""You exceeded your current quota, please check your plan and billing details""",make a python script use openai api get error script follow declare shebang python use pyenv think work do  api request assume error code,2023-03-31 11:58:04,2023-05-26 18:13:00,2023-06-04 22:03:11,2023-06-04 22:03:11,<python><prompt><openai-api><completion><chatgpt-api>,5,4,34,68395,,2.0,10347145.0,"<p><strong>TL;DR: To upgrade to a paid plan, set up a paid account, add a credit or debit card, and generate a new API key if your old one was generated before the upgrade.</strong></p>
<h2>Problem</h2>
<p>As stated in the official <a href=""https://platform.openai.com/docs/guides/error-codes/python-library-error-types"" rel=""noreferrer"">OpenAI documentation</a>:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>TYPE</th>
<th>OVERVIEW</th>
</tr>
</thead>
<tbody>
<tr>
<td>RateLimitError</td>
<td><strong>Cause:</strong> You have hit your assigned rate limit. <br><strong>Solution:</strong> Pace your requests. Read more in our <a href=""https://platform.openai.com/docs/guides/rate-limits"" rel=""noreferrer"">rate limit guide</a>.</td>
</tr>
</tbody>
</table>
</div>
<p>Also, read more about <a href=""https://help.openai.com/en/articles/6891831-error-code-429-you-exceeded-your-current-quota-please-check-your-plan-and-billing-details"" rel=""noreferrer"">Error Code 429 - You exceeded your current quota, please check your plan and billing details</a>:</p>
<blockquote>
<p>This (i.e., <code>429</code>) error message indicates that you have hit your maximum monthly
spend (hard limit) for the API. This means that you have consumed all
the credits or units allocated to your plan and have reached the limit
of your billing cycle. This could happen for several reasons, such as:</p>
<ul>
<li><p>You are using a high-volume or complex service that consumes a lot of credits or units per request.</p>
</li>
<li><p>You are using a large or diverse data set that requires a lot of requests to process.</p>
</li>
<li><p>Your limit is set too low for your organization’s usage.</p>
</li>
</ul>
</blockquote>
<br>
<h3>Did you sign up some time ago?</h3>
<p><strong>You're getting error <code>429</code> because either you used all your free tokens or 3 months have passed since you signed up.</strong></p>
<p>As stated in the official <a href=""https://help.openai.com/en/articles/4936830-what-happens-after-i-use-my-free-tokens-or-the-3-months-is-up-in-the-free-trial"" rel=""noreferrer"">OpenAI article</a>:</p>
<blockquote>
<p>To explore and experiment with the API, all new users get $5
worth of free tokens. These tokens expire after 3 months.</p>
<p>After the quota has passed you can choose to enter <a href=""https://platform.openai.com/account/billing"" rel=""noreferrer"">billing information</a>
to upgrade to a paid plan and continue your use of the API on
pay-as-you-go basis. If no billing information is entered you will
still have login access, but will be unable to make any further API
requests.</p>
<p>Please see the <a href=""https://openai.com/pricing"" rel=""noreferrer"">pricing</a> page for the latest information on
pay-as-you-go pricing.</p>
</blockquote>
<p><em>Note: If you signed up earlier (e.g., in December 2022), you got $18 worth of free tokens.</em></p>
<p>Check your API usage in the <a href=""https://platform.openai.com/account/usage"" rel=""noreferrer"">usage dashboard</a>.</p>
<p>For example, my free trial expires tomorrow and this is what I see right now in the usage dashboard:</p>
<p><a href=""https://i.stack.imgur.com/nfa3e.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/nfa3e.png"" alt=""Before expiration"" /></a></p>
<p>This is how my dashboard looks after expiration:</p>
<p><a href=""https://i.stack.imgur.com/EfsOf.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/EfsOf.png"" alt=""After expiration"" /></a></p>
<p>If I run a simple script after my free trial has expired, I get the following error:</p>
<pre><code>openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.
</code></pre>
<br>
<h3>Did you create your second account?</h3>
<p><strong>You're getting error <code>429</code> because you created a second OpenAI account with the same phone number. It seems like free credit is given based on phone numbers.</strong></p>
<p>As explained on the official <a href=""https://community.openai.com/t/how-can-i-get-free-trial-credits/26742/19"" rel=""noreferrer"">OpenAI forum by @SapphireFelineBytes</a>:</p>
<blockquote>
<p>I created an Open AI account in November and my $18 credits expired on
March 1st. So, like many of you here, I tried creating a new account
with a different email address, but same number. They gave me $0
credits.</p>
<p>I tried now with a different phone number and email. This time I got
$5 credits.</p>
</blockquote>
<p><strong>UPDATE:</strong>
It's confirmed that free credit is given based on phone numbers, as explained on the official <a href=""https://community.openai.com/t/how-can-i-get-free-trial-credits/26742/27"" rel=""noreferrer"">OpenAI forum by @logankilpatrick</a>:</p>
<blockquote>
<p>Also note, you only get free credits for the first account associated
with your phone number. Subsequent accounts are not granted free credits.</p>
</blockquote>
<h2>Solution</h2>
<p>Try to do the following:</p>
<ol>
<li><a href=""https://platform.openai.com/account/billing/overview"" rel=""noreferrer"">Set up paid account</a> and <a href=""https://platform.openai.com/account/billing/payment-methods"" rel=""noreferrer"">add a credit or debit card</a>.</li>
<li><a href=""https://platform.openai.com/account/api-keys"" rel=""noreferrer"">Generate a new API key</a> if your old API key was generated before you upgraded to the paid plan.</li>
</ol>
",2023-03-31 12:47:39,13.0,48.0
75812086,1,1009073.0,75812753.0,How can I stream using ChatGPT with Delphi?,play chatgpt delphi use openai library support stream not figure chatgpt mechanism stream create a chat data return message try use stream error follow console code work fine submit chat entire answer event like behavior chatgpt website tokens display generate code follow code work try turn stream econversionerror input value not a valid object cause chatgpt return invalid response,2023-03-22 12:10:45,,2023-04-10 18:39:30,2023-04-10 18:39:30,<delphi><openai-api><chatgpt-api>,1,0,1,407,,2.0,7409428.0,"<p>Because in this mode, it responds in this case not with a JSON object, but in its own special format.</p>
<pre><code>data: {&quot;id&quot;: &quot;cmpl-6wsVxtkU0TZrRAm4xPf5iTxyw9CTf&quot;, &quot;object&quot;: &quot;text_completion&quot;, &quot;created&quot;: 1679490597, &quot;choices&quot;: [{&quot;text&quot;: &quot;\r&quot;, &quot;index&quot;: 0, &quot;logprobs&quot;: null, &quot;finish_reason&quot;: null}], &quot;model&quot;: &quot;text-davinci-003&quot;}

data: {&quot;id&quot;: &quot;cmpl-6wsVxtkU0TZrRAm4xPf5iTxyw9CTf&quot;, &quot;object&quot;: &quot;text_completion&quot;, &quot;created&quot;: 1679490597, &quot;choices&quot;: [{&quot;text&quot;: &quot;\n&quot;, &quot;index&quot;: 0, &quot;logprobs&quot;: null, &quot;finish_reason&quot;: null}], &quot;model&quot;: &quot;text-davinci-003&quot;}

data: {&quot;id&quot;: &quot;cmpl-6wsVxtkU0TZrRAm4xPf5iTxyw9CTf&quot;, &quot;object&quot;: &quot;text_completion&quot;, &quot;created&quot;: 1679490597, &quot;choices&quot;: [{&quot;text&quot;: &quot;1&quot;, &quot;index&quot;: 0, &quot;logprobs&quot;: null, &quot;finish_reason&quot;: null}], &quot;model&quot;: &quot;text-davinci-003&quot;}

data: {&quot;id&quot;: &quot;cmpl-6wsVxtkU0TZrRAm4xPf5iTxyw9CTf&quot;, &quot;object&quot;: &quot;text_completion&quot;, &quot;created&quot;: 1679490597, &quot;choices&quot;: [{&quot;text&quot;: &quot;,&quot;, &quot;index&quot;: 0, &quot;logprobs&quot;: null, &quot;finish_reason&quot;: null}], &quot;model&quot;: &quot;text-davinci-003&quot;}

data: {&quot;id&quot;: &quot;cmpl-6wsVxtkU0TZrRAm4xPf5iTxyw9CTf&quot;, &quot;object&quot;: &quot;text_completion&quot;, &quot;created&quot;: 1679490597, &quot;choices&quot;: [{&quot;text&quot;: &quot; 2&quot;, &quot;index&quot;: 0, &quot;logprobs&quot;: null, &quot;finish_reason&quot;: null}], &quot;model&quot;: &quot;text-davinci-003&quot;}
...
</code></pre>
<p>I can start working on such a mode for the library.</p>
",2023-03-22 13:17:57,0.0,3.0
75717246,1,4966876.0,75748927.0,"How can I stream in a Vercel Serverless function? It's working in local, but not once deployed",test chatgpt functionalities stream responses type realtime able reproduce correctly local reason deploy response show message load vercel documentation state able documentation about stream edge function not serverless compare work local not deploy repo specific handler help make work prod,2023-03-13 00:45:12,,2023-03-13 14:33:29,2023-03-15 18:48:21,<node.js><artificial-intelligence><serverless><vercel><chatgpt-api>,2,0,2,656,,2.0,4966876.0,"<p>As noted by @AcclaimHosting, there are many differences.</p>
<p>Ended up following this example with Edge Functions instead of serverless:
<a href=""https://vercel.com/blog/gpt-3-app-next-js-vercel-edge-functions#the-frontend"" rel=""nofollow noreferrer"">https://vercel.com/blog/gpt-3-app-next-js-vercel-edge-functions#the-frontend</a></p>
",2023-03-15 18:48:21,0.0,0.0
76110114,1,7745865.0,,Fix for Google-served ads on screens with replicated content,create app base chatgpt openai use api work a chatbot google ads app work normally recently ads restrict a googleserved ads screen replicate content issue guess google consider chat gpt answer replicate content a way solve accept app not use google ads anymore remove banners chat issue persist,2023-04-26 11:17:44,,,2023-04-28 21:44:07,<admob><openai-api><chatgpt-api>,1,1,2,155,,,,,,,
76145131,1,18008840.0,,How to make my GPT bot answer questions using the data from a database?,research create a chatbot provide information user apis databases not document watch a couple videos youtube langchain gpt bots gather gpt bots need prompt need a dataset answer question need gpt bot domain specific refuse answer kinds question extract intent data provide user input make api provide api endpoints user password respective url data query parameters get data api respond data retrieve a nutshell not need bot train document just need gpt api understand intent user input carry a conversation just need fetch data dynamically databaseapis example input tell order do receive yesterday intent fetch order data yesterday webservice answer list order abcd qty  efgh qty  example earlier think leverage langchain not sure confuse search internet requirement really need guidance possible look wrong technology not thank,2023-05-01 06:36:32,,,2023-05-01 06:36:32,<chatbot><openai-api><chatgpt-api><langchain>,0,3,0,147,,,,,,,
76151528,1,11094220.0,,Stream interrupted (client disconnected),use error code follow file error use work help solve problem,2023-05-02 03:24:11,,,2023-05-02 03:24:11,<openai-api><chatgpt-api><fine-tune>,0,0,0,119,,,,,,,
76165297,1,4175296.0,,When did chatgpt / gpt become a thing developers widely used,hear pro devs start use about  months ago hit news powerful complete script do people use production a lot feature late game start recently increase productivity coders a fair a look try ask chatgpt not know understand specifics want a person opinion ai,2023-05-03 14:54:33,,,2023-05-03 14:54:33,<workflow><chatgpt-api>,0,0,0,24,,,,,,,
76166441,1,1975127.0,,How do I get accurate results with ChatGPT API?,type a prompt use chatgpt plugin log query plugin receive queiries look like simplify versions prompt example type  word query simplify a  word sentance extract believe key info wonder able make use initial message generate a complete response base exact prompt without omit detail override exact answer exact question not a simplify version interpret chatgpt case quantity important let say need a quantity  chat gpt ignore send a quantity  make understand quantity need exact example prompt chatgpt response help appreciate accurate result thank,2023-05-03 17:08:43,,2023-05-04 17:17:02,2023-05-08 10:17:47,<openai-api><chatgpt-api>,0,0,0,320,,,,,,,
76020058,1,772481.0,76176390.0,Chat completions /v1/chat/completions results is very different than the ChatGPT result,api vchatcompletions result different web page result api response q content birthday george washington result web page longer,2023-04-15 02:17:50,,2023-04-16 06:46:33,2023-06-15 21:24:18,<openai-api><chatgpt-api>,3,2,0,1400,,2.0,21819159.0,"<p>Unfortunately, ChatGPT-4 is not willing to spill the beans either. While it is possible to tweak the temperature via API and find a good balance, I'd be curious as well what the default temperate on Web actually is.</p>
<p>Question for ChatGPT-4 via Web:
What is the default temperature when using ChatGPT via web instead of the API?</p>
<p>ChatGPT-4 answer:
The default temperature when using ChatGPT via web interface might not be explicitly stated. However, when using OpenAI's API, the default temperature is typically set to 0.7. This value provides a good balance between creativity and coherence. You can adjust the temperature to control the randomness of the generated text: a lower temperature (e.g., 0.2) makes the output more focused and deterministic, while a higher temperature (e.g., 1.0) makes it more random and creative. Keep in mind that the web interface and the API may have different default values or behaviors.</p>
",2023-05-04 18:36:38,0.0,1.0
76164749,1,769449.0,,"Use python, AutoGPT and ChatGPT to extract data from downloaded HTML page",note downvoting least share a lot effort write question share code do research not sure add use scrapy crawl websites successfully extract specific data a webpage use css selectors time consume setup error prone want able pass raw html chatgpt ask a question like right now run max chat length  character decide send page chunk a simple question like price object expect answer  just get a bunch text wonder do wrong hear autogpt offer a new layer flexibility wonder a solution code,2023-05-03 13:59:44,,2023-06-22 12:57:19,2023-06-24 19:41:25,<python><openai-api><chatgpt-api><autogpt>,1,5,-3,665,,,,,,,
76181606,1,15329359.0,,Unable to create dataset to upload on OpenAI,try create upload dataset openai finetuning chatgpt try json jsonl none work continuosly show error create dataset easily upload openai fine tune code upload json data finetuning model code upload jsonl format code json data train kindly help carry process uplaoding finetuning model,2023-05-05 11:18:15,,,2023-05-05 11:18:15,<json><python-3.x><dataset><openai-api><chatgpt-api>,0,0,0,89,,,,,,,
76192480,1,21840769.0,,What model is used for the free version of ChatGPT?,chatgpt plus version offer gpt turbo gpt able easily official documentation about model versions use free version chatgpt say check official chatgpt documentation saw different versions gpt model chatgpt default mode call actually lead turbo model,2023-05-07 06:24:46,,,2023-05-18 23:42:45,<openai-api><chatgpt-api>,1,0,1,788,,,,,,,
76232902,1,11432290.0,,How to incrementally build index using chatgpt dev api,use chatgpt dev api train a model custom data need incrementally train not ideal create index docs time new data add cost calculate complete list docs correct way charge new data append index update new data implementation,2023-05-12 03:19:25,,,2023-05-12 05:51:28,<openai-api><chatgpt-api>,1,0,0,118,,,,,,,
76166611,1,6013354.0,,Chatgpt integration with Django for parallel connection,use django framework multiple chatgpt connection time make complete code haltdown chatgpt response encounter use async django channel block django server serve resource command run django server code call chatgpt python code send response channel,2023-05-03 17:34:17,,,2023-05-03 17:55:06,<python><django><asynchronous><openai-api><chatgpt-api>,1,0,0,92,,,,,,,
76166932,1,5289186.0,,What is the difference between ChatGPT and other chatbot frameworks like Dialogflow or Rasa?,key differences chatgpt chatbot frameworks like dialogflow rasa term natural language understand customization integration scalability cost framework best fit a chatbot project specific requirements term complexity customization integration informations no comparsion technologies intersect a clear sucess technology,2023-05-03 18:20:37,2023-05-06 18:44:02,,2023-05-06 18:39:44,<dialogflow-es><rasa-nlu><chatgpt-api>,1,0,-1,113,,,,,,,
76172138,1,282855.0,,How to manage a function of a third-party library that stops returning value after a while?,a function function gptall object stop return value a process a bunch query without errorsexceptions try set a timeout function execution control cause error write return value prompt function a file like recommendations overcome situation simply skip iteration continue process input script add a comment stop return a value a,2023-05-04 10:23:07,,2023-05-04 11:06:43,2023-05-04 11:06:43,<python><chatgpt-api>,1,0,0,59,,,,,,,
76040193,1,16659327.0,76045751.0,"How can i update my chatbot with chatgpt from ""text-davinci-003"" to ""gpt-3.5-turbo"" in python",new python want a little hand code develop a smart chatbot use openai api use app piece code responsible chatgpt response code moment code model textdavinci want turn gptturbo good soul interest help obs msg ask whatsapp piece code,2023-04-18 00:20:20,,2023-04-18 00:23:11,2023-05-19 12:17:16,<python><chatbot><whatsapp><openai-api><chatgpt-api>,2,0,2,792,,2.0,5702.0,"<p>To update your code to <code>gpt-3.5-turbo</code>, there are four areas you need to modify:</p>
<ol>
<li>Call <code>openai.ChatCompletion.create</code> instead of <code>openai.Completion.create</code></li>
<li>Set <code>model='gpt-3.5-turbo'</code></li>
<li>Change <code>messages=</code> to an array as shown below</li>
<li>Change the way you are assigning <code>repsonse</code> to your <code>resposta</code> variable so that you are reading from the <code>messages</code> key</li>
</ol>
<p>This tested example takes into account those changes:</p>
<pre class=""lang-python prettyprint-override""><code>response=openai.ChatCompletion.create(
    model=&quot;gpt-3.5-turbo&quot;,
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: questao }],
    temperature=0.1,
    max_tokens=270,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0.6,
)

resposta=response['choices'][0]['message']['content']
</code></pre>
<p>Additionally, since more than one choice can be returned from the model, instead of only looking at <code>[0]</code> you may be interested in iterating over them to see what you're getting, something like:</p>
<pre class=""lang-python prettyprint-override""><code>for choice in response.choices:
            outputText = choice.message.content
            print(outputText)
            print(&quot;------&quot;)
print(&quot;\n&quot;)
</code></pre>
<p>Note that you don't need to do that if you are calling <code>openai.ChatCompletion.create</code> with 'n=1'</p>
<p>Additionally, your example is setting both <code>temperature</code> and <code>top_p</code>, however the <a href=""https://platform.openai.com/docs/api-reference/chat/create#chat/create-temperature"" rel=""nofollow noreferrer"">docs suggest to only set one of those variables</a>.</p>
",2023-04-18 14:20:29,3.0,1.0
75897965,1,10161976.0,75906424.0,Is it possible to handle stream api call in Angular using ChatGPT API?,link stackblitz project a mini app work chatgpt api hide api key work questionanswer big take time exceed token limit chatgpt possible response stream chunk chunk not figure provide code try receive chunk solution glad help,2023-03-31 11:23:26,,,2023-04-01 15:38:06,<angular><http><chatgpt-api>,1,1,3,290,,2.0,20242413.0,"<p>You can get response in stream with <code>fetch</code> and <a href=""https://developer.mozilla.org/en-US/docs/Web/API/ReadableStream"" rel=""nofollow noreferrer""><code>ReadableStream</code></a>. Here is an example:</p>
<pre class=""lang-js prettyprint-override""><code>chatStream(url, body, apikey) {
    return new Observable&lt;string&gt;(observer =&gt; {
      fetch(url, {
        method: 'POST',
        body: body,
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apikey}`,
        },
      }).then(response =&gt; {
        const reader = response.body?.getReader();
        const decoder = new TextDecoder();
        if (!response.ok) {
           // handle response error 
           observer.error();
        }

        function push() {
          return reader?.read().then(({ done, value }) =&gt; {
            if (done) {
              observer.complete();
              return;
            }

            //parse text content from response
            const events = decoder.decode(value).split('\n\n');
            let content = '';
            for (let i = 0; i &lt; events.length; i++) {
              const event = events[i];
              if (event === 'data: [DONE]') break;
              if (event &amp;&amp; event.slice(0, 6) === 'data: ') {
                const data = JSON.parse(event.slice(6));
                content += data.choices[0].delta?.content || '';
              }
            }
            observer.next(content);
            push();
          });
        }

        push();
      }).catch((err: Error) =&gt; {
        // handle fetch error
        observer.error();
      });
    });
  }
</code></pre>
<p>And then subscribe like this</p>
<pre class=""lang-js prettyprint-override""><code>let botMessage = ''

chatStream().subscribe({
    next: (text) =&gt; {
      botMessage += text
    },
    complete: () =&gt; {

    },
    error: () =&gt; {

    }
  });
</code></pre>
<p>Check out my complete application <a href=""https://github.com/ocherry341/custom-chatgpt"" rel=""nofollow noreferrer"">here</a>. Each part can be found at <a href=""https://github.com/ocherry341/custom-chatgpt/blob/main/src/app/%40core/services/http-api.service.ts"" rel=""nofollow noreferrer"">app/@core/http-api.service.ts</a> and
<a href=""https://github.com/ocherry341/custom-chatgpt/blob/main/src/app/pages/chat/chat.component.ts"" rel=""nofollow noreferrer"">app/pages/chat/chat.component.ts</a>.</p>
<p>If you found this helpful, I would greatly appreciate it if you could give me a star.</p>
",2023-04-01 13:16:32,0.0,1.0
76242228,1,4736890.0,,"OpenAI use case —> data insights, storytelling",use case draw insights data storytelling tabular data row columns ecommerce store purchase time series chatgpt good purpose data not textual ’ s user behaviorial data flow ecommerce store validation currently copypaste raw data table chatgpt window — prompt — output insights work great esp data storytelling now want scale build app use openai api ’ ll soon run max_token limit prompt feed data scale  years data time series data ex architecture help solve challenge idea lead highly appreciable,2023-05-13 10:22:13,,,2023-05-13 10:22:13,<openai-api><chatgpt-api>,0,0,0,18,,,,,,,
76251778,1,1608906.0,,I want to train a open source LLM model on my custom dataset [don't want to use openai],try use a open source llm model ggmlgptalllbsnoozybin download want use model embeddings create a ques answer chat bot custom data use lanchain llama_index library create vector store read document dir code not return result not sure do wrong,2023-05-15 07:28:30,,2023-05-16 07:22:35,2023-05-16 07:22:35,<chatgpt-api><langchain><llama-index>,0,2,2,823,,,,,,,
76261677,1,12774913.0,,ChatGPT API - creating longer JSON response bigger than gpt-3.5-turbo token limit,use case chatgpt api not know handle create python app method create request instructions data rewrite chatgpt look like instructions data just sample example provide article content really long model limit reach response fine json short create content just break json content not finish token limit try pass response request limit reach,2023-05-16 09:55:40,,,2023-06-14 04:57:22,<python><openai-api><chatgpt-api>,1,0,0,360,,,,,,,
76185628,1,708436.0,,"How to prompt chatGPT API to give completely machine-readable responses, without superfluous commentary?",try write prompt chatgpt api want respond purely machine readable json responses contain information want want appraise a description a project json specify properties appraisal estimated_hours_of_work not want text outside request json format code evaluate use response not engineer a prompt response purely json extra commentary want use gptturbo gpt,2023-05-05 20:08:04,,,2023-05-05 20:08:04,<openai-api><chatgpt-api>,0,1,0,389,,,,,,,
76186253,1,15329359.0,,Unable to create a dataset using OpenAI module,try use dataset attribute openai module create a dataset use uplaod openai use finetuning continuously show th error attributeerror module openai no attribute ataset havetried update openai latest version come  not know update latest version kinldy help make work new work openai apis,2023-05-05 22:28:42,,,2023-05-05 22:28:42,<python><python-3.x><api><openai-api><chatgpt-api>,0,0,0,24,,,,,,,
76206459,1,18678260.0,,How to continue incomplete response of openai API,openai api programmatically check response incomplete add command like continue expand programmatically continue perfectly experience know response incomplete api return do not work response exceed  tokens need pass previous response conversation new response conversation response  return  tokens not remain  tokens max tokens conversation  tokens correct wrong code note prompt just a sample prompt reality prompt long not fine tune gpt  need train base prompt,2023-05-09 06:33:09,,2023-05-09 14:06:14,2023-06-17 15:19:43,<python><machine-learning><artificial-intelligence><openai-api><chatgpt-api>,1,1,8,1110,,,,,,,
76297924,1,21213741.0,,Error in integration of ChatGPT API in SAS. Invalid context type header,like integrate chatgpt sas execute code error show image error message show invalidcontent type header expexted applicationjson try different solution,2023-05-21 01:13:55,,2023-05-21 18:06:24,2023-05-24 23:35:22,<sas><openai-api><chatgpt-api>,2,0,1,77,,,,,,,
75840731,1,6201311.0,75878074.0,How can I translate _multiple_ strings at once?,research idea translate html page language translate visible text specific split html markup text chunk now need translate text chatgpt idea need translate n piece text strictly n piece currently best experiment widely use phrase chatgpt not resist temptation join string example join phrase not want receive email click link high probability cause case mismatch number texts number translations fatal method force chatgpt transform n string n string,2023-03-25 09:22:01,,2023-05-23 14:20:05,2023-05-23 14:20:05,<chatgpt-api>,2,0,0,177,,2.0,6201311.0,"<p>Looks like I found appropriate query:</p>
<pre><code>Translate this N strings to &lt;language&gt; preserving its number: [ 1. &quot;Line1&quot;, 2. &quot;Line2&quot;, ... , N. &quot;LineN&quot;]
</code></pre>
<p>It produces stable output like:</p>
<pre><code>1. &quot;Translation1&quot;
2. &quot;Translation2&quot;
...
N. &quot;TranslationN&quot;
</code></pre>
",2023-03-29 13:44:01,0.0,0.0
76276691,1,9848794.0,,How to get Open AI GPT-3.5-turbo and grain access to provide requests,sorry silly question give get model official web page a price choose buy want just a request form need write company info like webpage zip a single developer just want try model try send request like just check face  error mean block access not tokens gpt response,2023-05-17 22:56:59,,2023-05-17 23:05:20,2023-05-18 10:27:58,<openai-api><chatgpt-api>,0,0,0,119,,,,,,,
76284509,1,2595659.0,,How to upload files with the OpenAI API,order make a finetuned chatgpt model need upload a json file train data openai doc file upload not append actual file information parameter file not file obvious miss,2023-05-18 21:09:04,,,2023-05-18 21:09:04,<openai-api><chatgpt-api>,0,1,1,512,,,,,,,
76301928,1,21936869.0,,How to incorporate context/chat history in OpenAI ChatBot using ChatGPT and langchain in Python?,bear literally major code write openai chatgpt api intend code load a pdf document a group pdf document split not use tokens user ask question relate say document s bot respond thing have trouble want bot understand context ask new question instance q a lady bug a a ladybug a type beetle blah blah blah q color a come sort color blah blah blah q a ladybugs world code run instead output ask a follow question require bot know context code no idea try everytime try manage make worse leave hop help thank advance,2023-05-21 21:09:00,,2023-05-22 03:04:13,2023-05-22 03:04:13,<python><openai-api><chatgpt-api><langchain><py-langchain>,0,0,3,693,,,,,,,
76336152,1,14782006.0,,ChatGPT will not communicate correctly using Javascript API and whatsapp-web.js,issue have chatgpt give weird responses normal information copy code want try initialise node install package start chat type stop use code not worry about random command start a conversation like prompt hi response absolutely no idea go somebody like help appreciate,2023-05-25 20:44:27,,,2023-05-25 20:44:27,<javascript><node.js><openai-api><chatgpt-api><whatsapi>,0,0,0,55,,,,,,,
76350108,1,16904882.0,,Easiest way to hide api key using Next.js and Vercel?,buidling app use openai api nextjs vercel order make work let frontend make api directly do expose api key browser know never safe store secrets frontend wonder easiest way run a backend service make api goal hide api key prefer a super lightweight solution thank guy try use environment variables vercel require a backend service,2023-05-28 05:39:22,,,2023-05-29 06:06:14,<reactjs><api-key><openai-api><secret-key><chatgpt-api>,2,0,1,110,,,,,,,
76366589,1,9658149.0,,How to select the correct tool in a specific order for an agent using Langchain?,think not understand agent choose a tool a vector database chroma embed internal knowledge want agent look answer not chroma database answer question use information openai use train external knowledge case question a natural conversation want agent take a role answer code try just use knowledge external base tool want decide best tool a correct planexecuteorchestrator agent take correct tool right order,2023-05-30 15:53:22,,,2023-06-22 14:56:46,<python><python-3.x><chatgpt-api><langchain>,1,0,0,158,,,,,,,
76298294,1,15472787.0,,How to Augment Two Embeddings of Different Dimension Sizes?,try implement solution have issue res not define ok look docs not sure res come screenshot error code figure res response define code get errors try use define do not work try augment query combine retrieve context html data embeddings original query question gpt,2023-05-21 04:25:12,,,2023-05-21 04:25:12,<openai-api><python-embedding><chatgpt-api><vector-database>,0,0,0,59,,,,,,,
76324985,1,21265659.0,,GPT Commit Generator for Visual Studio 2022,cool visual studio code extensions generate automatic commit message use chatgpt api like extension like visual studio  not not a way use github copilot repo gitlab maybe use,2023-05-24 15:07:48,,,2023-05-24 15:07:48,<visual-studio><commit><visual-studio-2022><chatgpt-api><chatgpt-plugin>,0,0,1,36,,,,,,,
76338342,1,13438752.0,,Getting error while calling Unity Web Request the payload is displaying in the Unity Editor Console but Empty in WebGL Chrome Console,call openai apis receive error provide model parameter work fine editor not webgl debuglog result load editer chrome pass openai chat completion parameters,2023-05-26 06:59:47,,2023-06-02 14:57:14,2023-06-02 14:57:14,<unity-game-engine><openai-api><unity-webgl><chatgpt-api>,0,4,0,27,,,,,,,
76345057,1,21968880.0,,ChatGPT Integration with Java gives 429 Too Many Requests,hi get  request error try time help assist solution complete error stacktrace orgspringframeworkwebclienthttpclienterrorexception toomanyrequests  request error message exceed current quota check plan bill detail type insufficient_quota param null code null,2023-05-27 02:02:29,,,2023-05-27 02:02:29,<chatgpt-api>,0,1,0,93,,,,,,,
76374831,1,17168063.0,,Chrome Extension ChatGPT with Groovy Script Editor,try build a simple chrome extension idea a sap cloud integration developer a developer open a groovy script editor page write a comment write a groovy code comment pass a prompt chatgpt response chatgpt load script editor chatgpt need open tab able work note chatgpt not correct code a start template base requirement start work instead write scratch default template provide want template base code a developer want write specially nice addition developer manually write prompt chatgpt copypasting script editor issue add extension chrome neither work nor able log console help mistake cod extension contain  file indexhtml manifestjson groovyscripteditorjs backgroundjs chatgptscriptjs manifestjson groovyscripteditorjs backgroundjs chatgptscriptjs,2023-05-31 14:56:42,,2023-06-22 17:19:34,2023-06-22 17:19:34,<javascript><google-chrome-extension><sap-cpi><chatgpt-plugin>,1,1,-1,63,,,,,,,
76408530,1,19362622.0,,Open AI: Remember the last conversation,follow course udemy make a chatgpt kind app flutter give code want bot remember conversation example user explain ai bot artificial intelligence user explain  years old bot artificial intelligence code give above code actually try best get nosuchmethoderror really thankful help look bot remember previous conversation,2023-06-05 16:55:26,,,2023-06-05 16:55:26,<flutter><chatbot><openai-api><chatgpt-api>,0,4,0,51,,,,,,,
76408677,1,19827956.0,,Streaming response line chatgpt,do know display chatgptlike stream response streamlit use streamlit_chat message need like message streamingtrue alternative code segment streamlit_chat import message import streamlit st range len stsession_state generate    message stsession_state past is_usertrue keystr _user message stsession_state generate keystr expect response stream like chatgpt steamlit app,2023-06-05 17:17:05,,,2023-06-07 22:06:13,<streamlit><chatgpt-api><llm>,1,0,0,75,,,,,,,
76425570,1,20212696.0,,Replacing UI with LLMs,replace ui application llm chat window bot able use natural language end user do not click button view options a menu heshe able tell simple sentence trigger usual apis event clickhover drive exist project github a definite approach solve,2023-06-07 16:59:49,,,2023-06-07 16:59:49,<nlp><openai-api><chatgpt-api><langchain><large-language-model>,0,4,1,20,,,,,,,
76432988,1,12075506.0,,how to generate mindmap from Chatgpt discussion through ChatGPT Api,idea create app draw mindmap chatgpt discussion chatgpt api write a prompt control chatgpt write a mindmap a fix format parse,2023-06-08 14:39:21,,,2023-06-15 21:17:48,<openai-api><chatgpt-api>,1,0,0,53,,,,,,,
76441559,1,8941316.0,,"How to create the correct prompt in LLM? Example: GPT, MPT and Falcon etc",input output python code perform task change prompt right skill list skill_descriptions get match input text,2023-06-09 15:17:41,,2023-06-09 15:26:27,2023-06-09 15:26:27,<python-3.x><openai-api><chatgpt-api>,0,0,0,50,,,,,,,
76385146,1,15729369.0,,"OpenAI API error: Why do I still get the ""module 'openai' has no attribute 'ChatCompletion'"" error after I upgraded the OpenAI package and Python?",get follow error check post say upgrade openai python package upgrade python do do not fix python openai python package,2023-06-01 19:33:16,,2023-06-04 19:52:30,2023-06-09 15:23:37,<openai-api><chatgpt-api>,1,1,-1,91,,,,,,,
76385672,1,22003177.0,,How can I use Flask and OpenAI APIs to implement ChatGPT in Python?,python flask chatgpt not request url not server enter url manually check spell try apppy subfolder templatesindexhtml apppy html iam try send text trough a button chatgpt want answer chatgpt say apichat miss,2023-06-01 20:59:42,,,2023-06-01 20:59:42,<python><flask><openai-api><chatgpt-api>,0,0,-2,67,,,,,,,
76493184,1,22085771.0,,What is the best Chat GPT Chat Bot builder app to simulate a virtual patient in therapy?,team like create a virtual therapy patient therapists practice have conversations diagnose like use chat gpt algorithms generate speech train chat bot use data feed specific information about background patient do suggestions best apps program create a chatgpt chatbot minimal cod technical expertise thank currently use dialogue flow like switch chat gpt,2023-06-16 19:48:04,2023-06-16 20:00:26,,2023-06-16 19:48:04,<chatgpt-api>,0,0,-2,14,,,,,,,
76499219,1,248925.0,,When should I want to create a ChatGPT Plugin exposing my company's data?,’ m have a hard time understand idea benefit create chatgpt plugin ’ m not sure chatgpt andor plugin play a role follow scenario help shed light company a frontend application call api turn fetch data oracle database frontend application not accessible company frontend application authenticate users azure ad sort claim transformation occur turn create a second claimsidentity use claim second claimsidentity access not frontend application a company know a protect frontend call inhouse api get appropriate result want leverage care create a chatgpt plugin basically call inhouse api maybe above scenario doesn ’ t fall a good case study move a chatgpt plugin counter arguments make ’ m not see a traditional way do things chatgpt plugins hope make sense sincerely,2023-06-18 05:55:19,2023-06-18 06:36:08,,2023-06-18 05:55:19,<c#><openai-api><chatgpt-api><chatgpt-plugin>,0,10,-1,30,,,,,,,
76509434,1,22098564.0,,"While trying to run gpt-engineer, I tried this command 'python main.py example', but recieved 'No such file or directory'",error get use tutorial follow get stick time   video dont know fix issue command expect command run use prompt file projectsexample dont think issue code,2023-06-19 19:04:07,,2023-06-19 20:17:41,2023-06-19 20:17:41,<ubuntu><openai-api><chatgpt-api>,0,0,0,30,,,,,,,
76522693,1,2641825.0,,How to check the validity of the OpenAI key from python?,ask chat gpt complete a message a a python method check key valid,2023-06-21 11:16:50,,,2023-06-21 13:00:02,<python><openai-api><chatgpt-api>,2,1,1,46,,,,,,,
76533895,1,22115795.0,,Chat GPT API Key Troubleshooting,currently try make a dashboard excel allow utilize chat gpt answer basic excel question coworkers problem keep return say usage api key chat gpt api a pay account base office script video link list do change code video available errors keep occur excel remove api key guy thoughts video link error message trouble shoot try use new api key buy a pay account chat gpt api honestly think get pay account error message away create a new api key,2023-06-22 16:26:43,,2023-06-22 16:38:46,2023-06-22 16:38:46,<javascript><office-scripts><chatgpt-api><chatgpt-plugin>,0,1,0,32,,,,,,,
76534252,1,11566621.0,,Is there a Golang/Java alternative for Langchain to build LLM model over ChatGPT,want create a support bot initially provide pdf contain documentation index data documentation base able answer query ask users plan use chatgpt need sort a langchain alternative golang prefer java,2023-06-22 17:13:07,2023-06-22 18:11:20,,2023-06-22 17:17:16,<java><go><openai-api><langchain><chatgpt-api>,1,0,-3,50,,,,,,,
76450212,1,19860105.0,,"Making a request for openAi api using chatgpt3.5, for making chatbox",build a chat application flutter use chat_gpt_sdk package integrate openai gpt model encounter error try make a request use oncompletionstream method a send_message function handle send a message user receive a response gpt model relevant code snippet update necessary dependencies include dart sdk flutter sdk chat_gpt_sdk package get follow errors argument type tring not assign parameter type model method oncompletionstream not define type openai unsure errors occur resolve appreciate guidance insights properly make a request use completetext method gpt model receive responses use oncompletionstream method additionally redact api token security reason thank advance help,2023-06-11 11:10:14,,,2023-06-11 11:10:14,<flutter><api><request><openai-api><chatgpt-api>,0,0,-1,30,,,,,,,
76450929,1,22040878.0,,why do i get Client.create_tweet() takes 1 positional argument but 2 were given,way say create_tweet doesnt  pos arguments code above box,2023-06-11 14:23:03,,,2023-06-11 19:40:40,<tweepy><chatgpt-api>,1,5,0,15,,,,,,,
76457607,1,5704368.0,,Browsing skill with Semantic Kernel (ChatGPT),create skill browse work browser open chatgpt navigate url provide action page problem chatgpt not aware page guess help send html chatgpt not know help repo browserskillcs try send html code load page chatgpt doesnt work lenght end simulation chatgpt send wrong xpath not base html provide,2023-06-12 14:24:41,,,2023-06-15 04:47:05,<c#><browser><artificial-intelligence><openai-api><chatgpt-api>,1,0,-2,51,,,,,,,
76464905,1,22066719.0,,Use ChatGPT trough Microsoft Teams,try use api chat gpt connect a chat team guide above show use power automate connect api chatgpt a channel team pretty easy part not clear fact guide say end tutorial guy do screen video guide reason not manage thing instead receive error screen result people encounter issue none solve not receive help guy tutorial encounter issue follow tutorial search answer comment section ask chatgpt figure nothing work,2023-06-13 12:23:25,,,2023-06-13 12:23:25,<api><microsoft-teams><power-automate><openai-api><chatgpt-api>,0,1,0,35,,,,,,,
76540414,1,22120193.0,,azure ai studio error: Missing header 'chatgpt_url' in request,greet fellow developers face obstacle use chatgpt playground preview azure openai studio new openai azure platform seek assistance a specific error encounter problem face step take problem attempt utilize session chat feature chatgpt playground preview receive follow error message miss header chatgpt_url request setup step take utilize azure openai studio successfully integrate external data azure subscription use a storage service cognitive search service effort explore capabilities chatgpt playground preview initiate session chat functionality mention error message appear leave unsure cause course action insights suggestions solutions regard problem greatly appreciate thank assistance advance best regard suma,2023-06-23 13:05:54,,,2023-06-23 13:05:54,<openai-api><chatgpt-api><azure-openai>,0,0,0,12,,,,,,,
76546004,1,5564764.0,,Why would you use something like LlamaIndex instead of training a custom model?,just get start work llms particularly openais oss model a lot guide use llamaindex create a store document query try a sample document discover query get super expensive quickly think use a page pdf document a summarization query cost usd query a lot tokens send across assume send entire document query give want use thousands millions record not like llamaindex really useful a costeffective manner hand openai allow train a chatgpt model not use custom train llms cheaper effective query data want set llamaindex,2023-06-24 12:12:32,,2023-06-24 18:27:17,2023-06-24 18:27:17,<openai-api><chatgpt-api><language-model><llama-index>,0,0,1,16,,,,,,,
76547020,1,16486628.0,,Need a simple text to speech converter fro chat gpt,a need api code convert chat gpt answer speech automatically without manually copy paste apis integrate a simple site api need easily accesible free work seamlessly transform text speech link link provide idea mean guide process integrate personal site code extensions not favourable reason pertain ease access automation,2023-06-24 16:34:29,2023-06-24 16:47:34,,2023-06-24 16:34:29,<artificial-intelligence><chatgpt-api>,0,0,-3,7,,,,,,,
75816148,1,19917133.0,,ChatGPT wrapper in python as a command line interpreter,a commandline interpreter chatgpt work fine problem wait chatgpt response fully finish print result like print response chatgpt think maybe thread work follow code nothing special see stackoverflow post lead problem chat chatgpt use python,2023-03-22 18:43:15,,,2023-03-22 19:16:15,<python><openai-api><chatgpt-api>,1,1,0,249,,,,,,,
75823578,1,2666883.0,,"OpenAI ChatGPT (GPT-3.5) API error 400: ""Unexpected response code 400 for https://api.openai.com/v1/chat/completions"" (migrating GPT-3 to GPT-3.5 API)",get follow error code,2023-03-23 13:23:10,,2023-03-29 16:05:49,2023-03-29 16:05:49,<android><kotlin><openai-api><chatgpt-api>,1,1,-3,343,,,,,,,
75909209,1,14523375.0,,"OpenAI ChatGPT (GPT-3.5) API error 404: ""Request failed with status code 404""",work a chatgptapp use react axios make api request openai gpt api encounter a  error try make a request hop help identify issue guide fix appjs indexjs code error message frontend appjs backend indexjs error message verify api key correct url point right direction cause error help greatly appreciate thank,2023-04-01 22:39:38,,2023-04-03 09:27:03,2023-04-03 09:30:30,<javascript><node.js><reactjs><openai-api><chatgpt-api>,1,0,0,2322,,,,,,,
76481857,1,21859743.0,,Integration of TinyMCE editor and ChatGPT,try integrate tunymce editor chatgpt use openai feature try implement example error  error  try run example tutorial use api key error do know solution way code thank help,2023-06-15 11:09:13,,2023-06-15 11:09:45,2023-06-15 11:09:45,<tinymce><openai-api><chatgpt-api><tinymce-6>,0,0,0,26,,,,,,,
76496786,1,22088832.0,,Is it possible to get a huge response from chatGPT?,work require write large text materials chatgpt write about  word answer recently saw news about appearance gptturbok try use despite enlarge context give a response size way limit grateful ideas,2023-06-17 15:02:56,2023-06-19 01:34:20,,2023-06-17 15:02:56,<python><openai-api><chatgpt-api>,0,9,-6,49,,,,,,,
76503607,1,22094556.0,,"ChatWithPdf plugin error when used, how to make it work",use chatwithpdf plugin chatgpt say error communicate plugin chatwithpdf wrong delete download make no differenceis mistake plugin things,2023-06-19 04:28:00,,,2023-06-19 04:28:00,<chatgpt-api>,0,0,0,77,,,,,,,
75924544,1,6116668.0,,How to write text back into correct spot in JSON file after translating all text at once,a json file structure look like follow goal grab text inside parameters code  grab text translate want spot currently use follow function extract text give a list string need translate translate list use method like problem start normally translate line line easily retain position grab raw text write command drawbacks main issue translation quality suffer greatly machine do not context text dialogue require knowledge just say context cost higher line line vs giant batch time take translation greater larger number request choice translate text list a single request avoid above pitfalls leave a translation blob differ length nearly impossible know sentence  cod try use delimiters mark group  end gpt like randomly addremove delimiters throw frankly think about a long time like impossible task maybe community a good idea try group delimiters forcefully match list result a small mismatch position  throw order file cause bug,2023-04-03 23:58:13,,,2023-04-03 23:58:13,<python><json><chatgpt-api><rpgmakermv>,0,2,0,32,,,,,,,
75943817,1,20025261.0,,Integrating ChatGPT into SwiftUI Application,click send button enter input no response chatgpt api not sure cause a look miss actually add api key code not provide privacy reason package dependency use,2023-04-05 20:44:06,,2023-04-06 13:58:18,2023-04-06 13:58:18,<swift><swiftui><openai-api><chatgpt-api>,0,18,0,300,,,,,,,
76504003,1,7276189.0,,Using AI to implement talking avatar on website,a question possible use ai tool a talk avatar person a website ask a question a site site give a generate answer chatgpt example answer present avatar talk mouth movement voice know tool generate a video example possible a website recommendations thank advance try a solution google not deep develop a solid overview,2023-06-19 06:09:01,2023-06-19 18:44:53,,2023-06-19 06:09:01,<javascript><html><artificial-intelligence><openai-api><chatgpt-api>,0,1,-4,27,,,,,,,
76519027,1,5029968.0,,add memory to create_pandas_dataframe_agent in Langchain,try add memory create_pandas_dataframe_agent perform post process a model train use langchain use follow code moment try add memory conversationbuffermemory memory_key chat_history didnt help,2023-06-20 23:41:05,,2023-06-21 15:51:32,2023-06-21 15:51:32,<openai-api><langchain><chatgpt-api><llm>,0,0,0,43,,,,,,,
76519562,1,16692083.0,,Invalid URL (POS /v1/chat/completions) with SQL Server MSXML2.ServerXMLHTTP.6.0,create procedure sql server use openai api execute procedure generate error result try excel use msxmlserverxmlhttp code work correctly do know,2023-06-21 02:35:04,,2023-06-21 02:40:42,2023-06-21 02:40:42,<sql-server><openai-api><chatgpt-api>,0,3,0,18,,,,,,,
76527422,1,22111241.0,,"AutoGPT, AgentGPT and God Mode - How do I get them to review contents of my public Google Drive Folder?",try ai tool review content google drive able accomplish goals not retrieve file google drive enable google drive api console api amp service create api key share google drive folder link api key get no luck help appreciate thank,2023-06-21 21:59:55,2023-06-22 00:10:07,,2023-06-21 21:59:55,<openai-api><chatgpt-api><autogpt><chatgpt-plugin>,0,0,-2,16,,,,,,,
76527607,1,2360477.0,,Summarising long documents,task work summarise annual financial report sample file work annual half year report anz trade asx exchange file create a python script accept a pdf file python script convert pdf file text basically split file chunk fee chatgpt api summaries summaries aggregate pass chatgpt api use method describe website problems document fairly large end  chunk take minutes a summary across chunk phase  stage stage  combine chunk summaries overall chunk size exceed chatgpt limit fail stage not really come a neater solution summarise long document chatgpt happy use a dedicate offtheshelf summarisation api tool get use work fairly ideally a abstractive summarisation oppose extractive summarisation,2023-06-21 22:49:36,,,2023-06-21 22:49:36,<python><summarization><chatgpt-api>,0,7,-2,34,,,,,,,
75616048,1,9773423.0,,"OpenAI ChatGPT (GPT-3.5) API error 400: ""Unexpected response code 400 for https://api.openai.com/v1/completions"" (migrating GPT-3 to GPT-3.5 API)",android application currently use chat gpt  completions work fine now release chat gpt  turbo change base request example throw  errors appreciate help thank code gpt  work fine now switch  turbo use gptturbo model error use chat gpt  turbo model use chat gpt  work base documentation,2023-03-02 13:25:32,,2023-03-24 19:31:50,2023-03-28 09:34:17,<java><android><openai-api><chatgpt-api>,1,0,0,1199,,,,,,,
75621565,1,21322319.0,,adding chatgpt's api to a discord command in discord.js,code fine far know keep hit error api key correct code good api work get error code code check unless not know wrong error error do say a  not tell fix,2023-03-02 22:50:19,,,2023-04-30 11:07:22,<javascript><node.js><discord.js><openai-api><chatgpt-api>,0,5,3,614,,,,,,,
75630847,1,19614071.0,,Gradio ouputs keys of a dictionary instead of strings while using openai.ChatCompletion API and GPT-3.5-turbo,try create a gptturbo chatbot a gradio interface chatbot work perfectly fine command line not implement gradio able send input receive response response get return gradio do not properly display result reply role content dictionary key instead chat string goal able a simple chat conversation history record web interface try return string sort different section dictionary completely a loss return a string predict function complain want enumerate send a list string no luck return dictionary do not kick error display key not value a image error occur gradio interface current code,2023-03-03 18:51:40,,2023-03-03 21:59:03,2023-03-07 22:15:58,<python><chatbot><openai-api><gradio><chatgpt-api>,2,0,-2,342,,,,,,,
75633269,1,7723375.0,,Has the react-native openai-api module been modified to access the ChatGPT API?,reactnative openaiapi module modify access chatgpt api use expo follow error use not work reactnative code,2023-03-04 02:21:12,,2023-03-04 02:29:14,2023-03-04 04:55:34,<react-native><openai-api><chatgpt-api>,1,1,0,418,,,,,,,
75661997,1,14740582.0,,7 tokens is add to prompt_tokens in gpt-3.5-turbo,use api response  tokens add prompt_tokens token calculation different documentation prompt_tokens  content pear documentation api response ,2023-03-07 12:31:23,,,2023-04-08 07:49:43,<openai-api><chatgpt-api>,1,0,0,367,,,,,,,
76074574,1,20044208.0,,summarizer pdf with langchain and openAI/ChatGTP,use follow code summarize long pdfs work fine pdf use a second pdf change file path pdf put summary pdf embeddings pdfprevious round store not delete behavior hold true restart python try a number pdfs start rename object help right now rename object put summary previous pdf confuse about behavior clue delete vectors previous round fix,2023-04-21 15:23:44,,,2023-05-05 13:53:22,<python><word-embedding><openai-api><chatgpt-api><langchain>,1,0,0,1287,,,,,,,
75973933,1,16446701.0,,How Do I Set Up LLMPredictor To Be able To Create Indexes?,follow video video make a chatgpt bot fine end try create model index bot copy code directly video creator notebook code run without problems run code error try directly video creators notebook get error miss fix,2023-04-10 02:33:14,,,2023-04-21 09:18:42,<python><indexing><chatbot><chatgpt-api>,1,0,1,830,,,,,,,
75981657,1,20651596.0,,Cannot get the API REST result using Retrofit,search a long time do not help try use a openai api time get errors do wrong error code retrofitinstance chatgptinterceptor mainactivity chatgptendpoint,2023-04-11 00:37:05,,2023-04-16 14:29:36,2023-04-16 14:29:36,<kotlin><retrofit><openai-api><chatgpt-api>,0,1,1,48,,,,,,,
75995458,1,2405632.0,,OpenAI PHP embedding documentation Q&A wrong similarity response,follow guide website use openai create a qa laravelphp embed just remake php now a logical error firstly embeddings source document embed question try answer openai source document question problem function getanswer return letter a case obviously answer openai plain wrong not correct paste code file don ’ t know embeddings not work correctly,2023-04-12 12:28:49,,2023-04-16 14:26:54,2023-04-16 14:26:54,<php><openai-api><chatgpt-api>,0,3,0,243,,,,,,,
76011399,1,16428744.0,,"Error ""okhttp3.responsebody$Companion$asresponseBody$1@8d63711d""",currently develop app use chatgpt openai api provide conversational ai capabilities try send a message bot send error okhttpresponsebody companion asresponsebody dd tell solve error code,2023-04-14 03:28:30,,2023-05-04 15:50:19,2023-05-04 15:50:19,<android><api><kotlin><chatgpt-api>,0,0,0,193,,,,,,,
75729386,1,21263382.0,,OpenAI ChatGPT (GPT-3.5) API: Can I fine-tune the gpt-3.5-turbo model?,a sql table contain huge data need train sql table data chatgpt use chat completion api try generate a sql query use chatgpt do not work expect generate inappropriate query,2023-03-14 05:30:42,,2023-03-22 17:36:11,2023-06-16 14:20:22,<openai-api><chatgpt-api>,2,1,0,7192,,,,,,,
75737523,1,12624118.0,,openAI api - is it possible save chat state \history by the api (without resending it)?,want develop a chat app use use nodejs like save state conversation user wo not send conversation prim time want accomplish similar chatbot ui do today possible,2023-03-14 19:23:17,,2023-03-18 16:16:44,2023-03-18 16:16:44,<node.js><openai-api><chatgpt-api>,1,0,1,896,,,,,,,
75780617,1,1444464.0,,Using PHP to access ChatGPT API,write a simple php script no dependencies access chatgpt api throw error not understand script far return error api afaik send message param array go wrong issue json_encode do not api think array thank advance,2023-03-19 07:28:32,2023-04-29 07:53:33,2023-03-28 05:13:56,2023-04-25 11:42:01,<openai-api><chatgpt-api>,2,9,1,9084,,,,,,,
75671878,1,2686197.0,,"How can I send back partial GPT-3.5-turbo responses, to an ajax call, to display response in real time",follow php code struggle correctly access partial message deliver api send ajax message appear a div real time wrong api ajax,2023-03-08 10:30:46,,2023-03-08 12:13:57,2023-03-08 12:13:57,<javascript><openai-api><chatgpt-api>,0,0,0,340,,,,,,,
75613656,1,2686197.0,75613704.0,OpenAI ChatGPT (GPT-3.5) API: How do I access the message content?,receive a response openai model able extract text response follow php code da vinci response code now try alter code work recently release model return response slightly differently question alter code grab content response message,2023-03-02 09:42:24,,2023-03-21 17:53:04,2023-06-13 13:01:45,<php><openai-api><chatgpt-api>,2,0,0,5423,,2.0,10347145.0,"<p><strong>Python:</strong></p>
<pre><code>print(response['choices'][0]['message']['content'])
</code></pre>
<p><strong>NodeJS:</strong></p>
<pre><code>console.log(response.data.choices[0].message.content);
</code></pre>
<p><strong>PHP:</strong></p>
<pre><code>var_dump($response-&gt;choices[0]-&gt;message-&gt;content);
</code></pre>
<br>
<h3>Working example in PHP</h3>
<p>If you run <code>test.php</code> the OpenAI API will return the following completion:</p>
<blockquote>
<p>string(40) &quot;</p>
<p>The capital city of England is London.&quot;</p>
</blockquote>
<p><strong>test.php</strong></p>
<pre><code>&lt;?php
    $ch = curl_init();

    $url = 'https://api.openai.com/v1/chat/completions';

    $api_key = 'sk-xxxxxxxxxxxxxxxxxxxx';

    $query = 'What is the capital city of England?';

    $post_fields = array(
        &quot;model&quot; =&gt; &quot;gpt-3.5-turbo&quot;,
        &quot;messages&quot; =&gt; array(
            array(
                &quot;role&quot; =&gt; &quot;user&quot;,
                &quot;content&quot; =&gt; $query
            )
        ),
        &quot;max_tokens&quot; =&gt; 12,
        &quot;temperature&quot; =&gt; 0
    );

    $header  = [
        'Content-Type: application/json',
        'Authorization: Bearer ' . $api_key
    ];

    curl_setopt($ch, CURLOPT_URL, $url);
    curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);
    curl_setopt($ch, CURLOPT_POST, 1);
    curl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($post_fields));
    curl_setopt($ch, CURLOPT_HTTPHEADER, $header);

    $result = curl_exec($ch);
    if (curl_errno($ch)) {
        echo 'Error: ' . curl_error($ch);
    }
    curl_close($ch);

    $response = json_decode($result);
    var_dump($response-&gt;choices[0]-&gt;message-&gt;content);
?&gt;
</code></pre>
",2023-03-02 09:47:00,7.0,6.0
75617865,1,17034564.0,75619702.0,"OpenAI ChatGPT (GPT-3.5) API error: ""InvalidRequestError: Unrecognized request argument supplied: messages""",currently try use openai recent model follow a basic tutorial work a google collab notebook make a request prompt a list prompt sake simplicity look like define a function follow error suggestions replace argument lead follow error,2023-03-02 15:55:35,,2023-06-01 12:49:13,2023-06-23 09:35:07,<python><openai-api><chatgpt-api>,3,6,18,24868,,2.0,10347145.0,"<h2>Problem</h2>
<p>You used the wrong function to get a completion. When using the OpenAI library (Python or NodeJS), you need to use the right function. Which is the right one? It depends on the model you want to use.</p>
<h2>Solution</h2>
<p>The tables below will help you figure out which function is the right one for a given OpenAI model.</p>
<p>First, find in the table below which API endpoint is compatible with the model you want to use.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>API endpoint</th>
<th>Model group</th>
<th>Model name</th>
</tr>
</thead>
<tbody>
<tr>
<td>/v1/chat/completions</td>
<td>GPT-3.5 and GPT-4</td>
<td>gpt-4, gpt-4-0613, gpt-4-32k, gpt-4-32k-0613, gpt-3.5-turbo, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613</td>
</tr>
<tr>
<td>/v1/completions</td>
<td>GPT-3</td>
<td>text-davinci-003, text-davinci-002, text-curie-001, text-babbage-001, text-ada-001</td>
</tr>
<tr>
<td>/v1/edits</td>
<td>Edits</td>
<td>text-davinci-edit-001, code-davinci-edit-001</td>
</tr>
<tr>
<td>/v1/audio/transcriptions</td>
<td>Whisper</td>
<td>whisper-1</td>
</tr>
<tr>
<td>/v1/audio/translations</td>
<td>Whisper</td>
<td>whisper-1</td>
</tr>
<tr>
<td>/v1/fine-tunes</td>
<td>GPT-3</td>
<td>davinci, curie, babbage, ada</td>
</tr>
<tr>
<td>/v1/embeddings</td>
<td>Embeddings</td>
<td>text-embedding-ada-002, text-search-ada-doc-001</td>
</tr>
<tr>
<td>/v1/moderations</td>
<td>Moderation</td>
<td>text-moderation-stable, text-moderation-latest</td>
</tr>
</tbody>
</table>
</div>
<p>Second, find in the table below which function you need to use.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>API endpoint</th>
<th>Python function</th>
<th>NodeJS function</th>
</tr>
</thead>
<tbody>
<tr>
<td>/v1/chat/completions</td>
<td>openai.ChatCompletion.create</td>
<td>openai.createChatCompletion</td>
</tr>
<tr>
<td>/v1/completions</td>
<td>openai.Completion.create</td>
<td>openai.createCompletion</td>
</tr>
<tr>
<td>/v1/edits</td>
<td>openai.Edit.create</td>
<td>openai.createEdit</td>
</tr>
<tr>
<td>/v1/audio/transcriptions</td>
<td>openai.Audio.transcribe</td>
<td>openai.createTranscription</td>
</tr>
<tr>
<td>/v1/audio/translations</td>
<td>openai.Audio.translate</td>
<td>openai.createTranslation</td>
</tr>
<tr>
<td>/v1/fine-tunes</td>
<td>openai.FineTune.create</td>
<td>openai.createFineTune</td>
</tr>
<tr>
<td>/v1/embeddings</td>
<td>openai.Embedding.create</td>
<td>openai.createEmbedding</td>
</tr>
<tr>
<td>/v1/moderations</td>
<td>openai.Moderation.create</td>
<td>openai.createModeration</td>
</tr>
</tbody>
</table>
</div><h4>Python working example for the <code>gpt-3.5-turbo</code> (i.e., <a href=""https://platform.openai.com/docs/guides/gpt/chat-completions-api"" rel=""nofollow noreferrer"">Chat Completions API</a>)</h4>
<p>If you run <code>test.py</code> the OpenAI API will return the following completion:</p>
<blockquote>
<p>Hello there! How can I assist you today?</p>
</blockquote>
<p><strong>test.py</strong></p>
<pre><code>import openai
import os

openai.api_key = os.getenv('OPENAI_API_KEY')

completion = openai.ChatCompletion.create(
  model = 'gpt-3.5-turbo',
  messages = [
    {'role': 'user', 'content': 'Hello!'}
  ],
  temperature = 0  
)

print(completion['choices'][0]['message']['content'])
</code></pre>
<br>
<h4>NodeJS working example for the <code>gpt-3.5-turbo</code> (i.e., <a href=""https://platform.openai.com/docs/guides/gpt/chat-completions-api"" rel=""nofollow noreferrer"">Chat Completions API</a>)</h4>
<p>If you run <code>test.js</code> the OpenAI API will return the following completion:</p>
<blockquote>
<p>Hello there! How can I assist you today?</p>
</blockquote>
<p><strong>test.js</strong></p>
<pre><code>const { Configuration, OpenAIApi } = require('openai');

const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});

const openai = new OpenAIApi(configuration);

async function getCompletionFromOpenAI() {
  const completion = await openai.createChatCompletion({
    model: 'gpt-3.5-turbo',
    messages: [
      { role: 'user', content: 'Hello!' }
    ],
    temperature: 0,
  });

  console.log(completion.data.choices[0].message.content);
}

getCompletionFromOpenAI();
</code></pre>
",2023-03-02 18:50:06,3.0,30.0
76383308,1,12467470.0,76383309.0,ruby-openai api gem in Ruby on Rails: how to implement a streaming conversation?,openai provide api allow implement ai service chagpt dale ruby rail application couple gems available obe work problem do not come stream conversation feature mean send question request a time without history track conversation word api forget question ask have send reply fix,2023-06-01 15:05:06,,2023-06-02 13:38:24,2023-06-02 13:38:24,<ruby-on-rails><ruby><openai-api><chatgpt-api>,1,1,0,86,,2.0,12467470.0,"<p>Basically you need to implement the whole behaviour yourself. Here are all the implementation step, including the implementation of the <code>dal-e</code> ai with a response with several pictures rather then just one.</p>
<p><strong>You can also find my whole repository <strong><a href=""https://github.com/OGsoundFX/ruby-open-ai"" rel=""nofollow noreferrer"">HERE</a></strong> and clone the app!!!</strong></p>
<h1>IMPLEMENTING A STREAM CONVERSATION FEATURE</h1>
<h2>Basic implementation</h2>
<p>Check out <a href=""https://github.com/dmbf29"" rel=""nofollow noreferrer"">Doug</a> Berkley's <a href=""https://doug-berkley.notion.site/doug-berkley/Rails-ChatGPT-Service-Object-Setup-21748fc969514b978bf6345f897b6d3e"" rel=""nofollow noreferrer"">Notion Page</a> for basic implementation of the API</p>
<h2>Implement a streaming conversation</h2>
<p>By default the <code>openai</code> gem does not come with that feature, hence having to implement it yourself</p>
<ol>
<li>Create your database with 3 tables (conversations, questions, answers) with thw following sctructure:</li>
</ol>
<pre class=""lang-rb prettyprint-override""><code># schema.rb
ActiveRecord::Schema[7.0].define(version: 2023_05_29_194913) do
  create_table &quot;answers&quot;, force: :cascade do |t|
    t.text &quot;content&quot;
    t.integer &quot;question_id&quot;, null: false
    t.datetime &quot;created_at&quot;, null: false
    t.datetime &quot;updated_at&quot;, null: false
    t.index [&quot;question_id&quot;], name: &quot;index_answers_on_question_id&quot;
  end

  create_table &quot;conversations&quot;, force: :cascade do |t|
    t.text &quot;initial_question&quot;
    t.datetime &quot;created_at&quot;, null: false
    t.datetime &quot;updated_at&quot;, null: false
    t.text &quot;historic&quot;
  end

  create_table &quot;questions&quot;, force: :cascade do |t|
    t.text &quot;content&quot;
    t.integer &quot;conversation_id&quot;, null: false
    t.datetime &quot;created_at&quot;, null: false
    t.datetime &quot;updated_at&quot;, null: false
    t.index [&quot;conversation_id&quot;], name: &quot;index_questions_on_conversation_id&quot;
  end

  add_foreign_key &quot;answers&quot;, &quot;questions&quot;
  add_foreign_key &quot;questions&quot;, &quot;conversations&quot;
end
</code></pre>
<ol start=""2"">
<li>Routes</li>
</ol>
<pre class=""lang-rb prettyprint-override""><code>Rails.application.routes.draw do
  root &quot;pages#home&quot; # supposes that you have a pages controller with a home action
  resources :conversations, only: [:create, :show]
  post &quot;question&quot;, to: &quot;conversations#ask_question&quot;
end
</code></pre>
<ol start=""3"">
<li>Home page view (with just a button that redirects to the create conversation action -- see bellow)</li>
</ol>
<pre class=""lang-html prettyprint-override""><code>&lt;h1&gt;Let's talk&lt;/h1&gt;
&lt;%= button_to &quot;Create New Conversation&quot;, conversations_path, method: :post, class: &quot;btn btn-primary my-3&quot; %&gt;
</code></pre>
<ol start=""4"">
<li>Controller <code>app/controllers/conversations_controller.rb</code></li>
</ol>
<pre class=""lang-rb prettyprint-override""><code>class ConversationsController &lt; ApplicationController
  def create
    @convo = Conversation.create
    redirect_to conversation_path(@convo)
  end

  def show
    @convo = Conversation.find(params[:id])
  end

  def ask_question
    @question = Question.new(content: params[:entry])
    conversation = Conversation.find(params[:conversation])
    @question.conversation = conversation
    @question.save
    if conversation.historic.nil?
      response = OpenaiService.new(params[:entry]).call 
      conversation.historic = &quot;#{@question.content}\n#{response}&quot;
    else
      response = OpenaiService.new(&quot;#{conversation.historic}\n#{params[:entry]}&quot;).call
      conversation.historic += &quot;\n#{@question.content}\n#{response}&quot;
    end
    conversation.save
    @answer = Answer.create(content: response, question: @question)
    redirect_to conversation_path(conversation)
  end
end
</code></pre>
<ol start=""5"">
<li>Show page <code>app/views/conversations/show.html.erb</code></li>
</ol>
<pre class=""lang-html prettyprint-override""><code>&lt;h1&gt;This is your conversation&lt;/h1&gt;
&lt;p&gt;Ask your question&lt;/p&gt;
&lt;form action=&quot;&lt;%= question_path %&gt;&quot;, method=&quot;post&quot;&gt;
  &lt;input type=&quot;hidden&quot; name=&quot;conversation&quot; value=&quot;&lt;%= @convo.id %&gt;&quot;&gt;
  &lt;textarea rows=&quot;5&quot; cols=&quot;33&quot; name=&quot;entry&quot;&gt;&lt;/textarea&gt;
  &lt;input type=&quot;submit&quot; class=&quot;btn btn-primary&quot;&gt;
&lt;/form&gt;

&lt;br&gt;

&lt;ul&gt;
  &lt;% @convo.questions.each do |question| %&gt;
    &lt;li&gt;
      Q: &lt;%= question.content.capitalize %&gt; &lt;%= &quot;?&quot; if question.content.strip.last != &quot;?&quot; %&gt;
    &lt;/li&gt;
    &lt;li&gt;
      A: &lt;%= question.answers.first.content %&gt;
    &lt;/li&gt;
  &lt;% end %&gt;
&lt;/ul&gt;

&lt;%= link_to &quot;Back&quot;, root_path %&gt;

</code></pre>
<ol start=""6"">
<li><code>rails s</code> and test :)</li>
</ol>
<h2>Resources:</h2>
<ul>
<li><a href=""https://github.com/OGsoundFX/ruby-open-ai"" rel=""nofollow noreferrer"">https://github.com/OGsoundFX/ruby-open-ai</a></li>
<li><a href=""https://doug-berkley.notion.site/doug-berkley/Rails-ChatGPT-Service-Object-Setup-21748fc969514b978bf6345f897b6d3e"" rel=""nofollow noreferrer"">https://doug-berkley.notion.site/doug-berkley/Rails-ChatGPT-Service-Object-Setup-21748fc969514b978bf6345f897b6d3e</a></li>
<li><a href=""https://github.com/alexrudall/ruby-openai"" rel=""nofollow noreferrer"">https://github.com/alexrudall/ruby-openai</a></li>
</ul>
<h2>Going Further:</h2>
<ul>
<li><a href=""https://gist.github.com/alexrudall/cb5ee1e109353ef358adb4e66631799d"" rel=""nofollow noreferrer"">https://gist.github.com/alexrudall/cb5ee1e109353ef358adb4e66631799d</a></li>
</ul>
",2023-06-01 15:05:06,0.0,0.0
76084296,1,21709806.0,76090137.0,"OpenAI ChatGPT (GPT-3.5) API error: ""Invalid URL (POST /v1/engines/gpt-3.5-turbo/chat/completions)""",use openai learn about api integration run across code run python program ask chatgpt about error do not right solutions note latest openai package instal code,2023-04-23 10:21:27,,2023-05-05 21:27:25,2023-05-05 21:27:25,<python><python-3.x><openai-api><chatgpt-api>,1,0,0,487,,2.0,10347145.0,"<h4>Problem</h4>
<p>The ChatGPT API (i.e., the GPT-3.5 API) has a <code>model</code> parameter (required). <strong>The <code>engine</code> parameter is not a valid parameter for the <code>/v1/chat/completions</code> API endpoint.</strong> See the official <a href=""https://platform.openai.com/docs/api-reference/chat"" rel=""nofollow noreferrer"">OpenAI documentation</a>.</p>
<h4>Solution</h4>
<p>Change this...</p>
<pre><code>engine = &quot;gpt-3.5-turbo&quot;
</code></pre>
<p>...to this.</p>
<pre><code>model = &quot;gpt-3.5-turbo&quot;
</code></pre>
<br>
<p>Also, change this...</p>
<pre><code>joke = response.choices[0].text.strip()
</code></pre>
<p>...to this.</p>
<pre><code>joke = response['choices'][0]['message']['content']
</code></pre>
",2023-04-24 08:48:47,1.0,5.0
76066707,1,8152261.0,76066764.0,How to get ChatGPT API to respond similarly to web version?,just start play chatgpt api wonder receive a response paragraph similar web a sample print openai website give a nice paragraph summarize ask do know exact parameters website emulate api,2023-04-20 17:16:26,,,2023-05-15 11:18:25,<python><python-3.x><openai-api><chatgpt-api>,2,3,0,1472,,2.0,4038800.0,"<p>You're using Text Davinci 003 and this is not what ChatGPT runs on.</p>
<p>ChatGPT runs on 3.5 turbo. <br>
<a href=""https://platform.openai.com/docs/models/gpt-3-5"" rel=""nofollow noreferrer"">As per documentation</a>, you can try using GPT 3.5 turbo. (<a href=""https://platform.openai.com/docs/models/gpt-3-5"" rel=""nofollow noreferrer"">There are multiple versions too</a>)</p>
<p>That being said...</p>
<p>You are very unlikely to get the same exact results for both (or if you run the same query twice) as the LLM is a non-deterministic solution, meaning that the same input will return different results each time (unless they employ some sort of caching which I am not aware of).
Additionally, ChatGPT is constantly updated using newer versions of 3.5 turbo (you can see which version they're running at the bottom of the chat page). So you're probably going to be running on a different version of 3.5 turbo than the one they're using.</p>
<p><a href=""https://en.wikipedia.org/wiki/Nondeterministic_algorithm"" rel=""nofollow noreferrer"">More on non-deterministic algorithms</a></p>
",2023-04-20 17:25:42,5.0,0.0
75682331,1,7400627.0,75729602.0,Does chatgpt-3.5-turbo API recounts the tokens in billing when sending the conversation history in api,create a chat app use chatgptturbo model do api consider tokens include assistant message old set message bill just message user count bill resend api request a new message append conversation,2023-03-09 08:57:17,,2023-03-10 07:25:39,2023-03-27 06:41:57,<openai-api><chatgpt-api>,1,0,2,1259,,2.0,15863196.0,"<p>As mentioned in <a href=""https://platform.openai.com/docs/guides/chat/introduction"" rel=""nofollow noreferrer"">OpenAI document</a>:</p>
<blockquote>
<p>The total number of tokens in an API call affects how much your API call costs, as you pay per token <br>
Both input and output tokens count toward these quantities. For example, if your API call used 10 tokens in the message input and you received 20 tokens in the message output, you would be billed for 30 tokens.</p>
</blockquote>
<p>To see how many tokens are used by an API call, check the usage field in the API response</p>
<pre><code>response['usage']['total_tokens']
</code></pre>
<p>Each time you <code>append</code> previous chats to <code>messages</code>, the number of <code>total_token</code> will increases. So all tokens of previous messages will be considered in the bill.</p>
",2023-03-14 06:09:41,0.0,0.0
76067104,1,1492337.0,76074046.0,Using Vicuna + langchain + llama_index for creating a self hosted LLM model,want create a self host llm model able a context custom data slack conversations matter hear vicuna a great alternative chatgpt code sadly hit error output run idea modify code make work,2023-04-20 18:14:37,,,2023-06-05 16:05:24,<python><machine-learning><pytorch><chatgpt-api><langchain>,2,0,6,4826,,2.0,9409701.0,"<p>length is too long, 9999 will consume huge amount of GPU RAM, especially using 13b model. try 7b model. And try using something like peft/bitsandbytes to reduce GPU RAM usage. set load_in_8bit=True is a good start.</p>
",2023-04-21 14:17:09,1.0,3.0
76240871,1,20293888.0,76257734.0,"How do i add memory to RetrievalQA.from_chain_type? or, how do I add a custom prompt to ConversationalRetrievalChain?",add memory retrievalqafrom_chain_type add a custom prompt conversationalretrievalchain past  weeks ive try make a chatbot chat document not just a semantic searchqa memory a custom prompt try combination chain far closest get conversationalretrievalchain without custom prompt retrievalqafrom_chain_type without memory,2023-05-13 02:43:50,,,2023-06-11 06:14:14,<python><openai-api><chatgpt-api><langchain><py-langchain>,3,0,4,2645,,2.0,2799941.0,"<p>Here's a solution with <code>ConversationalRetrievalChain</code>, with memory and custom prompts, using the default <code>'stuff'</code> chain type.</p>
<p>There are two prompts that can be customized here. First, the prompt that condenses conversation history plus current user input (<code>condense_question_prompt</code>), and second, the prompt that instructs the Chain on how to return a final response to the user (which happens in the <code>combine_docs_chain</code>).</p>
<pre><code>from langchain import PromptTemplate

# note that the input variables ('question', etc) are defaults, and can be changed

condense_prompt = PromptTemplate.from_template(
    ('Do X with user input ({question}), and do Y with chat history ({chat_history}).')
)

combine_docs_custom_prompt = PromptTemplate.from_template(
    ('Write a haiku about a dolphin.\n\n'
     'Completely ignore any context, such as {context}, or the question ({question}).')
)
</code></pre>
<p>Now we can initialize the <code>ConversationalRetrievalChain</code> with the custom prompts.</p>
<pre><code>from langchain.llms import OpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(memory_key=&quot;chat_history&quot;, return_messages=True)

chain = ConversationalRetrievalChain.from_llm(
    OpenAI(temperature=0), 
    vectorstore.as_retriever(), # see below for vectorstore definition
    memory=memory,
    condense_question_prompt=condense_prompt,
    combine_docs_chain_kwargs=dict(prompt=combine_docs_custom_prompt)
)
</code></pre>
<p>Note that this calls <a href=""https://github.com/hwchase17/langchain/blob/49ce5ce1ca70657e34b63c2f239222e9557be115/langchain/chains/question_answering/__init__.py#L54"" rel=""nofollow noreferrer""><code>_load_stuff_chain()</code></a> under the hood, which allows for an optional <code>prompt</code> kwarg (that's what we can customize). This is used to set the <a href=""https://github.com/hwchase17/langchain/blob/49ce5ce1ca70657e34b63c2f239222e9557be115/langchain/chains/question_answering/__init__.py#L63"" rel=""nofollow noreferrer""><code>LLMChain</code></a> , which then goes to initialize the <code>StuffDocumentsChain</code>.</p>
<p>We can test the setup with a simple query to the vectorstore (see below for example vectorstore data) - you can see how the output is determined completely by the custom prompt:</p>
<pre><code>chain(&quot;What color is mentioned in the document about cats?&quot;)['answer']
#'\n\nDolphin leaps in sea\nGraceful and playful in blue\nJoyful in the waves'
</code></pre>
<p>And memory is working correctly:</p>
<pre><code>chain.memory
#ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='What color is mentioned in the document about cats?', additional_kwargs={}), AIMessage(content='\n\nDolphin leaps in sea\nGraceful and playful in blue\nJoyful in the waves', additional_kwargs={})]), output_key=None, input_key=None, return_messages=True, human_prefix='Human', ai_prefix='AI', memory_key='chat_history')
</code></pre>
<p>Example vectorstore dataset with ephemeral ChromaDB instance:</p>
<pre><code>from langchain.vectorstores import Chroma
from langchain.document_loaders import DataFrameLoader
from langchain.embeddings.openai import OpenAIEmbeddings

data = {
    'index': ['001', '002', '003'], 
    'text': [
        'title: cat friend\ni like cats and the color blue.', 
        'title: dog friend\ni like dogs and the smell of rain.', 
        'title: bird friend\ni like birds and the feel of sunshine.'
    ]
}

df = pd.DataFrame(data)
loader = DataFrameLoader(df, page_content_column=&quot;text&quot;)
docs = loader.load()

embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(docs, embeddings)
</code></pre>
",2023-05-15 20:31:07,1.0,1.0
75724406,1,14301369.0,75724477.0,OpenAI API 404 response,try use chatgpt telegram bot use use textdavinci model work fine now work fine not satisfy responses now try change model gptturbo throw a  response code text error request fail status code  nothing code,2023-03-13 16:17:07,,2023-04-03 09:26:40,2023-06-02 22:36:57,<javascript><node.js><telegram-bot><openai-api><chatgpt-api>,1,1,5,3657,,2.0,11692562.0,"<p>Try to use <code>createChatCompletion</code> rather than <code>createCompletion</code>:</p>
<pre class=""lang-js prettyprint-override""><code>const response = async (message) =&gt; {
  const response = await openai.createChatCompletion({
    model: &quot;gpt-3.5-turbo&quot;,
    messages: [{ role: &quot;user&quot;, content: &quot;Hello world&quot; }],
  });

  return response.data.choices[0].message.content;
};
</code></pre>
",2023-03-13 16:24:00,0.0,12.0
76030084,1,772481.0,76030158.0,ChatGPT completion /v1/chat/completions memorize across multiple requests,use parameter memory not persist across multiple request let model memorize across multiple request message xxx remember chatgpt api send time purpose user variable not use remember things,2023-04-16 20:07:27,,2023-04-17 00:29:15,2023-04-17 00:29:15,<openai-api><chatgpt-api>,1,0,0,1039,,2.0,13269702.0,"<p>Do you mean that previous messages are deleted? You'll have to remember them. The API doesn't do that. Make <code>messages</code> a variable and append the response to it. Next time send the <code>messages</code> variable which contains the previous response</p>
",2023-04-16 20:27:19,3.0,2.0
75811594,1,245549.0,75811803.0,OpenAI ChatGPT (GPT-3.5) API: Can I fine-tune a GPT-3.5 model?,finetuned language model able access model method not access finetuned model research a bite problem not finetuning fact original model not accessible loop model model accessible not accessible contrast remain model accessible not accessible question confirm find write documentation page second question possible finetune a model support chat dialogue example official page do right finetune model not support chat dialogue,2023-03-22 11:20:25,2023-04-20 15:25:40,2023-03-22 17:34:12,2023-06-15 10:10:26,<openai-api><chatgpt-api>,1,3,-4,1233,,2.0,10347145.0,"<p><strong>Question 1:</strong></p>
<blockquote>
<p>I found out that only <code>gpt-3.5-turbo</code> model is accessible via
<code>openai.ChatCompletion.create</code> and it is not accessible via
<code>openai.Completion.create</code>. In contrast, the remaining four models are
accessible via <code>openai.Completion.create</code> but are not accessible via
<code>openai.ChatCompletion.create</code>.</p>
<p>So, my first question if someone can confirm my finding?</p>
</blockquote>
<p><strong>Answer 1:</strong></p>
<p>Yes, correct. The reason why this is the case is that the <code>gpt-3.5.-turbo</code> model is a GPT-3.5 model. All the other models you mentioned (i.e., <code>davinci</code>, <code>curie</code>, <code>babbage</code>, and <code>ada</code>) are GPT-3 models.</p>
<p><a href=""https://platform.openai.com/docs/api-reference/chat/create"" rel=""nofollow noreferrer"">GPT-3.5 models</a> use a different API endpoint than <a href=""https://platform.openai.com/docs/api-reference/completions/create"" rel=""nofollow noreferrer"">GPT-3 models</a>. This is not explicitly written in the documentation, but it's very clear if you read the whole documentation.</p>
<hr />
<p><strong>Question 2:</strong></p>
<blockquote>
<p>My second question is if it is possible to fine-tune a model that
supports Chat / Dialogue?</p>
</blockquote>
<p><strong>Answer 2:</strong></p>
<p>No, it's <a href=""https://platform.openai.com/docs/guides/fine-tuning/what-models-can-be-fine-tuned"" rel=""nofollow noreferrer"">not possible</a>. You want to fine-tune a GPT-3.5 model, which is not possible as of March 2023. Also, it doesn't seem this will change in the near future, if ever. Why?</p>
<p>I strongly recommend you to read the official <a href=""https://openai.com/blog/how-should-ai-systems-behave"" rel=""nofollow noreferrer"">OpenAI article</a> on how ChatGPT's behavior is shaped to understand why you can't fine-tune a GPT-3.5 model. I want to emphasize that the article doesn't discuss specifically the fine-tuning of a GPT-3.5 model, or better yet, its inability to do so, but rather ChatGPT's behavior. <strong>It's important to emphasize that ChatGPT is not the same as the GPT-3.5 model, but ChatGPT uses chat models, which GPT-3.5 belongs to, along with GPT-4 models.</strong></p>
<p>As stated in the article:</p>
<blockquote>
<p>Unlike ordinary software, our models are massive neural networks.
Their behaviors are learned from a broad range of data, not programmed
explicitly. /.../ An initial “pre-training” phase comes first, in
which the model learns to predict the next word in a sentence,
informed by its exposure to lots of Internet text (and to a vast array
of perspectives). This is followed by a second phase in which we
“fine-tune” our models to narrow down system behavior.</p>
<p><strong>First, we “pre-train” models by having them predict what comes next in
a big dataset that contains parts of the Internet.</strong> They might learn to
complete the sentence “instead of turning left, she turned ___.” By
learning from billions of sentences, our models learn grammar, many
facts about the world, and some reasoning abilities. They also learn
some of the biases present in those billions of sentences.</p>
<p><strong>Then, we “fine-tune” these models on a more narrow dataset that we
carefully generate with human reviewers who follow guidelines that we
provide them.</strong> /.../ Then, while they are in use, the models generalize
from this reviewer feedback in order to respond to a wide array of
specific inputs provided by a given user.</p>
</blockquote>
<p>Visual representation (<a href=""https://openai.com/blog/how-should-ai-systems-behave"" rel=""nofollow noreferrer"">source</a>):</p>
<p><a href=""https://i.stack.imgur.com/kj9S6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kj9S6.png"" alt=""Screenshot"" /></a></p>
<p><strong>As you can see, chat models (i.e., GPT-3.5 and GPT-4 models) are already &quot;fine-tuned&quot; by the OpenAI. This is the reason why you can only fine-tune base models: <code>davinci</code>, <code>curie</code>, <code>babbage</code>, and <code>ada</code>. These are the original models that do not have any instruction following training.</strong></p>
",2023-03-22 11:39:50,0.0,1.0
